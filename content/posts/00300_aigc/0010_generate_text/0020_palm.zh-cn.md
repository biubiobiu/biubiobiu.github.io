---
title: "PaLM"
date: 2023-08-05T12:30:40+08:00
menu:
  sidebar:
    name: PaLM
    identifier: aigc-text-palm
    parent: aigc-text
    weight: 20
author:
  name: biubiobiu
  image: /images/author/john.png
tags: ["aigc","PaLM"]
categories: ["Basic"]
math: true
mermaid: true
enableEmoji: true
---

## 一、简介

### 1、PaLM 1
<a href="https://arxiv.org/pdf/2204.02311.pdf" target="bland">《PaLM: Scaling Language Modeling with Pathways》</a> <br>
这篇文章87页，并没有深度的讨论模型算法的结构，数据的清洗技巧，或者是训练的方式（估计感觉这块的创新性不是特别明显，也不是文章的主要目的）。
而是花了大量的篇幅去评估这个模型在multi-task的能力，比如翻译，代码修改，生成，问答等等。<br>
其中模型版本于训练集大小：
<p align="center"><img src="/datasets/posts/nlp/palm_0.png" width="90%" height="90%"></p>
<p align="center"><img src="/datasets/posts/nlp/palm_1.png" width="90%" height="90%"></p>


Google PaLM 是一个 540B 参数密集型 Transformer 语言模型，在 780B 高质量、多样化文本的标记上进行训练。 它已经针对 3 种不同的尺寸进行了训练：8B、62B 和 540B，使用 6144 TPU v4 芯片使用 Pathways，这是一种新的 ML 系统，可跨多个 TPU（张量处理单元）Pod 进行高效训练。 当它被引入时，它在数百个 NLU 和 NLG 基准测试中产生了 SOTA 小样本学习结果。 这包括 Big-Bench 任务的性能大幅提升，以及多语言 NLG 和源代码生成功能的显着改进。 它还被证明可以使用思维链提示来解释笑话或逻辑推理，从而产生很好的解释。<br>

PaLM超越了许多之前的SOTA。作者归功于
1. 更好的数据的清理，
2. 更多的数据，
3. 模型规模的进一步提升。

模型算法的改进比较少，从Model Architecture那一章看出，其实模型结构的变化并不明显，在激活层，ShareEmbedding，PosEmbedding等模块做了一些结构优选。核心的TransformerBlock的变种选择也更多是为了优化模型的训练效率。谷歌作为搜索技术的天花板，数据清洗的积累，以及对于数据的理解肯定是OpenAI这些公司无法比拟的。个人感觉这块是个比较明显的优势。<br>

与GPT-3相比的变化：
> 1. 多查询注意力（Multi-query attention）：在每个注意力头中共享K/V（Key/Value）嵌入，但使用独立的Q（Query）嵌入。这样做可以在推理阶段显著提高模型的速度。
> 2. 并行Transformer块：使用并行的Transformer块来提高训练时间，相较于传统的串行设计，可以减少约15%的训练时间。
> 3. SwiGLU激活函数：与GPT-3使用的GELU激活函数不同，这里采用了SwiGLU激活函数。
> 4. 旋转位置编码RoPE嵌入：使用RoPE（Relative Positional Encodings）嵌入代替学习得到的嵌入方式，在长文本上具有更好的性能 。
> 5. 输入-输出嵌入共享：输入和输出embedding矩阵是共享的。
> 6. 无偏置向量：在mlp、normlayer等算法中，都不使用bias，对于大模型，可以提高训练稳定性。
> 7. SentencePiece与256k标记：使用SentencePiece进行分词处理，标记数量为256k。

### 2、PaLM 2

<a href="https://arxiv.org/pdf/2305.10403.pdf" target="bland">《PaLM 2 Technical Report》</a> <br>

这篇报告-总结：
1. 实验验证了模型规模和数据规模是随算力同比增加的，大致为1:1时，性价比更好（消耗同样算力得到的模型表现更好）。
2. 对于更大的模型（larger models），增加多语种数据不会降低其在英语上的表现。支持一百多种语言
3. 数据量比v1大很多（v1的数据量是780B），据CNBC报道，v2数据量是3.6T（不保真）。
4. 模型结构没有具体说，仅说了基于Transformer。参数规模上，这次PaLM2的模型家族中，最大的那个都会显著比v1版本（540B）的小很多（siginificantly smaller）。
5. 对部分有害语料加入控制字符，来保证生成可控。
6. 增加context长度不会损害模型在不需要这么长输入的任务上的表现。
7. 刷点上，PaLM 2基本上全面优于v1版本，和GPT-4的比较，突出一个势均力敌，甚至隐隐有超过。
8. 通过注入canaries实验（可以理解问人造一些数据），他们发现相对于正常数据，离群点（outlier point）仅需更小的重复次数就会被记忆。但是在真实语言数据中，这种现象又不是那么明显。
9. 训练框架上，硬件自然是TPUv4，软件是Pathways，Paxml，JAX和GSPMD，看起来没过气网红tf的事了。

**不同版本**：
<p align="center"><img src="/datasets/posts/nlp/palm_2.png" width="90%" height="90%"></p>



## 二、网络结构

基于Transformer解码器

