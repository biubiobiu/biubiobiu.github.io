---
title: "GPT"
date: 2023-08-05T12:30:40+08:00
menu:
  sidebar:
    name: GPT
    identifier: aigc-text-gpt
    parent: aigc-text
    weight: 11
author:
  name: biubiobiu
  image: /images/author/john.png
tags: ["aigc","GPT"]
categories: ["Basic"]
math: true
mermaid: true
enableEmoji: true
---

## 一、简介


## 二、GPT-1
<a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="blank">GPT(2018-06)</a> <br>

<a href="/zh-cn/posts/00200_nlp/0080_gpt/0020_gpt1_detail">详细参考</a><br>

其创造性的提出以Transformer的解码器来训练生成式模型，后面Bert的作者估计是看到了这篇论文，据说两个月时间就发表了以Transformer编码器训练的Bert模型。总结下GPT-1模型：

1. GPT-1 使用了一个仅有解码器的 Transformer 结构，每一个作为一个Layer，共有12层；
2. 使用了一个 768 维的嵌入向量来表示输入序列中的每个词或标记，使用了 12 个并行的注意力头（attention heads）；
3. 使用Adam优化器进行模型训练，在训练过程中，使用了学习率的 warmup 阶段和余弦退火调度机制，以平衡训练速度和模型性能；
4. 模型权重被初始化为均值为 0、标准差为 0.02 的正态分布（N(0, 0.02)），使用字节对编码（Byte Pair Encoding，BPE）来对文本进行分词处理，分词后得到的词汇表大小为 40000；
5. 激活函数是 GELU；
6. 文本输入序列固定长度是512；
7. 参数量 117M;
8. 使用了学习得到的位置嵌入向量(position embedding)，而不是Attention is All You Need中使用的正弦位置嵌入向量；

## 三、GPT-2
<a href="https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf" target="blank">GPT-2(2019-02)</a>

**GPT-2的改进**:

GPT-2 是GPT语言模型开始变大的地方，这是 OpenAI 第一次训练超过 1B 个参数的模型。
<font color=#f00000>通过提升模型的规模，来凸显GPT的优势</font>。在 GPT-1 中，作者训练了单个模型，但在这里，作者训练了一系列模型。
与GPT-1相比，架构上有如下差异：
<p align="center"><img src="/datasets/posts/nlp/gpt1-2.png" width=70% height=70%></p>

1. 层归一化操作，有原来的post-norm换成了pre-norm，<font color=#f00000>以加速训练和提高模型性能</font>。此外，在最后一个自注意力块的输出上添加了额外的层归一化；
2. 在权重初始化时，通过 $\frac{1}{\sqrt n}$ 进行缩放。这种缩放<font color=#f00000>有助于减少梯度更新的方差，使训练过程更加稳定</font>；
3. 扩大了其词汇表的大小，词汇表大小约为 <font color=#f00000>50,000</font>（相比于约 40,000）；
4. 增大文本输入序列长度 1024（相比于 512）这使得模型能够更好地理解和生成更长的文本；
5. batch size大小为 512（相比于 64）较大的批次大小有助于提高训练效率和模型并行计算的能力。
6. 最大的模型具有约 15 亿个参数。
7. 数据集：GPT-2 构造了一个新数据集，<a href="https://paperswithcode.com/dataset/webtext" target="bland">WebText</a>。全部来自于 Reddit 的外链，而且是那些获得至少三个赞的外链，经过清洗、去重后，得到8百万网页共计 <font color=#f00000>40GB 文本数据</font>。
  WebText 数据集的特点在于<font color=#f00000>全面而干净</font>。
  

**GPT-2的不同版本**:
|模型|Layers|d_size|ff_size|Heads|Parameters|
|:--|:--|:--|:--|:--|:--|
|GPT2-base|12|768|3072|12|1.17亿|
|GPT2-medium|24|1024|4096|16|3.45亿|
|GPT2-large|36|1280|5120|20|7.74亿|
|GPT2-xl|48|1600|6400|25|15.58亿|


## 四、GPT-3
<a href="https://arxiv.org/pdf/2005.14165.pdf" target="blank">GPT-3(2020-05)</a> <br>
GPT-3是大语言模型开始受到关注的开始。在论文中，作者训练了 10 个模型，参数范围从 1.25亿 个参数（“GPT-3 Small”）到 175B 个参数（“GPT-3”）。
<p align="center"><img src="/datasets/posts/nlp/gpt-3-model.png" width=90% height=90%></p>

在GPT-3中，模型的架构与GPT-2完全相同。唯一的区别是它们在transformer的各层中使用了<font color=#f00000>“交替的稠密和本地带状稀疏注意力模式”</font>。简单来说，GPT-3在注意力机制上进行了优化，引入了稀疏注意力的概念。<br>

> 1. 传统的点积注意力在计算复杂度上较高，而稀疏注意力可以提供更高的扩展性，并且在处理长序列时具有更高的效率。这种改进使得GPT-3能够更好地处理长文本输入，并且在计算效率和模型表现方面有所提升。
>2. GPT-3引入稀疏注意力的原因尚不清楚，也许是因为计算限制造成的，论文中并没详细的说明如何如何使用模型并行性训练模型，使得论文更难以复现。

## 五、chatGPT/InstructGPT

chatGPT(2022-12)<br>
<a href="https://arxiv.org/pdf/2203.02155.pdf" target="bland">InstructGPT</a> <br>

ChatGPT的博客中讲到ChatGPT和InstructGPT的训练方式相同，不同点仅仅是它们采集数据上有所不同，但是并没有更多的资料来讲数据采集上有哪些细节上的不同。

<p align="center"><img src="/datasets/posts/nlp/InstructGPT_0.png" width="90%" height="90%" title="InstructGPT" alt="InstructGPT"></p>

通过人类的反馈，在GPT3上做微调。


### 1、SFT模型
设计了一些prompt，<font color=#f00000>人工写答案</font>，搜集一批数据，用来fine-tune GPT3，得到一个SFT模型 (Supervised Fine-tune)，即：有监督的微调

### 2、RW模型：
由于SFT的标注数据，成本比较大。这搞一个便宜点的。
1. 设计一批prompt，每条prompt用GPT3采用很多条结果（生成模型的输出是概率性的，每次结果大概率是不一样的，generate有参数可以控制这些概率性）
2. <font color=#f00000>人工标注：每个prompt的生成结果的排序</font> （打分标注，可比写答案的标注快多了）
3. 训练一个奖励模型，这个奖励模型 就是对GPT3-6B的输出 进行打分，这个输出的分数 满足 人工标注的顺序。<br>
    * 作者没有采用GPT-175B模型，是因为在训练的过程中175B的不稳定，loss容易爆炸。<br>
    * 由于标注的是排序，RW模型的输出是score，所以有一个排序到score的映射。比如：一个prompt有K个答案。从k个答案中选2个，有 $C^2_k$种 结果对。每个结果对都是有人工标注的顺序的，在计算loss的时候保证这个顺序就行。每个prompt有 $C^2_k$ 个结果对，在算loss的时候，这 $C^2_k$ 个结果对一起计算。<br> 
    * loss的话是一个标准的 Pairwise的 Ranking Loss 
    $$loss(\theta) = - \frac{1}{C^2_k} E_{x,y_w,y_l \in D} log(\sigma[r_\theta (x, y_w) - r_\theta (x, y_l)])$$
    其中，$r_\theta ()$ 表示GPT3-6B的输出score值，$\sigma()$ 表示 Sigmoid函数。学习的目标是最大化这个loss。

### 3、强化学习SFT模型

**在强化学习的框架下调整SFT模型**：<br> 
用PPO强化学习方法，fine-tune 之前的SFT模型，得出的模型就是InstructGPT，大小只有1.3B。<br>
作者尝试把预训练的梯度整合到PPO中，如下：
$$
objective(\phi) = E_{(x,y) \in D_{\pi_\phi^{RL}}} [r_\theta(x, y) - \beta log(\frac{\pi^{RL}_\phi(y|x)}{\pi^{SFT}(y|x)})] + 
$$

$$
\gamma E_{x \in D_{pretrain}} [log(\pi^{RL}_\phi(x))]
$$

其中，$r_{\theta}()$ 就是第二步的RM模型；$\pi_\phi^{RL}$ 表示新的环境；$\pi_\phi^{SFT}$ 表示原来的环境；<br>
$log(\frac{\pi^{RL}_\phi(y|x)}{\pi^{SFT}(y|x)}) $ 表示新环境 与 旧环境差异，是一个正则项，作者希望与旧环境不能差异太大，所以添加了这个，类似KL散度。<br>

$ E_{x \in D_{pretrain}} [log(\pi^{RL}_\phi(x))]$ 表示原GPT3预训练时的损失函数，目的是防止模型遗忘。

在训练之前，是用第一步的SFT模型来初始化，$\pi_\phi^{SFT}$ 就是原来的SFT模型。<br>

<p align="center"><img src="/datasets/posts/nlp/InstructGPT_1.png" width="90%" height="90%" title="InstructGPT" alt="InstructGPT"></p>
从结果来看：还是人工打标的数据对模型的提升较大。可以看到经过监督训练的1.3B的模型，效果是好于175B的大模型。


## 四、GPT-4

<a href="https://arxiv.org/abs/2303.08774" target="bland">《GPT-4 Technical Report》</a>


## 相关

**RLHF**<br>
RLHF的相关技术，首次出自这篇文章：
<a href="https://arxiv.org/abs/1909.08593" target="bland">《Fine-Tuning Language Models from Human Preferences》</a>
该论文的作者Tom创建Anthropic公司，是该公司的CEO。<br>
后续，Anthropic公司发布了自己的大语言模型Claude，可以了解一下。<br>

**KL散度**：<br>
$$
D_{KL}(p||q) = E_p[log \frac{p}{q}] = \sum {p log \frac{p}{q}}
$$

其中，$p$ 分布往往是多峰。$D_{KL}(p||q)$ 与 $D_{KL}(q||p)$ 是不一样的<br>
1. $D_{KL}(p||q)$ 表示前向散度，在监督学习中使用
2. $D_{KL}(q||p)$ 表示反向散度，在强化学习中使用
