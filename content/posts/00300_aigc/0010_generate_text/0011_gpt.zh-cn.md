---
title: "GPT"
date: 2023-08-05T12:30:40+08:00
menu:
  sidebar:
    name: GPT
    identifier: aigc-text-gpt
    parent: aigc-text
    weight: 11
author:
  name: biubiobiu
  image: /images/author/john.png
tags: ["aigc","GPT"]
categories: ["Basic"]
math: true
mermaid: true
enableEmoji: true
---

## 一、简介


## 二、InstructGPT

<a href="https://arxiv.org/pdf/2203.02155.pdf" target="bland">InstructGPT</a> <br>

<p align="center"><img src="/datasets/posts/nlp/InstructGPT_0.png" width="90%" height="90%" title="InstructGPT" alt="InstructGPT"></p>

通过人类的反馈，在GPT3上做微调。


### 1、SFT模型
设计了一些prompt，<font color=#f00000>人工写答案</font>，搜集一批数据，用来fine-tune GPT3，得到一个SFT模型 (Supervised Fine-tune)，即：有监督的微调

### 2、RW模型：
由于SFT的标注数据，成本比较大。这搞一个便宜点的。
1. 设计一批prompt，每条prompt用GPT3采用很多条结果（生成模型的输出是概率性的，每次结果大概率是不一样的，generate有参数可以控制这些概率性）
2. <font color=#f00000>人工标注：每个prompt的生成结果的排序</font> （打分标注，可比写答案的标注快多了）
3. 训练一个奖励模型，这个奖励模型 就是对GPT3-6B的输出 进行打分，这个输出的分数 满足 人工标注的顺序。<br>
    * 作者没有采用GPT-175B模型，是因为在训练的过程中175B的不稳定，loss容易爆炸。<br>
    * 由于标注的是排序，RW模型的输出是score，所以有一个排序到score的映射。比如：一个prompt有K个答案。从k个答案中选2个，有 $C^2_k$种 结果对。每个结果对都是有人工标注的顺序的，在计算loss的时候保证这个顺序就行。每个prompt有 $C^2_k$ 个结果对，在算loss的时候，这 $C^2_k$ 个结果对一起计算。<br> 
    * loss的话是一个标准的 Pairwise的 Ranking Loss 
    $$loss(\theta) = - \frac{1}{C^2_k} E_{x,y_w,y_l \in D} log(\sigma[r_\theta (x, y_w) - r_\theta (x, y_l)])$$
    其中，$r_\theta ()$ 表示GPT3-6B的输出score值，$\sigma()$ 表示 Sigmoid函数。学习的目标是最大化这个loss。

### 3、强化学习SFT模型

**在强化学习的框架下调整SFT模型**：<br> 
用PPO强化学习方法，fine-tune 之前的SFT模型，得出的模型就是InstructGPT，大小只有1.3B。<br>
作者尝试把预训练的梯度整合到PPO中，如下：
$$
objective(\phi) = E_{(x,y) \in D_{\pi_\phi^{RL}}} [r_\theta(x, y) - \beta log(\frac{\pi^{RL}_\phi(y|x)}{\pi^{SFT}(y|x)})] + 
$$

$$
\gamma E_{x \in D_{pretrain}} [log(\pi^{RL}_\phi(x))]
$$

其中，$r_{\theta}()$ 就是第二步的RM模型；$\pi_\phi^{RL}$ 表示新的环境；$\pi_\phi^{SFT}$ 表示原来的环境；<br>
$log(\frac{\pi^{RL}_\phi(y|x)}{\pi^{SFT}(y|x)}) $ 表示新环境 与 旧环境差异，是一个正则项，作者希望与旧环境不能差异太大，所以添加了这个，类似KL散度。<br>

$ E_{x \in D_{pretrain}} [log(\pi^{RL}_\phi(x))]$ 表示原GPT3预训练时的损失函数，目的是防止模型遗忘。

在训练之前，是用第一步的SFT模型来初始化，$\pi_\phi^{SFT}$ 就是原来的SFT模型。<br>

<p align="center"><img src="/datasets/posts/nlp/InstructGPT_1.png" width="90%" height="90%" title="InstructGPT" alt="InstructGPT"></p>
从结果来看：还是人工打标的数据对模型的提升较大。可以看到经过监督训练的1.3B的模型，效果是好于175B的大模型。

## 三、相关

**RLHF**<br>
RLHF的相关技术，首次出自这篇文章：
<a href="https://arxiv.org/abs/1909.08593" target="bland">《Fine-Tuning Language Models from Human Preferences》</a>
该论文的作者Tom创建Anthropic公司，是该公司的CEO。<br>
后续，Anthropic公司发布了自己的大语言模型Claude，可以了解一下。<br>

**KL散度**：<br>
$$
D_{KL}(p||q) = E_p[log \frac{p}{q}] = \sum {p log \frac{p}{q}}
$$

其中，$p$ 分布往往是多峰。$D_{KL}(p||q)$ 与 $D_{KL}(q||p)$ 是不一样的<br>
1. $D_{KL}(p||q)$ 表示前向散度，在监督学习中使用
2. $D_{KL}(q||p)$ 表示反向散度，在强化学习中使用


## 四、GPT-4

<a href="https://arxiv.org/abs/2303.08774" target="bland">《GPT-4 Technical Report》</a>


