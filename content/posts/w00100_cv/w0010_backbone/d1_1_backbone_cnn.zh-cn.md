---
title: "CNN"
date: 2021-09-09T06:00:20+06:00
menu:
  sidebar:
    name: CNN
    identifier: cv-backbone-cnn
    parent: cv-backbone
    weight: 10
author:
  name: biubiobiu
  image: /images/author/john.png
tags: ["卷积","cnn"]
categories: ["Basic"]
math: true
---


## 一、卷积  

实际上，卷积操作需要对卷积核进行上下/左右翻转，然后用卷积核对输入进行滑动计算。由于网络学习目的是要"学习"出一个近最优解的权重，即：近最优解情况下卷积核的值，所以在卷积操作时，也就没必要在做翻转操作，反正卷积核的值是要被"调教"的，最终的卷积核的状态：可以看成是已经被上下/左右翻转过了。卷积操作也就变成互相关运算。  

<p align="center"><img src="/datasets/posts/cnn/conv2d.png" title="卷积" width="60%" height="60%" alt="卷积"></p>

卷积层解决的问题：
- 卷积层保留输入图片的形状，使图像的像素在高/宽两个方向上的相关性均可能被有效识别。
- 卷积层通过滑动窗口，将同一卷积核与不同位置的输入重复计算，参数共享，避免参数尺寸过大。  

在卷积操作时，会有两个超参数：填充(padding)和步幅(stride)，根据输入尺寸和卷积核改变输出形状：  
假设：输入尺寸：n<sub>h</sub> * n<sub>w</sub>，卷积核尺寸：k<sub>h</sub> * k<sub>w</sub>  
- 填充(padding)：在输入高和宽的两侧填充元素(通常是0)，一般来说：在高的两侧一共填充`p`<sub>`h`</sub>行；在宽的两侧一同填充`p`<sub>`w`</sub>列，一般填充的是偶数，即：nn.Conv2D(padding=(p<sub>h</sub>/2, p<sub>w</sub>/2))
- 步幅(stride)：在滑动计算时，每次滑动的步长，假设：在高上步幅为`s`<sub>`h`</sub>，在宽上步幅为`s`<sub>`w`</sub>  

则：输出尺寸：
$$ \tag{公式1} o_h = \frac{n_h-k_h+p_h+s_h} {s_h }, o_w = \frac{n_w-k_w+p_w+s_w} {s_w } $$

### 1. 1*1卷积层
1*1卷积层：被看作是卷积操作的全连接层。这是为什么呢？  
<p align="center"><img src="/datasets/posts/cnn/conv2d_1.png" title="1*1卷积" width="50%" height="50%" alt="1*1卷积"></p>  
1*1卷积的计算发生在通道维度上：输出的每个元素，来自输入中相同位置的元素在不同通道之间按权重叠加。假设我们将通道维度当作特征维度，将宽高维度上的元素看作数据样本，那么1*1卷积层的作用与全连接等价。

## 二、池化层
池化层(pooling)：缓解卷积层对位置的过渡敏感性。
- 浅层网络获取的是图像的细节信息，比如：纹理特征、边缘；高层网络获取的是图像的整体特征。池化层把感受野扩大，把图像的整体特征传递下去，网络越深感受野越大 
- 池化层一般是最大池化或者平均池化，类比生物学的神经细胞：只有电解质信号超过一定阈值，才能激活下一个神经元，才能把信号传递下去。

## 三、批量归一化

batch normalization：在一个batch内，算出平均值a, 方差：b^2；然后对每个样本做归一化：c*(x-a)/b+d。其中c、d是需要训练的。<br>
$$
x_{i+1} = \gamma \frac{x_i - \mu}{\sigma} + \beta
$$
由于数据的差异性，在卷积后可能会存在较大的波动。$\frac{x_i - \mu}{\sigma}$ 的作用就是把数据统一拉回N(0,1)的标准正态分布；但是每个特征的分布不一定是标准正态分布，所以添加了可学习的参数：$\gamma, \beta$。 通过训练来调节实际的均值 $\beta$ 和标准差 $\gamma$ ，不过 $\beta$ 和 $\gamma$ 是在一定的范围内，不能波动太大。<br>
作用：
  1. 避免梯度的消失/爆炸，这是因为通过归一化(a的作用是偏移，b的作用是拉伸)，把原来可能波动比较大的数据，限制在一定的范围内，从而减弱了梯度 消失/爆炸 的问题。
  2. 批量归一化为啥会有效，有的说：通过 a,b的偏移和拉伸，相当于添加了一个随机噪声，因为a,b是在当前样本上算出来的，包含了随机性。由于c,d是可以学习的，所以这两个值是比较稳定的。
  3. 批量归一化，可以加速收敛，可以调大学习率。

