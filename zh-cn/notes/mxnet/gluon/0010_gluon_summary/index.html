<!doctype html><html><head><title>Gluon实例</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0d88336a9cbd3e68c7714efae786b0a9_38044_42x0_resize_box_2.png><meta property="og:title" content="Gluon实例"><meta property="og:description" content="实例-单层感知机 模型：o = w1*x1 + w2*x2 + b 输出o作为线性回归的输出，输入层是2维特征；输入层不涉及计算，该神经网络只有输出层1层。
神经元：输出层中负责计算o的单元。
该神经元，依赖于输入层的全部特征，也就是说输出层中的神经元和输入层中各个输入完全连接，所以，这里的输出层又叫作全连接层(fully connected layer)或者稠密层(dense layer)
    生成数据集 目标： o = 2x1 - 3.4x2 + 4.2 其中： 样本集：features: [w1, w2]， labels: [真实值+噪声]
from IPython import display from matplotlib import pyplot as plt from mxnet import autograd, nd import random num_inputs = 2 num_examples = 1000 true_w = [2, -3.4] true_b = 4.2 features = nd.random.normal(scale=1, shape=(num_examples, num_inputs)) labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b labels += nd."><meta property="og:type" content="article"><meta property="og:url" content="https://biubiobiu.github.io/zh-cn/notes/mxnet/gluon/0010_gluon_summary/"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/notes.css><link rel=stylesheet href=/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/zh-cn><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png alt=Logo>
biubiobiu's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu547c3206082786c1d32d36034e2a655a_40863_42x0_resize_box_2.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/zh-cn/search><input type=text name=keyword placeholder=搜索 data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/zh-cn/notes data-filter=all>笔记</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/notes/computer_algorithm/>Computer Algorithm</a><ul><li><a href=/zh-cn/notes/computer_algorithm/0010_data_struct/ title=数据结构>数据结构</a></li><li><a href=/zh-cn/notes/computer_algorithm/0015_comm_ideas/ title=常见算法思路>常见算法思路</a></li><li><a href=/zh-cn/notes/computer_algorithm/0020_tree_search/ title=二叉树-遍历>二叉树-遍历</a></li><li><a href=/zh-cn/notes/computer_algorithm/0030_five_algorithms/ title=五大常用算法>五大常用算法</a></li><li><a href=/zh-cn/notes/computer_algorithm/0040_sliding_window/ title=滑动窗口>滑动窗口</a></li><li><a href=/zh-cn/notes/computer_algorithm/0500_leedcode_list/ title=LeedCode刷库记录>LeedCode刷库记录</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/notes/mxnet/>MxNet</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/notes/mxnet/ndarray/>NdArray</a><ul><li><a href=/zh-cn/notes/mxnet/ndarray/0010_ndarray_summary/ title=NdArray使用>NdArray使用</a></li><li><a href=/zh-cn/notes/mxnet/ndarray/0020_technic_gather/ title=NdArray技巧搜集>NdArray技巧搜集</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/notes/mxnet/gluon/>Gluon</a><ul class=active><li><a class=active href=/zh-cn/notes/mxnet/gluon/0010_gluon_summary/ title=Gluon实例>Gluon实例</a></li><li><a href=/zh-cn/notes/mxnet/gluon/0020_module_gather/ title=Gluon模块简介>Gluon模块简介</a></li><li><a href=/zh-cn/notes/mxnet/gluon/0030_module_gluon_nn/ title=Gluon-nn模块>Gluon-nn模块</a></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class="content container-fluid" id=content><div class="container-fluid note-card-holder" id=note-card-holder><div class=note-card><div class=item><h5 class=note-title><span>实例-单层感知机</span></h5><div class=card><div class=card-body><p><img src=/datasets/posts/dp_summary/single_perceptron.jpg alt=单层感知机></p><p>模型：o = w<sub>1</sub>*x<sub>1</sub> + w<sub>2</sub>*x<sub>2</sub> + b</p></p><p>输出<code>o</code>作为线性回归的输出，输入层是2维特征；输入层不涉及计算，该神经网络只有输出层1层。</p><code>神经元</code>：输出层中负责计算o的单元。</p>该神经元，依赖于输入层的全部特征，也就是说输出层中的神经元和输入层中各个输入完全连接，所以，这里的输出层又叫作<code>全连接层(fully connected layer)</code>或者<code>稠密层(dense layer)</code></p></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>生成数据集</span></h5><div class=card><div class=card-body><p>目标： o = 2<em>x<sub>1</sub> - 3.4</em>x<sub>2</sub> + 4.2 其中：
样本集：features: [w<sub>1</sub>, w<sub>2</sub>]， labels: [真实值+噪声]</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> IPython <span style=color:#f92672>import</span> display
<span style=color:#f92672>from</span> matplotlib <span style=color:#f92672>import</span> pyplot <span style=color:#66d9ef>as</span> plt
<span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> autograd, nd
<span style=color:#f92672>import</span> random

num_inputs <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>
num_examples <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span>
true_w <span style=color:#f92672>=</span> [<span style=color:#ae81ff>2</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>3.4</span>]
true_b <span style=color:#f92672>=</span> <span style=color:#ae81ff>4.2</span>
features <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(scale<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, shape<span style=color:#f92672>=</span>(num_examples, num_inputs))
labels <span style=color:#f92672>=</span> true_w[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>*</span> features[:, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>+</span> true_w[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>*</span> features[:, <span style=color:#ae81ff>1</span>] <span style=color:#f92672>+</span> true_b
labels <span style=color:#f92672>+=</span> nd<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(scale<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, shape<span style=color:#f92672>=</span>labels<span style=color:#f92672>.</span>shape)

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>读取数据集 - 从零实现</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>data_iter</span>(batch_size, features, labels):
    num_examples <span style=color:#f92672>=</span> len(features)
    indices <span style=color:#f92672>=</span> list(range(num_examples))
    random<span style=color:#f92672>.</span>shuffle(indices)  <span style=color:#75715e># 样本的读取顺序是随机的</span>
    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, num_examples, batch_size):
        j <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>array(indices[i: min(i <span style=color:#f92672>+</span> batch_size, num_examples)])
        <span style=color:#75715e># take函数根据索引返回对应元素</span>
        <span style=color:#66d9ef>yield</span> features<span style=color:#f92672>.</span>take(j), labels<span style=color:#f92672>.</span>take(j)  

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>读取数据集 - Gluon实现</span></h5><div class=card><div class=card-body><p><code>DataLoader</code> 返回一个迭代器，一次返回batch_size个样本</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> data <span style=color:#66d9ef>as</span> gdata

batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
<span style=color:#75715e># 将训练数据的特征和标签组合</span>
dataset <span style=color:#f92672>=</span> gdata<span style=color:#f92672>.</span>ArrayDataset(features, labels)
<span style=color:#75715e># 随机读取小批量</span>
data_iter <span style=color:#f92672>=</span> gdata<span style=color:#f92672>.</span>DataLoader(dataset, batch_size, shuffle<span style=color:#f92672>=</span>True)

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型定义 - 从零实现</span></h5><div class=card><div class=card-body><p>手动定义模型参数，一定要开辟存储梯度的内存。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 将权重初始化为：均值为0、标准差为0.01的正太随机数</span>
w <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(scale<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, shape<span style=color:#f92672>=</span>(num_inputs, <span style=color:#ae81ff>1</span>))
b <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>zeros(shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>,))
<span style=color:#75715e># 开辟存储梯度的内存</span>
w<span style=color:#f92672>.</span>attach_grad()
b<span style=color:#f92672>.</span>attach_grad()
<span style=color:#75715e># 定义线性回归的模型</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>linreg</span>(X, w, b): 
    <span style=color:#66d9ef>return</span> nd<span style=color:#f92672>.</span>dot(X, w) <span style=color:#f92672>+</span> b



</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型定义 - Gluon实现</span></h5><div class=card><div class=card-body><p><code>Gluon</code>模块：提供了大量预定义的层。<code>nn</code>模块(neural networks)的缩写，所以里面定义了大量神经网络 的层。
<code>Sequential</code>：可以看作是一个 串联各个层的容器，在构建模型时，在该容器中一次添加层。当给定输入数据时，
容器中的每一层的输出作为下一层的输入。</p><code>init</code>模块：initializer的缩写。该模块提供了模型参数初始化的各种方法。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> nn
<span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> init
<span style=color:#75715e># 定义模型</span>
net <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential()
net<span style=color:#f92672>.</span>add(nn<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>1</span>))
<span style=color:#75715e># 初始化模型参数</span>
net<span style=color:#f92672>.</span>initialize(init<span style=color:#f92672>.</span>Normal(sigma<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>))
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>定义损失函数 - 从零实现</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>squared_loss</span>(y_hat, y):
    <span style=color:#66d9ef>return</span> (y_hat <span style=color:#f92672>-</span> y<span style=color:#f92672>.</span>reshape(y_hat<span style=color:#f92672>.</span>shape)) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>/</span> <span style=color:#ae81ff>2</span>
loss <span style=color:#f92672>=</span> squared_loss
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>定义损失函数 - Gluon实现</span></h5><div class=card><div class=card-body><p><code>Gluon</code>中的<code>loss</code>模块：定义了各种损失函数。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> loss <span style=color:#66d9ef>as</span> gloss
loss <span style=color:#f92672>=</span> gloss<span style=color:#f92672>.</span>L2Loss()  <span style=color:#75715e># 平方损失又称L2范数损失</span>
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>定义优化算法 - 从零实现</span></h5><div class=card><div class=card-body><p><code>param.grad</code>自动求梯度模块计算得来的梯度是一个批量样本的梯度和。在迭代模型参数时，需要除以批量大小来得到平均值。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sgd</span>(params, lr, batch_size):
    <span style=color:#66d9ef>for</span> param <span style=color:#f92672>in</span> params:
        param[:] <span style=color:#f92672>=</span> param <span style=color:#f92672>-</span> lr <span style=color:#f92672>*</span> param<span style=color:#f92672>.</span>grad <span style=color:#f92672>/</span> batch_size

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>定义优化算法 - Gluon实现</span></h5><div class=card><div class=card-body><p><code>Gluon</code>模块中的<code>Trainer</code>类，用来迭代模型中的全部参数。这些参数可以通过<code>collect_params</code>函数获取。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> Trainer
trainer <span style=color:#f92672>=</span> Trainer(net<span style=color:#f92672>.</span>collect_params(), <span style=color:#e6db74>&#39;sgd&#39;</span>, {<span style=color:#e6db74>&#39;learning_rate&#39;</span>: <span style=color:#ae81ff>0.03</span>})
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>训练模型 - 从零实现</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>lr <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.03</span>
num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
net <span style=color:#f92672>=</span> linreg
<span style=color:#75715e># 训练模型一共需要num_epochs个迭代周期</span>
<span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(num_epochs): 
    <span style=color:#75715e># 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）</span>
    <span style=color:#75715e># x和y分别是小批量样本的特征和标签</span>
    <span style=color:#66d9ef>for</span> X, y <span style=color:#f92672>in</span> data_iter(batch_size, features, labels):
        <span style=color:#66d9ef>with</span> autograd<span style=color:#f92672>.</span>record():
            l <span style=color:#f92672>=</span> loss(net(X, w, b), y)  <span style=color:#75715e># l是有关小批量X和y的损失</span>
        l<span style=color:#f92672>.</span>backward()  <span style=color:#75715e># 小批量的损失对模型参数求梯度</span>
        sgd([w, b], lr, batch_size)  <span style=color:#75715e># 使用小批量随机梯度下降迭代模型参数</span>
    train_l <span style=color:#f92672>=</span> loss(net(features, w, b), labels)
    <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;epoch </span><span style=color:#e6db74>%d</span><span style=color:#e6db74>, loss </span><span style=color:#e6db74>%f</span><span style=color:#e6db74>&#39;</span> <span style=color:#f92672>%</span> (epoch <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>, train_l<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>asnumpy()))

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>训练模型 - Gluon实现</span></h5><div class=card><div class=card-body><p>通过<code>Trainer</code>实例的<code>step</code>函数来迭代模型参数。由于loss是长度为batch_size的向量，在执行l.backward()时，
等价于执行l.sum().backward()。所以要用batch_size做平均。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
<span style=color:#75715e># 训练模型一共需要num_epochs个迭代周期</span>
<span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>, num_epochs <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>):
    <span style=color:#75715e># 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）</span>
    <span style=color:#75715e># x和y分别是小批量样本的特征和标签</span>
    <span style=color:#66d9ef>for</span> X, y <span style=color:#f92672>in</span> data_iter:
        <span style=color:#66d9ef>with</span> autograd<span style=color:#f92672>.</span>record():
            l <span style=color:#f92672>=</span> loss(net(X), y)  <span style=color:#75715e># l是有关小批量X和y的损失</span>
        l<span style=color:#f92672>.</span>backward() <span style=color:#75715e># 等价于l.sum().backward()</span>
        trainer<span style=color:#f92672>.</span>step(batch_size)  <span style=color:#75715e># 指定batch_size，从而对批量样本梯度求平均</span>
    l <span style=color:#f92672>=</span> loss(net(features), labels)
    <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;epoch </span><span style=color:#e6db74>%d</span><span style=color:#e6db74>, loss: </span><span style=color:#e6db74>%f</span><span style=color:#e6db74>&#39;</span> <span style=color:#f92672>%</span> (epoch, l<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>asnumpy()))

</code></pre></div></div></div></div></div></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>导航</h5><ul><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#about>关于</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#skills>技能</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#experiences>经验</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#projects>项目</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#education>教育</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#recent-posts>最新博客</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#accomplishments>造诣</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#achievements>业绩</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>联系方式:</h5><ul><li><span>电话:</span> <span>+0123456789</span></li><li><span>邮箱:</span> <span>bsgshyn8@163.com</span></li></ul></div></div></div><hr><div class=container><p id=disclaimer><strong>免责声明:</strong> 这个主题是MIT许可的。因此，您可以将其用于非商业、商业或者私人用途。您可以修改或分发主题，而无须经 作者任何许可。但是，主题作者不对主题的任何问题提供任何保证或承担任何责任。</p></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/imagesloaded.pkgd.min.js></script><script src=/js/note.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>