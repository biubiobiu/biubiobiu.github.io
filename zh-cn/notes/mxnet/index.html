<!doctype html><html><head><title>MxNet</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0d88336a9cbd3e68c7714efae786b0a9_38044_42x0_resize_box_2.png><meta property="og:title" content="MxNet"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://biubiobiu.github.io/zh-cn/notes/mxnet/"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/notes.css><link rel=stylesheet href=/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/zh-cn><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png alt=Logo>
biubiobiu's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=languageSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="flag-icon flag-icon-zh-cn"></span>简体中文</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/notes/mxnet><span class="flag-icon flag-icon-gb"></span>English</a></div></li><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu547c3206082786c1d32d36034e2a655a_40863_42x0_resize_box_2.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/zh-cn/search><input type=text name=keyword placeholder=搜索 data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/zh-cn/notes data-filter=all>笔记</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/notes/computer_algorithm/>Computer Algorithm</a><ul><li><a href=/zh-cn/notes/computer_algorithm/data_struct/ title=数据结构>数据结构</a></li><li><a href=/zh-cn/notes/computer_algorithm/torch_summary/ title=二叉树-遍历>二叉树-遍历</a></li><li><a href=/zh-cn/notes/computer_algorithm/dynamic_plan/ title=五大常用算法>五大常用算法</a></li><li><a href=/zh-cn/notes/computer_algorithm/sliding_window/ title=滑动窗口>滑动窗口</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/notes/mxnet/>MxNet</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/notes/mxnet/ndarray/>NdArray</a><ul><li><a href=/zh-cn/notes/mxnet/ndarray/ndarray_summary/ title=NdArray使用>NdArray使用</a></li><li><a href=/zh-cn/notes/mxnet/ndarray/technic_gather/ title=NdArray技巧搜集>NdArray技巧搜集</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/notes/mxnet/gluon/>Gluon</a><ul><li><a href=/zh-cn/notes/mxnet/gluon/gluon_summary/ title=Gluon实例>Gluon实例</a></li><li><a href=/zh-cn/notes/mxnet/gluon/module_gather/ title=Gluon模块简介>Gluon模块简介</a></li><li><a href=/zh-cn/notes/mxnet/gluon/module_gluon_nn/ title=Gluon-nn模块>Gluon-nn模块</a></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class="content container-fluid" id=content><div class="container-fluid note-card-holder" id=note-card-holder><div class=note-card><div class=item><h5 class=note-title><span>查阅文档</span></h5><div class=card><div class=card-body><p>怎么查阅相关文档？ <a href=https://mxnet.apache.org/ target=blank>官网</a></p><h3 id=1-查阅模块里的所有函数和类>1. 查阅<code>模块</code>里的所有函数和类</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
<span style=color:#66d9ef>print</span>(dir(nd<span style=color:#f92672>.</span>random))
</code></pre></div><ol><li>__开头和结尾的函数 (python的特别对象) 可以忽略</li><li>_开头的函数 (一般为内部函数) 可以忽略</li><li>其余成员，可以根据名字 大致猜出是什么意思。</li></ol><h3 id=2-查阅特定函数和类的使用>2. 查阅特定<code>函数和类</code>的使用</h3><p>想了解某个函数或者类的具体用法，可以使用help函数。以NDArray中的ones_like函数为例。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>help(nd<span style=color:#f92672>.</span>ones_like)
</code></pre></div><p>注意：</p></p><ol><li>jupyter记事本里，使用<code>?</code>来将文档显示在另外一个窗口中。例如：<code>nd.ones_like?</code> 与 <code>help(nd.ones_like)</code>效果一样。<code>nd.ones_like??</code>会额外显示该函数实现的代码。</li></ol></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>内存开销</span></h5><div class=card><div class=card-body><ol><li><p>原始操作</p>首先来个例子：Y = Y + X &ndash;> 每个操作会新开内存来存储运算结果。
上例中，X，Y 变量首先存储在内存中，相加的计算结果会另外开辟内存来存储；然后变量Y在指向新的内存。
内存使用情况：</p>内存id_x &lt;&ndash; X</p>内存id_y &lt;&ndash; Y</p>内存id_x+y &lt;&ndash; Y</p></li><li><p>Y[:] = X + Y 或者 Y += X</p>通过<code>[:]</code>把X+Y的结果写进Y对应的内存中。上述操作中，需要另外开辟内存来存储计算结果。
内存使用情况：</p>内存id_x &lt;&ndash; X</p>内存id_y &lt;&ndash; Y</p>内存id_x+y &ndash;> 把<code>内存id_x+y</code>中数值复制到<code>内存id_y</code>中</p></li><li><p>使用运算符全名函数中的out参数</p>可以避免临时内存开销，使用运算符全名函数：<code>nd.elemwise_add(X, Y, out=Y)</code>。内存使用情况：</p>内存id_x &lt;&ndash; X</p>内存id_y &lt;&ndash; Y</p>内存id_y &lt;&ndash; 直接存放 X+Y 的计算结果</p></li></ol></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>自动求梯度</span></h5><div class=card><div class=card-body><p>MXNet提供的autograd模块，可以自动求梯度(gradient)</p></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> autograd, nd
<span style=color:#75715e># 1. 创建变量 x，并赋初值</span>
x <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>arrange(<span style=color:#ae81ff>4</span>)<span style=color:#f92672>.</span>reshape((<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>1</span>))
<span style=color:#75715e># 2. 为了求变量x的梯度，先调用attach_grad函数来申请存储梯度所需要的内存 </span>
x<span style=color:#f92672>.</span>attach_grad()
<span style=color:#75715e># 3. 为了减少计算和内存开销，默认条件下MXNet是不会记录：求梯度的计算，</span>
<span style=color:#75715e>#    需要调用record函数来要求MXNet记录与求梯度有关的计算。</span>
<span style=color:#66d9ef>print</span>(autograd<span style=color:#f92672>.</span>is_training())    <span style=color:#75715e># False</span>
<span style=color:#66d9ef>with</span> autograd<span style=color:#f92672>.</span>record():
  <span style=color:#66d9ef>print</span>(autograd<span style=color:#f92672>.</span>is_training())  <span style=color:#75715e># True</span>
  y <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>nd<span style=color:#f92672>.</span>dot(x<span style=color:#f92672>.</span>T, x)
<span style=color:#75715e># 4. 调用backward函数自动求梯度。y必须是一个标量，</span>
<span style=color:#75715e>#  如果y不是标量：MXNet会先对y中元素求和，然后对该和值求有关x的梯度</span>
y<span style=color:#f92672>.</span>backward() 

</code></pre></div><p>注意：</p></p><ol><li>在调用record函数后，MXNet会记录并计算梯度；</li><li>默认情况下，autograd会改变运行模式：从预测模式转为训练模式。可以通过调用is_training函数来查看。</li></ol></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>样例</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>sum/mean等操作 - 保留原维度数</span></h5><div class=card><div class=card-body><p><code>keepdims</code>: 保留原维度数。例如：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>softmax</span>(X):
    X_exp <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>exp()  <span style=color:#75715e># shape = (n, m)</span>
    <span style=color:#75715e># shape = (n, 1) 而并不是 (n,)</span>
    partition <span style=color:#f92672>=</span> X_exp<span style=color:#f92672>.</span>sum(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, keepdims<span style=color:#f92672>=</span>True)  
    <span style=color:#66d9ef>return</span> X_exp <span style=color:#f92672>/</span> partition  <span style=color:#75715e># 这里应用了广播机制</span>
X <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>5</span>))
X_prob <span style=color:#f92672>=</span> softmax(X)

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>B的值作为A的索引 - 取值</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
y_hat <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>0.1</span>, <span style=color:#ae81ff>0.3</span>, <span style=color:#ae81ff>0.6</span>], [<span style=color:#ae81ff>0.3</span>, <span style=color:#ae81ff>0.2</span>, <span style=color:#ae81ff>0.5</span>]])
y <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>], dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;int32&#39;</span>)
nd<span style=color:#f92672>.</span>pick(y_hat, y)
<span style=color:#75715e># 结果: [0.1, 0.5]</span>

<span style=color:#75715e># 应用的实例：交叉熵的实现</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>cross_entropy</span>(y_hat, y):
    <span style=color:#66d9ef>return</span> <span style=color:#f92672>-</span>nd<span style=color:#f92672>.</span>pick(y_hat, y)<span style=color:#f92672>.</span>log()
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>样例</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型基类-Block</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> Block, nn
<span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> ndarray <span style=color:#66d9ef>as</span> F

<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Model</span>(Block):
	<span style=color:#66d9ef>def</span> __init__(self, <span style=color:#f92672>**</span>kwargs):
		super(Model, self)<span style=color:#f92672>.</span>__init__(<span style=color:#f92672>**</span>kwargs)
		<span style=color:#75715e># use name_scope to give child Blocks appropriate names.</span>
		<span style=color:#66d9ef>with</span> self<span style=color:#f92672>.</span>name_scope():
			self<span style=color:#f92672>.</span>dense0 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>20</span>)
			self<span style=color:#f92672>.</span>dense1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>20</span>)

	<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
		x <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>dense0(x))
		<span style=color:#66d9ef>return</span> F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>dense1(x))

model <span style=color:#f92672>=</span> Model()
model<span style=color:#f92672>.</span>initialize(ctx<span style=color:#f92672>=</span>mx<span style=color:#f92672>.</span>cpu(<span style=color:#ae81ff>0</span>))
model(F<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>10</span>), ctx<span style=color:#f92672>=</span>mx<span style=color:#f92672>.</span>cpu(<span style=color:#ae81ff>0</span>)))

</code></pre></div><p>class Block(builtins.object)<br>网络的最基础的类，搭建网络时必须继承此Block类<br>—————————————————<br>Block的两个参数：<br></p><ul><li><code>prefix</code> : str; 前缀的作用就像一个命名空间。在父模块的作用域下创建的子模块都有父模块的前缀(prefix).</li><li><code>params</code> : ParameterDict or None; 共享参数。<br>　　例如：dense1共享dense0的参数。<br>　　　　dense0 = nn.Dense(20)<br>　　　　dense1 = nn.Dense(20, params=dense0.collect_params())</li></ul><p>—————————————————<br>Block的方法：</p><ul><li><code>collect_params</code>(self, select=None) 返回一个<code>ParameterDict</code>类。默认包含所有的参数；同时也可以正则匹配:<br>例如：选出特定的参数 [&lsquo;conv1_weight&rsquo;, &lsquo;conv1_bias&rsquo;, &lsquo;fc_weight&rsquo;, &lsquo;fc_bias&rsquo;]<br>即：model.collect_params(&lsquo;conv1_weight|conv1_bias|fc_weight|fc_bias&rsquo;)<br>　　<code>Parameters</code>：空 或者 正则表达式<br>　　<code>Returns</code>: py:class:ParameterDict</li><li><code>forward</code>(self, *args) 完成前向计算，输入是NDArray列表<br>　　Parameters：*args : list of NDArray</li><li><code>hybridize</code>(self, active=True, **kwargs) 激活/不激活HybridBlock的递归<br>　　Parameters： bool, default True</li><li><code>initialize</code>(self, init=&lt;mxnet.initializer.Uniform object>, ctx=None, verbose=False, force_reinit=False)<br>　　对模型的参数初始化，默认是均匀分布。<br>　　等价于：block.collect_params().initialize(&mldr;)<br>　　Parameters：<br>　　　　init : Initializer 初始化方法<br>　　　　ctx : 设备 或者 设备列表。会把模型copy到所有指定的设备上<br>　　　　verbose : bool, default False 是否在初始化时粗略地打印细节。<br>　　　　force_reinit : bool, default False 是否重新初始化，即使已经初始化</li><li><code>load_parameters</code>(self, filename, ctx=None, allow_missing=False, ignore_extra=False, cast_dtype=False, dtype_source=&lsquo;current&rsquo;)<br>　　加载模型参数从 用<code>save_parameters</code>保存的模型文件中。<br>　　Parameters：<br>　　　　filename : str 模型文件路径<br>　　　　ctx : 设备 或者设备列表。默认使用CPU<br>　　　　allow_missing : bool, default False 是否默默跳过模型文件中不存<br>　　　　　　在的模型参数。<br>　　　　ignore_extra : bool, default False 是否默默忽略模型中不存在的参<br>　　　　　　数(模型文件中有，模型定义中没有)<br>　　　　cast_dtype : bool, default False 从checkpointload模型时，是否根<br>　　　　　　据传入转换NDArray的数据类型<br>　　　　dtype_source : str, default &lsquo;current&rsquo; 枚举值：{&lsquo;current&rsquo;, &lsquo;saved&rsquo;}<br>　　　　　　只有再cast_dtype=True时有效，指定模型参数的数据类型</li><li><code>name_scope</code>(self) 返回一个命名空间，用来管理Block和参数names。<br>　　必须在with语句中使用：<br>　　with self.name_scope():<br>　　　　self.dense = nn.Dense(20)</li><li><code>register_child</code>(self, block, name=None) 将block注册为子节点，block的属<br>　　性将自动注册。</li><li><code>save_parameters</code>(self, filename) 保持模型参数到磁盘。该方法只保存模型<br>　　参数的权重，不保存模型的结构。如果想要保存模型的结构，请使<br>　　用:py:meth:<code>HybridBlock.export</code>.<br>　　Parameters：Path to file.</li><li><code>summary</code>(self, *inputs) 打印模型的输出和参数的摘要。模型必须被初始化</li></ul><p>—————————————————<br>数据描述：</p><ul><li><code>name</code> :py:class:Block 的名字</li><li><code>params</code>：返回一个参数字典（不包含子节点的参数）</li><li><code>prefix</code>：返回py:class:Block的前缀</li></ul></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型参数-Parameter</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>ctx <span style=color:#f92672>=</span> mx<span style=color:#f92672>.</span>gpu(<span style=color:#ae81ff>0</span>)
x <span style=color:#f92672>=</span> mx<span style=color:#f92672>.</span>nd<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>100</span>), ctx<span style=color:#f92672>=</span>ctx)
w <span style=color:#f92672>=</span> mx<span style=color:#f92672>.</span>gluon<span style=color:#f92672>.</span>Parameter(<span style=color:#e6db74>&#39;fc_weight&#39;</span>, shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>100</span>), init<span style=color:#f92672>=</span>mx<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>Xavier())
b <span style=color:#f92672>=</span> mx<span style=color:#f92672>.</span>gluon<span style=color:#f92672>.</span>Parameter(<span style=color:#e6db74>&#39;fc_bias&#39;</span>, shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>64</span>,), init<span style=color:#f92672>=</span>mx<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>Zero())
w<span style=color:#f92672>.</span>initialize(ctx<span style=color:#f92672>=</span>ctx)
b<span style=color:#f92672>.</span>initialize(ctx<span style=color:#f92672>=</span>ctx)
out <span style=color:#f92672>=</span> mx<span style=color:#f92672>.</span>nd<span style=color:#f92672>.</span>FullyConnected(x, w<span style=color:#f92672>.</span>data(ctx), b<span style=color:#f92672>.</span>data(ctx), num_hidden<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>)
</code></pre></div><p>class:Parameter 一个存放Blocks的参数的权重的容器。初始化后<code>Parameter.initialize(...)</code>，会copy所有参数权重到每个设备上。如果<code>grad_req</code>不为null，在每个设备上，该容器会拥有一个梯度向量。</p><p>Parameter(name,<br>　　　　　grad_req=&lsquo;write&rsquo;,<br>　　　　　shape=None,<br>　　　　　dtype=&lt;class &lsquo;numpy.float32&rsquo;>,<br>　　　　　lr_mult=1.0,<br>　　　　　wd_mult=1.0,<br>　　　　　init=None,<br>　　　　　allow_deferred_init=False,<br>　　　　　differentiable=True,<br>　　　　　stype=&lsquo;default&rsquo;,<br>　　　　　grad_stype=&lsquo;default&rsquo;)</p><p>形参：<br>——————————</p><ul><li><code>name</code> :
str类型；参数的名字。</li><li><code>grad_req</code> : 枚举值：{&lsquo;write&rsquo;, &lsquo;add&rsquo;, &lsquo;null&rsquo;}, 默认值：&lsquo;write&rsquo;。指定怎么更新梯度到梯度向量。<br>　　<code>'write'</code>:每次把梯度值写到 梯度向量中<br>　　<code>'add'</code>: 每次把计算的梯度值add到梯度向量中. 在每次迭代之前，<br>　　　　你需要手动调用<code>zero_grad()</code>来清理梯度缓存。<br>　　<code>'null'</code>: 参数不需要计算梯度，不会分配梯度向量。</li><li><code>shape</code> : int or tuple of int, default None. 参数的尺寸.</li><li><code>dtype</code> : numpy.dtype or str, default &lsquo;float32&rsquo;. 参数的数据类型</li><li><code>lr_mult</code> : float, default 1.0, 学习率.</li><li><code>wd_mult</code> : float, default 1.0, 权重衰减率 L2</li><li><code>init</code> : Initializer, default None. 参数的初始化，默认全局初始化</li><li><code>stype</code>: 枚举值: {&lsquo;default&rsquo;, &lsquo;row_sparse&rsquo;, &lsquo;csr&rsquo;}, defaults to &lsquo;default&rsquo;. 参数的存储类型。</li><li><code>grad_stype</code>: 枚举值: {&lsquo;default&rsquo;, &lsquo;row_sparse&rsquo;, &lsquo;csr&rsquo;}, defaults to &lsquo;default&rsquo;. 参数梯度的存储类型</li></ul><p>属性:<br>——————————</p><ul><li><code>grad_req</code> : 枚举值:{&lsquo;write&rsquo;, &lsquo;add&rsquo;, &lsquo;null&rsquo;} 可以在初始化之前/之后设置。当不需要计算参数的梯度时，设置为<code>null</code>，以节省内存和计算量。</li><li><code>lr_mult</code> : float 学习率</li><li><code>wd_mult</code> : float 权重衰减率</li></ul><p>定义的函数：<br>——————————</p><ul><li><p><code>cast(self, dtype)</code> 转换参数的值/梯度的数据类型。<br>　　dtype : str or numpy.dtype 新的数据类型</p></li><li><p><code>data(self, ctx=None)</code> 获取这个参数在设备<code>ctx</code>上的值，参数必须已经初始化了。<br>　　ctx : 指定设备<br>　　Returns：NDArray on ctx</p></li><li><p><code>grad(self, ctx=None)</code> 获取这个参数在设备<code>ctx</code>上的梯度值。<br>　　ctx : 指定设备</p></li><li><p><code>initialize</code>(self, init=None, ctx=None,<br>　　default_init=&lt;mxnet.initializer.Uniform>,<br>　　force_reinit=False) 初始化参数和梯度向量<br>　　<code>init</code> : Initializer 初始化参数的值<br>　　<code>ctx</code> : 设备/设备列表, 默认使用:py:meth:<code>context.current_context()</code>.<br>　　<code>default_init</code> : Initializer 当:py:func:<code>init</code>和:py:meth:<code>Parameter.init</code>都为none时，使用该默认的初始化.<br>　　<code>force_reinit</code> : bool, default False 当参数已经被初始化，是否再次初始化。</p></li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>weight <span style=color:#f92672>=</span> mx<span style=color:#f92672>.</span>gluon<span style=color:#f92672>.</span>Parameter(<span style=color:#e6db74>&#39;weight&#39;</span>, shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>))
weight<span style=color:#f92672>.</span>initialize(ctx<span style=color:#f92672>=</span>mx<span style=color:#f92672>.</span>cpu(<span style=color:#ae81ff>0</span>))
weight<span style=color:#f92672>.</span>data()
<span style=color:#75715e>#　　[[-0.01068833  0.01729892]</span>
<span style=color:#75715e>#　　 [ 0.02042518 -0.01618656]]</span>
<span style=color:#75715e>#　　&lt;NDArray 2x2 @cpu(0)&gt;</span>
weight<span style=color:#f92672>.</span>grad()
<span style=color:#75715e>#　　[[ 0.  0.]</span>
<span style=color:#75715e>#　　 [ 0.  0.]]</span>
<span style=color:#75715e>#　　&lt;NDArray 2x2 @cpu(0)&gt;</span>
weight<span style=color:#f92672>.</span>initialize(ctx<span style=color:#f92672>=</span>[mx<span style=color:#f92672>.</span>gpu(<span style=color:#ae81ff>0</span>), mx<span style=color:#f92672>.</span>gpu(<span style=color:#ae81ff>1</span>)])
weight<span style=color:#f92672>.</span>data(mx<span style=color:#f92672>.</span>gpu(<span style=color:#ae81ff>0</span>))
<span style=color:#75715e>#　　[[-0.00873779 -0.02834515]</span>
<span style=color:#75715e>#　　 [ 0.05484822 -0.06206018]]</span>
<span style=color:#75715e>#　　&lt;NDArray 2x2 @gpu(0)&gt;</span>
weight<span style=color:#f92672>.</span>data(mx<span style=color:#f92672>.</span>gpu(<span style=color:#ae81ff>1</span>))
<span style=color:#75715e>#　　[[-0.00873779 -0.02834515]</span>
<span style=color:#75715e>#　　 [ 0.05484822 -0.06206018]]</span>
<span style=color:#75715e>#　　&lt;NDArray 2x2 @gpu(1)&gt;</span>
</code></pre></div><ul><li><code>list_ctx(self)</code> 返回参数初始化在那些设备上</li><li><code>list_data(self)</code> 按照顺序返回所有设备上的参数值
　　Returns: list of NDArrays</li><li><code>list_grad(self)</code> 按照顺序返回所有设备上的梯度值</li><li><code>list_row_sparse_data(self, row_id)</code> 按照顺序返回所有设备上的 <code>行稀疏</code>的参数。<br>　　row_id: 指定看哪一行的数据<br>　　Returns: list of NDArrays</li><li><code>reset_ctx(self, ctx)</code> 重新设定设备，把参数copy到该设备上<br>　　ctx : Context or list of Context, default <code>context.current_context()</code></li><li><code>row_sparse_data(self, row_id)</code><br>　　row_id: NDArray 指定看哪一行的数据<br>　　Returns: NDArray on row_id&rsquo;s context</li><li><code>set_data(self, data)</code> 在所有设备上，设置该参数的值。</li><li><code>var(self)</code> 返回一个代表该参数的符号</li><li><code>zero_grad(self)</code> 将所有设备上的梯度缓存清零</li></ul><p>数据描述:<br>——————————</p><ul><li><code>dtype</code> 参数的数据类型</li><li><code>grad_req</code></li><li><code>shape</code> 参数的尺寸</li></ul></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型参数-访问</span></h5><div class=card><div class=card-body><p><code>ToTensor</code>：将图像数据从uint8格式变换成32位浮点数格式，并除以255使得所有像素的数值均在0到1之间</p><code>transform_first函数</code>：数据集的函数。将<code>ToTensor</code>的变换应用在每个数据样本（图像和标签）的第一个元素，即图像之上.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
<span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> nn

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>网络设计</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
<span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> nn
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型初始化</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
<span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> nn
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型初始化</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
<span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> nn
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>实例-单层感知机</span></h5><div class=card><div class=card-body><p><img src=/datasets/posts/dp_summary/single_perceptron.jpg alt=单层感知机></p><p>模型：o = w<sub>1</sub>*x<sub>1</sub> + w<sub>2</sub>*x<sub>2</sub> + b</p></p><p>输出<code>o</code>作为线性回归的输出，输入层是2维特征；输入层不涉及计算，该神经网络只有输出层1层。</p><code>神经元</code>：输出层中负责计算o的单元。</p>该神经元，依赖于输入层的全部特征，也就是说输出层中的神经元和输入层中各个输入完全连接，所以，这里的输出层又叫作<code>全连接层(fully connected layer)</code>或者<code>稠密层(dense layer)</code></p></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>生成数据集</span></h5><div class=card><div class=card-body><p>目标： o = 2<em>x<sub>1</sub> - 3.4</em>x<sub>2</sub> + 4.2 其中：
样本集：features: [w<sub>1</sub>, w<sub>2</sub>]， labels: [真实值+噪声]</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> IPython <span style=color:#f92672>import</span> display
<span style=color:#f92672>from</span> matplotlib <span style=color:#f92672>import</span> pyplot <span style=color:#66d9ef>as</span> plt
<span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> autograd, nd
<span style=color:#f92672>import</span> random

num_inputs <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>
num_examples <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span>
true_w <span style=color:#f92672>=</span> [<span style=color:#ae81ff>2</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>3.4</span>]
true_b <span style=color:#f92672>=</span> <span style=color:#ae81ff>4.2</span>
features <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(scale<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, shape<span style=color:#f92672>=</span>(num_examples, num_inputs))
labels <span style=color:#f92672>=</span> true_w[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>*</span> features[:, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>+</span> true_w[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>*</span> features[:, <span style=color:#ae81ff>1</span>] <span style=color:#f92672>+</span> true_b
labels <span style=color:#f92672>+=</span> nd<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(scale<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, shape<span style=color:#f92672>=</span>labels<span style=color:#f92672>.</span>shape)

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>读取数据集 - 从零实现</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>data_iter</span>(batch_size, features, labels):
    num_examples <span style=color:#f92672>=</span> len(features)
    indices <span style=color:#f92672>=</span> list(range(num_examples))
    random<span style=color:#f92672>.</span>shuffle(indices)  <span style=color:#75715e># 样本的读取顺序是随机的</span>
    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, num_examples, batch_size):
        j <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>array(indices[i: min(i <span style=color:#f92672>+</span> batch_size, num_examples)])
        <span style=color:#75715e># take函数根据索引返回对应元素</span>
        <span style=color:#66d9ef>yield</span> features<span style=color:#f92672>.</span>take(j), labels<span style=color:#f92672>.</span>take(j)  

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>读取数据集 - Gluon实现</span></h5><div class=card><div class=card-body><p><code>DataLoader</code> 返回一个迭代器，一次返回batch_size个样本</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> data <span style=color:#66d9ef>as</span> gdata

batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
<span style=color:#75715e># 将训练数据的特征和标签组合</span>
dataset <span style=color:#f92672>=</span> gdata<span style=color:#f92672>.</span>ArrayDataset(features, labels)
<span style=color:#75715e># 随机读取小批量</span>
data_iter <span style=color:#f92672>=</span> gdata<span style=color:#f92672>.</span>DataLoader(dataset, batch_size, shuffle<span style=color:#f92672>=</span>True)

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型定义 - 从零实现</span></h5><div class=card><div class=card-body><p>手动定义模型参数，一定要开辟存储梯度的内存。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 将权重初始化为：均值为0、标准差为0.01的正太随机数</span>
w <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(scale<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, shape<span style=color:#f92672>=</span>(num_inputs, <span style=color:#ae81ff>1</span>))
b <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>zeros(shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>,))
<span style=color:#75715e># 开辟存储梯度的内存</span>
w<span style=color:#f92672>.</span>attach_grad()
b<span style=color:#f92672>.</span>attach_grad()
<span style=color:#75715e># 定义线性回归的模型</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>linreg</span>(X, w, b): 
    <span style=color:#66d9ef>return</span> nd<span style=color:#f92672>.</span>dot(X, w) <span style=color:#f92672>+</span> b



</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型定义 - Gluon实现</span></h5><div class=card><div class=card-body><p><code>Gluon</code>模块：提供了大量预定义的层。<code>nn</code>模块(neural networks)的缩写，所以里面定义了大量神经网络 的层。
<code>Sequential</code>：可以看作是一个 串联各个层的容器，在构建模型时，在该容器中一次添加层。当给定输入数据时，
容器中的每一层的输出作为下一层的输入。</p><code>init</code>模块：initializer的缩写。该模块提供了模型参数初始化的各种方法。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> nn
<span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> init
<span style=color:#75715e># 定义模型</span>
net <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential()
net<span style=color:#f92672>.</span>add(nn<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>1</span>))
<span style=color:#75715e># 初始化模型参数</span>
net<span style=color:#f92672>.</span>initialize(init<span style=color:#f92672>.</span>Normal(sigma<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>))
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>定义损失函数 - 从零实现</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>squared_loss</span>(y_hat, y):
    <span style=color:#66d9ef>return</span> (y_hat <span style=color:#f92672>-</span> y<span style=color:#f92672>.</span>reshape(y_hat<span style=color:#f92672>.</span>shape)) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>/</span> <span style=color:#ae81ff>2</span>
loss <span style=color:#f92672>=</span> squared_loss
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>定义损失函数 - Gluon实现</span></h5><div class=card><div class=card-body><p><code>Gluon</code>中的<code>loss</code>模块：定义了各种损失函数。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> loss <span style=color:#66d9ef>as</span> gloss
loss <span style=color:#f92672>=</span> gloss<span style=color:#f92672>.</span>L2Loss()  <span style=color:#75715e># 平方损失又称L2范数损失</span>
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>定义优化算法 - 从零实现</span></h5><div class=card><div class=card-body><p><code>param.grad</code>自动求梯度模块计算得来的梯度是一个批量样本的梯度和。在迭代模型参数时，需要除以批量大小来得到平均值。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sgd</span>(params, lr, batch_size):
    <span style=color:#66d9ef>for</span> param <span style=color:#f92672>in</span> params:
        param[:] <span style=color:#f92672>=</span> param <span style=color:#f92672>-</span> lr <span style=color:#f92672>*</span> param<span style=color:#f92672>.</span>grad <span style=color:#f92672>/</span> batch_size

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>定义优化算法 - Gluon实现</span></h5><div class=card><div class=card-body><p><code>Gluon</code>模块中的<code>Trainer</code>类，用来迭代模型中的全部参数。这些参数可以通过<code>collect_params</code>函数获取。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> Trainer
trainer <span style=color:#f92672>=</span> Trainer(net<span style=color:#f92672>.</span>collect_params(), <span style=color:#e6db74>&#39;sgd&#39;</span>, {<span style=color:#e6db74>&#39;learning_rate&#39;</span>: <span style=color:#ae81ff>0.03</span>})
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>训练模型 - 从零实现</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>lr <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.03</span>
num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
net <span style=color:#f92672>=</span> linreg
<span style=color:#75715e># 训练模型一共需要num_epochs个迭代周期</span>
<span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(num_epochs): 
    <span style=color:#75715e># 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）</span>
    <span style=color:#75715e># x和y分别是小批量样本的特征和标签</span>
    <span style=color:#66d9ef>for</span> X, y <span style=color:#f92672>in</span> data_iter(batch_size, features, labels):
        <span style=color:#66d9ef>with</span> autograd<span style=color:#f92672>.</span>record():
            l <span style=color:#f92672>=</span> loss(net(X, w, b), y)  <span style=color:#75715e># l是有关小批量X和y的损失</span>
        l<span style=color:#f92672>.</span>backward()  <span style=color:#75715e># 小批量的损失对模型参数求梯度</span>
        sgd([w, b], lr, batch_size)  <span style=color:#75715e># 使用小批量随机梯度下降迭代模型参数</span>
    train_l <span style=color:#f92672>=</span> loss(net(features, w, b), labels)
    <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;epoch </span><span style=color:#e6db74>%d</span><span style=color:#e6db74>, loss </span><span style=color:#e6db74>%f</span><span style=color:#e6db74>&#39;</span> <span style=color:#f92672>%</span> (epoch <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>, train_l<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>asnumpy()))

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>训练模型 - Gluon实现</span></h5><div class=card><div class=card-body><p>通过<code>Trainer</code>实例的<code>step</code>函数来迭代模型参数。由于loss是长度为batch_size的向量，在执行l.backward()时，
等价于执行l.sum().backward()。所以要用batch_size做平均。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
<span style=color:#75715e># 训练模型一共需要num_epochs个迭代周期</span>
<span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>, num_epochs <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>):
    <span style=color:#75715e># 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）</span>
    <span style=color:#75715e># x和y分别是小批量样本的特征和标签</span>
    <span style=color:#66d9ef>for</span> X, y <span style=color:#f92672>in</span> data_iter:
        <span style=color:#66d9ef>with</span> autograd<span style=color:#f92672>.</span>record():
            l <span style=color:#f92672>=</span> loss(net(X), y)  <span style=color:#75715e># l是有关小批量X和y的损失</span>
        l<span style=color:#f92672>.</span>backward() <span style=color:#75715e># 等价于l.sum().backward()</span>
        trainer<span style=color:#f92672>.</span>step(batch_size)  <span style=color:#75715e># 指定batch_size，从而对批量样本梯度求平均</span>
    l <span style=color:#f92672>=</span> loss(net(features), labels)
    <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;epoch </span><span style=color:#e6db74>%d</span><span style=color:#e6db74>, loss: </span><span style=color:#e6db74>%f</span><span style=color:#e6db74>&#39;</span> <span style=color:#f92672>%</span> (epoch, l<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>asnumpy()))

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>gluon模块-结构</span></h5><div class=card><div class=card-body>路径.mxnet/gluon/下的树状结构:<br>│　　block.py <code>类：Block, HybridBlock</code><br>│　　loss.py <code>各种loss函数</code><br>│　　parameter.py <code>类：Parameter, Constant, ParameterDict</code><br>│　　trainer.py <code>类：Trainer</code><br>│　　utils.py <code>优化操作</code><br>│　　<strong>init</strong>.py<br>│<br>├─contrib<br>│　　│<br>│　　├─cnn<br>│　　│　　└─ conv_layers.py<br>│　　├─data<br>│　　│　　└─ sampler.py<br>│　　│<br>│　　├─estimator<br>│　　│　　│　　estimator.py<br>│　　│　　└─ event_handler.py<br>│　　│<br>│　　├─nn<br>│　　│　　└─ basic_layers.py<br>│　　│<br>│　　└─rnn<br>│　　　　 │　　conv_rnn_cell.py<br>│　　　　 └─ rnn_cell.py<br>│<br>├─data <code>主要是数据处理操作</code><br>│　　│　　dataloader.py <code>类：DataLoader</code><br>│　　│　　dataset.py <code>常用类: ArrayDataset</code><br>│　　│　　sampler.py<br>│　　│<br>│　　└─vision<br>│　　　　 │　　datasets.py <code>可用的数据集-各个类</code><br>│　　　　 └─ transforms.py <code>数据预处理-各个类</code><br>│<br>├─model_zoo<br>│　　│　　model_store.py<br>│　　│<br>│　　└─vision<br>│　　　　 │　　alexnet.py<br>│　　　　 │　　densenet.py<br>│　　　　 │　　inception.py<br>│　　　　 │　　mobilenet.py<br>│　　　　 │　　resnet.py<br>│　　　　 │　　squeezenet.py<br>│　　　　 └─ vgg.py<br>├─nn <code>网络结构</code><br>│　　│　　activations.py <code>定义了各种激活层</code><br>│　　│　　basic_layers.py <code>定义了网络的基础层,例如：BN,Dropout等</code><br>│　　└─ conv_layers.py <code>定义了各种卷积池化层等</code><br>│<br>└─rnn<br>　　 │　　rnn_cell.py<br>　　 └─ rnn_layer.py</div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>gluon模块-导入</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># data</span>
<span style=color:#f92672>from</span> mxnet.gluon.data <span style=color:#f92672>import</span> ArrayDataset, DataLoader
<span style=color:#f92672>from</span> mxnet.gluon.data.vision.transforms <span style=color:#f92672>import</span> ToTensor, Normalize
<span style=color:#75715e># nn</span>
<span style=color:#f92672>from</span> mxnet.gluon.nn <span style=color:#f92672>import</span> Block, HybridBlock, Sequential, HybridSequential, Dropout, BatchNorm, Dense, PReLU, Conv2D
<span style=color:#75715e># 模型参数</span>
<span style=color:#f92672>from</span> mxnet.gluon.parameter <span style=color:#f92672>import</span> Parameter, Constant, ParameterDict
<span style=color:#75715e># 训练</span>
<span style=color:#f92672>from</span> mxnet.gluon.trainer <span style=color:#f92672>import</span> Trainer
<span style=color:#75715e># 损失函数</span>
<span style=color:#f92672>from</span> mxnet.gluon. <span style=color:#f92672>import</span> loss        <span style=color:#75715e># 损失函数 [&#39;Loss&#39;, &#39;L2Loss&#39;, &#39;L1Loss&#39;, &#39;SigmoidBinaryCrossEntropyLoss&#39;, &#39;SigmoidBCELoss&#39;, &#39;SoftmaxCrossEntropyLoss&#39;, &#39;SoftmaxCELoss&#39;, &#39;KLDivLoss&#39;, &#39;CTCLoss&#39;, &#39;HuberLoss&#39;, &#39;HingeLoss&#39;, &#39;SquaredHingeLoss&#39;, &#39;LogisticLoss&#39;, &#39;TripletLoss&#39;, &#39;PoissonNLLLoss&#39;, &#39;CosineEmbeddingLoss&#39;]</span>

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>数据集 - data</span></h5><div class=card><div class=card-body><p><code>ToTensor</code>：将图像数据从uint8格式变换成32位浮点数格式，并除以255使得所有像素的数值均在0到1之间</p><code>transform_first函数</code>：数据集的函数。将<code>ToTensor</code>的变换应用在每个数据样本（图像和标签）的第一个元素，即图像之上.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> data <span style=color:#66d9ef>as</span> gdata
batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>256</span>
transformer <span style=color:#f92672>=</span> gdata<span style=color:#f92672>.</span>vision<span style=color:#f92672>.</span>transforms<span style=color:#f92672>.</span>ToTensor()
<span style=color:#66d9ef>if</span> sys<span style=color:#f92672>.</span>platform<span style=color:#f92672>.</span>startswith(<span style=color:#e6db74>&#39;win&#39;</span>):
    num_workers <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>  <span style=color:#75715e># 0表示不用额外的进程来加速读取数据</span>
<span style=color:#66d9ef>else</span>:
    num_workers <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>

train_iter <span style=color:#f92672>=</span> gdata<span style=color:#f92672>.</span>DataLoader(mnist_train<span style=color:#f92672>.</span>transform_first(transformer),
                              batch_size, 
                              shuffle<span style=color:#f92672>=</span>True,
                              num_workers<span style=color:#f92672>=</span>num_workers)
test_iter <span style=color:#f92672>=</span> gdata<span style=color:#f92672>.</span>DataLoader(mnist_test<span style=color:#f92672>.</span>transform_first(transformer),
                             batch_size, 
                             shuffle<span style=color:#f92672>=</span>False,
                             num_workers<span style=color:#f92672>=</span>num_workers)

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型初始化 - init</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> init
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>损失函数 - loss</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> loss <span style=color:#66d9ef>as</span> gloss
<span style=color:#75715e># 平方损失又称L2范数损失</span>
loss <span style=color:#f92672>=</span> gloss<span style=color:#f92672>.</span>L2Loss()
<span style=color:#75715e># 包含了softmax运算和交叉熵损失运算</span>
loss <span style=color:#f92672>=</span> gloss<span style=color:#f92672>.</span>SoftmaxCrossEntropyLoss()
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>优化算法 - Trainer</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> Trainer
</code></pre></div></div></div></div></div></div><div class=paginator></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>导航</h5><ul><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#about>关于</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#skills>技能</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#experiences>经验</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#projects>项目</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#education>教育</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#recent-posts>最新博客</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#accomplishments>造诣</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#achievements>业绩</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>联系方式:</h5><ul><li><span>电话:</span> <span>+0123456789</span></li><li><span>邮箱:</span> <span>bsgshyn8@163.com</span></li></ul></div></div></div><hr><div class=container><p id=disclaimer><strong>免责声明:</strong> 这个主题是MIT许可的。因此，您可以将其用于非商业、商业或者私人用途。您可以修改或分发主题，而无须经 作者任何许可。但是，主题作者不对主题的任何问题提供任何保证或承担任何责任。</p></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/imagesloaded.pkgd.min.js></script><script src=/js/note.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>