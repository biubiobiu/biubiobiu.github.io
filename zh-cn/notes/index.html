<!doctype html><html><head><title>笔记</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0d88336a9cbd3e68c7714efae786b0a9_38044_42x0_resize_box_2.png><meta property="og:title" content="笔记"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://biubiobiu.github.io/zh-cn/notes/"><meta property="og:updated_time" content="2010-06-08T08:06:25+06:00"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/notes.css><link rel=stylesheet href=/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/zh-cn><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png alt=Logo>
biubiobiu's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=languageSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="flag-icon flag-icon-zh-cn"></span>简体中文</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/notes><span class="flag-icon flag-icon-gb"></span>English</a></div></li><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu547c3206082786c1d32d36034e2a655a_40863_42x0_resize_box_2.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/zh-cn/search><input type=text name=keyword placeholder=搜索 data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/zh-cn/notes data-filter=all>笔记</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/notes/computer_algorithm/>Computer Algorithm</a><ul><li><a href=/zh-cn/notes/computer_algorithm/0010_data_struct/ title=数据结构>数据结构</a></li><li><a href=/zh-cn/notes/computer_algorithm/0015_comm_ideas/ title=常见算法思路>常见算法思路</a></li><li><a href=/zh-cn/notes/computer_algorithm/0020_tree_search/ title=二叉树-遍历>二叉树-遍历</a></li><li><a href=/zh-cn/notes/computer_algorithm/0030_five_algorithms/ title=五大常用算法>五大常用算法</a></li><li><a href=/zh-cn/notes/computer_algorithm/0040_sliding_window/ title=滑动窗口>滑动窗口</a></li><li><a href=/zh-cn/notes/computer_algorithm/0500_leedcode_list/ title=LeedCode刷库记录>LeedCode刷库记录</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/notes/mxnet/>MxNet</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/notes/mxnet/ndarray/>NdArray</a><ul><li><a href=/zh-cn/notes/mxnet/ndarray/0010_ndarray_summary/ title=NdArray使用>NdArray使用</a></li><li><a href=/zh-cn/notes/mxnet/ndarray/0020_technic_gather/ title=NdArray技巧搜集>NdArray技巧搜集</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/notes/mxnet/gluon/>Gluon</a><ul><li><a href=/zh-cn/notes/mxnet/gluon/0010_gluon_summary/ title=Gluon实例>Gluon实例</a></li><li><a href=/zh-cn/notes/mxnet/gluon/0020_module_gather/ title=Gluon模块简介>Gluon模块简介</a></li><li><a href=/zh-cn/notes/mxnet/gluon/0030_module_gluon_nn/ title=Gluon-nn模块>Gluon-nn模块</a></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class="content container-fluid" id=content><div class="container-fluid note-card-holder" id=note-card-holder><div class=note-card><div class=item><h5 class=note-title><span>查阅文档</span></h5><div class=card><div class=card-body><p>怎么查阅相关文档？ <a href=https://mxnet.apache.org/ target=blank>官网</a></p><h3 id=1-查阅模块里的所有函数和类>1. 查阅<code>模块</code>里的所有函数和类</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
<span style=color:#66d9ef>print</span>(dir(nd<span style=color:#f92672>.</span>random))
</code></pre></div><ol><li>__开头和结尾的函数 (python的特别对象) 可以忽略</li><li>_开头的函数 (一般为内部函数) 可以忽略</li><li>其余成员，可以根据名字 大致猜出是什么意思。</li></ol><h3 id=2-查阅特定函数和类的使用>2. 查阅特定<code>函数和类</code>的使用</h3><p>想了解某个函数或者类的具体用法，可以使用help函数。以NDArray中的ones_like函数为例。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>help(nd<span style=color:#f92672>.</span>ones_like)
</code></pre></div><p>注意：</p></p><ol><li>jupyter记事本里，使用<code>?</code>来将文档显示在另外一个窗口中。例如：<code>nd.ones_like?</code> 与 <code>help(nd.ones_like)</code>效果一样。<code>nd.ones_like??</code>会额外显示该函数实现的代码。</li></ol></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>内存开销</span></h5><div class=card><div class=card-body><ol><li><p>原始操作</p>首先来个例子：Y = Y + X &ndash;> 每个操作会新开内存来存储运算结果。
上例中，X，Y 变量首先存储在内存中，相加的计算结果会另外开辟内存来存储；然后变量Y在指向新的内存。
内存使用情况：</p>内存id_x &lt;&ndash; X</p>内存id_y &lt;&ndash; Y</p>内存id_x+y &lt;&ndash; Y</p></li><li><p>Y[:] = X + Y 或者 Y += X</p>通过<code>[:]</code>把X+Y的结果写进Y对应的内存中。上述操作中，需要另外开辟内存来存储计算结果。
内存使用情况：</p>内存id_x &lt;&ndash; X</p>内存id_y &lt;&ndash; Y</p>内存id_x+y &ndash;> 把<code>内存id_x+y</code>中数值复制到<code>内存id_y</code>中</p></li><li><p>使用运算符全名函数中的out参数</p>可以避免临时内存开销，使用运算符全名函数：<code>nd.elemwise_add(X, Y, out=Y)</code>。内存使用情况：</p>内存id_x &lt;&ndash; X</p>内存id_y &lt;&ndash; Y</p>内存id_y &lt;&ndash; 直接存放 X+Y 的计算结果</p></li></ol></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>自动求梯度</span></h5><div class=card><div class=card-body><p>MXNet提供的autograd模块，可以自动求梯度(gradient)</p></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> autograd, nd
<span style=color:#75715e># 1. 创建变量 x，并赋初值</span>
x <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>arrange(<span style=color:#ae81ff>4</span>)<span style=color:#f92672>.</span>reshape((<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>1</span>))
<span style=color:#75715e># 2. 为了求变量x的梯度，先调用attach_grad函数来申请存储梯度所需要的内存 </span>
x<span style=color:#f92672>.</span>attach_grad()
<span style=color:#75715e># 3. 为了减少计算和内存开销，默认条件下MXNet是不会记录：求梯度的计算，</span>
<span style=color:#75715e>#    需要调用record函数来要求MXNet记录与求梯度有关的计算。</span>
<span style=color:#66d9ef>print</span>(autograd<span style=color:#f92672>.</span>is_training())    <span style=color:#75715e># False</span>
<span style=color:#66d9ef>with</span> autograd<span style=color:#f92672>.</span>record():
  <span style=color:#66d9ef>print</span>(autograd<span style=color:#f92672>.</span>is_training())  <span style=color:#75715e># True</span>
  y <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>nd<span style=color:#f92672>.</span>dot(x<span style=color:#f92672>.</span>T, x)
<span style=color:#75715e># 4. 调用backward函数自动求梯度。y必须是一个标量，</span>
<span style=color:#75715e>#  如果y不是标量：MXNet会先对y中元素求和，然后对该和值求有关x的梯度</span>
y<span style=color:#f92672>.</span>backward() 

</code></pre></div><p>注意：</p></p><ol><li>在调用record函数后，MXNet会记录并计算梯度；</li><li>默认情况下，autograd会改变运行模式：从预测模式转为训练模式。可以通过调用is_training函数来查看。</li></ol></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>样例</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>sum/mean等操作 - 保留原维度数</span></h5><div class=card><div class=card-body><p><code>keepdims</code>: 保留原维度数。例如：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>softmax</span>(X):
    X_exp <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>exp()  <span style=color:#75715e># shape = (n, m)</span>
    <span style=color:#75715e># shape = (n, 1) 而并不是 (n,)</span>
    partition <span style=color:#f92672>=</span> X_exp<span style=color:#f92672>.</span>sum(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, keepdims<span style=color:#f92672>=</span>True)  
    <span style=color:#66d9ef>return</span> X_exp <span style=color:#f92672>/</span> partition  <span style=color:#75715e># 这里应用了广播机制</span>
X <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>5</span>))
X_prob <span style=color:#f92672>=</span> softmax(X)

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>B的值作为A的索引 - 取值</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
y_hat <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>0.1</span>, <span style=color:#ae81ff>0.3</span>, <span style=color:#ae81ff>0.6</span>], [<span style=color:#ae81ff>0.3</span>, <span style=color:#ae81ff>0.2</span>, <span style=color:#ae81ff>0.5</span>]])
y <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>], dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;int32&#39;</span>)
nd<span style=color:#f92672>.</span>pick(y_hat, y)
<span style=color:#75715e># 结果: [0.1, 0.5]</span>

<span style=color:#75715e># 应用的实例：交叉熵的实现</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>cross_entropy</span>(y_hat, y):
    <span style=color:#66d9ef>return</span> <span style=color:#f92672>-</span>nd<span style=color:#f92672>.</span>pick(y_hat, y)<span style=color:#f92672>.</span>log()
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>样例</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型基类-Block</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> Block, nn
<span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> ndarray <span style=color:#66d9ef>as</span> F

<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Model</span>(Block):
	<span style=color:#66d9ef>def</span> __init__(self, <span style=color:#f92672>**</span>kwargs):
		super(Model, self)<span style=color:#f92672>.</span>__init__(<span style=color:#f92672>**</span>kwargs)
		<span style=color:#75715e># use name_scope to give child Blocks appropriate names.</span>
		<span style=color:#66d9ef>with</span> self<span style=color:#f92672>.</span>name_scope():
			self<span style=color:#f92672>.</span>dense0 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>20</span>)
			self<span style=color:#f92672>.</span>dense1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>20</span>)

	<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
		x <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>dense0(x))
		<span style=color:#66d9ef>return</span> F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>dense1(x))

model <span style=color:#f92672>=</span> Model()
model<span style=color:#f92672>.</span>initialize(ctx<span style=color:#f92672>=</span>mx<span style=color:#f92672>.</span>cpu(<span style=color:#ae81ff>0</span>))
model(F<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>10</span>), ctx<span style=color:#f92672>=</span>mx<span style=color:#f92672>.</span>cpu(<span style=color:#ae81ff>0</span>)))

</code></pre></div><p>class Block(builtins.object)<br>网络的最基础的类，搭建网络时必须继承此Block类<br>—————————————————<br>Block的两个参数：<br></p><ul><li><code>prefix</code> : str; 前缀的作用就像一个命名空间。在父模块的作用域下创建的子模块都有父模块的前缀(prefix).</li><li><code>params</code> : ParameterDict or None; 共享参数。<br>　　例如：dense1共享dense0的参数。<br>　　　　dense0 = nn.Dense(20)<br>　　　　dense1 = nn.Dense(20, params=dense0.collect_params())</li></ul><p>—————————————————<br>Block的方法：</p><ul><li><code>collect_params</code>(self, select=None) 返回一个<code>ParameterDict</code>类。默认包含所有的参数；同时也可以正则匹配:<br>例如：选出特定的参数 [&lsquo;conv1_weight&rsquo;, &lsquo;conv1_bias&rsquo;, &lsquo;fc_weight&rsquo;, &lsquo;fc_bias&rsquo;]<br>即：model.collect_params(&lsquo;conv1_weight|conv1_bias|fc_weight|fc_bias&rsquo;)<br>　　<code>Parameters</code>：空 或者 正则表达式<br>　　<code>Returns</code>: py:class:ParameterDict</li><li><code>forward</code>(self, *args) 完成前向计算，输入是NDArray列表<br>　　Parameters：*args : list of NDArray</li><li><code>hybridize</code>(self, active=True, **kwargs) 激活/不激活HybridBlock的递归<br>　　Parameters： bool, default True</li><li><code>initialize</code>(self, init=&lt;mxnet.initializer.Uniform object>, ctx=None, verbose=False, force_reinit=False)<br>　　对模型的参数初始化，默认是均匀分布。<br>　　等价于：block.collect_params().initialize(&mldr;)<br>　　Parameters：<br>　　　　init : Initializer 初始化方法<br>　　　　ctx : 设备 或者 设备列表。会把模型copy到所有指定的设备上<br>　　　　verbose : bool, default False 是否在初始化时粗略地打印细节。<br>　　　　force_reinit : bool, default False 是否重新初始化，即使已经初始化</li><li><code>load_parameters</code>(self, filename, ctx=None, allow_missing=False, ignore_extra=False, cast_dtype=False, dtype_source=&lsquo;current&rsquo;)<br>　　加载模型参数从 用<code>save_parameters</code>保存的模型文件中。<br>　　Parameters：<br>　　　　filename : str 模型文件路径<br>　　　　ctx : 设备 或者设备列表。默认使用CPU<br>　　　　allow_missing : bool, default False 是否默默跳过模型文件中不存<br>　　　　　　在的模型参数。<br>　　　　ignore_extra : bool, default False 是否默默忽略模型中不存在的参<br>　　　　　　数(模型文件中有，模型定义中没有)<br>　　　　cast_dtype : bool, default False 从checkpointload模型时，是否根<br>　　　　　　据传入转换NDArray的数据类型<br>　　　　dtype_source : str, default &lsquo;current&rsquo; 枚举值：{&lsquo;current&rsquo;, &lsquo;saved&rsquo;}<br>　　　　　　只有再cast_dtype=True时有效，指定模型参数的数据类型</li><li><code>name_scope</code>(self) 返回一个命名空间，用来管理Block和参数names。<br>　　必须在with语句中使用：<br>　　with self.name_scope():<br>　　　　self.dense = nn.Dense(20)</li><li><code>register_child</code>(self, block, name=None) 将block注册为子节点，block的属<br>　　性将自动注册。</li><li><code>save_parameters</code>(self, filename) 保持模型参数到磁盘。该方法只保存模型<br>　　参数的权重，不保存模型的结构。如果想要保存模型的结构，请使<br>　　用:py:meth:<code>HybridBlock.export</code>.<br>　　Parameters：Path to file.</li><li><code>summary</code>(self, *inputs) 打印模型的输出和参数的摘要。模型必须被初始化</li></ul><p>—————————————————<br>数据描述：</p><ul><li><code>name</code> :py:class:Block 的名字</li><li><code>params</code>：返回一个参数字典（不包含子节点的参数）</li><li><code>prefix</code>：返回py:class:Block的前缀</li></ul></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型参数-Parameter</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>ctx <span style=color:#f92672>=</span> mx<span style=color:#f92672>.</span>gpu(<span style=color:#ae81ff>0</span>)
x <span style=color:#f92672>=</span> mx<span style=color:#f92672>.</span>nd<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>100</span>), ctx<span style=color:#f92672>=</span>ctx)
w <span style=color:#f92672>=</span> mx<span style=color:#f92672>.</span>gluon<span style=color:#f92672>.</span>Parameter(<span style=color:#e6db74>&#39;fc_weight&#39;</span>, shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>100</span>), init<span style=color:#f92672>=</span>mx<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>Xavier())
b <span style=color:#f92672>=</span> mx<span style=color:#f92672>.</span>gluon<span style=color:#f92672>.</span>Parameter(<span style=color:#e6db74>&#39;fc_bias&#39;</span>, shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>64</span>,), init<span style=color:#f92672>=</span>mx<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>Zero())
w<span style=color:#f92672>.</span>initialize(ctx<span style=color:#f92672>=</span>ctx)
b<span style=color:#f92672>.</span>initialize(ctx<span style=color:#f92672>=</span>ctx)
out <span style=color:#f92672>=</span> mx<span style=color:#f92672>.</span>nd<span style=color:#f92672>.</span>FullyConnected(x, w<span style=color:#f92672>.</span>data(ctx), b<span style=color:#f92672>.</span>data(ctx), num_hidden<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>)
</code></pre></div><p>class:Parameter 一个存放Blocks的参数的权重的容器。初始化后<code>Parameter.initialize(...)</code>，会copy所有参数权重到每个设备上。如果<code>grad_req</code>不为null，在每个设备上，该容器会拥有一个梯度向量。</p><p>Parameter(name,<br>　　　　　grad_req=&lsquo;write&rsquo;,<br>　　　　　shape=None,<br>　　　　　dtype=&lt;class &lsquo;numpy.float32&rsquo;>,<br>　　　　　lr_mult=1.0,<br>　　　　　wd_mult=1.0,<br>　　　　　init=None,<br>　　　　　allow_deferred_init=False,<br>　　　　　differentiable=True,<br>　　　　　stype=&lsquo;default&rsquo;,<br>　　　　　grad_stype=&lsquo;default&rsquo;)</p><p>形参：<br>——————————</p><ul><li><code>name</code> :
str类型；参数的名字。</li><li><code>grad_req</code> : 枚举值：{&lsquo;write&rsquo;, &lsquo;add&rsquo;, &lsquo;null&rsquo;}, 默认值：&lsquo;write&rsquo;。指定怎么更新梯度到梯度向量。<br>　　<code>'write'</code>:每次把梯度值写到 梯度向量中<br>　　<code>'add'</code>: 每次把计算的梯度值add到梯度向量中. 在每次迭代之前，<br>　　　　你需要手动调用<code>zero_grad()</code>来清理梯度缓存。<br>　　<code>'null'</code>: 参数不需要计算梯度，不会分配梯度向量。</li><li><code>shape</code> : int or tuple of int, default None. 参数的尺寸.</li><li><code>dtype</code> : numpy.dtype or str, default &lsquo;float32&rsquo;. 参数的数据类型</li><li><code>lr_mult</code> : float, default 1.0, 学习率.</li><li><code>wd_mult</code> : float, default 1.0, 权重衰减率 L2</li><li><code>init</code> : Initializer, default None. 参数的初始化，默认全局初始化</li><li><code>stype</code>: 枚举值: {&lsquo;default&rsquo;, &lsquo;row_sparse&rsquo;, &lsquo;csr&rsquo;}, defaults to &lsquo;default&rsquo;. 参数的存储类型。</li><li><code>grad_stype</code>: 枚举值: {&lsquo;default&rsquo;, &lsquo;row_sparse&rsquo;, &lsquo;csr&rsquo;}, defaults to &lsquo;default&rsquo;. 参数梯度的存储类型</li></ul><p>属性:<br>——————————</p><ul><li><code>grad_req</code> : 枚举值:{&lsquo;write&rsquo;, &lsquo;add&rsquo;, &lsquo;null&rsquo;} 可以在初始化之前/之后设置。当不需要计算参数的梯度时，设置为<code>null</code>，以节省内存和计算量。</li><li><code>lr_mult</code> : float 学习率</li><li><code>wd_mult</code> : float 权重衰减率</li></ul><p>定义的函数：<br>——————————</p><ul><li><p><code>cast(self, dtype)</code> 转换参数的值/梯度的数据类型。<br>　　dtype : str or numpy.dtype 新的数据类型</p></li><li><p><code>data(self, ctx=None)</code> 获取这个参数在设备<code>ctx</code>上的值，参数必须已经初始化了。<br>　　ctx : 指定设备<br>　　Returns：NDArray on ctx</p></li><li><p><code>grad(self, ctx=None)</code> 获取这个参数在设备<code>ctx</code>上的梯度值。<br>　　ctx : 指定设备</p></li><li><p><code>initialize</code>(self, init=None, ctx=None,<br>　　default_init=&lt;mxnet.initializer.Uniform>,<br>　　force_reinit=False) 初始化参数和梯度向量<br>　　<code>init</code> : Initializer 初始化参数的值<br>　　<code>ctx</code> : 设备/设备列表, 默认使用:py:meth:<code>context.current_context()</code>.<br>　　<code>default_init</code> : Initializer 当:py:func:<code>init</code>和:py:meth:<code>Parameter.init</code>都为none时，使用该默认的初始化.<br>　　<code>force_reinit</code> : bool, default False 当参数已经被初始化，是否再次初始化。</p></li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>weight <span style=color:#f92672>=</span> mx<span style=color:#f92672>.</span>gluon<span style=color:#f92672>.</span>Parameter(<span style=color:#e6db74>&#39;weight&#39;</span>, shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>))
weight<span style=color:#f92672>.</span>initialize(ctx<span style=color:#f92672>=</span>mx<span style=color:#f92672>.</span>cpu(<span style=color:#ae81ff>0</span>))
weight<span style=color:#f92672>.</span>data()
<span style=color:#75715e>#　　[[-0.01068833  0.01729892]</span>
<span style=color:#75715e>#　　 [ 0.02042518 -0.01618656]]</span>
<span style=color:#75715e>#　　&lt;NDArray 2x2 @cpu(0)&gt;</span>
weight<span style=color:#f92672>.</span>grad()
<span style=color:#75715e>#　　[[ 0.  0.]</span>
<span style=color:#75715e>#　　 [ 0.  0.]]</span>
<span style=color:#75715e>#　　&lt;NDArray 2x2 @cpu(0)&gt;</span>
weight<span style=color:#f92672>.</span>initialize(ctx<span style=color:#f92672>=</span>[mx<span style=color:#f92672>.</span>gpu(<span style=color:#ae81ff>0</span>), mx<span style=color:#f92672>.</span>gpu(<span style=color:#ae81ff>1</span>)])
weight<span style=color:#f92672>.</span>data(mx<span style=color:#f92672>.</span>gpu(<span style=color:#ae81ff>0</span>))
<span style=color:#75715e>#　　[[-0.00873779 -0.02834515]</span>
<span style=color:#75715e>#　　 [ 0.05484822 -0.06206018]]</span>
<span style=color:#75715e>#　　&lt;NDArray 2x2 @gpu(0)&gt;</span>
weight<span style=color:#f92672>.</span>data(mx<span style=color:#f92672>.</span>gpu(<span style=color:#ae81ff>1</span>))
<span style=color:#75715e>#　　[[-0.00873779 -0.02834515]</span>
<span style=color:#75715e>#　　 [ 0.05484822 -0.06206018]]</span>
<span style=color:#75715e>#　　&lt;NDArray 2x2 @gpu(1)&gt;</span>
</code></pre></div><ul><li><code>list_ctx(self)</code> 返回参数初始化在那些设备上</li><li><code>list_data(self)</code> 按照顺序返回所有设备上的参数值
　　Returns: list of NDArrays</li><li><code>list_grad(self)</code> 按照顺序返回所有设备上的梯度值</li><li><code>list_row_sparse_data(self, row_id)</code> 按照顺序返回所有设备上的 <code>行稀疏</code>的参数。<br>　　row_id: 指定看哪一行的数据<br>　　Returns: list of NDArrays</li><li><code>reset_ctx(self, ctx)</code> 重新设定设备，把参数copy到该设备上<br>　　ctx : Context or list of Context, default <code>context.current_context()</code></li><li><code>row_sparse_data(self, row_id)</code><br>　　row_id: NDArray 指定看哪一行的数据<br>　　Returns: NDArray on row_id&rsquo;s context</li><li><code>set_data(self, data)</code> 在所有设备上，设置该参数的值。</li><li><code>var(self)</code> 返回一个代表该参数的符号</li><li><code>zero_grad(self)</code> 将所有设备上的梯度缓存清零</li></ul><p>数据描述:<br>——————————</p><ul><li><code>dtype</code> 参数的数据类型</li><li><code>grad_req</code></li><li><code>shape</code> 参数的尺寸</li></ul></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型参数-访问</span></h5><div class=card><div class=card-body><p><code>ToTensor</code>：将图像数据从uint8格式变换成32位浮点数格式，并除以255使得所有像素的数值均在0到1之间</p><code>transform_first函数</code>：数据集的函数。将<code>ToTensor</code>的变换应用在每个数据样本（图像和标签）的第一个元素，即图像之上.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
<span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> nn

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>网络设计</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
<span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> nn
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型初始化</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
<span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> nn
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型初始化</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
<span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> nn
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>实例-单层感知机</span></h5><div class=card><div class=card-body><p><img src=/datasets/posts/dp_summary/single_perceptron.jpg alt=单层感知机></p><p>模型：o = w<sub>1</sub>*x<sub>1</sub> + w<sub>2</sub>*x<sub>2</sub> + b</p></p><p>输出<code>o</code>作为线性回归的输出，输入层是2维特征；输入层不涉及计算，该神经网络只有输出层1层。</p><code>神经元</code>：输出层中负责计算o的单元。</p>该神经元，依赖于输入层的全部特征，也就是说输出层中的神经元和输入层中各个输入完全连接，所以，这里的输出层又叫作<code>全连接层(fully connected layer)</code>或者<code>稠密层(dense layer)</code></p></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>生成数据集</span></h5><div class=card><div class=card-body><p>目标： o = 2<em>x<sub>1</sub> - 3.4</em>x<sub>2</sub> + 4.2 其中：
样本集：features: [w<sub>1</sub>, w<sub>2</sub>]， labels: [真实值+噪声]</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> IPython <span style=color:#f92672>import</span> display
<span style=color:#f92672>from</span> matplotlib <span style=color:#f92672>import</span> pyplot <span style=color:#66d9ef>as</span> plt
<span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> autograd, nd
<span style=color:#f92672>import</span> random

num_inputs <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>
num_examples <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span>
true_w <span style=color:#f92672>=</span> [<span style=color:#ae81ff>2</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>3.4</span>]
true_b <span style=color:#f92672>=</span> <span style=color:#ae81ff>4.2</span>
features <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(scale<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, shape<span style=color:#f92672>=</span>(num_examples, num_inputs))
labels <span style=color:#f92672>=</span> true_w[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>*</span> features[:, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>+</span> true_w[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>*</span> features[:, <span style=color:#ae81ff>1</span>] <span style=color:#f92672>+</span> true_b
labels <span style=color:#f92672>+=</span> nd<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(scale<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, shape<span style=color:#f92672>=</span>labels<span style=color:#f92672>.</span>shape)

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>读取数据集 - 从零实现</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>data_iter</span>(batch_size, features, labels):
    num_examples <span style=color:#f92672>=</span> len(features)
    indices <span style=color:#f92672>=</span> list(range(num_examples))
    random<span style=color:#f92672>.</span>shuffle(indices)  <span style=color:#75715e># 样本的读取顺序是随机的</span>
    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, num_examples, batch_size):
        j <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>array(indices[i: min(i <span style=color:#f92672>+</span> batch_size, num_examples)])
        <span style=color:#75715e># take函数根据索引返回对应元素</span>
        <span style=color:#66d9ef>yield</span> features<span style=color:#f92672>.</span>take(j), labels<span style=color:#f92672>.</span>take(j)  

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>读取数据集 - Gluon实现</span></h5><div class=card><div class=card-body><p><code>DataLoader</code> 返回一个迭代器，一次返回batch_size个样本</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> data <span style=color:#66d9ef>as</span> gdata

batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
<span style=color:#75715e># 将训练数据的特征和标签组合</span>
dataset <span style=color:#f92672>=</span> gdata<span style=color:#f92672>.</span>ArrayDataset(features, labels)
<span style=color:#75715e># 随机读取小批量</span>
data_iter <span style=color:#f92672>=</span> gdata<span style=color:#f92672>.</span>DataLoader(dataset, batch_size, shuffle<span style=color:#f92672>=</span>True)

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型定义 - 从零实现</span></h5><div class=card><div class=card-body><p>手动定义模型参数，一定要开辟存储梯度的内存。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 将权重初始化为：均值为0、标准差为0.01的正太随机数</span>
w <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(scale<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, shape<span style=color:#f92672>=</span>(num_inputs, <span style=color:#ae81ff>1</span>))
b <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>zeros(shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>,))
<span style=color:#75715e># 开辟存储梯度的内存</span>
w<span style=color:#f92672>.</span>attach_grad()
b<span style=color:#f92672>.</span>attach_grad()
<span style=color:#75715e># 定义线性回归的模型</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>linreg</span>(X, w, b): 
    <span style=color:#66d9ef>return</span> nd<span style=color:#f92672>.</span>dot(X, w) <span style=color:#f92672>+</span> b



</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型定义 - Gluon实现</span></h5><div class=card><div class=card-body><p><code>Gluon</code>模块：提供了大量预定义的层。<code>nn</code>模块(neural networks)的缩写，所以里面定义了大量神经网络 的层。
<code>Sequential</code>：可以看作是一个 串联各个层的容器，在构建模型时，在该容器中一次添加层。当给定输入数据时，
容器中的每一层的输出作为下一层的输入。</p><code>init</code>模块：initializer的缩写。该模块提供了模型参数初始化的各种方法。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> nn
<span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> init
<span style=color:#75715e># 定义模型</span>
net <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential()
net<span style=color:#f92672>.</span>add(nn<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>1</span>))
<span style=color:#75715e># 初始化模型参数</span>
net<span style=color:#f92672>.</span>initialize(init<span style=color:#f92672>.</span>Normal(sigma<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>))
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>定义损失函数 - 从零实现</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>squared_loss</span>(y_hat, y):
    <span style=color:#66d9ef>return</span> (y_hat <span style=color:#f92672>-</span> y<span style=color:#f92672>.</span>reshape(y_hat<span style=color:#f92672>.</span>shape)) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>/</span> <span style=color:#ae81ff>2</span>
loss <span style=color:#f92672>=</span> squared_loss
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>定义损失函数 - Gluon实现</span></h5><div class=card><div class=card-body><p><code>Gluon</code>中的<code>loss</code>模块：定义了各种损失函数。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> loss <span style=color:#66d9ef>as</span> gloss
loss <span style=color:#f92672>=</span> gloss<span style=color:#f92672>.</span>L2Loss()  <span style=color:#75715e># 平方损失又称L2范数损失</span>
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>定义优化算法 - 从零实现</span></h5><div class=card><div class=card-body><p><code>param.grad</code>自动求梯度模块计算得来的梯度是一个批量样本的梯度和。在迭代模型参数时，需要除以批量大小来得到平均值。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sgd</span>(params, lr, batch_size):
    <span style=color:#66d9ef>for</span> param <span style=color:#f92672>in</span> params:
        param[:] <span style=color:#f92672>=</span> param <span style=color:#f92672>-</span> lr <span style=color:#f92672>*</span> param<span style=color:#f92672>.</span>grad <span style=color:#f92672>/</span> batch_size

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>定义优化算法 - Gluon实现</span></h5><div class=card><div class=card-body><p><code>Gluon</code>模块中的<code>Trainer</code>类，用来迭代模型中的全部参数。这些参数可以通过<code>collect_params</code>函数获取。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> Trainer
trainer <span style=color:#f92672>=</span> Trainer(net<span style=color:#f92672>.</span>collect_params(), <span style=color:#e6db74>&#39;sgd&#39;</span>, {<span style=color:#e6db74>&#39;learning_rate&#39;</span>: <span style=color:#ae81ff>0.03</span>})
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>训练模型 - 从零实现</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>lr <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.03</span>
num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
net <span style=color:#f92672>=</span> linreg
<span style=color:#75715e># 训练模型一共需要num_epochs个迭代周期</span>
<span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(num_epochs): 
    <span style=color:#75715e># 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）</span>
    <span style=color:#75715e># x和y分别是小批量样本的特征和标签</span>
    <span style=color:#66d9ef>for</span> X, y <span style=color:#f92672>in</span> data_iter(batch_size, features, labels):
        <span style=color:#66d9ef>with</span> autograd<span style=color:#f92672>.</span>record():
            l <span style=color:#f92672>=</span> loss(net(X, w, b), y)  <span style=color:#75715e># l是有关小批量X和y的损失</span>
        l<span style=color:#f92672>.</span>backward()  <span style=color:#75715e># 小批量的损失对模型参数求梯度</span>
        sgd([w, b], lr, batch_size)  <span style=color:#75715e># 使用小批量随机梯度下降迭代模型参数</span>
    train_l <span style=color:#f92672>=</span> loss(net(features, w, b), labels)
    <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;epoch </span><span style=color:#e6db74>%d</span><span style=color:#e6db74>, loss </span><span style=color:#e6db74>%f</span><span style=color:#e6db74>&#39;</span> <span style=color:#f92672>%</span> (epoch <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>, train_l<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>asnumpy()))

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>训练模型 - Gluon实现</span></h5><div class=card><div class=card-body><p>通过<code>Trainer</code>实例的<code>step</code>函数来迭代模型参数。由于loss是长度为batch_size的向量，在执行l.backward()时，
等价于执行l.sum().backward()。所以要用batch_size做平均。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
<span style=color:#75715e># 训练模型一共需要num_epochs个迭代周期</span>
<span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>, num_epochs <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>):
    <span style=color:#75715e># 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）</span>
    <span style=color:#75715e># x和y分别是小批量样本的特征和标签</span>
    <span style=color:#66d9ef>for</span> X, y <span style=color:#f92672>in</span> data_iter:
        <span style=color:#66d9ef>with</span> autograd<span style=color:#f92672>.</span>record():
            l <span style=color:#f92672>=</span> loss(net(X), y)  <span style=color:#75715e># l是有关小批量X和y的损失</span>
        l<span style=color:#f92672>.</span>backward() <span style=color:#75715e># 等价于l.sum().backward()</span>
        trainer<span style=color:#f92672>.</span>step(batch_size)  <span style=color:#75715e># 指定batch_size，从而对批量样本梯度求平均</span>
    l <span style=color:#f92672>=</span> loss(net(features), labels)
    <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;epoch </span><span style=color:#e6db74>%d</span><span style=color:#e6db74>, loss: </span><span style=color:#e6db74>%f</span><span style=color:#e6db74>&#39;</span> <span style=color:#f92672>%</span> (epoch, l<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>asnumpy()))

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>gluon模块-结构</span></h5><div class=card><div class=card-body>路径.mxnet/gluon/下的树状结构:<br>│　　block.py <code>类：Block, HybridBlock</code><br>│　　loss.py <code>各种loss函数</code><br>│　　parameter.py <code>类：Parameter, Constant, ParameterDict</code><br>│　　trainer.py <code>类：Trainer</code><br>│　　utils.py <code>优化操作</code><br>│　　<strong>init</strong>.py<br>│<br>├─contrib<br>│　　│<br>│　　├─cnn<br>│　　│　　└─ conv_layers.py<br>│　　├─data<br>│　　│　　└─ sampler.py<br>│　　│<br>│　　├─estimator<br>│　　│　　│　　estimator.py<br>│　　│　　└─ event_handler.py<br>│　　│<br>│　　├─nn<br>│　　│　　└─ basic_layers.py<br>│　　│<br>│　　└─rnn<br>│　　　　 │　　conv_rnn_cell.py<br>│　　　　 └─ rnn_cell.py<br>│<br>├─data <code>主要是数据处理操作</code><br>│　　│　　dataloader.py <code>类：DataLoader</code><br>│　　│　　dataset.py <code>常用类: ArrayDataset</code><br>│　　│　　sampler.py<br>│　　│<br>│　　└─vision<br>│　　　　 │　　datasets.py <code>可用的数据集-各个类</code><br>│　　　　 └─ transforms.py <code>数据预处理-各个类</code><br>│<br>├─model_zoo<br>│　　│　　model_store.py<br>│　　│<br>│　　└─vision<br>│　　　　 │　　alexnet.py<br>│　　　　 │　　densenet.py<br>│　　　　 │　　inception.py<br>│　　　　 │　　mobilenet.py<br>│　　　　 │　　resnet.py<br>│　　　　 │　　squeezenet.py<br>│　　　　 └─ vgg.py<br>├─nn <code>网络结构</code><br>│　　│　　activations.py <code>定义了各种激活层</code><br>│　　│　　basic_layers.py <code>定义了网络的基础层,例如：BN,Dropout等</code><br>│　　└─ conv_layers.py <code>定义了各种卷积池化层等</code><br>│<br>└─rnn<br>　　 │　　rnn_cell.py<br>　　 └─ rnn_layer.py</div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>gluon模块-导入</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># data</span>
<span style=color:#f92672>from</span> mxnet.gluon.data <span style=color:#f92672>import</span> ArrayDataset, DataLoader
<span style=color:#f92672>from</span> mxnet.gluon.data.vision.transforms <span style=color:#f92672>import</span> ToTensor, Normalize
<span style=color:#75715e># nn</span>
<span style=color:#f92672>from</span> mxnet.gluon.nn <span style=color:#f92672>import</span> Block, HybridBlock, Sequential, HybridSequential, Dropout, BatchNorm, Dense, PReLU, Conv2D
<span style=color:#75715e># 模型参数</span>
<span style=color:#f92672>from</span> mxnet.gluon.parameter <span style=color:#f92672>import</span> Parameter, Constant, ParameterDict
<span style=color:#75715e># 训练</span>
<span style=color:#f92672>from</span> mxnet.gluon.trainer <span style=color:#f92672>import</span> Trainer
<span style=color:#75715e># 损失函数</span>
<span style=color:#f92672>from</span> mxnet.gluon. <span style=color:#f92672>import</span> loss        <span style=color:#75715e># 损失函数 [&#39;Loss&#39;, &#39;L2Loss&#39;, &#39;L1Loss&#39;, &#39;SigmoidBinaryCrossEntropyLoss&#39;, &#39;SigmoidBCELoss&#39;, &#39;SoftmaxCrossEntropyLoss&#39;, &#39;SoftmaxCELoss&#39;, &#39;KLDivLoss&#39;, &#39;CTCLoss&#39;, &#39;HuberLoss&#39;, &#39;HingeLoss&#39;, &#39;SquaredHingeLoss&#39;, &#39;LogisticLoss&#39;, &#39;TripletLoss&#39;, &#39;PoissonNLLLoss&#39;, &#39;CosineEmbeddingLoss&#39;]</span>

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>数据集 - data</span></h5><div class=card><div class=card-body><p><code>ToTensor</code>：将图像数据从uint8格式变换成32位浮点数格式，并除以255使得所有像素的数值均在0到1之间</p><code>transform_first函数</code>：数据集的函数。将<code>ToTensor</code>的变换应用在每个数据样本（图像和标签）的第一个元素，即图像之上.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> data <span style=color:#66d9ef>as</span> gdata
batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>256</span>
transformer <span style=color:#f92672>=</span> gdata<span style=color:#f92672>.</span>vision<span style=color:#f92672>.</span>transforms<span style=color:#f92672>.</span>ToTensor()
<span style=color:#66d9ef>if</span> sys<span style=color:#f92672>.</span>platform<span style=color:#f92672>.</span>startswith(<span style=color:#e6db74>&#39;win&#39;</span>):
    num_workers <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>  <span style=color:#75715e># 0表示不用额外的进程来加速读取数据</span>
<span style=color:#66d9ef>else</span>:
    num_workers <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>

train_iter <span style=color:#f92672>=</span> gdata<span style=color:#f92672>.</span>DataLoader(mnist_train<span style=color:#f92672>.</span>transform_first(transformer),
                              batch_size, 
                              shuffle<span style=color:#f92672>=</span>True,
                              num_workers<span style=color:#f92672>=</span>num_workers)
test_iter <span style=color:#f92672>=</span> gdata<span style=color:#f92672>.</span>DataLoader(mnist_test<span style=color:#f92672>.</span>transform_first(transformer),
                             batch_size, 
                             shuffle<span style=color:#f92672>=</span>False,
                             num_workers<span style=color:#f92672>=</span>num_workers)

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型初始化 - init</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> init
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>损失函数 - loss</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> loss <span style=color:#66d9ef>as</span> gloss
<span style=color:#75715e># 平方损失又称L2范数损失</span>
loss <span style=color:#f92672>=</span> gloss<span style=color:#f92672>.</span>L2Loss()
<span style=color:#75715e># 包含了softmax运算和交叉熵损失运算</span>
loss <span style=color:#f92672>=</span> gloss<span style=color:#f92672>.</span>SoftmaxCrossEntropyLoss()
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>优化算法 - Trainer</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> Trainer
</code></pre></div></div></div></div></div><p><a href=https://blog.algomooc.com/ target=blank>参考</a></p><div class=note-card><div class=item><h5 class=note-title><span>第一周：链表、栈、队列</span></h5><div class=card><div class=card-body><ol><li><input disabled type=checkbox> 链表的基础知识：单链表</li><li><input disabled type=checkbox> 反转链表（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 206</a> ）</li><li><input disabled type=checkbox> 相交链表（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 160</a> ）</li><li><input disabled type=checkbox> 合并两个有序链表 （ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 21</a> ）</li><li><input disabled type=checkbox> 分隔链表 （ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 86</a> ）</li><li><input disabled type=checkbox> 环形链表 II （ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 142</a> ）</li><li><input disabled type=checkbox> 反转链表 II （ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 92</a> ）</li><li><input disabled type=checkbox> 复制带随机指针的链表（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 138</a> ）</li><li><input disabled type=checkbox> 栈的基础知识</li><li><input disabled type=checkbox> 有效的括号（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 20</a> ）</li><li><input disabled type=checkbox> 基本计算器（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 224</a> ）</li><li><input disabled type=checkbox> 最小栈（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 155</a> ）</li><li><input disabled type=checkbox> 验证栈序列（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 946</a> ）</li><li><input disabled type=checkbox> 每日温度（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 739</a> ）</li><li><input disabled type=checkbox> 接雨水（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 42</a> ）</li><li><input disabled type=checkbox> 队列的基础知识</li><li><input disabled type=checkbox> 用栈实现队列 （ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 232</a> ）</li><li><input disabled type=checkbox> 滑动窗口最大值（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 239</a> ）</li><li><input disabled type=checkbox> 设计循环双端队列（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 641</a> ）</li><li><input disabled type=checkbox> 移除链表元素（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 203</a> ）</li><li><input disabled type=checkbox> K 个一组翻转链表（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 25</a> ）</li><li><input disabled type=checkbox> 回文链表（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 234</a> ）</li><li><input disabled type=checkbox> 奇偶链表（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 328</a> ）</li><li><input disabled type=checkbox> 从尾到头打印链表（ 剑指Offer 06</a> ）</li><li><input disabled type=checkbox> 链表中倒数第 k 个节点（ 剑指Offer 22</a> ）</li></ol></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>第二周，递归、排序、贪心</span></h5><div class=card><div class=card-body><ol><li><input disabled type=checkbox> 递归基础知识</li><li><input disabled type=checkbox> 冒泡排序基础知识</li><li><input disabled type=checkbox> 选择排序基础知识</li><li><input disabled type=checkbox> 插入排序基础知识</li><li><input disabled type=checkbox> 快速排序基础知识</li><li><input disabled type=checkbox> 计数排序基础知识</li><li><input disabled type=checkbox> 归并排序</li><li><input disabled type=checkbox> 桶排序（了解即 </a>可）</li><li><input disabled type=checkbox> 堆排序</li><li><input disabled type=checkbox> 基数排序（了解即 </a>可）</li><li><input disabled type=checkbox> 希尔排序（了解即 </a>可）</li><li><input disabled type=checkbox> 合并两个有序数组( <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 88</a> )</li><li><input disabled type=checkbox> 颜色分类( <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 75</a> )</li><li><input disabled type=checkbox> 部分排序 （面试题 16）</li><li><input disabled type=checkbox> 计算右侧小于当前元素的个数 ( <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 315</a> )</li><li><input disabled type=checkbox> 合并 K 个升序链表（<a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 23 </a>）</li><li><input disabled type=checkbox> 有序数组的平方( <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 977</a> )</li><li><input disabled type=checkbox> 盛最多水的容器 ( <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 11</a> )</li><li><input disabled type=checkbox> 两数之和（<a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 1 </a>）</li><li><input disabled type=checkbox> 二叉堆基础知识</li><li><input disabled type=checkbox> 分发饼干（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 455</a> ）</li><li><input disabled type=checkbox> 柠檬水找零（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 860</a> ）</li><li><input disabled type=checkbox> 用最少数量的箭引爆气球（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 452</a> ）</li><li><input disabled type=checkbox> 移掉 K 位数字（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 402</a> ）</li><li><input disabled type=checkbox> 跳跃游戏（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 55</a> ）</li><li><input disabled type=checkbox> 摆动序列（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 376</a> ）</li><li><input disabled type=checkbox> 买卖股票的最佳时机 II（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 122</a> ）</li><li><input disabled type=checkbox> 三数之和（<a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 15 </a>）</li><li><input disabled type=checkbox> 最接近三数之和（<a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 16 </a>）</li><li><input disabled type=checkbox> 加油站（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 134</a> ）</li><li><input disabled type=checkbox> 合并区间（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 56</a> ）</li></ol></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>第三周，搜索算法、回溯算法、位运算、二分查找</span></h5><div class=card><div class=card-body><ol><li><input disabled type=checkbox> 二分查找基础知识</li><li><input disabled type=checkbox> 二分查找（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 704</a> ）</li><li><input disabled type=checkbox> 搜索插入位置（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 35</a> ）</li><li><input disabled type=checkbox> 在排序数组中查找元素的第一个和最后一个位置（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 34</a> ）</li><li><input disabled type=checkbox> 搜索旋转排序数组（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 33</a> ）</li><li><input disabled type=checkbox> 搜索二维矩阵（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 74</a> ）</li><li><input disabled type=checkbox> 寻找两个正序数组的中位数（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 4</a> ）</li><li><input disabled type=checkbox> 有效三角形的个数（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 611</a> ）</li><li><input disabled type=checkbox> 剑指 Offer 53 – II. 0～n-1中缺失的数字</li><li><input disabled type=checkbox> 剑指 Offer 53 – I. 在排序数组中查找数字 I</li><li><input disabled type=checkbox> 剑指 Offer 51. 数组中的逆序对</li><li><input disabled type=checkbox> 寻找峰值（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 162</a> ）</li><li><input disabled type=checkbox> 第一个错误的版本（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 278</a> ）</li><li><input disabled type=checkbox> 山脉数组的峰顶索引（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 852</a> ）</li><li><input disabled type=checkbox> 有效的完全平方数（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 367</a> ）</li><li><input disabled type=checkbox> 位运算基础知识</li><li><input disabled type=checkbox> 丢失的数字（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 268</a> ）</li><li><input disabled type=checkbox> 2 的幂（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 231</a> ）</li><li><input disabled type=checkbox> 比特位计数（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 338</a> ）</li><li><input disabled type=checkbox> 位 1 的个数（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 191</a> ）</li><li><input disabled type=checkbox> 只出现一次的数字 II（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 137</a> ）</li><li><input disabled type=checkbox> 只出现一次的数字 III（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 260</a> ）</li><li><input disabled type=checkbox> 最大单词长度乘积（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 318</a> ）</li><li><input disabled type=checkbox> 汉明距离（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 461</a> ）</li><li><input disabled type=checkbox> 回溯基础知识</li><li><input disabled type=checkbox> 岛屿数量（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 200</a> ）</li><li><input disabled type=checkbox> N 皇后（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 51</a> ）</li><li><input disabled type=checkbox> 子集（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 78</a> ）</li><li><input disabled type=checkbox> 组合总和 II（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 40</a> ）</li><li><input disabled type=checkbox> 括号生成（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 22</a> ）</li><li><input disabled type=checkbox> 火柴拼正方形（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 437</a> ）</li><li><input disabled type=checkbox> 接雨水 II（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 407</a> ）</li><li><input disabled type=checkbox> 组合（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 77</a> ）</li><li><input disabled type=checkbox> 组合总和 II（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 216</a> ）</li><li><input disabled type=checkbox> 分割回文串（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 131</a> ）</li><li><input disabled type=checkbox> 全排列（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 46</a> ）</li></ol></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>第四周，二叉树</span></h5><div class=card><div class=card-body><ol><li><input disabled type=checkbox> 二叉树基础知识</li><li><input disabled type=checkbox> 二叉树的前序遍历（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 144</a> ）</li><li><input disabled type=checkbox> 二叉树的中序遍历（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 94</a> ）</li><li><input disabled type=checkbox> 二叉树的后序遍历（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 145</a> ）</li><li><input disabled type=checkbox> 二叉树的层序遍历（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 102</a> ）</li><li><input disabled type=checkbox> 二叉树的锯齿形层序遍历（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 103</a> ）</li><li><input disabled type=checkbox> 从前序与中序遍历序列构造二叉树（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 105</a> ）</li><li><input disabled type=checkbox> 路径总和 II（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 113</a> ）</li><li><input disabled type=checkbox> 二叉树的最近公共祖先（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 236</a> ）</li><li><input disabled type=checkbox> 二叉树的右视图（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 199</a> ）</li><li><input disabled type=checkbox> 二叉树展开为链表（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 114</a> ）</li><li><input disabled type=checkbox> 将有序数组转换为二叉搜索树（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 108</a> ）</li><li><input disabled type=checkbox> 把二叉搜索树转换为累加树（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 538</a> ）</li><li><input disabled type=checkbox> 删除二叉搜索树中的节点（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 450</a> ）</li><li><input disabled type=checkbox> 二叉树的序列化与反序列化（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 297</a> ）</li><li><input disabled type=checkbox> 完全二叉树的节点个数（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 222</a> ）</li><li><input disabled type=checkbox> 二叉树的最大深度（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 104</a> ）</li><li><input disabled type=checkbox> 二叉树的最小深度（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 111</a> ）</li><li><input disabled type=checkbox> 二叉树的所有路径（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 257</a> ）</li><li><input disabled type=checkbox> 平衡二叉树（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 110</a> ）</li><li><input disabled type=checkbox> 左叶子之和（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 404</a> ）</li><li><input disabled type=checkbox> 找树左下角的值（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 513</a> ）</li><li><input disabled type=checkbox> 修剪二叉搜索树（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 669</a> ）</li><li><input disabled type=checkbox> 二叉搜索树的最近公共祖先（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 235</a> ）</li><li><input disabled type=checkbox> 二叉搜索树的最小绝对差（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 530</a> ）</li><li><input disabled type=checkbox> 最大二叉树（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 654</a> ）</li></ol></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>第五周，动态规划、背包问题</span></h5><div class=card><div class=card-body><ol><li><input disabled type=checkbox> 动态规划基础知识和解题步骤</li><li><input disabled type=checkbox> 爬楼梯（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 70</a> ）</li><li><input disabled type=checkbox> 斐波那契数（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 509</a> ）</li><li><input disabled type=checkbox> 最大子序和（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 53</a> ）</li><li><input disabled type=checkbox> 零钱兑换（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 322</a> ）</li><li><input disabled type=checkbox> 零钱兑换 II（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 518</a> ）</li><li><input disabled type=checkbox> 最小路径和（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 64</a> ）</li><li><input disabled type=checkbox> 编辑距离（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 72</a> ）</li><li><input disabled type=checkbox> 买卖股票的最佳时机（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 121</a> ）</li><li><input disabled type=checkbox> 买卖股票的最佳时机II（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 122</a> ）</li><li><input disabled type=checkbox> 买卖股票的最佳时机III（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 123</a> ）</li><li><input disabled type=checkbox> 买卖股票的最佳时机IV（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 188</a> ）</li><li><input disabled type=checkbox> 最佳买卖股票时机含冷冻期(<a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 309</a> )</li><li><input disabled type=checkbox> 买卖股票的最佳时机含手续费(<a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 714</a> )</li><li><input disabled type=checkbox> 完全平方数（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 279</a> ）</li><li><input disabled type=checkbox> 三角形最小路径和（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 120</a> ）</li><li><input disabled type=checkbox> 不同路径（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 62</a> ）</li><li><input disabled type=checkbox> 不同路径II（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 63</a> ）</li><li><input disabled type=checkbox> 整数拆分（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 343</a> ）</li><li><input disabled type=checkbox> 不同的二叉搜索树（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 96</a> ）</li><li><input disabled type=checkbox> 地下城游戏（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 174</a> ）</li><li><input disabled type=checkbox> 打家劫舍（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 198</a> ）</li><li><input disabled type=checkbox> 打家劫舍II（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 213</a> ）</li><li><input disabled type=checkbox> 打家劫舍III（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 337</a> ）</li><li><input disabled type=checkbox> 最长递增子序列（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 300</a> ）</li><li><input disabled type=checkbox> 最长连续递增序列（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 674</a> ）</li><li><input disabled type=checkbox> 分割等和子集（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 416</a> ）</li><li><input disabled type=checkbox> 最长重复子数组（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 718</a> ）</li><li><input disabled type=checkbox> 最长公共子序列（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 1143</a> ）</li><li><input disabled type=checkbox> 最长回文子序列（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 516</a> ）</li><li><input disabled type=checkbox> 最长回文子串（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 5</a> ）</li><li><input disabled type=checkbox> 01 背包问题</li><li><input disabled type=checkbox> 目标和（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 494</a> ）</li><li><input disabled type=checkbox> 最后一块石头的重量 II（ <a href=https://leetcode.cn/problemset/all/ target=blank>LeetCode 1049</a> ）</li></ol></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>第六周，剑指offer</span></h5><div class=card><div class=card-body><ul><li><input disabled type=checkbox> 剑指 Offer 03. 数组中重复的数字</li><li><input disabled type=checkbox> 剑指 Offer 04. 二维数组中的查找</li><li><input disabled type=checkbox> 剑指 Offer 05. 替换空格</li><li><input disabled type=checkbox> 剑指 Offer 06. 从尾到头打印链表</li><li><input disabled type=checkbox> 剑指 Offer 09. 用两个栈实现队列</li><li><input disabled type=checkbox> 剑指 Offer 11. 旋转数组的最小数字</li><li><input disabled type=checkbox> 剑指 Offer 12. 矩阵中的路径</li><li><input disabled type=checkbox> 剑指 Offer 18. 删除链表的节点</li><li><input disabled type=checkbox> 剑指 Offer 21. 调整数组顺序使奇数位于偶数前面</li><li><input disabled type=checkbox> 剑指 Offer 22. 链表中倒数第k个节点</li><li><input disabled type=checkbox> 剑指 Offer 24. 反转链表</li><li><input disabled type=checkbox> 剑指 Offer 25. 合并两个排序的链表</li><li><input disabled type=checkbox> 剑指 Offer 26. 树的子结构</li><li><input disabled type=checkbox> 剑指 Offer 30. 包含min函数的栈</li><li><input disabled type=checkbox> 剑指 Offer 32 - I. 从上到下打印二叉树</li><li><input disabled type=checkbox> 剑指 Offer 32 - II. 从上到下打印二叉树 II</li><li><input disabled type=checkbox> 剑指 Offer 32 - III. 从上到下打印二叉树 III</li><li><input disabled type=checkbox> 剑指 Offer 33. 二叉搜索树的后序遍历序列</li><li><input disabled type=checkbox> 剑指 Offer 41. 数据流中的中位数</li><li><input disabled type=checkbox> 剑指 Offer 42. 连续子数组的最大和</li><li><input disabled type=checkbox> 剑指 Offer 45. 把数组排成最小的数</li><li><input disabled type=checkbox> 剑指 Offer 46. 把数字翻译成字符串</li><li><input disabled type=checkbox> 剑指 Offer 47. 礼物的最大价值</li><li><input disabled type=checkbox> 剑指 Offer 50. 第一个只出现一次的字符</li><li><input disabled type=checkbox> 剑指 Offer 51. 数组中的逆序对</li><li><input disabled type=checkbox> 剑指 Offer 52. 两个链表的第一个公共节点</li><li><input disabled type=checkbox> 剑指 Offer 53 - I. 在排序数组中查找数字 I</li><li><input disabled type=checkbox> 剑指 Offer 53 - II. 0～n-1中缺失的数字</li><li><input disabled type=checkbox> 剑指 Offer 54. 二叉搜索树的第k大节点</li><li><input disabled type=checkbox> 剑指 Offer 55 - I. 二叉树的深度</li><li><input disabled type=checkbox> 剑指 Offer 57. 和为s的两个数字</li><li><input disabled type=checkbox> 剑指 Offer 58 - II. 左旋转字符串</li><li><input disabled type=checkbox> 剑指 Offer 61. 扑克牌中的顺子</li><li><input disabled type=checkbox> 剑指 Offer 66. 构建乘积数组</li></ul></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>Depth First Search(DFS)遍历</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># -*- coding: utf-8 -*-</span>
<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TreeNode</span>:
    <span style=color:#66d9ef>def</span> __init__(self, value):
        self<span style=color:#f92672>.</span>value <span style=color:#f92672>=</span> value
        self<span style=color:#f92672>.</span>left <span style=color:#f92672>=</span> None
        self<span style=color:#f92672>.</span>right <span style=color:#f92672>=</span> None

<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Tree_Method</span>:

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>DFS</span>(self, root):
        <span style=color:#e6db74>&#39;&#39;&#39;
</span><span style=color:#e6db74>        深度优先遍历，即先访问根节点，然后遍历左子树接着遍历右子树。
</span><span style=color:#e6db74>        主要利用栈的特点，先将右子树压栈，再将左子树压栈，这样左子树就位于栈顶，
</span><span style=color:#e6db74>        可以结点的左子树先与右子树被遍历。
</span><span style=color:#e6db74>        &#39;&#39;&#39;</span>
 
        <span style=color:#66d9ef>if</span> root <span style=color:#f92672>==</span> None:
            <span style=color:#66d9ef>return</span> None
        stack <span style=color:#f92672>=</span> []
        <span style=color:#e6db74>&#39;&#39;&#39;用列表模仿入栈&#39;&#39;&#39;</span>
        stack<span style=color:#f92672>.</span>append(root)          
        <span style=color:#66d9ef>while</span> stack:
            <span style=color:#e6db74>&#39;&#39;&#39;将栈顶元素出栈&#39;&#39;&#39;</span>
            current_node <span style=color:#f92672>=</span> stack<span style=color:#f92672>.</span>pop()
            <span style=color:#66d9ef>print</span>(current_node<span style=color:#f92672>.</span>value, end<span style=color:#f92672>=</span><span style=color:#e6db74>&#39; &#39;</span>)
            <span style=color:#e6db74>&#39;&#39;&#39;判断该节点是否有右孩子，有就入栈&#39;&#39;&#39;</span>
            <span style=color:#66d9ef>if</span> current_node<span style=color:#f92672>.</span>right:
                stack<span style=color:#f92672>.</span>append(current_node<span style=color:#f92672>.</span>right)
            <span style=color:#e6db74>&#39;&#39;&#39;判断该节点是否有左孩子，有就入栈&#39;&#39;&#39;</span>
            <span style=color:#66d9ef>if</span> current_node<span style=color:#f92672>.</span>left:

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>preOrder</span>(self, root):
        <span style=color:#e6db74>&#39;&#39;&#39;先序遍历&#39;&#39;&#39;</span>
        <span style=color:#66d9ef>if</span> root <span style=color:#f92672>==</span> None:
            <span style=color:#66d9ef>return</span> None
        <span style=color:#66d9ef>print</span>(root<span style=color:#f92672>.</span>value)
        self<span style=color:#f92672>.</span>preOrder(root<span style=color:#f92672>.</span>left)
        self<span style=color:#f92672>.</span>preOrder(root<span style=color:#f92672>.</span>right)
 
    <span style=color:#75715e># 先序打印二叉树（非递归）</span>
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>preOrderTravese</span>(node):
        stack <span style=color:#f92672>=</span> [node]
        <span style=color:#66d9ef>while</span> len(stack) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>:
            <span style=color:#66d9ef>print</span>(node<span style=color:#f92672>.</span>val)
            <span style=color:#66d9ef>if</span> node<span style=color:#f92672>.</span>right <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> None:
                stack<span style=color:#f92672>.</span>append(node<span style=color:#f92672>.</span>right)
            <span style=color:#66d9ef>if</span> node<span style=color:#f92672>.</span>left <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> None:
                stack<span style=color:#f92672>.</span>append(node<span style=color:#f92672>.</span>left)
            node <span style=color:#f92672>=</span> stack<span style=color:#f92672>.</span>pop()
 
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>minOrder</span>(self, root):
        <span style=color:#e6db74>&#39;&#39;&#39;中序遍历&#39;&#39;&#39;</span>
        <span style=color:#66d9ef>if</span> root <span style=color:#f92672>==</span> None:
            <span style=color:#66d9ef>return</span> None
        self<span style=color:#f92672>.</span>minOrder(root<span style=color:#f92672>.</span>left)
        <span style=color:#66d9ef>print</span>(root<span style=color:#f92672>.</span>value)
        self<span style=color:#f92672>.</span>minOrder(root<span style=color:#f92672>.</span>right)
 
    <span style=color:#75715e># 中序打印二叉树（非递归）</span>
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>inOrderTraverse</span>(node):
        stack <span style=color:#f92672>=</span> []
        pos <span style=color:#f92672>=</span> node
        <span style=color:#66d9ef>while</span> pos <span style=color:#f92672>or</span> stack:
            <span style=color:#66d9ef>if</span> pos:
                stack<span style=color:#f92672>.</span>append(pos)
                pos <span style=color:#f92672>=</span> pos<span style=color:#f92672>.</span>left
            <span style=color:#66d9ef>else</span>:
                pos <span style=color:#f92672>=</span> stack<span style=color:#f92672>.</span>pop()
                <span style=color:#66d9ef>print</span>(pos<span style=color:#f92672>.</span>val)
                pos <span style=color:#f92672>=</span> pos<span style=color:#f92672>.</span>right
 
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>postOrder</span>(self, root):
        <span style=color:#e6db74>&#39;&#39;&#39;后序遍历&#39;&#39;&#39;</span>
        <span style=color:#66d9ef>if</span> root <span style=color:#f92672>==</span> None:
            <span style=color:#66d9ef>return</span> None
        self<span style=color:#f92672>.</span>postOrder(root<span style=color:#f92672>.</span>left)
        self<span style=color:#f92672>.</span>postOrder(root<span style=color:#f92672>.</span>right)
        <span style=color:#66d9ef>print</span>(root<span style=color:#f92672>.</span>value)
 
    <span style=color:#75715e># 后序打印二叉树（非递归）</span>
    <span style=color:#75715e># 使用两个栈结构</span>
    <span style=color:#75715e># 第一个栈进栈顺序：左节点-&gt;右节点-&gt;跟节点</span>
    <span style=color:#75715e># 第一个栈弹出顺序： 跟节点-&gt;右节点-&gt;左节点(先序遍历栈弹出顺序：跟-&gt;左-&gt;右)</span>
    <span style=color:#75715e># 第二个栈存储为第一个栈的每个弹出依次进栈</span>
    <span style=color:#75715e># 最后第二个栈依次出栈</span>
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>postOrderTraverse</span>(node):
        stack <span style=color:#f92672>=</span> [node]
        stack2 <span style=color:#f92672>=</span> []
        <span style=color:#66d9ef>while</span> stack:
            node <span style=color:#f92672>=</span> stack<span style=color:#f92672>.</span>pop()
            stack2<span style=color:#f92672>.</span>append(node)
            <span style=color:#66d9ef>if</span> node<span style=color:#f92672>.</span>left:
                stack<span style=color:#f92672>.</span>append(node<span style=color:#f92672>.</span>left)
            <span style=color:#66d9ef>if</span> node<span style=color:#f92672>.</span>right:
                stack<span style=color:#f92672>.</span>append(node<span style=color:#f92672>.</span>right)
        <span style=color:#66d9ef>while</span> stack2:
            <span style=color:#66d9ef>print</span>(stack2<span style=color:#f92672>.</span>pop()<span style=color:#f92672>.</span>val)
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>Breadth First Search(BFS)遍历</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># -*- coding: utf-8 -*-</span>
<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TreeNode</span>:
    <span style=color:#66d9ef>def</span> __init__(self, value):
        self<span style=color:#f92672>.</span>value <span style=color:#f92672>=</span> value
        self<span style=color:#f92672>.</span>left <span style=color:#f92672>=</span> None
        self<span style=color:#f92672>.</span>right <span style=color:#f92672>=</span> None
 
<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Tree_Method</span>:
    
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>create_tree</span>(self, arr):
        <span style=color:#e6db74>&#39;&#39;&#39;
</span><span style=color:#e6db74>        利用二叉树的三个组成部分：根节点-左子树-右子树；
</span><span style=color:#e6db74>        传入的arr是一个多维列表，每一维最大为3，
</span><span style=color:#e6db74>        每一维中的内容依次表示根节点-左子树-右子树。然后递归的进行构建
</span><span style=color:#e6db74>        &#39;&#39;&#39;</span>
 
        length <span style=color:#f92672>=</span> len(arr)  <span style=color:#75715e>#计算每一维的大小</span>
        root <span style=color:#f92672>=</span> TreeNode(arr[<span style=color:#ae81ff>0</span>]) <span style=color:#75715e>#获取每一维的根节点</span>
        <span style=color:#66d9ef>if</span> length <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>2</span>:         <span style=color:#75715e>#判断是否有左子树</span>
            root<span style=color:#f92672>.</span>left <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>create_tree(arr[<span style=color:#ae81ff>1</span>])
        <span style=color:#66d9ef>if</span> length <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>3</span>:         <span style=color:#75715e>#判断是否有右子树</span>
            root<span style=color:#f92672>.</span>right <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>create_tree(arr[<span style=color:#ae81ff>2</span>])
        <span style=color:#66d9ef>return</span> root
 
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>BFS</span>(self, root):
        <span style=color:#e6db74>&#39;&#39;&#39;
</span><span style=color:#e6db74>        广度优先遍历，即从上到下，从左到右遍历。
</span><span style=color:#e6db74>        主要利用队列先进先出的特性，入队的时候，是按根左右的顺序，那么只要按照这个顺序出队就可以了
</span><span style=color:#e6db74>        &#39;&#39;&#39;</span>
 
        <span style=color:#66d9ef>if</span> root <span style=color:#f92672>==</span> None:
            <span style=color:#66d9ef>return</span> None
        queue <span style=color:#f92672>=</span> []
        <span style=color:#e6db74>&#39;&#39;&#39;用列表模仿入队&#39;&#39;&#39;</span>
        queue<span style=color:#f92672>.</span>append(root)          
        <span style=color:#66d9ef>while</span> queue:
            <span style=color:#e6db74>&#39;&#39;&#39;将队首元素出栈&#39;&#39;&#39;</span>
            current_node <span style=color:#f92672>=</span> queue<span style=color:#f92672>.</span>pop(<span style=color:#ae81ff>0</span>)
            <span style=color:#66d9ef>print</span>(current_node<span style=color:#f92672>.</span>value, end<span style=color:#f92672>=</span><span style=color:#e6db74>&#39; &#39;</span>)
            <span style=color:#e6db74>&#39;&#39;&#39;判断该节点是否有左孩子，有就入队&#39;&#39;&#39;</span>
            <span style=color:#66d9ef>if</span> current_node<span style=color:#f92672>.</span>left:
                queue<span style=color:#f92672>.</span>append(current_node<span style=color:#f92672>.</span>left)
            <span style=color:#e6db74>&#39;&#39;&#39;判断该节点是否有右孩子，有就入队&#39;&#39;&#39;</span>
            <span style=color:#66d9ef>if</span> current_node<span style=color:#f92672>.</span>right:
                queue<span style=color:#f92672>.</span>append(current_node<span style=color:#f92672>.</span>right)
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>分治法</span></h5><div class=card><div class=card-body><p>分治法(divide and conquer)的工作原理：</p><ol><li>找出简单的基线条件。</li><li>确定如何缩小问题的规模，使其符合基线条件。</li></ol><p>分治法：并非可用于解决问题的算法，而是一种解决问题的思路。</p><ol><li>待解决复杂问题，能够简化为若干个小规模相同的问题，<font color=#a020f0>各个子问题独立存在</font>，并且与原问题形式相同；</li><li>递归地解决各个子问题；</li><li>将各个子问题的解合并，得到原问题的解。</li></ol><p><strong>实例1：</strong><br>N和M的最大公约数（把一块农田均分成方块，求方块最大值）</p><p><strong>实例2:</strong><br>快速排序：</p><ol><li>基线条件：空数组或者只有一个元素的数组，直接返回</li><li>选中一个基准值后，小于基准值的放在左边，大于基准值的放在右边。</li></ol><p align=center><img src=/datasets/note/quik_sort.png width=100% height=100% title=merge_sort alt=merge_sort></p><p><strong>实例3：</strong><br>归并排序</p><p align=center><img src=/datasets/note/merge_sort.png width=100% height=100% title=merge_sort alt=merge_sort></p></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>贪心算法</span></h5><div class=card><div class=card-body><blockquote><p><strong>近似算法</strong>：approximation algorithm. 在获得精确解需要的时间太长时，可使用近似算法。判断近似算法优劣的标准如下：</p><ol><li>速度有多快</li><li>得到的近似解与最优解的接近程度</li></ol></blockquote></div></div><div class=card><div class=card-body><blockquote><p><font color=#a020f0>NP完全问题</font>：就是以难解著称的问题。很多非常聪明的人都认为，根本不可能编写出可快速解决这些问题的算法。</p><ol><li><kbd>集合覆盖问题</kbd>：有n个广播站，每个广播站可能覆盖几个省(覆盖有重复)，想要覆盖全国，最少需要选那几个广播站。每个广播站覆盖的范围：是一个集合。想要全集：选最少个集合，并集是全集。</li><li><kbd>旅行商问题</kbd>：旅行商打算旅行n个城市，找出前往这n个城市的最短路径。如果要找最优解：有<mark>n!</mark>种可能。</li></ol></blockquote><blockquote><p><font color=#a020f0>如何识别NP完全问题</font>：如果能够判断是NP完全问题，这样就好了，就不用去寻找完美的解决方案，而是使用近似算法即可。</p><ol><li>元素较少时算法的运行速度非常快，但随着元素数量的增加，速度会变得非常慢</li><li>涉及<font color=#a020f0>所有组合</font>的问题，通常是NP完全问题</li><li>不能将问题分成小问题，必须考虑各种可能的情况，这<font color=#a020f0>可能是NP完全问题</font></li><li>如果问题涉及<font color=#a020f0>序列(比如：旅行商问题中的城市序列) 且难以解决</font>，可能就是NP完全问题</li><li>如果问题涉及<font color=#a020f0>集合(比如：广播台集合) 且难以解决</font>，可能就是NP完全问题</li><li>如果问题<font color=#a020f0>可转换为集合问题、旅行商问题</font>，它肯定就是NP完全问题。</li></ol></blockquote></div></div><div class=card><div class=card-body><blockquote><p><font color=#a020f0>贪心算法</font>：</p><ol><li>贪心算法，是寻找局部最优解，企图以这种方式获得全局最优解</li><li>面对<kbd>NP完全问题</kbd>，还没有找到快速解决方案。最佳的做法是使用<kbd>近似算法</kbd></li><li>贪心算法，是一种易于实现、运行速度快 的近似算法。</li></ol></blockquote><blockquote><p>贪心算法解决的问题：</p><ol><li>教室，安排课程</li><li>旅行商问题</li><li>序列全排列问题</li></ol></blockquote></div></div><div class=card><div class=card-body><p><strong>贪心算法</strong>：在对问题求解时，总是做出在当前看来是做好的选择。即：<font color=#a020f0>当考虑做何种选择的时候，我们只考虑对当前问题最佳的选择而不考虑子问题的结果，这是贪心算法可行的第一个基本要素</font>。不从整体最优上考虑，而是仅仅在某种意义上的局部最优解。贪心算法以迭代的方式作出相继的贪心选择，每做一次贪心选择就将问题简化为规模更小的子问题。</p><p><strong>何时采用贪心算法</strong>：对于一个具体问题，要确定它是否具有贪心选择性质，必须证明<font color=#a020f0>每一步所作的贪心选择最终导致问题的整体最优解。</font></p><p><strong>示例：</strong><br>完全背包问题、均分纸牌、最大整数</p><p>实际上，贪心算法适用的情况很少。需要先证明：<code>局部最优解会得出整体最优解</code>，才可以使用。一旦证明能成立，它就是一种高效的算法。<br>例如【0-1背包问题】：即：对于每个物品，要么装要么不装(0或1)<br>有一个背包，背包容量是M=150。有7个物品，物品可以分割成任意大小。要求尽可能让装入背包中的物品总价值最大，但不能超过总容量。<br>物品： A B C D E F G<br>重量： 35 30 60 50 40 10 25<br>价值： 10 40 30 50 35 40 30<br>目标函数： ∑pi最大<br></p><p>利用贪心算法，可以这样：</p><ol><li>每次挑选价值最大的物品装入背包，（是否是最优解？）</li><li>每次选择重量最小的物品装入背包，（是否是最优解？）</li><li>每次选择单位重量价值最大的物品，（是否是最优解？）</li></ol><p>上面的3中贪心策略，都无法成立，所以不能采用贪心算法。所以，<code>贪心算法虽然简单高效，但是能证明可以使用该算法的场景比较少。</code></p></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>动态规划</span></h5><div class=card><div class=card-body><p><strong>与分治法的不同</strong>：<br>动态规划与分治法相似，都是组合子问题的解来解决原问题，与分治法的不同在于：</p><ol><li>分治法：将原问题划分为一个个<code>不相交</code>的子问题（比如：归并排序，将数组不断地划分为一个个的子数组进行排序，再将返回的两个有序数组进行合并排序）</li><li>动态规划：要解决的是<code>子问题有重叠</code>的问题，例如0-1背包问题。即：不同的子问题有公共的子子问题，这些重叠的子问题在动态规划中是不应该也不需要重新计算的，而是应该将其解以一定方式保存起来，提供给父问题使用。</li></ol><p><strong>设计步骤</strong>：<br>动态规划通常用来求解<code>最优解问题</code>，这类问题会有很多个解，每个解都对应一个值，而我们则希望在这些解中找到最优解（最大值或者最小值）。
通常四个步骤设计一个动态规划算法：</p><ol><li>定义dp数组以及下标的含义；</li><li>推导出：<code>递推公式</code></li><li>dp数组的初始化</li><li>遍历顺序</li><li>打印出dp数组</li></ol><p><strong>实现方法</strong>：</p><ol><li>递归，属于自顶向下的计算方法：如果子问题有重复计算的情况下，需要一个<code>备忘录</code>来辅助实现，<code>备忘录</code>主要用来保存每一个子问题的解，当每个子问题只求一次，如果后续需要子问题的解，只需要查找备忘录中保存的结果，不必重复计算。</li><li>动态规划，属于自底向上的计算方法：此方法最常用，必须明确每个子问题规模的概念，使得任何子问题的求解都依赖于子子问题的解来进行求解。</li></ol><p><strong>示例：</strong><br><a href=https://www.bilibili.com/read/cv12924751 target=blank>0-1背包问题</a></p></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>回溯法</span></h5><div class=card><div class=card-body><p><strong>回溯法：</strong>
是一种类似枚举的搜索尝试过程，在搜索尝试过程中寻找问题的解，当发现已不满足条件时，就<code>回溯</code>返回，尝试别的路径。<br>回溯法是一种选优搜索法，通常是创建一棵树，从根节点出发，按照<code>深度优先搜索</code>的策略进行搜索，到达某一节点后，搜索该节点是否包含该问题的解：</p><ul><li>设计状态：表示求解问题的不同阶段，在回溯的时候，要有<code>状态重置</code></li><li>如果包含，则进入下一个节点进行搜索；</li><li>如果不包含，则<code>回溯</code>到父节点选择其他支路进行搜索。</li></ul><p><strong>何时采用回溯算法：</strong> 必须有标志性操作——<code>搜索时不满足条件就剪枝 + 所有解</code></p><p><strong>设计步骤</strong>:<br></p><ol><li>针对所给的原问题，定义问题的解空间，设计状态，用于记录不同阶段</li><li>确定易于搜索的解空间结构；</li><li>以深度优先搜索解空间，并在搜索过程中用剪枝函数除去无效搜索。</li></ol><p><strong>示例：</strong><br><code>全排列</code>、旅行商问题、八皇后问题<br>例如：<code>全排列</code></p><p align=center><img src=/datasets/note/permute.png width=100% height=100% title=全排列 alt=全排列></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Solution</span>(object):
    <span style=color:#75715e># 方法二：回溯：深度遍历dfs</span>
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>permute</span>(self, nums):
        n <span style=color:#f92672>=</span> len(nums)
        rst <span style=color:#f92672>=</span> []
        <span style=color:#75715e># 状态used，记录该元素是否已经被使用过</span>
        used <span style=color:#f92672>=</span> [False] <span style=color:#f92672>*</span> n
        <span style=color:#75715e># 已经处理的</span>
        path <span style=color:#f92672>=</span> []

        <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>mute</span>(nums, depth):
            <span style=color:#66d9ef>if</span> depth <span style=color:#f92672>==</span> n:
                rst<span style=color:#f92672>.</span>append([i <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> path])
            <span style=color:#66d9ef>else</span>:
                <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(n):
                    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> used[i]:
                        <span style=color:#75715e># 更新状态</span>
                        path<span style=color:#f92672>.</span>append(nums[i])
                        used[i] <span style=color:#f92672>=</span> True
                        <span style=color:#75715e># 深度搜索</span>
                        mute(nums, depth<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)
                        <span style=color:#75715e># 回溯，把状态重置</span>
                        used[i] <span style=color:#f92672>=</span> False
                        path<span style=color:#f92672>.</span>pop()
        mute(nums, <span style=color:#ae81ff>0</span>)
        <span style=color:#66d9ef>return</span> rst
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>分支限界法</span></h5><div class=card><div class=card-body><p><strong>分支限界法(branch and bound method)：</strong> 和回溯法类似，也是一种搜索算法，与回溯法不同的是：</p><ol><li>回溯法：找出问题的许多解；通常用<code>深度优先</code>的方式搜索解空间树；</li><li>分支限界法：找出原问题的一个解，或者 在满足约束条件的解中找出使某一目标函数的极大解/极小解。通常以<code>广度优先或最小耗费优先</code>的方式搜索解空间树。</li></ol><p>在当前节点(<code>扩展节点</code>)处，生成其所有的子节点(分支)，然后再从当前节点的子节点表中选择下一个<code>扩展节点</code>。为了有效地选择下一个<code>扩展节点</code>，加速搜索的进程，在每个节点处，计算一个<code>限界</code>，从其子节点表中选择一个最有利的节点作为<code>扩展节点</code>，使搜索朝着解空间上最优解的分支推进。</p><p><strong>何时采用分支界限法：</strong> 必须有标志性操作——<code>搜索时不满足限界就剪枝 + 最优解</code></p><p><strong>示例：</strong><br>0-1背包问题：<code>限界</code>就是背包的大小，一个节点的子节点表中，如果有超过<code>限界</code>的就直接剪枝。如下图所示：</p><p align=center><img src=/datasets/note/branch_bound.jpg width=100% height=100% title=branch_bound alt=branch_bound></p></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>递归</span></h5><div class=card><div class=card-body><p>Leigh Caldwell在Stack Overflow上说的一句话: “如果使用循环，程序的性能可能更高;如果使用递归，程序可能更容易理解。如何选择要看什么对你来说更重要。”<br></p><p>编写递归函数时，必须告诉它何时停止递归。正因为如此，每个递归函数都有两部分:</p><ol><li>基线条件(base case)。指的是函数不再调用自己，从而避免形成无限循环。</li><li>递归条件(recursive case)。指的是函数调用自己。</li></ol></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>最大公约数</span></h5><div class=card><div class=card-body><ol><li>原始：求两个数(N, M)的最大公约数。</li><li>变形1：假设你是农场主，有一小块土地。你要将这块地均匀地分成方块，且分出的方块要尽可能大。</li></ol><pre><code>伪代码-思路：  
假设：N表示较小的数，M表示较大的数。  
重复一下操作，直到 N=0
新N = M % N
新M = 原N


</code></pre></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>图的搜索</span></h5><div class=card><div class=card-body><blockquote><p><font color=#a020f0>广度优先搜索</font>：可以回答两类问题，即：适合<font color=#a020f0>非加权图</font></p><ol><li>从节点A出发，有往节点B的路径吗？</li><li>从节点A出发，前往节点B的那条<font color=#a020f0>路径最短</font>。</li></ol></blockquote></div></div><div class=card><div class=card-body><blockquote><p><font color=#a020f0>狄克斯特拉算法</font>：适合 <font color=#a020f0>没有负权边的加权图</font>。</p><ol><li>狄克斯特拉算法，假设：对于处理过的节点，没有前往该节点的更短路径。这种假设仅在<font color=#a020f0>没有负权边</font>时才成立。</li></ol></blockquote></div></div><div class=card><div class=card-body><blockquote><p><font color=#a020f0>贝尔曼-福德算法</font>：适合 <font color=#a020f0>包含负权边</font>的加权图</p></blockquote></div></div><div class=card><div class=card-body><p>狄克斯特拉算法包括4个步骤</p><blockquote><ol><li>找出”最便宜“的节点，即：可在最短时间内到达的节点</li><li>更新该节点的邻居的开销，检查是否有前往它们的更短路径，如果有，就更新其开销。</li><li>重复这个过程，直到对图中的每个节点都这样做了</li><li>计算最终路径</li></ol></blockquote><p>例如：乐谱 -换-> 钢琴</p><p align=center><img src=/datasets/note/dikesi.png width=100% height=100% title=dikesi alt=dikesi></p><p><strong>第一步</strong>：找出最便宜的节点。这里，换海报最便宜了，不需要支付额外的费用。<br><strong>第二步</strong>：计算前往该节点的各个邻居的开销。</p><p align=center><img src=/datasets/note/dikesi-2.png width=100% height=100% title=dikesi alt=dikesi></p>父节点：代表该节点的上一级最便宜节点。<br><p><strong>第三步</strong>：目前条件(未遍历：<font color=#a020f0>黑胶唱片、吉他、架子鼓</font>；已遍历：海报)，在目前未遍历节点中找下一个最便宜的节点是 ”黑胶唱片“；更新 ”黑胶唱片“ 的各个邻居的开销。</p><p align=center><img src=/datasets/note/dikesi-3.png width=100% height=100% title=dikesi alt=dikesi></p>下一个最便宜的是 吉他，因此更新其邻居的开销：<p align=center><img src=/datasets/note/dikesi-4.png width=100% height=100% title=dikesi alt=dikesi></p>下一个最便宜的是 架子鼓，因此更新其邻居的开销：<p align=center><img src=/datasets/note/dikesi-5.png width=100% height=100% title=dikesi alt=dikesi></p><p><strong>第四步</strong>：所有节点都已遍历完了，当前，我们直到最短路径的开销是35美元，但如何确定这条路径呢？为此，可以根据父节点寻找。</p></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>狄克斯特拉算法：python实例：乐谱 -换-> 钢琴</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Solution</span>(object):
    <span style=color:#66d9ef>def</span> __init__(self):
        <span style=color:#66d9ef>pass</span>
    
    <span style=color:#a6e22e>@staticmethod</span>
    <span style=color:#75715e># 在未处理的节点中找出开销最小的节点</span>
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>find_lowest_cost_node</span>(costs, processed):
        lowest_cost <span style=color:#f92672>=</span> float(<span style=color:#e6db74>&#39;inf&#39;</span>)
        lowest_cost_node <span style=color:#f92672>=</span> None
        <span style=color:#66d9ef>for</span> node <span style=color:#f92672>in</span> costs:
            cost <span style=color:#f92672>=</span> costs[node]
            <span style=color:#66d9ef>if</span> cost <span style=color:#f92672>&lt;</span> lowest_cost <span style=color:#f92672>and</span> node <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> processed:
                lowest_cost <span style=color:#f92672>=</span> cost
                lowest_cost_node <span style=color:#f92672>=</span> node

        <span style=color:#66d9ef>return</span> lowest_cost_node

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>dikesi</span>(self, graph):

        <span style=color:#75715e># 开销-散列表。未知节点的开销，先设置为无穷大</span>
        costs <span style=color:#f92672>=</span> {
          <span style=color:#e6db74>&#39;A&#39;</span>: <span style=color:#ae81ff>0</span>, 
          <span style=color:#e6db74>&#39;B&#39;</span>: <span style=color:#ae81ff>5</span>, 
          <span style=color:#e6db74>&#39;C&#39;</span>: <span style=color:#ae81ff>0</span>, 
          <span style=color:#e6db74>&#39;D&#39;</span>: float(<span style=color:#e6db74>&#39;inf&#39;</span>), 
          <span style=color:#e6db74>&#39;E&#39;</span>: float(<span style=color:#e6db74>&#39;inf&#39;</span>), 
          <span style=color:#e6db74>&#39;F&#39;</span>: float(<span style=color:#e6db74>&#39;inf&#39;</span>)
        }
        <span style=color:#75715e># 父节点-散列表</span>
        parents <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;B&#39;</span>: <span style=color:#e6db74>&#39;A&#39;</span>, <span style=color:#e6db74>&#39;C&#39;</span>: <span style=color:#e6db74>&#39;A&#39;</span>, <span style=color:#e6db74>&#39;F&#39;</span>: None}
        <span style=color:#75715e># 已处理过的节点</span>
        processed <span style=color:#f92672>=</span> []

        node <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;A&#39;</span>
        <span style=color:#66d9ef>while</span> node <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> None:
            cost <span style=color:#f92672>=</span> costs[node]
            neighbors <span style=color:#f92672>=</span> graph[node]
            <span style=color:#75715e># 遍历当前节点的所有邻居</span>
            <span style=color:#66d9ef>for</span> n <span style=color:#f92672>in</span> neighbors:
                new_cost <span style=color:#f92672>=</span> cost <span style=color:#f92672>+</span> neighbors[n]
                <span style=color:#75715e># 如果当前节点前往邻居更近，就更新该邻居的开销；同时更新该邻居的父节点</span>
                <span style=color:#66d9ef>if</span> costs[n] <span style=color:#f92672>&gt;</span> new_cost:
                    costs[n] <span style=color:#f92672>=</span> new_cost
                    parents[n] <span style=color:#f92672>=</span> node
            processed<span style=color:#f92672>.</span>append(node)
            node <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>find_lowest_cost_node(costs, processed)

        <span style=color:#66d9ef>return</span> costs[<span style=color:#e6db74>&#39;F&#39;</span>]

<span style=color:#75715e># 图-散列表</span>
graph <span style=color:#f92672>=</span> {}
graph[<span style=color:#e6db74>&#39;A&#39;</span>] <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;B&#39;</span>: <span style=color:#ae81ff>5</span>, <span style=color:#e6db74>&#39;C&#39;</span>: <span style=color:#ae81ff>0</span>}
graph[<span style=color:#e6db74>&#39;B&#39;</span>] <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;D&#39;</span>: <span style=color:#ae81ff>15</span>, <span style=color:#e6db74>&#39;E&#39;</span>: <span style=color:#ae81ff>20</span>}
graph[<span style=color:#e6db74>&#39;C&#39;</span>] <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;D&#39;</span>: <span style=color:#ae81ff>30</span>, <span style=color:#e6db74>&#39;E&#39;</span>: <span style=color:#ae81ff>35</span>}
graph[<span style=color:#e6db74>&#39;D&#39;</span>] <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;F&#39;</span>: <span style=color:#ae81ff>20</span>}
graph[<span style=color:#e6db74>&#39;E&#39;</span>] <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;F&#39;</span>: <span style=color:#ae81ff>10</span>}
graph[<span style=color:#e6db74>&#39;F&#39;</span>] <span style=color:#f92672>=</span> {}
alpha <span style=color:#f92672>=</span> Solution()
rst <span style=color:#f92672>=</span> alpha<span style=color:#f92672>.</span>dikesi(graph)
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>数据结构</span></h5><div class=card><div class=card-body><p>常见的数据结构可分为「线性数据结构」与「非线性数据结构」，具体为：「数组」、「链表」、「栈」、「队列」、「树」、「图」、「散列表」、「堆」。</p><p align=center><img src=/datasets/note/data-struct.png width=80% height=80% title=data-struct alt=data-struct></p></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>数组与链表</span></h5><div class=card><div class=card-body><blockquote><p>数组： 在内存中是连续的一整块。</p><ol><li>随机访问，数组在内存中是连续的一整块，所以支持随机访问。</li><li>增/删操作，费事。增加元素时，如果内存不够一整块，还得整体迁移</li></ol></blockquote></div></div><div class=card><div class=card-body><blockquote><p>链表： 可以存储在内存的任何地方。</p><ol><li>顺序访问，由于存在任何地方，每个元素都存储了下一个元素的地址，所以只能从头开始逐个查询。</li><li>增/删操作，不费事。只要修改一下 <font color=#a020f0>下一元素地址</font> 就行。</li></ol></blockquote></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>栈</span></h5><div class=card><div class=card-body><font color=#a020f0>递归</font>操作，就是使用的调用栈。即：把每个递归调用函数，都压入栈，完成一个弹出一个，直到空栈。</div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>队列</span></h5><div class=card><div class=card-body><p><font color=#a020f0>队列</font>(First In First Out)：先进先出的数据结构。<br></p><blockquote><ol><li>图的广度优先搜索，就是先把1级元素压入队列，然后在一个一个出队遍历时，把其邻居压入队列</li><li></li></ol></blockquote></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>散列表</span></h5><div class=card><div class=card-body><p><font color=#a020f0>散列函数</font>：将任何输入映射到数字。<br></p><p>在pyhton中 散列表的实现为字典 dict()</p><p>散列表是一种非线性数据结构，通过利用 Hash 函数将指定的「键 key」映射至对应的「值 value」，以实现高效的元素查找。<br>比如：通过输入学号，在名字库里找到对应的名字。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 输入：学号</span>
<span style=color:#75715e># 小力: 10001</span>
<span style=color:#75715e># 小特: 10002</span>
<span style=color:#75715e># 小扣: 10003</span>
<span style=color:#75715e># 名字库</span>
names <span style=color:#f92672>=</span> [ <span style=color:#e6db74>&#34;小力&#34;</span>, <span style=color:#e6db74>&#34;小特&#34;</span>, <span style=color:#e6db74>&#34;小扣&#34;</span> ]
<span style=color:#75715e># Hash函数的目的：把学号，映射为序号index，</span>
<span style=color:#75715e># 这个序号index就是 名字库names的名字对应序号</span>
</code></pre></div><p align=center><img src=/datasets/note/hash.png width=100% height=100% title=hash alt=hash></p></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>堆</span></h5><div class=card><div class=card-body><p>堆是一种基于「完全二叉树」的数据结构，可使用数组实现。以堆为原理的排序算法称为「堆排序」，基于堆实现的数据结构为「优先队列」。堆分为「大顶堆」和「小顶堆」，大（小）顶堆：任意节点的值不大于（小于）其父节点的值。</p><p>完全二叉树定义： 设二叉树深度为 k，若二叉树除第 k 层外的其它各层（第 1 至 k−1 层）的节点达到最大个数，且处于第 k 层的节点都连续集中在最左边，则称此二叉树为完全二叉树。</p><p align=center><img src=/datasets/note/heap.png width=30% height=30% title=heap alt=heap></p><p>上图就是一个「小顶堆」，堆的操作：</p><ol><li>搜索：$O(1)$，就是访问 堆顶的元素。</li><li>添加：就是要满足堆的定义：<font color=#a020f0>任意节点的值不大于（小于）其父节点的值。</font></li><li>删除：跟添加一样，就是要满足堆的定义：<font color=#a020f0>任意节点的值不大于（小于）其父节点的值。</font></li></ol></div></div></div></div><blockquote></blockquote></div><div class=paginator><ul class=pagination><li class=page-item><a href=/zh-cn/notes/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class="page-item disabled"><a class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class="page-item active"><a class=page-link href=/zh-cn/notes/>1</a></li><li class=page-item><a class=page-link href=/zh-cn/notes/page/2/>2</a></li><li class=page-item><a class=page-link href=/zh-cn/notes/page/3/>3</a></li><li class=page-item><a href=/zh-cn/notes/page/2/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/zh-cn/notes/page/3/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>导航</h5><ul><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#about>关于</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#skills>技能</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#experiences>经验</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#projects>项目</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#education>教育</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#recent-posts>最新博客</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#accomplishments>造诣</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#achievements>业绩</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>联系方式:</h5><ul><li><span>电话:</span> <span>+0123456789</span></li><li><span>邮箱:</span> <span>bsgshyn8@163.com</span></li></ul></div></div></div><hr><div class=container><p id=disclaimer><strong>免责声明:</strong> 这个主题是MIT许可的。因此，您可以将其用于非商业、商业或者私人用途。您可以修改或分发主题，而无须经 作者任何许可。但是，主题作者不对主题的任何问题提供任何保证或承担任何责任。</p></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/imagesloaded.pkgd.min.js></script><script src=/js/note.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>