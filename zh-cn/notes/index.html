<!doctype html><html><head><title>笔记</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0d88336a9cbd3e68c7714efae786b0a9_38044_42x0_resize_box_2.png><meta property="og:title" content="笔记"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://biubiobiu.github.io/zh-cn/notes/"><meta property="og:updated_time" content="2010-06-08T08:06:25+06:00"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/notes.css><link rel=stylesheet href=/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/zh-cn><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png alt=Logo>
biubiobiu's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=languageSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="flag-icon flag-icon-zh-cn"></span>简体中文</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/notes><span class="flag-icon flag-icon-gb"></span>English</a></div></li><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu547c3206082786c1d32d36034e2a655a_40863_42x0_resize_box_2.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/zh-cn/search><input type=text name=keyword placeholder=搜索 data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/zh-cn/notes data-filter=all>笔记</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/notes/computer_algorithm/>Computer Algorithm</a><ul><li><a href=/zh-cn/notes/computer_algorithm/data_struct/ title=数据结构>数据结构</a></li><li><a href=/zh-cn/notes/computer_algorithm/torch_summary/ title=二叉树-遍历>二叉树-遍历</a></li><li><a href=/zh-cn/notes/computer_algorithm/dynamic_plan/ title=五大常用算法>五大常用算法</a></li><li><a href=/zh-cn/notes/computer_algorithm/sliding_window/ title=滑动窗口>滑动窗口</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/notes/mxnet/>MxNet</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/notes/mxnet/ndarray/>NdArray</a><ul><li><a href=/zh-cn/notes/mxnet/ndarray/ndarray_summary/ title=NdArray使用>NdArray使用</a></li><li><a href=/zh-cn/notes/mxnet/ndarray/technic_gather/ title=NdArray技巧搜集>NdArray技巧搜集</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/notes/mxnet/gluon/>Gluon</a><ul><li><a href=/zh-cn/notes/mxnet/gluon/gluon_summary/ title=Gluon实例>Gluon实例</a></li><li><a href=/zh-cn/notes/mxnet/gluon/module_gather/ title=Gluon模块简介>Gluon模块简介</a></li><li><a href=/zh-cn/notes/mxnet/gluon/module_gluon_nn/ title=Gluon-nn模块>Gluon-nn模块</a></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class="content container-fluid" id=content><div class="container-fluid note-card-holder" id=note-card-holder><div class=note-card><div class=item><h5 class=note-title><span>查阅文档</span></h5><div class=card><div class=card-body><p>怎么查阅相关文档？ <a href=https://mxnet.apache.org/ target=blank>官网</a></p><h3 id=1-查阅模块里的所有函数和类>1. 查阅<code>模块</code>里的所有函数和类</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
<span style=color:#66d9ef>print</span>(dir(nd<span style=color:#f92672>.</span>random))
</code></pre></div><ol><li>__开头和结尾的函数 (python的特别对象) 可以忽略</li><li>_开头的函数 (一般为内部函数) 可以忽略</li><li>其余成员，可以根据名字 大致猜出是什么意思。</li></ol><h3 id=2-查阅特定函数和类的使用>2. 查阅特定<code>函数和类</code>的使用</h3><p>想了解某个函数或者类的具体用法，可以使用help函数。以NDArray中的ones_like函数为例。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>help(nd<span style=color:#f92672>.</span>ones_like)
</code></pre></div><p>注意：</p></p><ol><li>jupyter记事本里，使用<code>?</code>来将文档显示在另外一个窗口中。例如：<code>nd.ones_like?</code> 与 <code>help(nd.ones_like)</code>效果一样。<code>nd.ones_like??</code>会额外显示该函数实现的代码。</li></ol></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>内存开销</span></h5><div class=card><div class=card-body><ol><li><p>原始操作</p>首先来个例子：Y = Y + X &ndash;> 每个操作会新开内存来存储运算结果。
上例中，X，Y 变量首先存储在内存中，相加的计算结果会另外开辟内存来存储；然后变量Y在指向新的内存。
内存使用情况：</p>内存id_x &lt;&ndash; X</p>内存id_y &lt;&ndash; Y</p>内存id_x+y &lt;&ndash; Y</p></li><li><p>Y[:] = X + Y 或者 Y += X</p>通过<code>[:]</code>把X+Y的结果写进Y对应的内存中。上述操作中，需要另外开辟内存来存储计算结果。
内存使用情况：</p>内存id_x &lt;&ndash; X</p>内存id_y &lt;&ndash; Y</p>内存id_x+y &ndash;> 把<code>内存id_x+y</code>中数值复制到<code>内存id_y</code>中</p></li><li><p>使用运算符全名函数中的out参数</p>可以避免临时内存开销，使用运算符全名函数：<code>nd.elemwise_add(X, Y, out=Y)</code>。内存使用情况：</p>内存id_x &lt;&ndash; X</p>内存id_y &lt;&ndash; Y</p>内存id_y &lt;&ndash; 直接存放 X+Y 的计算结果</p></li></ol></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>自动求梯度</span></h5><div class=card><div class=card-body><p>MXNet提供的autograd模块，可以自动求梯度(gradient)</p></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> autograd, nd
<span style=color:#75715e># 1. 创建变量 x，并赋初值</span>
x <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>arrange(<span style=color:#ae81ff>4</span>)<span style=color:#f92672>.</span>reshape((<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>1</span>))
<span style=color:#75715e># 2. 为了求变量x的梯度，先调用attach_grad函数来申请存储梯度所需要的内存 </span>
x<span style=color:#f92672>.</span>attach_grad()
<span style=color:#75715e># 3. 为了减少计算和内存开销，默认条件下MXNet是不会记录：求梯度的计算，</span>
<span style=color:#75715e>#    需要调用record函数来要求MXNet记录与求梯度有关的计算。</span>
<span style=color:#66d9ef>print</span>(autograd<span style=color:#f92672>.</span>is_training())    <span style=color:#75715e># False</span>
<span style=color:#66d9ef>with</span> autograd<span style=color:#f92672>.</span>record():
  <span style=color:#66d9ef>print</span>(autograd<span style=color:#f92672>.</span>is_training())  <span style=color:#75715e># True</span>
  y <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>nd<span style=color:#f92672>.</span>dot(x<span style=color:#f92672>.</span>T, x)
<span style=color:#75715e># 4. 调用backward函数自动求梯度。y必须是一个标量，</span>
<span style=color:#75715e>#  如果y不是标量：MXNet会先对y中元素求和，然后对该和值求有关x的梯度</span>
y<span style=color:#f92672>.</span>backward() 

</code></pre></div><p>注意：</p></p><ol><li>在调用record函数后，MXNet会记录并计算梯度；</li><li>默认情况下，autograd会改变运行模式：从预测模式转为训练模式。可以通过调用is_training函数来查看。</li></ol></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>样例</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>sum/mean等操作 - 保留原维度数</span></h5><div class=card><div class=card-body><p><code>keepdims</code>: 保留原维度数。例如：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>softmax</span>(X):
    X_exp <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>exp()  <span style=color:#75715e># shape = (n, m)</span>
    <span style=color:#75715e># shape = (n, 1) 而并不是 (n,)</span>
    partition <span style=color:#f92672>=</span> X_exp<span style=color:#f92672>.</span>sum(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, keepdims<span style=color:#f92672>=</span>True)  
    <span style=color:#66d9ef>return</span> X_exp <span style=color:#f92672>/</span> partition  <span style=color:#75715e># 这里应用了广播机制</span>
X <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>5</span>))
X_prob <span style=color:#f92672>=</span> softmax(X)

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>B的值作为A的索引 - 取值</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
y_hat <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>0.1</span>, <span style=color:#ae81ff>0.3</span>, <span style=color:#ae81ff>0.6</span>], [<span style=color:#ae81ff>0.3</span>, <span style=color:#ae81ff>0.2</span>, <span style=color:#ae81ff>0.5</span>]])
y <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>], dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;int32&#39;</span>)
nd<span style=color:#f92672>.</span>pick(y_hat, y)
<span style=color:#75715e># 结果: [0.1, 0.5]</span>

<span style=color:#75715e># 应用的实例：交叉熵的实现</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>cross_entropy</span>(y_hat, y):
    <span style=color:#66d9ef>return</span> <span style=color:#f92672>-</span>nd<span style=color:#f92672>.</span>pick(y_hat, y)<span style=color:#f92672>.</span>log()
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>样例</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型基类-Block</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> Block, nn
<span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> ndarray <span style=color:#66d9ef>as</span> F

<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Model</span>(Block):
	<span style=color:#66d9ef>def</span> __init__(self, <span style=color:#f92672>**</span>kwargs):
		super(Model, self)<span style=color:#f92672>.</span>__init__(<span style=color:#f92672>**</span>kwargs)
		<span style=color:#75715e># use name_scope to give child Blocks appropriate names.</span>
		<span style=color:#66d9ef>with</span> self<span style=color:#f92672>.</span>name_scope():
			self<span style=color:#f92672>.</span>dense0 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>20</span>)
			self<span style=color:#f92672>.</span>dense1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>20</span>)

	<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
		x <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>dense0(x))
		<span style=color:#66d9ef>return</span> F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>dense1(x))

model <span style=color:#f92672>=</span> Model()
model<span style=color:#f92672>.</span>initialize(ctx<span style=color:#f92672>=</span>mx<span style=color:#f92672>.</span>cpu(<span style=color:#ae81ff>0</span>))
model(F<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>10</span>), ctx<span style=color:#f92672>=</span>mx<span style=color:#f92672>.</span>cpu(<span style=color:#ae81ff>0</span>)))

</code></pre></div><p>class Block(builtins.object)<br>网络的最基础的类，搭建网络时必须继承此Block类<br>—————————————————<br>Block的两个参数：<br></p><ul><li><code>prefix</code> : str; 前缀的作用就像一个命名空间。在父模块的作用域下创建的子模块都有父模块的前缀(prefix).</li><li><code>params</code> : ParameterDict or None; 共享参数。<br>　　例如：dense1共享dense0的参数。<br>　　　　dense0 = nn.Dense(20)<br>　　　　dense1 = nn.Dense(20, params=dense0.collect_params())</li></ul><p>—————————————————<br>Block的方法：</p><ul><li><code>collect_params</code>(self, select=None) 返回一个<code>ParameterDict</code>类。默认包含所有的参数；同时也可以正则匹配:<br>例如：选出特定的参数 [&lsquo;conv1_weight&rsquo;, &lsquo;conv1_bias&rsquo;, &lsquo;fc_weight&rsquo;, &lsquo;fc_bias&rsquo;]<br>即：model.collect_params(&lsquo;conv1_weight|conv1_bias|fc_weight|fc_bias&rsquo;)<br>　　<code>Parameters</code>：空 或者 正则表达式<br>　　<code>Returns</code>: py:class:ParameterDict</li><li><code>forward</code>(self, *args) 完成前向计算，输入是NDArray列表<br>　　Parameters：*args : list of NDArray</li><li><code>hybridize</code>(self, active=True, **kwargs) 激活/不激活HybridBlock的递归<br>　　Parameters： bool, default True</li><li><code>initialize</code>(self, init=&lt;mxnet.initializer.Uniform object>, ctx=None, verbose=False, force_reinit=False)<br>　　对模型的参数初始化，默认是均匀分布。<br>　　等价于：block.collect_params().initialize(&mldr;)<br>　　Parameters：<br>　　　　init : Initializer 初始化方法<br>　　　　ctx : 设备 或者 设备列表。会把模型copy到所有指定的设备上<br>　　　　verbose : bool, default False 是否在初始化时粗略地打印细节。<br>　　　　force_reinit : bool, default False 是否重新初始化，即使已经初始化</li><li><code>load_parameters</code>(self, filename, ctx=None, allow_missing=False, ignore_extra=False, cast_dtype=False, dtype_source=&lsquo;current&rsquo;)<br>　　加载模型参数从 用<code>save_parameters</code>保存的模型文件中。<br>　　Parameters：<br>　　　　filename : str 模型文件路径<br>　　　　ctx : 设备 或者设备列表。默认使用CPU<br>　　　　allow_missing : bool, default False 是否默默跳过模型文件中不存<br>　　　　　　在的模型参数。<br>　　　　ignore_extra : bool, default False 是否默默忽略模型中不存在的参<br>　　　　　　数(模型文件中有，模型定义中没有)<br>　　　　cast_dtype : bool, default False 从checkpointload模型时，是否根<br>　　　　　　据传入转换NDArray的数据类型<br>　　　　dtype_source : str, default &lsquo;current&rsquo; 枚举值：{&lsquo;current&rsquo;, &lsquo;saved&rsquo;}<br>　　　　　　只有再cast_dtype=True时有效，指定模型参数的数据类型</li><li><code>name_scope</code>(self) 返回一个命名空间，用来管理Block和参数names。<br>　　必须在with语句中使用：<br>　　with self.name_scope():<br>　　　　self.dense = nn.Dense(20)</li><li><code>register_child</code>(self, block, name=None) 将block注册为子节点，block的属<br>　　性将自动注册。</li><li><code>save_parameters</code>(self, filename) 保持模型参数到磁盘。该方法只保存模型<br>　　参数的权重，不保存模型的结构。如果想要保存模型的结构，请使<br>　　用:py:meth:<code>HybridBlock.export</code>.<br>　　Parameters：Path to file.</li><li><code>summary</code>(self, *inputs) 打印模型的输出和参数的摘要。模型必须被初始化</li></ul><p>—————————————————<br>数据描述：</p><ul><li><code>name</code> :py:class:Block 的名字</li><li><code>params</code>：返回一个参数字典（不包含子节点的参数）</li><li><code>prefix</code>：返回py:class:Block的前缀</li></ul></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型参数-Parameter</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>ctx <span style=color:#f92672>=</span> mx<span style=color:#f92672>.</span>gpu(<span style=color:#ae81ff>0</span>)
x <span style=color:#f92672>=</span> mx<span style=color:#f92672>.</span>nd<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>100</span>), ctx<span style=color:#f92672>=</span>ctx)
w <span style=color:#f92672>=</span> mx<span style=color:#f92672>.</span>gluon<span style=color:#f92672>.</span>Parameter(<span style=color:#e6db74>&#39;fc_weight&#39;</span>, shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>100</span>), init<span style=color:#f92672>=</span>mx<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>Xavier())
b <span style=color:#f92672>=</span> mx<span style=color:#f92672>.</span>gluon<span style=color:#f92672>.</span>Parameter(<span style=color:#e6db74>&#39;fc_bias&#39;</span>, shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>64</span>,), init<span style=color:#f92672>=</span>mx<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>Zero())
w<span style=color:#f92672>.</span>initialize(ctx<span style=color:#f92672>=</span>ctx)
b<span style=color:#f92672>.</span>initialize(ctx<span style=color:#f92672>=</span>ctx)
out <span style=color:#f92672>=</span> mx<span style=color:#f92672>.</span>nd<span style=color:#f92672>.</span>FullyConnected(x, w<span style=color:#f92672>.</span>data(ctx), b<span style=color:#f92672>.</span>data(ctx), num_hidden<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>)
</code></pre></div><p>class:Parameter 一个存放Blocks的参数的权重的容器。初始化后<code>Parameter.initialize(...)</code>，会copy所有参数权重到每个设备上。如果<code>grad_req</code>不为null，在每个设备上，该容器会拥有一个梯度向量。</p><p>Parameter(name,<br>　　　　　grad_req=&lsquo;write&rsquo;,<br>　　　　　shape=None,<br>　　　　　dtype=&lt;class &lsquo;numpy.float32&rsquo;>,<br>　　　　　lr_mult=1.0,<br>　　　　　wd_mult=1.0,<br>　　　　　init=None,<br>　　　　　allow_deferred_init=False,<br>　　　　　differentiable=True,<br>　　　　　stype=&lsquo;default&rsquo;,<br>　　　　　grad_stype=&lsquo;default&rsquo;)</p><p>形参：<br>——————————</p><ul><li><code>name</code> :
str类型；参数的名字。</li><li><code>grad_req</code> : 枚举值：{&lsquo;write&rsquo;, &lsquo;add&rsquo;, &lsquo;null&rsquo;}, 默认值：&lsquo;write&rsquo;。指定怎么更新梯度到梯度向量。<br>　　<code>'write'</code>:每次把梯度值写到 梯度向量中<br>　　<code>'add'</code>: 每次把计算的梯度值add到梯度向量中. 在每次迭代之前，<br>　　　　你需要手动调用<code>zero_grad()</code>来清理梯度缓存。<br>　　<code>'null'</code>: 参数不需要计算梯度，不会分配梯度向量。</li><li><code>shape</code> : int or tuple of int, default None. 参数的尺寸.</li><li><code>dtype</code> : numpy.dtype or str, default &lsquo;float32&rsquo;. 参数的数据类型</li><li><code>lr_mult</code> : float, default 1.0, 学习率.</li><li><code>wd_mult</code> : float, default 1.0, 权重衰减率 L2</li><li><code>init</code> : Initializer, default None. 参数的初始化，默认全局初始化</li><li><code>stype</code>: 枚举值: {&lsquo;default&rsquo;, &lsquo;row_sparse&rsquo;, &lsquo;csr&rsquo;}, defaults to &lsquo;default&rsquo;. 参数的存储类型。</li><li><code>grad_stype</code>: 枚举值: {&lsquo;default&rsquo;, &lsquo;row_sparse&rsquo;, &lsquo;csr&rsquo;}, defaults to &lsquo;default&rsquo;. 参数梯度的存储类型</li></ul><p>属性:<br>——————————</p><ul><li><code>grad_req</code> : 枚举值:{&lsquo;write&rsquo;, &lsquo;add&rsquo;, &lsquo;null&rsquo;} 可以在初始化之前/之后设置。当不需要计算参数的梯度时，设置为<code>null</code>，以节省内存和计算量。</li><li><code>lr_mult</code> : float 学习率</li><li><code>wd_mult</code> : float 权重衰减率</li></ul><p>定义的函数：<br>——————————</p><ul><li><p><code>cast(self, dtype)</code> 转换参数的值/梯度的数据类型。<br>　　dtype : str or numpy.dtype 新的数据类型</p></li><li><p><code>data(self, ctx=None)</code> 获取这个参数在设备<code>ctx</code>上的值，参数必须已经初始化了。<br>　　ctx : 指定设备<br>　　Returns：NDArray on ctx</p></li><li><p><code>grad(self, ctx=None)</code> 获取这个参数在设备<code>ctx</code>上的梯度值。<br>　　ctx : 指定设备</p></li><li><p><code>initialize</code>(self, init=None, ctx=None,<br>　　default_init=&lt;mxnet.initializer.Uniform>,<br>　　force_reinit=False) 初始化参数和梯度向量<br>　　<code>init</code> : Initializer 初始化参数的值<br>　　<code>ctx</code> : 设备/设备列表, 默认使用:py:meth:<code>context.current_context()</code>.<br>　　<code>default_init</code> : Initializer 当:py:func:<code>init</code>和:py:meth:<code>Parameter.init</code>都为none时，使用该默认的初始化.<br>　　<code>force_reinit</code> : bool, default False 当参数已经被初始化，是否再次初始化。</p></li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>weight <span style=color:#f92672>=</span> mx<span style=color:#f92672>.</span>gluon<span style=color:#f92672>.</span>Parameter(<span style=color:#e6db74>&#39;weight&#39;</span>, shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>))
weight<span style=color:#f92672>.</span>initialize(ctx<span style=color:#f92672>=</span>mx<span style=color:#f92672>.</span>cpu(<span style=color:#ae81ff>0</span>))
weight<span style=color:#f92672>.</span>data()
<span style=color:#75715e>#　　[[-0.01068833  0.01729892]</span>
<span style=color:#75715e>#　　 [ 0.02042518 -0.01618656]]</span>
<span style=color:#75715e>#　　&lt;NDArray 2x2 @cpu(0)&gt;</span>
weight<span style=color:#f92672>.</span>grad()
<span style=color:#75715e>#　　[[ 0.  0.]</span>
<span style=color:#75715e>#　　 [ 0.  0.]]</span>
<span style=color:#75715e>#　　&lt;NDArray 2x2 @cpu(0)&gt;</span>
weight<span style=color:#f92672>.</span>initialize(ctx<span style=color:#f92672>=</span>[mx<span style=color:#f92672>.</span>gpu(<span style=color:#ae81ff>0</span>), mx<span style=color:#f92672>.</span>gpu(<span style=color:#ae81ff>1</span>)])
weight<span style=color:#f92672>.</span>data(mx<span style=color:#f92672>.</span>gpu(<span style=color:#ae81ff>0</span>))
<span style=color:#75715e>#　　[[-0.00873779 -0.02834515]</span>
<span style=color:#75715e>#　　 [ 0.05484822 -0.06206018]]</span>
<span style=color:#75715e>#　　&lt;NDArray 2x2 @gpu(0)&gt;</span>
weight<span style=color:#f92672>.</span>data(mx<span style=color:#f92672>.</span>gpu(<span style=color:#ae81ff>1</span>))
<span style=color:#75715e>#　　[[-0.00873779 -0.02834515]</span>
<span style=color:#75715e>#　　 [ 0.05484822 -0.06206018]]</span>
<span style=color:#75715e>#　　&lt;NDArray 2x2 @gpu(1)&gt;</span>
</code></pre></div><ul><li><code>list_ctx(self)</code> 返回参数初始化在那些设备上</li><li><code>list_data(self)</code> 按照顺序返回所有设备上的参数值
　　Returns: list of NDArrays</li><li><code>list_grad(self)</code> 按照顺序返回所有设备上的梯度值</li><li><code>list_row_sparse_data(self, row_id)</code> 按照顺序返回所有设备上的 <code>行稀疏</code>的参数。<br>　　row_id: 指定看哪一行的数据<br>　　Returns: list of NDArrays</li><li><code>reset_ctx(self, ctx)</code> 重新设定设备，把参数copy到该设备上<br>　　ctx : Context or list of Context, default <code>context.current_context()</code></li><li><code>row_sparse_data(self, row_id)</code><br>　　row_id: NDArray 指定看哪一行的数据<br>　　Returns: NDArray on row_id&rsquo;s context</li><li><code>set_data(self, data)</code> 在所有设备上，设置该参数的值。</li><li><code>var(self)</code> 返回一个代表该参数的符号</li><li><code>zero_grad(self)</code> 将所有设备上的梯度缓存清零</li></ul><p>数据描述:<br>——————————</p><ul><li><code>dtype</code> 参数的数据类型</li><li><code>grad_req</code></li><li><code>shape</code> 参数的尺寸</li></ul></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型参数-访问</span></h5><div class=card><div class=card-body><p><code>ToTensor</code>：将图像数据从uint8格式变换成32位浮点数格式，并除以255使得所有像素的数值均在0到1之间</p><code>transform_first函数</code>：数据集的函数。将<code>ToTensor</code>的变换应用在每个数据样本（图像和标签）的第一个元素，即图像之上.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
<span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> nn

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>网络设计</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
<span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> nn
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型初始化</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
<span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> nn
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型初始化</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> nd
<span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> nn
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>实例-单层感知机</span></h5><div class=card><div class=card-body><p><img src=/datasets/posts/dp_summary/single_perceptron.jpg alt=单层感知机></p><p>模型：o = w<sub>1</sub>*x<sub>1</sub> + w<sub>2</sub>*x<sub>2</sub> + b</p></p><p>输出<code>o</code>作为线性回归的输出，输入层是2维特征；输入层不涉及计算，该神经网络只有输出层1层。</p><code>神经元</code>：输出层中负责计算o的单元。</p>该神经元，依赖于输入层的全部特征，也就是说输出层中的神经元和输入层中各个输入完全连接，所以，这里的输出层又叫作<code>全连接层(fully connected layer)</code>或者<code>稠密层(dense layer)</code></p></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>生成数据集</span></h5><div class=card><div class=card-body><p>目标： o = 2<em>x<sub>1</sub> - 3.4</em>x<sub>2</sub> + 4.2 其中：
样本集：features: [w<sub>1</sub>, w<sub>2</sub>]， labels: [真实值+噪声]</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> IPython <span style=color:#f92672>import</span> display
<span style=color:#f92672>from</span> matplotlib <span style=color:#f92672>import</span> pyplot <span style=color:#66d9ef>as</span> plt
<span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> autograd, nd
<span style=color:#f92672>import</span> random

num_inputs <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>
num_examples <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span>
true_w <span style=color:#f92672>=</span> [<span style=color:#ae81ff>2</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>3.4</span>]
true_b <span style=color:#f92672>=</span> <span style=color:#ae81ff>4.2</span>
features <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(scale<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, shape<span style=color:#f92672>=</span>(num_examples, num_inputs))
labels <span style=color:#f92672>=</span> true_w[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>*</span> features[:, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>+</span> true_w[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>*</span> features[:, <span style=color:#ae81ff>1</span>] <span style=color:#f92672>+</span> true_b
labels <span style=color:#f92672>+=</span> nd<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(scale<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, shape<span style=color:#f92672>=</span>labels<span style=color:#f92672>.</span>shape)

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>读取数据集 - 从零实现</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>data_iter</span>(batch_size, features, labels):
    num_examples <span style=color:#f92672>=</span> len(features)
    indices <span style=color:#f92672>=</span> list(range(num_examples))
    random<span style=color:#f92672>.</span>shuffle(indices)  <span style=color:#75715e># 样本的读取顺序是随机的</span>
    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, num_examples, batch_size):
        j <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>array(indices[i: min(i <span style=color:#f92672>+</span> batch_size, num_examples)])
        <span style=color:#75715e># take函数根据索引返回对应元素</span>
        <span style=color:#66d9ef>yield</span> features<span style=color:#f92672>.</span>take(j), labels<span style=color:#f92672>.</span>take(j)  

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>读取数据集 - Gluon实现</span></h5><div class=card><div class=card-body><p><code>DataLoader</code> 返回一个迭代器，一次返回batch_size个样本</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> data <span style=color:#66d9ef>as</span> gdata

batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
<span style=color:#75715e># 将训练数据的特征和标签组合</span>
dataset <span style=color:#f92672>=</span> gdata<span style=color:#f92672>.</span>ArrayDataset(features, labels)
<span style=color:#75715e># 随机读取小批量</span>
data_iter <span style=color:#f92672>=</span> gdata<span style=color:#f92672>.</span>DataLoader(dataset, batch_size, shuffle<span style=color:#f92672>=</span>True)

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型定义 - 从零实现</span></h5><div class=card><div class=card-body><p>手动定义模型参数，一定要开辟存储梯度的内存。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 将权重初始化为：均值为0、标准差为0.01的正太随机数</span>
w <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(scale<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, shape<span style=color:#f92672>=</span>(num_inputs, <span style=color:#ae81ff>1</span>))
b <span style=color:#f92672>=</span> nd<span style=color:#f92672>.</span>zeros(shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>,))
<span style=color:#75715e># 开辟存储梯度的内存</span>
w<span style=color:#f92672>.</span>attach_grad()
b<span style=color:#f92672>.</span>attach_grad()
<span style=color:#75715e># 定义线性回归的模型</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>linreg</span>(X, w, b): 
    <span style=color:#66d9ef>return</span> nd<span style=color:#f92672>.</span>dot(X, w) <span style=color:#f92672>+</span> b



</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型定义 - Gluon实现</span></h5><div class=card><div class=card-body><p><code>Gluon</code>模块：提供了大量预定义的层。<code>nn</code>模块(neural networks)的缩写，所以里面定义了大量神经网络 的层。
<code>Sequential</code>：可以看作是一个 串联各个层的容器，在构建模型时，在该容器中一次添加层。当给定输入数据时，
容器中的每一层的输出作为下一层的输入。</p><code>init</code>模块：initializer的缩写。该模块提供了模型参数初始化的各种方法。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> nn
<span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> init
<span style=color:#75715e># 定义模型</span>
net <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential()
net<span style=color:#f92672>.</span>add(nn<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>1</span>))
<span style=color:#75715e># 初始化模型参数</span>
net<span style=color:#f92672>.</span>initialize(init<span style=color:#f92672>.</span>Normal(sigma<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>))
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>定义损失函数 - 从零实现</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>squared_loss</span>(y_hat, y):
    <span style=color:#66d9ef>return</span> (y_hat <span style=color:#f92672>-</span> y<span style=color:#f92672>.</span>reshape(y_hat<span style=color:#f92672>.</span>shape)) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>/</span> <span style=color:#ae81ff>2</span>
loss <span style=color:#f92672>=</span> squared_loss
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>定义损失函数 - Gluon实现</span></h5><div class=card><div class=card-body><p><code>Gluon</code>中的<code>loss</code>模块：定义了各种损失函数。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> loss <span style=color:#66d9ef>as</span> gloss
loss <span style=color:#f92672>=</span> gloss<span style=color:#f92672>.</span>L2Loss()  <span style=color:#75715e># 平方损失又称L2范数损失</span>
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>定义优化算法 - 从零实现</span></h5><div class=card><div class=card-body><p><code>param.grad</code>自动求梯度模块计算得来的梯度是一个批量样本的梯度和。在迭代模型参数时，需要除以批量大小来得到平均值。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sgd</span>(params, lr, batch_size):
    <span style=color:#66d9ef>for</span> param <span style=color:#f92672>in</span> params:
        param[:] <span style=color:#f92672>=</span> param <span style=color:#f92672>-</span> lr <span style=color:#f92672>*</span> param<span style=color:#f92672>.</span>grad <span style=color:#f92672>/</span> batch_size

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>定义优化算法 - Gluon实现</span></h5><div class=card><div class=card-body><p><code>Gluon</code>模块中的<code>Trainer</code>类，用来迭代模型中的全部参数。这些参数可以通过<code>collect_params</code>函数获取。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> Trainer
trainer <span style=color:#f92672>=</span> Trainer(net<span style=color:#f92672>.</span>collect_params(), <span style=color:#e6db74>&#39;sgd&#39;</span>, {<span style=color:#e6db74>&#39;learning_rate&#39;</span>: <span style=color:#ae81ff>0.03</span>})
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>训练模型 - 从零实现</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>lr <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.03</span>
num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
net <span style=color:#f92672>=</span> linreg
<span style=color:#75715e># 训练模型一共需要num_epochs个迭代周期</span>
<span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(num_epochs): 
    <span style=color:#75715e># 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）</span>
    <span style=color:#75715e># x和y分别是小批量样本的特征和标签</span>
    <span style=color:#66d9ef>for</span> X, y <span style=color:#f92672>in</span> data_iter(batch_size, features, labels):
        <span style=color:#66d9ef>with</span> autograd<span style=color:#f92672>.</span>record():
            l <span style=color:#f92672>=</span> loss(net(X, w, b), y)  <span style=color:#75715e># l是有关小批量X和y的损失</span>
        l<span style=color:#f92672>.</span>backward()  <span style=color:#75715e># 小批量的损失对模型参数求梯度</span>
        sgd([w, b], lr, batch_size)  <span style=color:#75715e># 使用小批量随机梯度下降迭代模型参数</span>
    train_l <span style=color:#f92672>=</span> loss(net(features, w, b), labels)
    <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;epoch </span><span style=color:#e6db74>%d</span><span style=color:#e6db74>, loss </span><span style=color:#e6db74>%f</span><span style=color:#e6db74>&#39;</span> <span style=color:#f92672>%</span> (epoch <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>, train_l<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>asnumpy()))

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>训练模型 - Gluon实现</span></h5><div class=card><div class=card-body><p>通过<code>Trainer</code>实例的<code>step</code>函数来迭代模型参数。由于loss是长度为batch_size的向量，在执行l.backward()时，
等价于执行l.sum().backward()。所以要用batch_size做平均。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
<span style=color:#75715e># 训练模型一共需要num_epochs个迭代周期</span>
<span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>, num_epochs <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>):
    <span style=color:#75715e># 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）</span>
    <span style=color:#75715e># x和y分别是小批量样本的特征和标签</span>
    <span style=color:#66d9ef>for</span> X, y <span style=color:#f92672>in</span> data_iter:
        <span style=color:#66d9ef>with</span> autograd<span style=color:#f92672>.</span>record():
            l <span style=color:#f92672>=</span> loss(net(X), y)  <span style=color:#75715e># l是有关小批量X和y的损失</span>
        l<span style=color:#f92672>.</span>backward() <span style=color:#75715e># 等价于l.sum().backward()</span>
        trainer<span style=color:#f92672>.</span>step(batch_size)  <span style=color:#75715e># 指定batch_size，从而对批量样本梯度求平均</span>
    l <span style=color:#f92672>=</span> loss(net(features), labels)
    <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;epoch </span><span style=color:#e6db74>%d</span><span style=color:#e6db74>, loss: </span><span style=color:#e6db74>%f</span><span style=color:#e6db74>&#39;</span> <span style=color:#f92672>%</span> (epoch, l<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>asnumpy()))

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>gluon模块-结构</span></h5><div class=card><div class=card-body>路径.mxnet/gluon/下的树状结构:<br>│　　block.py <code>类：Block, HybridBlock</code><br>│　　loss.py <code>各种loss函数</code><br>│　　parameter.py <code>类：Parameter, Constant, ParameterDict</code><br>│　　trainer.py <code>类：Trainer</code><br>│　　utils.py <code>优化操作</code><br>│　　<strong>init</strong>.py<br>│<br>├─contrib<br>│　　│<br>│　　├─cnn<br>│　　│　　└─ conv_layers.py<br>│　　├─data<br>│　　│　　└─ sampler.py<br>│　　│<br>│　　├─estimator<br>│　　│　　│　　estimator.py<br>│　　│　　└─ event_handler.py<br>│　　│<br>│　　├─nn<br>│　　│　　└─ basic_layers.py<br>│　　│<br>│　　└─rnn<br>│　　　　 │　　conv_rnn_cell.py<br>│　　　　 └─ rnn_cell.py<br>│<br>├─data <code>主要是数据处理操作</code><br>│　　│　　dataloader.py <code>类：DataLoader</code><br>│　　│　　dataset.py <code>常用类: ArrayDataset</code><br>│　　│　　sampler.py<br>│　　│<br>│　　└─vision<br>│　　　　 │　　datasets.py <code>可用的数据集-各个类</code><br>│　　　　 └─ transforms.py <code>数据预处理-各个类</code><br>│<br>├─model_zoo<br>│　　│　　model_store.py<br>│　　│<br>│　　└─vision<br>│　　　　 │　　alexnet.py<br>│　　　　 │　　densenet.py<br>│　　　　 │　　inception.py<br>│　　　　 │　　mobilenet.py<br>│　　　　 │　　resnet.py<br>│　　　　 │　　squeezenet.py<br>│　　　　 └─ vgg.py<br>├─nn <code>网络结构</code><br>│　　│　　activations.py <code>定义了各种激活层</code><br>│　　│　　basic_layers.py <code>定义了网络的基础层,例如：BN,Dropout等</code><br>│　　└─ conv_layers.py <code>定义了各种卷积池化层等</code><br>│<br>└─rnn<br>　　 │　　rnn_cell.py<br>　　 └─ rnn_layer.py</div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>gluon模块-导入</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># data</span>
<span style=color:#f92672>from</span> mxnet.gluon.data <span style=color:#f92672>import</span> ArrayDataset, DataLoader
<span style=color:#f92672>from</span> mxnet.gluon.data.vision.transforms <span style=color:#f92672>import</span> ToTensor, Normalize
<span style=color:#75715e># nn</span>
<span style=color:#f92672>from</span> mxnet.gluon.nn <span style=color:#f92672>import</span> Block, HybridBlock, Sequential, HybridSequential, Dropout, BatchNorm, Dense, PReLU, Conv2D
<span style=color:#75715e># 模型参数</span>
<span style=color:#f92672>from</span> mxnet.gluon.parameter <span style=color:#f92672>import</span> Parameter, Constant, ParameterDict
<span style=color:#75715e># 训练</span>
<span style=color:#f92672>from</span> mxnet.gluon.trainer <span style=color:#f92672>import</span> Trainer
<span style=color:#75715e># 损失函数</span>
<span style=color:#f92672>from</span> mxnet.gluon. <span style=color:#f92672>import</span> loss        <span style=color:#75715e># 损失函数 [&#39;Loss&#39;, &#39;L2Loss&#39;, &#39;L1Loss&#39;, &#39;SigmoidBinaryCrossEntropyLoss&#39;, &#39;SigmoidBCELoss&#39;, &#39;SoftmaxCrossEntropyLoss&#39;, &#39;SoftmaxCELoss&#39;, &#39;KLDivLoss&#39;, &#39;CTCLoss&#39;, &#39;HuberLoss&#39;, &#39;HingeLoss&#39;, &#39;SquaredHingeLoss&#39;, &#39;LogisticLoss&#39;, &#39;TripletLoss&#39;, &#39;PoissonNLLLoss&#39;, &#39;CosineEmbeddingLoss&#39;]</span>

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>数据集 - data</span></h5><div class=card><div class=card-body><p><code>ToTensor</code>：将图像数据从uint8格式变换成32位浮点数格式，并除以255使得所有像素的数值均在0到1之间</p><code>transform_first函数</code>：数据集的函数。将<code>ToTensor</code>的变换应用在每个数据样本（图像和标签）的第一个元素，即图像之上.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> data <span style=color:#66d9ef>as</span> gdata
batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>256</span>
transformer <span style=color:#f92672>=</span> gdata<span style=color:#f92672>.</span>vision<span style=color:#f92672>.</span>transforms<span style=color:#f92672>.</span>ToTensor()
<span style=color:#66d9ef>if</span> sys<span style=color:#f92672>.</span>platform<span style=color:#f92672>.</span>startswith(<span style=color:#e6db74>&#39;win&#39;</span>):
    num_workers <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>  <span style=color:#75715e># 0表示不用额外的进程来加速读取数据</span>
<span style=color:#66d9ef>else</span>:
    num_workers <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>

train_iter <span style=color:#f92672>=</span> gdata<span style=color:#f92672>.</span>DataLoader(mnist_train<span style=color:#f92672>.</span>transform_first(transformer),
                              batch_size, 
                              shuffle<span style=color:#f92672>=</span>True,
                              num_workers<span style=color:#f92672>=</span>num_workers)
test_iter <span style=color:#f92672>=</span> gdata<span style=color:#f92672>.</span>DataLoader(mnist_test<span style=color:#f92672>.</span>transform_first(transformer),
                             batch_size, 
                             shuffle<span style=color:#f92672>=</span>False,
                             num_workers<span style=color:#f92672>=</span>num_workers)

</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>模型初始化 - init</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet <span style=color:#f92672>import</span> init
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>损失函数 - loss</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> loss <span style=color:#66d9ef>as</span> gloss
<span style=color:#75715e># 平方损失又称L2范数损失</span>
loss <span style=color:#f92672>=</span> gloss<span style=color:#f92672>.</span>L2Loss()
<span style=color:#75715e># 包含了softmax运算和交叉熵损失运算</span>
loss <span style=color:#f92672>=</span> gloss<span style=color:#f92672>.</span>SoftmaxCrossEntropyLoss()
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>优化算法 - Trainer</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> mxnet.gluon <span style=color:#f92672>import</span> Trainer
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>Depth First Search(DFS)遍历</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># -*- coding: utf-8 -*-</span>
<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TreeNode</span>:
    <span style=color:#66d9ef>def</span> __init__(self, value):
        self<span style=color:#f92672>.</span>value <span style=color:#f92672>=</span> value
        self<span style=color:#f92672>.</span>left <span style=color:#f92672>=</span> None
        self<span style=color:#f92672>.</span>right <span style=color:#f92672>=</span> None

<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Tree_Method</span>:

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>DFS</span>(self, root):
        <span style=color:#e6db74>&#39;&#39;&#39;
</span><span style=color:#e6db74>        深度优先遍历，即先访问根节点，然后遍历左子树接着遍历右子树。
</span><span style=color:#e6db74>        主要利用栈的特点，先将右子树压栈，再将左子树压栈，这样左子树就位于栈顶，
</span><span style=color:#e6db74>        可以结点的左子树先与右子树被遍历。
</span><span style=color:#e6db74>        &#39;&#39;&#39;</span>
 
        <span style=color:#66d9ef>if</span> root <span style=color:#f92672>==</span> None:
            <span style=color:#66d9ef>return</span> None
        stack <span style=color:#f92672>=</span> []
        <span style=color:#e6db74>&#39;&#39;&#39;用列表模仿入栈&#39;&#39;&#39;</span>
        stack<span style=color:#f92672>.</span>append(root)          
        <span style=color:#66d9ef>while</span> stack:
            <span style=color:#e6db74>&#39;&#39;&#39;将栈顶元素出栈&#39;&#39;&#39;</span>
            current_node <span style=color:#f92672>=</span> stack<span style=color:#f92672>.</span>pop()
            <span style=color:#66d9ef>print</span>(current_node<span style=color:#f92672>.</span>value, end<span style=color:#f92672>=</span><span style=color:#e6db74>&#39; &#39;</span>)
            <span style=color:#e6db74>&#39;&#39;&#39;判断该节点是否有右孩子，有就入栈&#39;&#39;&#39;</span>
            <span style=color:#66d9ef>if</span> current_node<span style=color:#f92672>.</span>right:
                stack<span style=color:#f92672>.</span>append(current_node<span style=color:#f92672>.</span>right)
            <span style=color:#e6db74>&#39;&#39;&#39;判断该节点是否有左孩子，有就入栈&#39;&#39;&#39;</span>
            <span style=color:#66d9ef>if</span> current_node<span style=color:#f92672>.</span>left:

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>preOrder</span>(self, root):
        <span style=color:#e6db74>&#39;&#39;&#39;先序遍历&#39;&#39;&#39;</span>
        <span style=color:#66d9ef>if</span> root <span style=color:#f92672>==</span> None:
            <span style=color:#66d9ef>return</span> None
        <span style=color:#66d9ef>print</span>(root<span style=color:#f92672>.</span>value)
        self<span style=color:#f92672>.</span>preOrder(root<span style=color:#f92672>.</span>left)
        self<span style=color:#f92672>.</span>preOrder(root<span style=color:#f92672>.</span>right)
 
    <span style=color:#75715e># 先序打印二叉树（非递归）</span>
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>preOrderTravese</span>(node):
        stack <span style=color:#f92672>=</span> [node]
        <span style=color:#66d9ef>while</span> len(stack) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>:
            <span style=color:#66d9ef>print</span>(node<span style=color:#f92672>.</span>val)
            <span style=color:#66d9ef>if</span> node<span style=color:#f92672>.</span>right <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> None:
                stack<span style=color:#f92672>.</span>append(node<span style=color:#f92672>.</span>right)
            <span style=color:#66d9ef>if</span> node<span style=color:#f92672>.</span>left <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> None:
                stack<span style=color:#f92672>.</span>append(node<span style=color:#f92672>.</span>left)
            node <span style=color:#f92672>=</span> stack<span style=color:#f92672>.</span>pop()
 
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>minOrder</span>(self, root):
        <span style=color:#e6db74>&#39;&#39;&#39;中序遍历&#39;&#39;&#39;</span>
        <span style=color:#66d9ef>if</span> root <span style=color:#f92672>==</span> None:
            <span style=color:#66d9ef>return</span> None
        self<span style=color:#f92672>.</span>minOrder(root<span style=color:#f92672>.</span>left)
        <span style=color:#66d9ef>print</span>(root<span style=color:#f92672>.</span>value)
        self<span style=color:#f92672>.</span>minOrder(root<span style=color:#f92672>.</span>right)
 
    <span style=color:#75715e># 中序打印二叉树（非递归）</span>
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>inOrderTraverse</span>(node):
        stack <span style=color:#f92672>=</span> []
        pos <span style=color:#f92672>=</span> node
        <span style=color:#66d9ef>while</span> pos <span style=color:#f92672>or</span> stack:
            <span style=color:#66d9ef>if</span> pos:
                stack<span style=color:#f92672>.</span>append(pos)
                pos <span style=color:#f92672>=</span> pos<span style=color:#f92672>.</span>left
            <span style=color:#66d9ef>else</span>:
                pos <span style=color:#f92672>=</span> stack<span style=color:#f92672>.</span>pop()
                <span style=color:#66d9ef>print</span>(pos<span style=color:#f92672>.</span>val)
                pos <span style=color:#f92672>=</span> pos<span style=color:#f92672>.</span>right
 
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>postOrder</span>(self, root):
        <span style=color:#e6db74>&#39;&#39;&#39;后序遍历&#39;&#39;&#39;</span>
        <span style=color:#66d9ef>if</span> root <span style=color:#f92672>==</span> None:
            <span style=color:#66d9ef>return</span> None
        self<span style=color:#f92672>.</span>postOrder(root<span style=color:#f92672>.</span>left)
        self<span style=color:#f92672>.</span>postOrder(root<span style=color:#f92672>.</span>right)
        <span style=color:#66d9ef>print</span>(root<span style=color:#f92672>.</span>value)
 
    <span style=color:#75715e># 后序打印二叉树（非递归）</span>
    <span style=color:#75715e># 使用两个栈结构</span>
    <span style=color:#75715e># 第一个栈进栈顺序：左节点-&gt;右节点-&gt;跟节点</span>
    <span style=color:#75715e># 第一个栈弹出顺序： 跟节点-&gt;右节点-&gt;左节点(先序遍历栈弹出顺序：跟-&gt;左-&gt;右)</span>
    <span style=color:#75715e># 第二个栈存储为第一个栈的每个弹出依次进栈</span>
    <span style=color:#75715e># 最后第二个栈依次出栈</span>
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>postOrderTraverse</span>(node):
        stack <span style=color:#f92672>=</span> [node]
        stack2 <span style=color:#f92672>=</span> []
        <span style=color:#66d9ef>while</span> stack:
            node <span style=color:#f92672>=</span> stack<span style=color:#f92672>.</span>pop()
            stack2<span style=color:#f92672>.</span>append(node)
            <span style=color:#66d9ef>if</span> node<span style=color:#f92672>.</span>left:
                stack<span style=color:#f92672>.</span>append(node<span style=color:#f92672>.</span>left)
            <span style=color:#66d9ef>if</span> node<span style=color:#f92672>.</span>right:
                stack<span style=color:#f92672>.</span>append(node<span style=color:#f92672>.</span>right)
        <span style=color:#66d9ef>while</span> stack2:
            <span style=color:#66d9ef>print</span>(stack2<span style=color:#f92672>.</span>pop()<span style=color:#f92672>.</span>val)
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>Breadth First Search(BFS)遍历</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># -*- coding: utf-8 -*-</span>
<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TreeNode</span>:
    <span style=color:#66d9ef>def</span> __init__(self, value):
        self<span style=color:#f92672>.</span>value <span style=color:#f92672>=</span> value
        self<span style=color:#f92672>.</span>left <span style=color:#f92672>=</span> None
        self<span style=color:#f92672>.</span>right <span style=color:#f92672>=</span> None
 
<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Tree_Method</span>:
    
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>create_tree</span>(self, arr):
        <span style=color:#e6db74>&#39;&#39;&#39;
</span><span style=color:#e6db74>        利用二叉树的三个组成部分：根节点-左子树-右子树；
</span><span style=color:#e6db74>        传入的arr是一个多维列表，每一维最大为3，
</span><span style=color:#e6db74>        每一维中的内容依次表示根节点-左子树-右子树。然后递归的进行构建
</span><span style=color:#e6db74>        &#39;&#39;&#39;</span>
 
        length <span style=color:#f92672>=</span> len(arr)  <span style=color:#75715e>#计算每一维的大小</span>
        root <span style=color:#f92672>=</span> TreeNode(arr[<span style=color:#ae81ff>0</span>]) <span style=color:#75715e>#获取每一维的根节点</span>
        <span style=color:#66d9ef>if</span> length <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>2</span>:         <span style=color:#75715e>#判断是否有左子树</span>
            root<span style=color:#f92672>.</span>left <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>create_tree(arr[<span style=color:#ae81ff>1</span>])
        <span style=color:#66d9ef>if</span> length <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>3</span>:         <span style=color:#75715e>#判断是否有右子树</span>
            root<span style=color:#f92672>.</span>right <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>create_tree(arr[<span style=color:#ae81ff>2</span>])
        <span style=color:#66d9ef>return</span> root
 
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>BFS</span>(self, root):
        <span style=color:#e6db74>&#39;&#39;&#39;
</span><span style=color:#e6db74>        广度优先遍历，即从上到下，从左到右遍历。
</span><span style=color:#e6db74>        主要利用队列先进先出的特性，入队的时候，是按根左右的顺序，那么只要按照这个顺序出队就可以了
</span><span style=color:#e6db74>        &#39;&#39;&#39;</span>
 
        <span style=color:#66d9ef>if</span> root <span style=color:#f92672>==</span> None:
            <span style=color:#66d9ef>return</span> None
        queue <span style=color:#f92672>=</span> []
        <span style=color:#e6db74>&#39;&#39;&#39;用列表模仿入队&#39;&#39;&#39;</span>
        queue<span style=color:#f92672>.</span>append(root)          
        <span style=color:#66d9ef>while</span> queue:
            <span style=color:#e6db74>&#39;&#39;&#39;将队首元素出栈&#39;&#39;&#39;</span>
            current_node <span style=color:#f92672>=</span> queue<span style=color:#f92672>.</span>pop(<span style=color:#ae81ff>0</span>)
            <span style=color:#66d9ef>print</span>(current_node<span style=color:#f92672>.</span>value, end<span style=color:#f92672>=</span><span style=color:#e6db74>&#39; &#39;</span>)
            <span style=color:#e6db74>&#39;&#39;&#39;判断该节点是否有左孩子，有就入队&#39;&#39;&#39;</span>
            <span style=color:#66d9ef>if</span> current_node<span style=color:#f92672>.</span>left:
                queue<span style=color:#f92672>.</span>append(current_node<span style=color:#f92672>.</span>left)
            <span style=color:#e6db74>&#39;&#39;&#39;判断该节点是否有右孩子，有就入队&#39;&#39;&#39;</span>
            <span style=color:#66d9ef>if</span> current_node<span style=color:#f92672>.</span>right:
                queue<span style=color:#f92672>.</span>append(current_node<span style=color:#f92672>.</span>right)
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>torch模块-样例</span></h5><div class=card><div class=card-body><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> torch
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>分治法</span></h5><div class=card><div class=card-body><p>分治法(divide and conquer)：</p><ol><li>待解决复杂问题，能够简化为若干个小规模相同的问题，<code>各个子问题独立存在</code>，并且与原问题形式相同；</li><li>递归地解决各个子问题；</li><li>将各个子问题的解合并，得到原问题的解。</li></ol><p><strong>示例：</strong><br>归并排序</p><p align=center><img src=/datasets/note/merge_sort.png width=100% height=100% title=merge_sort alt=merge_sort></p></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>动态规划</span></h5><div class=card><div class=card-body><p><strong>与分治法的不同</strong>：<br>动态规划与分治法相似，都是组合子问题的解来解决原问题，与分治法的不同在于：</p><ol><li>分治法：将原问题划分为一个个<code>不相交</code>的子问题（比如：归并排序，将数组不断地划分为一个个的子数组进行排序，再将返回的两个有序数组进行合并排序）</li><li>动态规划：要解决的是<code>子问题有重叠</code>的问题，例如0-1背包问题。即：不同的子问题有公共的子子问题，这些重叠的子问题在动态规划中是不应该也不需要重新计算的，而是应该将其解以一定方式保存起来，提供给父问题使用。</li></ol><p><strong>设计步骤</strong>：<br>动态规划通常用来求解<code>最优解问题</code>，这类问题会有很多个解，每个解都对应一个值，而我们则希望在这些解中找到最优解（最大值或者最小值）。
通常四个步骤设计一个动态规划算法：</p><ol><li>定义dp数组以及下标的含义；</li><li>推导出：<code>递推公式</code></li><li>dp数组的初始化</li><li>遍历顺序</li><li>打印出dp数组</li></ol><p><strong>实现方法</strong>：</p><ol><li>递归，属于自顶向下的计算方法：如果子问题有重复计算的情况下，需要一个<code>备忘录</code>来辅助实现，<code>备忘录</code>主要用来保存每一个子问题的解，当每个子问题只求一次，如果后续需要子问题的解，只需要查找备忘录中保存的结果，不必重复计算。</li><li>动态规划，属于自底向上的计算方法：此方法最常用，必须明确每个子问题规模的概念，使得任何子问题的求解都依赖于子子问题的解来进行求解。</li></ol><p><strong>示例：</strong><br><a href=https://www.bilibili.com/read/cv12924751 target=blank>0-1背包问题</a></p></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>贪心算法</span></h5><div class=card><div class=card-body><p><strong>贪心算法</strong>：在对问题求解时，总是做出在当前看来是做好的选择。即：<code>当考虑做何种选择的时候，我们只考虑对当前问题最佳的选择而不考虑子问题的结果，这是贪心算法可行的第一个基本要素</code>。不从整体最优上考虑，而是仅仅在某种意义上的局部最优解。贪心算法以迭代的方式作出相继的贪心选择，每做一次贪心选择就将问题简化为规模更小的子问题。</p><p><strong>何时采用贪心算法</strong>：对于一个具体问题，要确定它是否具有贪心选择性质，必须证明<code>每一步所作的贪心选择最终导致问题的整体最优解。</code></p><p><strong>示例：</strong><br>完全背包问题、均分纸牌、最大整数</p><p>实际上，贪心算法适用的情况很少。需要先证明：<code>局部最优解会得出整体最优解</code>，才可以使用。一旦证明能成立，它就是一种高效的算法。<br>例如【0-1背包问题】：即：对于每个物品，要么装要么不装(0或1)<br>有一个背包，背包容量是M=150。有7个物品，物品可以分割成任意大小。要求尽可能让装入背包中的物品总价值最大，但不能超过总容量。<br>物品： A B C D E F G<br>重量： 35 30 60 50 40 10 25<br>价值： 10 40 30 50 35 40 30<br>目标函数： ∑pi最大<br></p><p>利用贪心算法，可以这样：</p><ol><li>每次挑选价值最大的物品装入背包，（是否是最优解？）</li><li>每次选择重量最小的物品装入背包，（是否是最优解？）</li><li>每次选择单位重量价值最大的物品，（是否是最优解？）</li></ol><p>上面的3中贪心策略，都无法成立，所以不能采用贪心算法。所以，<code>贪心算法虽然简单高效，但是能证明可以使用该算法的场景比较少。</code></p></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>回溯法</span></h5><div class=card><div class=card-body><p><strong>回溯法：</strong>
是一种类似枚举的搜索尝试过程，在搜索尝试过程中寻找问题的解，当发现已不满足条件时，就<code>回溯</code>返回，尝试别的路径。<br>回溯法是一种选优搜索法，通常是创建一棵树，从根节点出发，按照<code>深度优先搜索</code>的策略进行搜索，到达某一节点后，搜索该节点是否包含该问题的解：</p><ul><li>设计状态：表示求解问题的不同阶段，在回溯的时候，要有<code>状态重置</code></li><li>如果包含，则进入下一个节点进行搜索；</li><li>如果不包含，则<code>回溯</code>到父节点选择其他支路进行搜索。</li></ul><p><strong>何时采用回溯算法：</strong> 必须有标志性操作——<code>搜索时不满足条件就剪枝 + 所有解</code></p><p><strong>设计步骤</strong>:<br></p><ol><li>针对所给的原问题，定义问题的解空间，设计状态，用于记录不同阶段</li><li>确定易于搜索的解空间结构；</li><li>以深度优先搜索解空间，并在搜索过程中用剪枝函数除去无效搜索。</li></ol><p><strong>示例：</strong><br><code>全排列</code>、旅行商问题、八皇后问题<br>例如：<code>全排列</code></p><p align=center><img src=/datasets/note/permute.png width=100% height=100% title=全排列 alt=全排列></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Solution</span>(object):
    <span style=color:#75715e># 方法二：回溯：深度遍历dfs</span>
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>permute</span>(self, nums):
        n <span style=color:#f92672>=</span> len(nums)
        rst <span style=color:#f92672>=</span> []
        <span style=color:#75715e># 状态used，记录该元素是否已经被使用过</span>
        used <span style=color:#f92672>=</span> [False] <span style=color:#f92672>*</span> n
        <span style=color:#75715e># 已经处理的</span>
        path <span style=color:#f92672>=</span> []

        <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>mute</span>(nums, depth):
            <span style=color:#66d9ef>if</span> depth <span style=color:#f92672>==</span> n:
                rst<span style=color:#f92672>.</span>append([i <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> path])
            <span style=color:#66d9ef>else</span>:
                <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(n):
                    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> used[i]:
                        <span style=color:#75715e># 更新状态</span>
                        path<span style=color:#f92672>.</span>append(nums[i])
                        used[i] <span style=color:#f92672>=</span> True
                        <span style=color:#75715e># 深度搜索</span>
                        mute(nums, depth<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)
                        <span style=color:#75715e># 回溯，把状态重置</span>
                        used[i] <span style=color:#f92672>=</span> False
                        path<span style=color:#f92672>.</span>pop()
        mute(nums, <span style=color:#ae81ff>0</span>)
        <span style=color:#66d9ef>return</span> rst
</code></pre></div></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>分支限界法</span></h5><div class=card><div class=card-body><p><strong>分支限界法(branch and bound method)：</strong> 和回溯法类似，也是一种搜索算法，与回溯法不同的是：</p><ol><li>回溯法：找出问题的许多解；通常用<code>深度优先</code>的方式搜索解空间树；</li><li>分支限界法：找出原问题的一个解，或者 在满足约束条件的解中找出使某一目标函数的极大解/极小解。通常以<code>广度优先或最小耗费优先</code>的方式搜索解空间树。</li></ol><p>在当前节点(<code>扩展节点</code>)处，生成其所有的子节点(分支)，然后再从当前节点的子节点表中选择下一个<code>扩展节点</code>。为了有效地选择下一个<code>扩展节点</code>，加速搜索的进程，在每个节点处，计算一个<code>限界</code>，从其子节点表中选择一个最有利的节点作为<code>扩展节点</code>，使搜索朝着解空间上最优解的分支推进。</p><p><strong>何时采用分支界限法：</strong> 必须有标志性操作——<code>搜索时不满足限界就剪枝 + 最优解</code></p><p><strong>示例：</strong><br>0-1背包问题：<code>限界</code>就是背包的大小，一个节点的子节点表中，如果有超过<code>限界</code>的就直接剪枝。如下图所示：</p><p align=center><img src=/datasets/note/branch_bound.jpg width=100% height=100% title=branch_bound alt=branch_bound></p></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>数据结构</span></h5><div class=card><div class=card-body><p>常见的数据结构可分为「线性数据结构」与「非线性数据结构」，具体为：「数组」、「链表」、「栈」、「队列」、「树」、「图」、「散列表」、「堆」。</p><p align=center><img src=/datasets/note/data-struct.png width=80% height=80% title=data-struct alt=data-struct></p></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>散列表</span></h5><div class=card><div class=card-body><p>散列表是一种非线性数据结构，通过利用 Hash 函数将指定的「键 key」映射至对应的「值 value」，以实现高效的元素查找。<br>比如：通过输入学号，在名字库里找到对应的名字。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 输入：学号</span>
<span style=color:#75715e># 小力: 10001</span>
<span style=color:#75715e># 小特: 10002</span>
<span style=color:#75715e># 小扣: 10003</span>
<span style=color:#75715e># 名字库</span>
names <span style=color:#f92672>=</span> [ <span style=color:#e6db74>&#34;小力&#34;</span>, <span style=color:#e6db74>&#34;小特&#34;</span>, <span style=color:#e6db74>&#34;小扣&#34;</span> ]
<span style=color:#75715e># Hash函数的目的：把学号，映射为序号index，</span>
<span style=color:#75715e># 这个序号index就是 名字库names的名字对应序号</span>
</code></pre></div><p align=center><img src=/datasets/note/hash.png width=100% height=100% title=hash alt=hash></p></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>堆</span></h5><div class=card><div class=card-body><p>堆是一种基于「完全二叉树」的数据结构，可使用数组实现。以堆为原理的排序算法称为「堆排序」，基于堆实现的数据结构为「优先队列」。堆分为「大顶堆」和「小顶堆」，大（小）顶堆：任意节点的值不大于（小于）其父节点的值。</p><p>完全二叉树定义： 设二叉树深度为 k，若二叉树除第 k 层外的其它各层（第 1 至 k−1 层）的节点达到最大个数，且处于第 k 层的节点都连续集中在最左边，则称此二叉树为完全二叉树。</p><p align=center><img src=/datasets/note/heap.png width=30% height=30% title=heap alt=heap></p><p>上图就是一个「小顶堆」，堆的操作：</p><ol><li>搜索：$O(1)$，就是访问 堆顶的元素。</li><li>添加：就是要满足堆的定义：<code>任意节点的值不大于（小于）其父节点的值。</code></li><li>删除：跟添加一样，就是要满足堆的定义：<code>任意节点的值不大于（小于）其父节点的值。</code></li></ol></div></div></div></div><div class=note-card><div class=item><h5 class=note-title><span>滑动窗口算法</span></h5><div class=card><div class=card-body><p><a href=https://www.cnblogs.com/huansky/p/13488234.html target=blank>参考</a></p><p>滑动窗口算法：是在给定特定窗口大小的数组或字符串上执行要求的操作，该技术可以将一部分问题中的嵌套循环转变为一个单循环，可以减少时间复杂度。即：在一个特定大小的字符串/数组上进行操作，而不是在整个字符串/数组上操作，这样就降低了问题的复杂度。<br>滑动：说明这个窗口是移动的；<br>窗口：窗口大小并不是固定的，可以不断扩容直到满足一定的条件；也可以不断缩小，直到找到一个满足条件的最小窗口；也可以是固定大小。<br></p><p>滑动窗口算法的思路：<br></p><ol><li>我们在字符串 S 中使用双指针中的左右指针技巧，初始化 left = right = 0，把索引闭区间 [left, right] 称为一个「窗口」。</li><li>我们先不断地增加 right 指针扩大窗口 [left, right]，直到窗口中的字符串符合要求（包含了 T 中的所有字符）。</li><li>此时，我们停止增加 right，转而不断增加 left 指针缩小窗口 [left, right]，直到窗口中的字符串不再符合要求（不包含 T 中的所有字符了）。同时，每次增加 left，我们都要更新一轮结果。</li><li>重复第 2 和第 3 步，直到 right 到达字符串 S 的尽头。</li></ol><p><code>对于固定窗口大小，框架总结如下：</code></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 固定窗口大小为k</span>
<span style=color:#75715e># 在s中 寻找窗口大小为k时的所包含最大元音字母个数</span>
right <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
<span style=color:#66d9ef>while</span> right<span style=color:#f92672>&lt;</span>len(s):
  window<span style=color:#f92672>.</span>append(s[right])
  right <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
  <span style=color:#75715e># 如果符合要求，说明窗口构造完成</span>
  <span style=color:#66d9ef>if</span> right<span style=color:#f92672>&gt;=</span>k:
    <span style=color:#75715e># 这已经是一个窗口了，根据条件做一些事情 ... 可以计算窗口最大值</span>
    <span style=color:#75715e># 最后不要忘记把 【right-k】位置元素从窗口里移除</span>
</code></pre></div><p><code>对于不固定窗口大小，框架总结如下：</code></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 在s中寻找 t 的 最小覆盖子串</span>
left, right <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>
<span style=color:#66d9ef>while</span> right<span style=color:#f92672>&lt;</span>len(s):
  right <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
  <span style=color:#75715e># 如果符合要求，说明窗口构造完成，移动left缩小窗口</span>
  <span style=color:#66d9ef>while</span> <span style=color:#e6db74>&#39;符合要求&#39;</span>:
    <span style=color:#75715e># 如果这个窗口的子串更短，则更新res</span>
    res <span style=color:#f92672>=</span> minLen(res, windown)
    window<span style=color:#f92672>.</span>remove(left)
    left <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
<span style=color:#66d9ef>return</span> res
</code></pre></div></div></div></div></div></div><div class=paginator><ul class=pagination><li class=page-item><a href=/zh-cn/notes/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class="page-item disabled"><a class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class="page-item active"><a class=page-link href=/zh-cn/notes/>1</a></li><li class=page-item><a class=page-link href=/zh-cn/notes/page/2/>2</a></li><li class=page-item><a class=page-link href=/zh-cn/notes/page/3/>3</a></li><li class=page-item><a href=/zh-cn/notes/page/2/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/zh-cn/notes/page/3/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>导航</h5><ul><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#about>关于</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#skills>技能</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#experiences>经验</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#projects>项目</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#education>教育</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#recent-posts>最新博客</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#accomplishments>造诣</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#achievements>业绩</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>联系方式:</h5><ul><li><span>电话:</span> <span>+0123456789</span></li><li><span>邮箱:</span> <span>bsgshyn8@163.com</span></li></ul></div></div></div><hr><div class=container><p id=disclaimer><strong>免责声明:</strong> 这个主题是MIT许可的。因此，您可以将其用于非商业、商业或者私人用途。您可以修改或分发主题，而无须经 作者任何许可。但是，主题作者不对主题的任何问题提供任何保证或承担任何责任。</p></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/imagesloaded.pkgd.min.js></script><script src=/js/note.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>