<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AIGC综述 on biubiobiu's Blog</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/</link><description>Recent content in AIGC综述 on biubiobiu's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Everything is mine</copyright><lastBuildDate>Sat, 05 Aug 2023 12:30:40 +0800</lastBuildDate><atom:link href="https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/index.xml" rel="self" type="application/rss+xml"/><item><title>LLM-数据集</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0002_aigc_data/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0002_aigc_data/</guid><description>Awesome-Chinese-LLM</description></item><item><title>大模型训练框架</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0010_aigc_train_p/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0010_aigc_train_p/</guid><description>目前训练超大规模语言模型主要有两条技术路线：
TPU + XLA + TensorFlow/JAX GPU + PyTorch + Megatron-LM + DeepSpeed。 前者由Google主导，由于TPU和自家云平台GCP深度绑定，对于非Googler来说， 只可远观而不可把玩，后者背后则有NVIDIA、Meta、MS大厂加持，社区氛围活跃，也更受到群众欢迎。
一、简介 1、并行计算 模型并行：将模型参数分布到多个GPU上
数据并行(Data parallelism, DP)：复制多份模型，每个副本被放置在不同设备上，并输入数据分片。该过程是并行完成的，所有模型副本在每个训练step结束时同步。 张量并行(Tensor parallelism, TP)：这种方式，我们不把整个激活张量或者梯度张量放在单个GPU上，而是切分参数矩阵，每个GPU计算一部分。该技术有时被称为水平并行或者层内模型并行。缺点是：需要额外通信，降低计算粒度 流水线并行(Pipeline parallelism, PP)：将网络分成多段并行。这有时也称为垂直并行。缺点是：引入流水线气泡 Zero Redundancy Optimizer(ZeRO)：将参数分布到数据并行组中，计算之前先获取模型参数。缺点是：需要额外通信 为了能够提升训练的效率，目前都采用混合精度训练，然而混合精度训练，是非常不稳定的，很容易导致梯度爆炸。这个原因是：在做Forword或者Backword的时候，需要把FP32位，降低到FP16位。这个操作有可能会导致精度溢出，从而导致loss爆炸。
2、混合精度(AMP) 混合精度 (Automatically Mixed Precision, AMP)
为加速训练，模型的参数是以FP16半精度存储的； 然后，输入数据也是 FP16半精度，与模型参数 foreword计算，激活结果也是FP16半精度； 计算loss，然后backword。在backword之前，需要对loss进行缩放，让他变成Fp32位 3、训练时的空间量 a. 模型参数（parameter） 需要的空间大小：跟模型大小一致。
b. 梯度（gradient） 需要的空间大小：跟模型大小一致。
c. 中间状态 以线性层为例：
Forword: $y = Wx$ Backword: $\nabla x = W^T \nabla y, \nabla W = \nabla y x^T$ 利用梯度更新模型参数时，需要用到：模型输入、输出。所以这些数据是要一直保存，直到参数更新完毕。 需要的空间大小：</description></item><item><title>模型小型化</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0015_aigc_train_mini/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0015_aigc_train_mini/</guid><description>一、简介 目前小型化的方案：
剪枝 Network Pruning 蒸馏 Knowledge Distillation 量化 Parameter Quantization Architecture Design Dynamic Computation 1、蒸馏 Knowledge Distillation 2、量化 Parameter Quantization 3、剪枝 Network Pruning 在权重W中，有些值非常接近于0，这些值好像没有啥作用。说明这些参数是冗余的，可以去掉。
二、TensorRT</description></item><item><title>模型应用策略</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0005_aigc_application/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0005_aigc_application/</guid><description>首先区分一下：fine-tuning、prompt-tuning、instruction-tuning
fine-tuning: 一般是指：SFT（superviseed fine-tuning）全参数的微调。 prompt-tuning: 原模型冻结，只训练部分参数 instruction-tuning：原模型不冻结，训练全部参数 一、简介 要想训练一个针对特定领域的大模型，如果采用全量参数微调（Full Parameter Futuing）的方法，一方面需要大量的高质量数据集、另一方需要较高的算力，那么，有没有不需要大量算力就能在特定领域数据上对大模型进行微调的方法呢？
下面，给大家介绍几种常见的大模型微调方法：
Adapter-Tuning Prefix-Tuning Prompt-Tuning(P-Tuning)、P-Tuning v2 LoRA 对于大语言模型应用的两种不同的使用方式：
“专才”：只精通指定任务。怎么让一个基础模型在指定任务上比较精通呢？有两种方式：
加外挂：比如：在bert后面添加几个fc层，完成指定任务 fine-tune： Adapter插件：固定原来模型，添加一个额外的模型插件。例如：Bitfit、AdapterBias、Houlsby、Prefix-tuning；ControlNet， LoRA，Text Inversion “全才”：模型有各种背景知识，用户可以通过使用prompt指令，来要求模型按照指令输出。
In-context Learning Instruction tuning Chain-of-Thought Prompting APE 1、Adapter插件 github: adapter-bert 有人提出 Adaptor 的概念，在预训练的模型中加入一些叫Apt(Adaptor)的层，在微调的时候，只微调Apt层。这篇文章中，将Adapter插在Feed-forward层之后，在预训练的时候是没有Adapter的，只有在微调的时候才插进去。并且在微调的时候，只调整Adapter层的参数。 2、Prefix-tuning github: PrefixTuning 根据《Prefix-Tuning》 ，前缀调整实现了与微调所有层相当的建模性能，同时只需要训练 0.1% 的参数——实验基于 GPT-2 模型。此外，在许多情况下，前缀调整甚至优于所有层的微调，这可能是因为涉及的参数较少，这有助于减少较小​​目标数据集上的过度拟合。
思路：在原来模型前面，训练一些参数，让这些权重学习到根据prompt来控制模型的输出。 3、Prompt-tuning 4、P-tuning github: P-tuning 论文：《GPT Understands》 在原输入中添加Prompt，可能会因为添加了一些词，影响模型效果。所以作者用（BiLSTM+MLP）构建了一个prompt encoder</description></item><item><title>混合精度训练</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0012_aigc_amp/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0012_aigc_amp/</guid><description>一、简介 目前，混合精度 (Automatically Mixed Precision, AMP) 训练已经成为了炼丹师的标配工具，仅仅只需几行代码，就能让显存占用减半，训练速度加倍。 AMP 技术是由百度和 NIVDIA 团队在 2017 年提出的 (Mixed Precision Training)，该成果发表在 ICLR 上。PyTorch 1.6之前，大家都是用 NVIDIA 的 apex 库来实现 AMP 训练。1.6 版本之后，PyTorch 出厂自带 AMP。
# 原代码 output = net(input) loss = loss_fn(output, target) loss.backward() optimizer.step() optimizer.zero_grad() # 使用混合精度训练 with torch.cuda.amp.autocast(): output = net(input) loss = loss_fn(output, target) scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() optimizer.zero_grad() 半精度浮点数 (FP16)： 是一种计算机使用的二进制浮点数数据类型，使用 2 字节 (16 位) 存储。而 PyTorch 默认使用 单精度浮点数 (FP32) 来进行网络模型的计算和权重存储。FP32 在内存中用 4 字节 (32 位) 存储。</description></item><item><title>生成式-问题</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0020_aigc_error/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0020_aigc_error/</guid><description>一、简介 问题1：在文本生成是，模型会一本正经的胡说八道，这种现象叫做模型的幻觉。 问题2：训练一个大模型，需要多少数据量呢？ 问题3：数据预处理，怎么过滤、去重 问题4：模型大小 与 数据大小 的关系？ 二、模型问题 1、Calibration 问题1：在文本生成是，模型会一本正经的胡说八道，这种现象叫做模型的幻觉。
产生幻觉的原因： LLM缺乏相关知识，或者内化了错误的知识 LLM有时高估了自己的能力 问题对齐过程误导LLM进入幻觉：在对齐过程中接受针对它们在预训练阶段尚未获得的知识的指示时，实际上是一种不对齐过程，鼓励LLMs产生幻觉。 LLMs采用的生成策略存在潜在风险：LLMs有时会过分坚持早期的错误，即使它们意识到这是不正确的。换句话说，LLMs可能更喜欢为了自身一致性而堆积幻觉，而不是从错误中恢复。 减轻幻觉的方案：
整理训练集：在预训练期间减轻幻觉主要集中在预训练语料库的策划上 SFT：监督训练，构建训练数据是减轻幻觉的一种方法 RLHF：人类监督强化学习。让模型学习到：诚实性、 在推理阶段： 设计解码策略 利用外部知识来减轻LLMs中的幻觉 《Language Models (Mostly) Know What They Know》 这篇论文发现：模型够大后，说谎才会心虚。
对于大模型，模型输出是正确的概率 VS 模型的自信度，这两个是相关的。当模型比较自信时，输出的结果是正确的概率就比较大。 对于小模型，模型输出是正确的概率 VS 模型的自信度，这两个是不相关的 其中，横轴：模型输出时的自信程度；纵轴：模型输出是正确的概率。黄色表示最大模型，自身表示最小模型。 三、数据问题 问题2：训练一个大模型，需要多少数据量呢？
训练一个大模型，需要多少数据量呢？《When Do You Need Billions of Words of Pretraining Data?》 问题3：数据预处理，怎么过滤、去重?
数据预处理：《Scaling Language Models: Methods, Analysis &amp;amp; Insights from Training Gopher》 过滤有害的内容，通过Google的审核接口 去掉一些 HTML 前端的一些tag 规则过滤，去掉低质量的文本。 去重 剔除测试数据 问题4：模型大小 与 数据大小 的关系？ 《Training Compute-Optimal Large Language Models》 这篇文章发现：</description></item><item><title>生成模型-评估</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0025_aigc_eval/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0025_aigc_eval/</guid><description>一、 测评指标：
1、BLEU(Bilingual evaluation understudy) $$ BLEU = BP \times e^{\sum^n_{n=1}W_n \times \log P_n}, BP = e^{1-\frac{lr}{lc}} 如果 lc &amp;lt;= lr $$
2、PPL(perplexity) 3、ROUGE(Recall-Oriented Understudy for Gisting Evaluation) 4、Distance-base Metrics Edit Dist
二、</description></item><item><title>综述</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0001_aigc_summary/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0001_aigc_summary/</guid><description>AIGC 的技术分类按照处理的模态来看，可以分为一下几类：
文本类：
主要包括：文章生成、文本风格转换、问答对话等生成或者编辑文本内容的AIGC技术。 音频类：
包括：文本转音频、语音转换、语音属性编辑等生成或者编辑语音内容的AIGC技术；以及音乐合成、场景声音编辑等生成或者编辑非语言内容的AIGC技术。例如：智能配音主播、虚拟歌手演唱、自动配乐、歌曲生成等 图像视频类：
包括：人脸生成、人脸替换、人物属性编辑、人类操控、姿态操控等AIGC技术；以及编辑图像、视频内容、图像生成、图像增强、图像修复等AIGC技术 虚拟空间类：
主要包括：三维重建、数字仿真等AIGC技术，以及编辑数字任务、虚拟场景相关的AIGC技术，例如：元宇宙、数字孪生、游戏引擎、3D建模、VR等。 在大语言模型的训练中，如果增大数据量，相应的应该减少学习率，这个跟原来的经验相反。
模型大小与模型效果 《Emergent Abilities of Large Language Models》 这篇文章指出：随着模型大小的增大，模型效果先不会有明显提升；增加到一定程度，模型有个突然顿悟时刻。
为什么需要预训练 《Visualizing and Understanding the Effectiveness of BERT》 这篇文章指出:
首先，预训练能在下游任务中达到一个良好的初始点，与从头开始训练相比，预训练能带来更宽的最优点，更容易优化。尽管 BERT 对下游任务的参数设置过高，但微调程序对过拟合具有很强的鲁棒性。 其次，可视化结果表明，由于最佳值平坦且宽广，以及训练损失面和泛化误差面之间的一致性，微调 BERT 趋向于更好地泛化。 第三，在微调过程中，BERT 的低层更具不变性，这表明靠近输入的层学习到了更多可迁移的语言表征。 一、文本生成 1、GPT 参考
GPT-4: 参数量1800B，训练集：1.3T token 2、PaLM 《PaLM: Scaling Language Modeling with Pathways》 PaLM才是真正的“大”模型。它是迄今为止训练的最大的密集语言模型，参数为 540B，需要 6144 个 TPU 来训练（这是 3 个完整的 TPU pod，每个包含 2048 个 TPU）。这太贵了！可能只有谷歌拥有资源+基础设施来做到这一点。使用的Token高达7800亿。PaLM是使用Google新一代PathWay分布式训练框架训练出来。
与GPT-3相比的变化：
多查询注意力（Multi-query attention）：在每个注意力头中共享K/V（Key/Value）嵌入，但使用独立的Q（Query）嵌入。这样做可以在推理阶段显著提高模型的速度。 并行Transformer块：使用并行的Transformer块来提高训练时间，相较于传统的串行设计，可以减少约15%的训练时间。 SwiGLU激活函数：与GPT-3使用的GELU激活函数不同，这里采用了SwiGLU激活函数。 旋转位置编码RoPE嵌入：使用RoPE（Relative Positional Encodings）嵌入代替学习得到的嵌入方式，在长文本上具有更好的性能 。 输入-输出嵌入共享：输入和输出embedding矩阵是共享的。 无偏置向量：在mlp、normlayer等算法中，都不使用bias，对于大模型，可以提高训练稳定性。 SentencePiece与256k标记：使用SentencePiece进行分词处理，标记数量为256k。 所以，有很多变化！同样，其中很多都是常见的，例如使用 GPT-3 的学习嵌入向量已经非常过时了，现在几乎没有人这样做。</description></item></channel></rss>