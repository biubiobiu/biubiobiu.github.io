<!doctype html><html><head><title>模型应用策略</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0d88336a9cbd3e68c7714efae786b0a9_38044_42x0_resize_box_2.png><meta property="og:title" content="模型应用策略"><meta property="og:description" content="首先区分一下：fine-tuning、prompt-tuning、instruction-tuning
 fine-tuning: 一般是指：SFT（superviseed fine-tuning）全参数的微调。 prompt-tuning: 原模型冻结，只训练部分参数 instruction-tuning：原模型不冻结，训练全部参数  一、简介 要想训练一个针对特定领域的大模型，如果采用全量参数微调（Full Parameter Futuing）的方法，一方面需要大量的高质量数据集、另一方需要较高的算力，那么，有没有不需要大量算力就能在特定领域数据上对大模型进行微调的方法呢？
下面，给大家介绍几种常见的大模型微调方法：
 Adapter-Tuning Prefix-Tuning Prompt-Tuning(P-Tuning)、P-Tuning v2 LoRA  对于大语言模型应用的两种不同的使用方式：
  “专才”：只精通指定任务。怎么让一个基础模型在指定任务上比较精通呢？有两种方式：
 加外挂：比如：在bert后面添加几个fc层，完成指定任务 fine-tune： Adapter插件：固定原来模型，添加一个额外的模型插件。例如：Bitfit、AdapterBias、Houlsby、Prefix-tuning；ControlNet， LoRA，Text Inversion    “全才”：模型有各种背景知识，用户可以通过使用prompt指令，来要求模型按照指令输出。
 In-context Learning Instruction tuning Chain-of-Thought Prompting APE      1、Adapter插件  github: adapter-bert 有人提出 Adaptor 的概念，在预训练的模型中加入一些叫Apt(Adaptor)的层，在微调的时候，只微调Apt层。这篇文章中，将Adapter插在Feed-forward层之后，在预训练的时候是没有Adapter的，只有在微调的时候才插进去。并且在微调的时候，只调整Adapter层的参数。   2、Prefix-tuning  github: PrefixTuning 根据《Prefix-Tuning》 ，前缀调整实现了与微调所有层相当的建模性能，同时只需要训练 0.1% 的参数——实验基于 GPT-2 模型。此外，在许多情况下，前缀调整甚至优于所有层的微调，这可能是因为涉及的参数较少，这有助于减少较小​​目标数据集上的过度拟合。
思路：在原来模型前面，训练一些参数，让这些权重学习到根据prompt来控制模型的输出。   3、Prompt-tuning 4、P-tuning github: P-tuning 论文：《GPT Understands》 在原输入中添加Prompt，可能会因为添加了一些词，影响模型效果。所以作者用（BiLSTM+MLP）构建了一个prompt encoder"><meta property="og:type" content="article"><meta property="og:url" content="https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0005_aigc_application/"><meta property="article:published_time" content="2023-08-05T12:30:40+08:00"><meta property="article:modified_time" content="2023-08-05T12:30:40+08:00"><meta name=description content="模型应用策略"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/zh-cn><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png alt=Logo>
biubiobiu's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu547c3206082786c1d32d36034e2a655a_40863_42x0_resize_box_2.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/zh-cn/search><input type=text name=keyword placeholder=搜索 data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/zh-cn/posts data-filter=all>博文</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00020_toha-tutorial/>Toha教程</a><ul><li><a href=/zh-cn/posts/00020_toha-tutorial/0010_toha-config/ title=Toha的配置>Toha的配置</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0011_write-blogs/ title=撰写文章>撰写文章</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0013_markdown-tutorial/ title=MarkDown入门>MarkDown入门</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0015_latax_formula/ title=Katex公式>Katex公式</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0020_shortcodes_samples/ title=区域块-实例>区域块-实例</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00025_math_knowledge/>数学知识</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/>概率论</a><ul><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0010_basic-conception/ title=基本概念>基本概念</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0020_distribution_of_variables/ title=随机变量及其分布>随机变量及其分布</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0030_law_of_large_numbers/ title=大数定律>大数定律</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0040_sample_distribution/ title=样本及抽样分布>样本及抽样分布</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0050_parameter_estimation/ title=假设检验>假设检验</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0060_analysis_variance_regression/ title=方差分析及回归分析>方差分析及回归分析</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0070_stochastic_process/ title=随机过程>随机过程</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0080_markov_process/ title=马尔科夫链>马尔科夫链</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0090_stationary_stochastic_process/ title=平稳随机过程>平稳随机过程</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00025_math_knowledge/0020_math_convex_optimization_theory/>凸优化</a><ul><li><a href=/zh-cn/posts/00025_math_knowledge/0020_math_convex_optimization_theory/0010_basic_conception/ title=基本概念>基本概念</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00030_deeplearning_summary/>深度学习</a><ul><li><a href=/zh-cn/posts/00030_deeplearning_summary/0005_deeplearning_summary/ title=深度学习开篇>深度学习开篇</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0010_deeplearning_problem/ title=深度学习-结构>深度学习-结构</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0015_deeplearning_norm/ title=归一化>归一化</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0020_deeplearing_init/ title=初始化>初始化</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0100_draw_map_for_dl/ title=神经网络画图篇>神经网络画图篇</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0200_cam/ title=CAM>CAM</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00033_reinforce/>强化学习</a><ul><li><a href=/zh-cn/posts/00033_reinforce/0001_reinforce_summary/ title=综述>综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/>编程语言</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/>Python</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/0010_build_env/>env</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0010_build_env/0010_pip_env/ title=py-env>py-env</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0010_build_env/0020_cuda_env/ title=cuda>cuda</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/>Internal</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0010_encode_mode/ title=字符编码>字符编码</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0020_basic_operator/ title=基础操作>基础操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0025_internal_module/ title=内置模块>内置模块</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0030_advance_operator/ title=进阶操作>进阶操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0040_error/ title=异常>异常</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0050_file/ title=文件读取>文件读取</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/>SDK</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0020_multiprocessing/ title=并行操作>并行操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0030_importlib/ title=importlib>importlib</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0040_ipdb/ title=ipdb>ipdb</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0050_re/ title=正则-re>正则-re</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0060_heapd/ title=堆-heapq>堆-heapq</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0070_request/ title=requests>requests</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0080_logging/ title=logging>logging</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0090_argparse/ title=argparse>argparse</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0100_pil/ title=PIL>PIL</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0110_opencv/ title=OpenCV>OpenCV</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0040_tf/>TensorFlow</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0040_tf/w0010_compat/>兼容1.x</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0040_tf/w0010_compat/0010_tf_compat_summary/ title=静态图>静态图</a></li><li><a href=/zh-cn/posts/00035_programming_language/0040_tf/w0010_compat/0020_tf_compat_train/ title=模型训练>模型训练</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/>PyTorch</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0010_torch_summary/ title=简介>简介</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0020_basic/ title=基础操作>基础操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0030_mathematical/ title=数学计算>数学计算</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0040_tensor/ title=Tensor和变量>Tensor和变量</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0050_train_model/ title=模型训练>模型训练</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0060_mxnet/>MxNet</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0060_mxnet/0010_ndarray/>NdArray</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0060_mxnet/0010_ndarray/0010_ndarray_summary/ title=NdArray使用>NdArray使用</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0070_hive/>数据库</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0070_hive/0010_sql/ title=MySQL>MySQL</a></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0070_hive/0020_hive/>Hive</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0070_hive/0020_hive/0010_hive_build/ title=创建库/表>创建库/表</a></li><li><a href=/zh-cn/posts/00035_programming_language/0070_hive/0020_hive/0020_hive_func/ title=常见函数>常见函数</a></li><li><a href=/zh-cn/posts/00035_programming_language/0070_hive/0020_hive/0030_hive_common/ title=常用操作>常用操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0070_hive/0020_hive/0040_hive_datatype/ title=数据类型>数据类型</a></li><li><a href=/zh-cn/posts/00035_programming_language/0070_hive/0020_hive/0050_hive_regular/ title=正则匹配>正则匹配</a></li></ul></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/>CV</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0010_backbone/>基础</a><ul><li><a href=/zh-cn/posts/00100_cv/0010_backbone/d1_1_backbone_cnn/ title=CNN>CNN</a></li><li><a href=/zh-cn/posts/00100_cv/0010_backbone/d1_2_optimizer/ title=optimizer>optimizer</a></li><li><a href=/zh-cn/posts/00100_cv/0010_backbone/d1_3_backbone_net/ title="backbone net">backbone net</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0020_contrastive_learning/>对比学习</a><ul><li><a href=/zh-cn/posts/00100_cv/0020_contrastive_learning/contrastive_learning/ title="contrastive learning">contrastive learning</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0030_vision_transformer/>ViT</a><ul><li><a href=/zh-cn/posts/00100_cv/0030_vision_transformer/vision_transformer/ title="vision transformer">vision transformer</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0050_detect_object/>目标检测</a><ul><li><a href=/zh-cn/posts/00100_cv/0050_detect_object/object_detection_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0055_semantic_segmentation/>语义分割</a><ul><li><a href=/zh-cn/posts/00100_cv/0055_semantic_segmentation/object_detection_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0060_image-matting/>抠图</a><ul><li><a href=/zh-cn/posts/00100_cv/0060_image-matting/image-matting-summary/ title=抠图综述>抠图综述</a></li><li><a href=/zh-cn/posts/00100_cv/0060_image-matting/image-matting-animal/ title="animal matting">animal matting</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/>NLP</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0010_word_embedding/>Word Embedding</a><ul><li><a href=/zh-cn/posts/00200_nlp/0010_word_embedding/word_embedding_summary/ title="Word Embedding综述">Word Embedding综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0020_rnn/>RNN</a><ul><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/rnn_summary/ title=RNN综述>RNN综述</a></li><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/gru/ title=GRU网络>GRU网络</a></li><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/lstm/ title=LSTM网络>LSTM网络</a></li><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/encode_decode/ title=编解码架构>编解码架构</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0030_transformer/>Transformer</a><ul><li><a href=/zh-cn/posts/00200_nlp/0030_transformer/0001_attention/ title=Attention>Attention</a></li><li><a href=/zh-cn/posts/00200_nlp/0030_transformer/0010_transformer_summary/ title=Transformer>Transformer</a></li><li><a href=/zh-cn/posts/00200_nlp/0030_transformer/0030_position/ title=位置编码>位置编码</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0080_gpt/>GPT</a><ul><li><a href=/zh-cn/posts/00200_nlp/0080_gpt/0010_gpt_summary/ title=GPT综述>GPT综述</a></li><li><a href=/zh-cn/posts/00200_nlp/0080_gpt/0020_gpt1_detail/ title=GPT-1>GPT-1</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0100_bert/>Bert</a><ul><li><a href=/zh-cn/posts/00200_nlp/0100_bert/0001_bert_summary/ title=Bert综述>Bert综述</a></li><li><a href=/zh-cn/posts/00200_nlp/0100_bert/0020_bert_family/ title=Bert家族>Bert家族</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0120_t5/>T5</a><ul><li><a href=/zh-cn/posts/00200_nlp/0120_t5/0010_t5_summary/ title=T5综述>T5综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0150_bart/>BART</a><ul><li><a href=/zh-cn/posts/00200_nlp/0150_bart/bart_summary/ title=BART综述>BART综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0200_electra/>ELECTRA</a><ul><li><a href=/zh-cn/posts/00200_nlp/0200_electra/electra_summary/ title=ELECTRA综述>ELECTRA综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/1000_code/>CODE</a><ul><li><a href=/zh-cn/posts/00200_nlp/1000_code/bart_summary/ title=code解析>code解析</a></li></ul></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/posts/00300_aigc/>AIGC</a><ul class=active><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/posts/00300_aigc/0005_summary/>AIGC综述</a><ul class=active><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0001_aigc_summary/ title=综述>综述</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0002_aigc_data/ title=LLM-数据集>LLM-数据集</a></li><li><a class=active href=/zh-cn/posts/00300_aigc/0005_summary/0005_aigc_application/ title=模型应用策略>模型应用策略</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0010_aigc_train_p/ title=大模型训练框架>大模型训练框架</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0012_aigc_amp/ title=混合精度训练>混合精度训练</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0015_aigc_train_mini/ title=模型小型化>模型小型化</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0020_aigc_error/ title=生成式-问题>生成式-问题</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0025_aigc_eval/ title=生成模型-评估>生成模型-评估</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/0010_generate_text/>文本生成</a><ul><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0011_gpt/ title=GPT>GPT</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0015_llama/ title=LLaMa>LLaMa</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0020_palm/ title=PaLM>PaLM</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0025_chatglm/ title=ChatGLM>ChatGLM</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0030_claude/ title=Claude>Claude</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0035_cohere/ title=Cohere>Cohere</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0040_falcon/ title=Falcon>Falcon</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0045_vicuna/ title=Vicuna>Vicuna</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/0020_generate_image/>图像生成</a><ul><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1000_summary/ title=综述>综述</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1002_gan_summary/ title=GAN>GAN</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1005_can_summary/ title=CAN>CAN</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1010_dall_e/ title=DALL-E>DALL-E</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1015_vqgan/ title=VQGAN>VQGAN</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1020_diffusion/ title=Diffusion>Diffusion</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1025_midjourney/ title=Midjourney>Midjourney</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1030_imagen/ title=Imagen>Imagen</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00400_vlp/>多模态</a><ul><li><a href=/zh-cn/posts/00400_vlp/0001_vlp_summary/ title=简介>简介</a></li><li><a href=/zh-cn/posts/00400_vlp/0005_clip/ title=CLIP>CLIP</a></li><li><a href=/zh-cn/posts/00400_vlp/0010_mllm/ title=MLLM>MLLM</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00500_video/>视频理解</a><ul><li><a href=/zh-cn/posts/00500_video/vidio_summary/ title=简介>简介</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/john_hu7f9991f5b5d471ecdfc6cff3db1a9fe6_6397_120x120_fit_box_2.png alt="Author Image"><h5 class=author-name>biubiobiu</h5><p>August 5, 2023</p></div><div class=title><h1>模型应用策略</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/zh-cn/tags/aigc class="btn, btn-sm">aigc</a></li><li class=rounded><a href=/zh-cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B class="btn, btn-sm">大模型</a></li><li class=rounded><a href=/zh-cn/tags/%E5%BA%94%E7%94%A8%E7%AD%96%E7%95%A5 class="btn, btn-sm">应用策略</a></li></ul></div><div class=post-content id=post-content><p>首先区分一下：fine-tuning、prompt-tuning、instruction-tuning</p><ol><li>fine-tuning: 一般是指：SFT（superviseed fine-tuning）全参数的微调。</li><li>prompt-tuning: 原模型冻结，只训练部分参数</li><li>instruction-tuning：原模型不冻结，训练全部参数</li></ol><p align=center><img src=/datasets/posts/nlp/p-tuning-0.png width=100% height=100%></p><h2 id=一简介>一、简介</h2><p>要想训练一个针对特定领域的大模型，如果采用<font color=#f00000>全量参数微调（Full Parameter Futuing）</font>的方法，一方面需要大量的高质量数据集、另一方需要较高的算力，那么，有没有不需要大量算力就能在特定领域数据上对大模型进行微调的方法呢？<br>下面，给大家介绍几种常见的大模型微调方法：</p><ol><li>Adapter-Tuning</li><li>Prefix-Tuning</li><li>Prompt-Tuning(P-Tuning)、P-Tuning v2</li><li>LoRA</li></ol><div class="alert alert-success"><strong><p>对于大语言模型应用的两种不同的使用方式：</p><ol><li><p>“专才”：只精通指定任务。怎么让一个基础模型在指定任务上比较精通呢？有两种方式：</p><ol><li>加外挂：比如：在bert后面添加几个fc层，完成指定任务</li><li>fine-tune：</li><li>Adapter插件：固定原来模型，添加一个额外的模型插件。例如：Bitfit、AdapterBias、Houlsby、Prefix-tuning；ControlNet， LoRA，Text Inversion</li></ol></li><li><p>“全才”：模型有各种背景知识，用户可以通过使用prompt指令，来要求模型按照指令输出。</p><ol><li>In-context Learning</li><li>Instruction tuning</li><li>Chain-of-Thought Prompting</li><li>APE</li></ol></li></ol></strong></div><h3 id=1adapter插件>1、Adapter插件</h3><div class=row><div class="col col-sm-12 col-lg-6"><p align=center><img src=/datasets/posts/nlp/adaptor_0.png width=100% height=100%></div><div class="col col-sm-12 col-lg-6">github: <a href=https://github.com/google-research/adapter-bert target=bland>adapter-bert</a><br>有人提出 <a href=https://arxiv.org/pdf/1902.00751.pdf target=bland>Adaptor</a> 的概念，在预训练的模型中加入一些叫Apt(Adaptor)的层，在微调的时候，只微调Apt层。这篇文章中，将Adapter插在Feed-forward层之后，在预训练的时候是没有Adapter的，只有在微调的时候才插进去。并且在微调的时候，只调整Adapter层的参数。</div></div><h3 id=2prefix-tuning>2、Prefix-tuning</h3><div class=row><div class="col col-sm-12 col-lg-6"><p align=center><img src=/datasets/posts/nlp/prefix-tuning.png width=100% height=100%></div><div class="col col-sm-12 col-lg-6">github: <a href=https://github.com/XiangLi1999/PrefixTuning target=bland>PrefixTuning</a><br>根据<a href=https://arxiv.org/pdf/2101.00190.pdf target=bland>《Prefix-Tuning》</a> ，前缀调整实现了与微调所有层相当的建模性能，同时只需要训练 0.1% 的参数——实验基于 GPT-2 模型。此外，在许多情况下，前缀调整甚至优于所有层的微调，这可能是因为涉及的参数较少，这有助于减少较小​​目标数据集上的过度拟合。<br>思路：<font color=#f00000>在原来模型前面，训练一些参数，让这些权重学习到根据prompt来控制模型的输出</font>。</div></div><h3 id=3prompt-tuning>3、Prompt-tuning</h3><h3 id=4p-tuning>4、P-tuning</h3><p>github: <a href=https://github.com/THUDM/P-tuning target=bland>P-tuning</a><br>论文：<a href=https://arxiv.org/pdf/2103.10385.pdf target=bland>《GPT Understands》</a><br></p><p align=center><img src=/datasets/posts/nlp/prompt-1.png width=100% height=100%></p><p>在原输入中添加Prompt，可能会因为添加了一些词，影响模型效果。所以作者用（BiLSTM+MLP）构建了一个prompt encoder</p><p>经典的Prompt tuning方式不涉及对底层模型的任何参数更新。相反，它侧重于精心制作可以指导预训练模型生成所需输出的输入提示或模板。<br>主要结构是</p><ol><li>利用了一个prompt encoder（BiLSTM+MLP），将一些pseudo prompt先encode（离散token）</li><li>再与input embedding进行拼接，同时利用LSTM进行 Reparamerization 加速训练，并引入少量自然语言提示的锚字符（Anchor，例如Britain）进一步提升效果。</li><li>然后结合（capital，Britain）生成得到结果，再优化生成的encoder部分。</li></ol><p>但是P-tuning v1有两个显著缺点：任务不通用和规模不通用。在一些复杂的自然语言理解NLU任务上效果很差，同时预训练模型的参数量不能过小。</p><blockquote><ol><li>先将一些为prompt输入到LSTM中，用LSTM输出的向量来替换原始的Prompt token</li><li>然后一起输入到 预训练模型中</li><li>LSTM和预训练模型一起训练</li></ol></blockquote><p>github: <a href=https://github.com/THUDM/P-tuning-v2 target=bland>P-Tuning v2</a><br>论文：<a href=https://arxiv.org/pdf/2110.07602.pdf target=bland>《P-Tuning v2》</a><br></p><p align=center><img src=/datasets/posts/nlp/prompt-2.png width=100% height=100%></p><p>p-tuning存在的问题：
1.</p><blockquote><p>p-tuning的改进版：不同层中的提示作为前缀token加入到输入序列中，并独立于其他层间(而不是由之前的transformer层计算)。<br></p><ol><li>在每一层都加入了Prompts tokens作为输入，而不是仅仅加在输入层。</li><li>一方面，通过这种方式，P-tuning v2有更多的可优化的特定任务参数(从0.01%到0.1%-3%)，以允许更多的每个任务容量，而它仍然比完整的预训练语言模型小得多。</li><li>另一方面，添加到更深层的提示可以对输出预测产生更直接和重大的影响，而中间的transformer层则更少</li></ol></blockquote><h3 id=5lora>5、LoRA</h3><div class=row><div class="col col-sm-12 col-lg-6"><p align=center><img src=/datasets/posts/nlp/lora-0.png width=100% height=100%></div><div class="col col-sm-12 col-lg-6"><p>github: <a href=https://github.com/microsoft/LoRA target=bland>LoRA</a><br>根据<a href=https://arxiv.org/pdf/2106.09685.pdf target=bland>《LoRA》</a><br></p><p>特点：</p><ol><li>训练速度更快</li><li>计算量更低</li><li>训练权重更小</li></ol></div></div><h3 id=6随便选一些参数>6、随便选一些参数</h3><p><strong>实验</strong>：随便选一些参数作为需要更新的，其他的参数冻结。<br><strong>结果</strong>：发现这样的结果与 全参数训练的结果也差不多。<br></p><p><strong>说明</strong>：这个可能是因为 模型太大了。模型增大到一定程度，模型结构就不是那么重要了。</p><h2 id=二大模型-使用范式>二、大模型-使用范式</h2><h3 id=1训练时>1、训练时</h3><p><strong>Instruction Tuning 与 Prompt Tuning 的区别</strong></p><blockquote><p>Prompt Tuning<br>针对每个任务，单独生成prompt模版，然后在每个任务上进行full-shot微调与评估，其中预训练模型参数是<font color=#f00000>冻结的</font>。<br>在T5中：冻结预训练模型，只更新添加在第一层的soft prompt。在full-shot上就能和finetune上效果相当。<br>prompt tuning 的一系列方法和adapter越来越像了。</p></blockquote><hr><blockquote><p>Instruction Tuning<br>针对每个任务，单独生成 Instruction（hard token），通过在若干个full-shot任务上进行微调，然后再具体的任务上进行评估泛化能力（zero-shot）。其中预训练模型参数是<font color=#f00000>不冻结的。</font><br>这两个方法的核心点：去挖掘语言模型本身具备的知识。不同点是：<br></p><ol><li>prompt 是激发语言模型的不全能力，比如：给出上半句生成下半句、完形填空</li><li>instruction 是激发语言模型的理解能力，通过给出更明显的指令，让模型去理解并做出正确的反馈。</li></ol></blockquote><h4 id=1-prompt-tuning>1. Prompt-tuning</h4><div class=row><div class="col col-sm-12 col-lg-6"><p align=center><img src=/datasets/posts/nlp/prompt-0.png width=100% height=100%></div><div class="col col-sm-12 col-lg-6">设计prompt：<br>每个任务都会设计一种prompt。</div></div><h5 id=1-人工设计prompt>1. 人工设计prompt</h5><p>根据具体任务，人工设计prompt。。。。</p><h5 id=2-自动生成prompt>2. 自动生成prompt</h5><p><strong>APE</strong><br><a href=https://arxiv.org/pdf/2211.01910.pdf target=bland>《LARGE LANGUAGE MODELS ARE HUMAN-LEVEL PROMPT ENGINEERS》</a><br></p><p>作者用大模型自己输出有用的prompt，通过筛选后，选出效果较好的。</p><p align=center><img src=/datasets/posts/nlp/cot_prompt_2.png width=90% height=90%></p><p>整体效果如下：</p><p align=center><img src=/datasets/posts/nlp/ape_prompt.png width=90% height=90%></p><h4 id=2-instruction-tuning>2. Instruction-tuning</h4><h5 id=1-flan>1. FLAN</h5><p>来自Google公司Jaon Wei的<a href=https://arxiv.org/pdf/2109.01652.pdf target=bland>《FINETUNED LANGUAGE MODELS ARE ZERO-SHOT
LEARNERS》</a> 2021-09-03<br></p><p>论文的结论：<font color=#f00000>模型可以学习到人类设定好的指令，并根据已学到的知识，在测试时对于未见过的指令，结果有好的表现</font>。<br></p><p><strong>Motivation</strong>：通过提升语言模型对Instructions的理解能力，来提高语言模型零样本学习能力。<br></p><h5 id=2-t0>2. T0</h5><p><a href=https://arxiv.org/pdf/2110.08207.pdf target=bland>《Multitask Prompted Training Enables Zero-Shot Task Generalization》</a> 2021-10-15<br></p><blockquote><p>T0和 FLAN 工作整体相似，区别是：</p><ol><li>增加了任务（171个NLP任务） 和 prompt 数量（1939个prompt）</li><li>FLAN使用了decoder-only，T0使用了encoder+decoder</li><li>FLAN每次针对测试一个任务训练一个模型，其他任务作为训练集，T0为了测试模型泛化能力，只在多任务数据集上训练一个模型。证明了隐式多任务学习能提升模型泛化和zero-shot能力。</li></ol></blockquote><h5 id=3-rlhf>3. RLHF</h5><p><a href=https://arxiv.org/pdf/2203.02155.pdf target=bland>《Multitask Prompted Training Enables Zero-Shot Task Generalization》</a> 2022-03-04<br></p><p><strong>Motivation</strong>：使用 人类反馈的强化学习（RLHF）技术，根据用户和API的交互结果，对模型的多个输出进行排序，然后再利用这些数据微调GPT-3，使得InstructGPT模型遵循指令方面比GPT-3更好。</p><h4 id=3-delta-tuning>3. Delta-tuning</h4><p><strong>Motivation</strong>：只微调少量的参数，效果可以达到全参数微调差不多的效果。<br><strong>可以这样理解</strong>：因为有了比较好的预训练模型，后续就不许再重点学习知识点了。后续的下游任务，主要是如何激活跟该任务相关的知识点。<br><strong>实现</strong>：固定预训练模型不变，添加、修改等一些网络，训练时只更新这些网络的参数。</p><p align=center><img src=/datasets/posts/nlp/delta-tuning-0.png width=90% height=90%></p><p>构建Delta的方式：</p><ol><li>增量式（addition-based）：例如：Adapter、LoRA、prefix-Tuning</li><li>指定式（Specification-base）：例如：BitFit（只微调bias）、随便选一些参数更新</li><li>重参数化 （Reparameterization-base）：例如：降维、降秩</li></ol><h3 id=2推理时>2、推理时</h3><h4 id=1in-context-learning>1、In-Context Learning</h4><h5 id=1-解释1>1. 解释1</h5><p><a href=https://arxiv.org/abs/2202.12837 target=bland>《Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?》</a><br>In-context learning是一种学习范式，它允许语言模型通过以演示形式组织的若干个示例或者指令来学习任务。In-context learning（ICL）的核心在于<font color=#f00000>从任务相关的类比样本中学习</font>，ICL要求若干示例以特定形式进行演示，然后将当前输入x跟上述示例通过prompt拼接到一起作为语言模型的输入。本质上，它利用训练有素的语言模型根据演示的示例来估计候选答案的可能性。简单理解，<font color=#f00000>就是通过若干个完整的示例，让语言模型更好地理解当前的任务，从而做出更加准确的预测</font>。</p><p align=center><img src=/datasets/posts/nlp/in_context_2.jpg width=90% height=90%></p><p>实验结论：</p><ol><li><font color=#f00000>ICL 中Ground Truth信息无关紧要</font>。<br>作者实验对比：没有示例、多个示例-且label是一一对应的、多个示例-且label是随机的。对比发现：<ol><li>随机label 与 正确label 的效果相当，性能只下降了 $ 0 - 5\%$。</li><li>没有示例，效果下降较多。</li></ol></li></ol><p align=center><img src=/datasets/posts/nlp/in_context_1.jpg width=90% height=90%></p>2. ICL的性能收益主要来自 <font color=#f00000>独立规范的输入空间和标签空间，以及正确一致的演示格式</font>。<br>作者实验了这4个因素：输入空间、标签空间、演示格式。对比实验：把输入换成外部语料；把标签换成英语单词；缺少输入或者label。实验发现：
1. 把输入换成外部语料；把标签换成英语单词；缺少输入或者label。这些操作都会使得效果明显下降。<p align=center><img src=/datasets/posts/nlp/in_context_0.jpg width=90% height=90%></p><p><strong>个人理解</strong>：比如做情感分析，示例：输入<sep>label。这种格式很重要，label是否正确不重要。<br>大模型通过预训练，对文本时有理解能力的。ICL 的prompt中有多个 样例的格式，是让大语言模型知道当前是在做情感分析任务，而不是在做其他任务，按照情感分析的思路输出。并不是根据 prompt中的几个样例学习，而是唤醒机器要执行什么样的任务。</p><h5 id=2-解释2>2. 解释2</h5><p>来自Google公司Jaon Wei的<a href=https://arxiv.org/pdf/2303.03846.pdf target=bland>《LARGER LANGUAGE MODELS DO IN-CONTEXT
LEARNING DIFFERENTLY》</a><br>Google的这篇文章解释：大语言模型在ICL中是有学习的，解释1的结论之所以成立，是因为解释1用的模型还不够大，在更大的模型中，在ICL中的学习会表现的更为明显。</p><h4 id=2chain-of-thought-prompting>2、Chain-of-Thought Prompting</h4><p>来自Google公司Jaon Wei的<a href=https://arxiv.org/pdf/2201.11903.pdf target=bland>《Chain-of-Thought Prompting Elicits Reasoning in Large Language Models》</a><br>发现：在推理的任务中，只是给一些示例，大模型的效果不好，如果<font color=#f00000>能给到推理的思路过程</font>，大模型的效果就会有明显提升。<br></p><p align=center><img src=/datasets/posts/nlp/cot_prompt_0.png width=90% height=90%></p><p>CoT的变形：<br>来自Google公司的<a href=https://arxiv.org/pdf/2205.11916.pdf target=bland>《Large Language Models are Zero-Shot Reasoners》</a>，<a href=https://arxiv.org/pdf/2205.10625.pdf target=bland>《LEAST-TO-MOST PROMPTING ENABLES COMPLEX
REASONING IN LARGE LANGUAGE MODELS》</a><br>发现：由于这个推理思路是人工写的，这些数据量比较少，而且操作比较麻烦。作者的操作：</p><ol><li>在生成答案之前，加了一个要求：<font color=#f00000>Let&rsquo;s think step by step.</font></li><li>生成多个答案，然后投票</li></ol><p align=center><img src=/datasets/posts/nlp/cot_prompt_1.png width=90% height=90%></p></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"><strong>分享:</strong>
<a class="btn btn-sm facebook-btn" href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00300_aigc%2f0005_summary%2f0005_aigc_application%2f" target=_blank><i class="fab fa-facebook"></i></a><a class="btn btn-sm twitter-btn" href="https://twitter.com/share?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00300_aigc%2f0005_summary%2f0005_aigc_application%2f&text=%e6%a8%a1%e5%9e%8b%e5%ba%94%e7%94%a8%e7%ad%96%e7%95%a5&via=biubiobiu%27s%20Blog" target=_blank><i class="fab fa-twitter"></i></a><a class="btn btn-sm reddit-btn" href="https://reddit.com/submit?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00300_aigc%2f0005_summary%2f0005_aigc_application%2f&title=%e6%a8%a1%e5%9e%8b%e5%ba%94%e7%94%a8%e7%ad%96%e7%95%a5" target=_blank><i class="fab fa-reddit"></i></a><a class="btn btn-sm linkedin-btn" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00300_aigc%2f0005_summary%2f0005_aigc_application%2f&title=%e6%a8%a1%e5%9e%8b%e5%ba%94%e7%94%a8%e7%ad%96%e7%95%a5" target=_blank><i class="fab fa-linkedin"></i></a><a class="btn btn-sm whatsapp-btn" href="https://api.whatsapp.com/send?text=%e6%a8%a1%e5%9e%8b%e5%ba%94%e7%94%a8%e7%ad%96%e7%95%a5 https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00300_aigc%2f0005_summary%2f0005_aigc_application%2f" target=_blank><i class="fab fa-whatsapp"></i></a><a class="btn btn-sm email-btn" href="mailto:?subject=%e6%a8%a1%e5%9e%8b%e5%ba%94%e7%94%a8%e7%ad%96%e7%95%a5&body=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00300_aigc%2f0005_summary%2f0005_aigc_application%2f" target=_blank><i class="fas fa-envelope-open-text"></i></a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/zh-cn/posts/00300_aigc/0005_summary/0002_aigc_data/ title=LLM-数据集 class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i>上一篇</div><div class=next-prev-text>LLM-数据集</div></a></div><div class="col-md-6 next-article"><a href=/zh-cn/posts/00300_aigc/0005_summary/0010_aigc_train_p/ title=大模型训练框架 class="btn btn-outline-info"><div>下一篇 <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>大模型训练框架</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">目录</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#一简介>一、简介</a><ul><li><a href=#1adapter插件>1、Adapter插件</a></li><li><a href=#2prefix-tuning>2、Prefix-tuning</a></li><li><a href=#3prompt-tuning>3、Prompt-tuning</a></li><li><a href=#4p-tuning>4、P-tuning</a></li><li><a href=#5lora>5、LoRA</a></li><li><a href=#6随便选一些参数>6、随便选一些参数</a></li></ul></li><li><a href=#二大模型-使用范式>二、大模型-使用范式</a><ul><li><a href=#1训练时>1、训练时</a><ul><li><a href=#1-prompt-tuning>1. Prompt-tuning</a><ul><li><a href=#1-人工设计prompt>1. 人工设计prompt</a></li><li><a href=#2-自动生成prompt>2. 自动生成prompt</a></li></ul></li><li><a href=#2-instruction-tuning>2. Instruction-tuning</a><ul><li><a href=#1-flan>1. FLAN</a></li><li><a href=#2-t0>2. T0</a></li><li><a href=#3-rlhf>3. RLHF</a></li></ul></li><li><a href=#3-delta-tuning>3. Delta-tuning</a></li></ul></li><li><a href=#2推理时>2、推理时</a><ul><li><a href=#1in-context-learning>1、In-Context Learning</a><ul><li><a href=#1-解释1>1. 解释1</a></li><li><a href=#2-解释2>2. 解释2</a></li></ul></li><li><a href=#2chain-of-thought-prompting>2、Chain-of-Thought Prompting</a></li></ul></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>导航</h5><ul><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#about>关于</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#skills>技能</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#experiences>经验</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#projects>项目</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#education>教育</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#recent-posts>最新博客</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#accomplishments>造诣</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#achievements>业绩</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>联系方式:</h5><ul><li><span>电话:</span> <span>+0123456789</span></li><li><span>邮箱:</span> <span>bsgshyn8@163.com</span></li></ul></div></div></div><hr><div class=container><p id=disclaimer><strong>免责声明:</strong> 这个主题是MIT许可的。因此，您可以将其用于非商业、商业或者私人用途。您可以修改或分发主题，而无须经 作者任何许可。但是，主题作者不对主题的任何问题提供任何保证或承担任何责任。</p></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=stylesheet href=/katex/katex.min.css><script type=text/javascript defer src=/katex/katex.min.js></script><script type=text/javascript defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body);></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false},{left:'\\(',right:'\\)',display:false},{left:'\\[',right:'\\]',display:true}],throwOnError:false});});</script><script src=/js/mermaid-8.14.0.min.js></script><script>mermaid.initialize({startOnLoad:true});</script></body></html>