<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>文本生成 on biubiobiu's Blog</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/</link><description>Recent content in 文本生成 on biubiobiu's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Everything is mine</copyright><lastBuildDate>Sat, 05 Aug 2023 12:30:40 +0800</lastBuildDate><atom:link href="https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/index.xml" rel="self" type="application/rss+xml"/><item><title>ChatGLM</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0025_chatglm/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0025_chatglm/</guid><description>一、简介 《GLM: General Language Model Pretraining with Autoregressive Blank Infilling》 参考 ChatGLM-6B： ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。 ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。
ChatGLM2-6B： ChatGLM2-6B 是开源中英双语对话模型 ChatGLM-6B 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，ChatGLM2-6B 引入了如下新特性：
更强大的性能： 基于 ChatGLM 初代模型的开发经验，我们全面升级了 ChatGLM2-6B 的基座模型。ChatGLM2-6B 使用了 GLM 的混合目标函数 经过了 1.4T 中英标识符的预训练与人类偏好对齐训练，评测结果显示，相比于初代模型，ChatGLM2-6B 在 MMLU（+23%）、CEval（+33%）、GSM8K（+571%） 、BBH（+60%）等数据集上的性能取得了大幅度的提升，在同尺寸开源模型中具有较强的竞争力。 更长的上下文：基于 FlashAttention 技术，我们将基座模型的上下文长度（Context Length）由 ChatGLM-6B 的 2K 扩展到了 32K，并在对话阶段使用 8K 的上下文长度训练。 更高效的推理：基于 Multi-Query Attention 技术，ChatGLM2-6B 有更高效的推理速度和更低的显存占用：在官方的模型实现下，推理速度相比初代提升了 42%，INT4 量化下，6G 显存支持的对话长度由 1K 提升到了 8K。 二、网络结构 按照自动编码的思想从输入文本中随机删除连续的标记span，并按照自回归预训练的思想训练模型顺序重建span。</description></item><item><title>Claude</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0030_claude/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0030_claude/</guid><description>一、简介 Anthropic公司推出的Claude。
二、网络结构</description></item><item><title>Cohere</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0035_cohere/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0035_cohere/</guid><description>一、简介 二、网络结构</description></item><item><title>Falcon</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0040_falcon/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0040_falcon/</guid><description>一、简介 二、网络结构</description></item><item><title>GPT</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0011_gpt/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0011_gpt/</guid><description>一、简介 二、GPT-1 GPT(2018-06) 详细参考
其创造性的提出以Transformer的解码器来训练生成式模型，后面Bert的作者估计是看到了这篇论文，据说两个月时间就发表了以Transformer编码器训练的Bert模型。总结下GPT-1模型：
GPT-1 使用了一个仅有解码器的 Transformer 结构，每一个作为一个Layer，共有12层； 使用了一个 768 维的嵌入向量来表示输入序列中的每个词或标记，使用了 12 个并行的注意力头（attention heads）； 使用Adam优化器进行模型训练，在训练过程中，使用了学习率的 warmup 阶段和余弦退火调度机制，以平衡训练速度和模型性能； 模型权重被初始化为均值为 0、标准差为 0.02 的正态分布（N(0, 0.02)），使用字节对编码（Byte Pair Encoding，BPE）来对文本进行分词处理，分词后得到的词汇表大小为 40000； 激活函数是 GELU； 文本输入序列固定长度是512； 参数量 117M; 使用了学习得到的位置嵌入向量(position embedding)，而不是Attention is All You Need中使用的正弦位置嵌入向量； 三、GPT-2 GPT-2(2019-02)
GPT-2的改进:
GPT-2 是GPT语言模型开始变大的地方，这是 OpenAI 第一次训练超过 1B 个参数的模型。 通过提升模型的规模，来凸显GPT的优势。在 GPT-1 中，作者训练了单个模型，但在这里，作者训练了一系列模型。 与GPT-1相比，架构上有如下差异：
层归一化操作，有原来的post-norm换成了pre-norm，以加速训练和提高模型性能。此外，在最后一个自注意力块的输出上添加了额外的层归一化； 在权重初始化时，通过 $\frac{1}{\sqrt n}$ 进行缩放。这种缩放有助于减少梯度更新的方差，使训练过程更加稳定； 扩大了其词汇表的大小，词汇表大小约为 50,000（相比于约 40,000）； 增大文本输入序列长度 1024（相比于 512）这使得模型能够更好地理解和生成更长的文本； batch size大小为 512（相比于 64）较大的批次大小有助于提高训练效率和模型并行计算的能力。 最大的模型具有约 15 亿个参数。 数据集：GPT-2 构造了一个新数据集，WebText。全部来自于 Reddit 的外链，而且是那些获得至少三个赞的外链，经过清洗、去重后，得到8百万网页共计 40GB 文本数据。 WebText 数据集的特点在于全面而干净。 GPT-2的不同版本:</description></item><item><title>LLaMa</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0015_llama/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0015_llama/</guid><description>LLaMa2 翻译
一、简介 数据方面
LLaMa2训练了2000B的tokens，训练语料比LLaMa多了40% 2000B 个token的预训练集，提供了良好的性能和成本权衡；对最真实的来源进行上采样，以增加知识并抑制幻觉，保持真实 调查数据，以便用户更好地了解模型的潜在能力和局限性，保证安全。 上下文长度从2048提升到了4096 LLaMa2-chat 模型还接受了超过100w的人类标注的训练数据 开源数据选了 LLaMa2 使用监督微调 LLaMa2-chat 使用人类反馈强化学习(RLHF)进行迭代细化；包括拒绝采样、近端策略优化 网络方面
LLaMa2 vs LLaMa，主要改动体现在 GQA 和 FFN 上:
由MHA改成GQA：整体参数量会减少 FFN模块矩阵维度有扩充：增强泛化能力，整体参数量增加。 RMSNorm 归一化 FFN中用swiGLU激活函数替换原来的Relu 旋转位置编码 RoPE 增加上下文长度 分组查询注意力 GQA 原始的 多头注意力：MHA 具有单个KV投影的原始多查询格式：MQA 具有8个KV投影的分组查询注意力变体：GQA 训练方面 预训练细节：
用AdamW优化器进行训练，其中： $β_1 =0.9，β_2 = 0.95，eps = 10−5$。 使用余弦调整学习率，预热2000steps，$lr$ 衰减到峰值的10% 使用0.</description></item><item><title>PaLM</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0020_palm/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0020_palm/</guid><description>一、简介 1、PaLM 1 《PaLM: Scaling Language Modeling with Pathways》 这篇文章87页，并没有深度的讨论模型算法的结构，数据的清洗技巧，或者是训练的方式（估计感觉这块的创新性不是特别明显，也不是文章的主要目的）。 而是花了大量的篇幅去评估这个模型在multi-task的能力，比如翻译，代码修改，生成，问答等等。
其中模型版本于训练集大小：
Google PaLM 是一个 540B 参数密集型 Transformer 语言模型，在 780B 高质量、多样化文本的标记上进行训练。 它已经针对 3 种不同的尺寸进行了训练：8B、62B 和 540B，使用 6144 TPU v4 芯片使用 Pathways，这是一种新的 ML 系统，可跨多个 TPU（张量处理单元）Pod 进行高效训练。 当它被引入时，它在数百个 NLU 和 NLG 基准测试中产生了 SOTA 小样本学习结果。 这包括 Big-Bench 任务的性能大幅提升，以及多语言 NLG 和源代码生成功能的显着改进。 它还被证明可以使用思维链提示来解释笑话或逻辑推理，从而产生很好的解释。
PaLM超越了许多之前的SOTA。作者归功于
更好的数据的清理， 更多的数据， 模型规模的进一步提升。 模型算法的改进比较少，从Model Architecture那一章看出，其实模型结构的变化并不明显，在激活层，ShareEmbedding，PosEmbedding等模块做了一些结构优选。核心的TransformerBlock的变种选择也更多是为了优化模型的训练效率。谷歌作为搜索技术的天花板，数据清洗的积累，以及对于数据的理解肯定是OpenAI这些公司无法比拟的。个人感觉这块是个比较明显的优势。
与GPT-3相比的变化：
多查询注意力（Multi-query attention）：在每个注意力头中共享K/V（Key/Value）嵌入，但使用独立的Q（Query）嵌入。这样做可以在推理阶段显著提高模型的速度。 并行Transformer块：使用并行的Transformer块来提高训练时间，相较于传统的串行设计，可以减少约15%的训练时间。 SwiGLU激活函数：与GPT-3使用的GELU激活函数不同，这里采用了SwiGLU激活函数。 旋转位置编码RoPE嵌入：使用RoPE（Relative Positional Encodings）嵌入代替学习得到的嵌入方式，在长文本上具有更好的性能 。 输入-输出嵌入共享：输入和输出embedding矩阵是共享的。 无偏置向量：在mlp、normlayer等算法中，都不使用bias，对于大模型，可以提高训练稳定性。 SentencePiece与256k标记：使用SentencePiece进行分词处理，标记数量为256k。 2、PaLM 2 《PaLM 2 Technical Report》 这篇报告-总结：</description></item><item><title>Vicuna</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0045_vicuna/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0045_vicuna/</guid><description>一、简介 二、网络结构</description></item></channel></rss>