<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>图像生成 on biubiobiu's Blog</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/</link><description>Recent content in 图像生成 on biubiobiu's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Everything is mine</copyright><lastBuildDate>Sat, 05 Aug 2023 12:30:40 +0800</lastBuildDate><atom:link href="https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/index.xml" rel="self" type="application/rss+xml"/><item><title>CAN</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1005_can_summary/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1005_can_summary/</guid><description>一、简介 It is coming soon.</description></item><item><title>DALL-E</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1010_dall_e/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1010_dall_e/</guid><description>一、简介 It is coming soon.</description></item><item><title>Diffusion</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1020_diffusion/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1020_diffusion/</guid><description>一、简介 Diffusion 过程：每一步添加一次noise，经过很多步后，图片就接近白噪声了。
但是，我们的目的是：从白噪声中，根据输入的文字描述，生成一张图片。是上面Diffusion过程的逆过程。
所以，根据Diffusion过程生成训练数据，然后训练一个：noise 生成器
从noise中生成图片的过程，也是一步一步删除noise的过程。
所以，训练一个noise 生成器，根据输入的图片、步数，生成图片中的噪声，然后减去输入图片中的这部分噪声，就是去除噪声的图片。
为啥设计个 noise 生成器，而不是直接设计个图片生成器？
noise 生成器：生成噪声，总比一步生成最终的优质图片要容易得多。 需要根据输入的文字描述，来生成相应的图片，那么文字特征是怎么输入的呢？
在 noise 生成器 中添加一个文本输入。 二、Diffusion Model Diffusion 的常用范式：
文本编码器 生成模型：产出中间产物（图片的压缩版本） 图像解码器：生成高质量的图片 文本编码器：
可以是 《CLIP》 生成模型：
在对图片做Diffusion时，是对图片不断地添加noise。
对于生成模型，是对latent representation 不断地添加noise。
所以，在真实的生成模块里，是对latent representation 不断地删除噪声，生成可用的中间产物。
图像编码器：
-- 图像编码器，可以是把低分辨率的小图，生成高分辨率的大图，如下图：
也可以是，把中间产物，生成高分辨率的大图，如下图：
1、Stable Diffusion 《Stable Diffusion》 2、DALL-E 《Hierarchical Text-Conditional Image Generation with CLIP Latents》 《Zero-Shot Text-to-Image Generation》 3、Imagen Imagen Home 《Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding》 三、评估指标 a.</description></item><item><title>GAN</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1002_gan_summary/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1002_gan_summary/</guid><description>一、简介 Discriminator
Generator
二、网络</description></item><item><title>Imagen</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1030_imagen/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1030_imagen/</guid><description>一、简介 It is coming soon.</description></item><item><title>Midjourney</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1025_midjourney/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1025_midjourney/</guid><description>一、简介 It is coming soon.</description></item><item><title>VQGAN</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1015_vqgan/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1015_vqgan/</guid><description>一、简介 It is coming soon.</description></item><item><title>综述</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1000_summary/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1000_summary/</guid><description>扩散模型 扩散模型通过向原始数据逐步加入噪声以破坏原始信息，然后在逆转这一过程来生成样本。相较于以往的深度生成模型，扩散模型生产的数据质量更高、更具多样性，并且扩散模型的结构更灵活。
用一个物理过程来通俗地解释扩散模型 把真实数据比作空气中的一团分子，它们相互交织，形成了具有特定结构的整体。由于分子团过于复杂，我们无法直接了解其结构。我们可以从无规则运动的某种粒子(服从标准高斯分布)出发，不断变换这些粒子的相对位置，将这些粒子的状态变换为我们想要的复杂的分子形态。也就是：从噪声开始，进行很多小的”噪声“变换，逐渐地将噪声的分布转换为数据的分布。这样就可以利用得到的数据分布进行采样，以便得到新的数据。
扩散模型发展历史 扩散模型：是一类生成式模型，用于高维复杂数据的概率分布的建模，核心思想：基于扩散过程描述数据的生成过程，通过逆向扩散过程从后验概率逐步推断出先验概率分布，从而实现对高维复杂数据的建模。该模型的发展历史：
郎之万动力学（Langevin Dynamics）：扩散模型最初的灵感来自郎之万动力学。郎之万动力学是一种用于模拟随机过程的方法，其中加入了随机噪声，类似与布朗运动。 去噪分数匹配（Denoising Score Matching）：2010年，Roux提出了一种名为 ”去噪分数匹配“ 的算法，利用郎之万动力学建立一个基于梯度的概率模型。这种方法利用加噪声的样本和其周围样本之间的梯度来训练模型，从而建立一个对高维数据建模的框架。 扩散过程（Diffusion Process）：2015年，Sohl-Dickstein等人提出了扩散模型，通过将郎之万动力学与扩散过程结合，建立了一个能够描述高维数据生成的模型。该模型使用扩散过程描述数据的生成过程，并通过逆向扩散过程推断出先验分布。 无参数扩散（Non-Parametric Diffusion）：2019年，Song等人提出了一种基于无参数扩散过程的生成模型，它将扩散模型过程嵌入流模型中，从而实现了对高维数据的建模。 扩散模型：2019年至今，深度学习快速发展，扩散模型先后出现了：DDPM、SGM、SDE等新的范式，大大提高了模型的生成效果。 得益于扩散模型的强大性能，目前实际生成中已经出现利用扩散模型进行创造性内容生成。
图像生成的应用包括：Stable Diffusion、DALL-E2、Midjourney等，这些模型，基于输入的引导生成符合条件的内容。这种引导可以是自然语句、部分图像，也可以用低分辨率的图像做为引导生成高分辨率的图像。 图像生成 1、GAN 2014年
2、CAN 2017年
3、DALL-E 2021年2月
根据文本描述绘画，绘画水平一般。
4、CLIP+VQGAN 2021年4月
根据文本描述绘画，绘画水平一般。
5、Disco Diffusion 2022年2月
根据文本描述绘画，具有原创性，图片精美，渲染时间长。
6、Midjourney 2022年3月
根据文本描述绘画，适合人像，细节突出
7、DALL-E2 2022年4月，OpenAI发布DALL-E 2，命名来源于著名画家Dali和机器人总动员Wall-E，是DALL-E的升级版，其分辨率是之前版本的4倍。
DALL-E 2 由三个模型组成：CLIP模型、先验模型、扩散模型。
CLIP模型主要是用来对齐文本和图像特征：获取文本编码 先验模型主要是将文本表征映射为图片表征：将文本编码映射为图片编码 扩散模型是根据图片表征来完成完整的图像：用图片编码生成完整的图片。 根据文本描述绘画，限制较多，对复杂文字理解准确，渲染快
8、Stable Diffusion 2022年8月，慕尼黑大学的Robin Rombach和Patrick Esser的团队提出的文本生成图像模型，交互简单，生成速度快。Stable Diffusion主要由三部分组成，分别是 VAE、U-Net、CLIP文本编码器：
首先使用CLIP模型将文本转换为表征形式 然后引导扩散模型U-Net在低维表征上进行扩散 最后将扩散后的低维表征送入VAE中的解码器，从而生成图像。 在GAN和CLIP的基础上，Stable Diffusion模型开源，直接推动了AIGC技术的突破性发展。
Stable Diffusion 扩散模型的原理是：先添加噪声后降噪。即：给现有的图像逐步添加噪声，直到图像被完全破坏，然后根据给定的高斯噪声，逆向逐步还原出原图。在模型训练完毕后，只需要输入一段随机的高斯噪声，就能生成一张图像。</description></item></channel></rss>