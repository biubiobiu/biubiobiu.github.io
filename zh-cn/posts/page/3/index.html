<!doctype html><html><head><title>博文</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0d88336a9cbd3e68c7714efae786b0a9_38044_42x0_resize_box_2.png><meta property="og:title" content="博文"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://biubiobiu.github.io/zh-cn/posts/"><meta property="og:updated_time" content="2023-08-08T06:00:20+08:00"><link rel=stylesheet href=/css/layouts/list.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/zh-cn><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png alt=Logo>
biubiobiu's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=languageSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="flag-icon flag-icon-zh-cn"></span>简体中文</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/posts><span class="flag-icon flag-icon-gb"></span>English</a></div></li><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu547c3206082786c1d32d36034e2a655a_40863_42x0_resize_box_2.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/zh-cn/search><input type=text name=keyword placeholder=搜索 data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/zh-cn/posts data-filter=all>博文</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00020_toha-tutorial/>Toha教程</a><ul><li><a href=/zh-cn/posts/00020_toha-tutorial/0010_toha-config/ title=Toha的配置>Toha的配置</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0011_write-blogs/ title=撰写文章>撰写文章</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0013_markdown-tutorial/ title=MarkDown入门>MarkDown入门</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0015_latax_formula/ title=Katex公式>Katex公式</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0020_shortcodes_samples/ title=区域块-实例>区域块-实例</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00025_math_knowledge/>数学知识</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/>概率论</a><ul><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0010_basic-conception/ title=基本概念>基本概念</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0020_distribution_of_variables/ title=随机变量及其分布>随机变量及其分布</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0030_law_of_large_numbers/ title=大数定律>大数定律</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0040_sample_distribution/ title=样本及抽样分布>样本及抽样分布</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0050_parameter_estimation/ title=假设检验>假设检验</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0060_analysis_variance_regression/ title=方差分析及回归分析>方差分析及回归分析</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0070_stochastic_process/ title=随机过程>随机过程</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0080_markov_process/ title=马尔科夫链>马尔科夫链</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0090_stationary_stochastic_process/ title=平稳随机过程>平稳随机过程</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00025_math_knowledge/0020_math_convex_optimization_theory/>凸优化</a><ul><li><a href=/zh-cn/posts/00025_math_knowledge/0020_math_convex_optimization_theory/0010_basic_conception/ title=基本概念>基本概念</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00030_deeplearning_summary/>深度学习</a><ul><li><a href=/zh-cn/posts/00030_deeplearning_summary/0005_deeplearning_summary/ title=深度学习开篇>深度学习开篇</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0010_deeplearning_problem/ title=深度学习-结构>深度学习-结构</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0015_deeplearning_norm/ title=归一化>归一化</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0020_deeplearing_init/ title=初始化>初始化</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0100_draw_map_for_dl/ title=神经网络画图篇>神经网络画图篇</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0200_cam/ title=CAM>CAM</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00033_reinforce/>强化学习</a><ul><li><a href=/zh-cn/posts/00033_reinforce/0001_reinforce_summary/ title=综述>综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/>编程语言</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/>Python</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/0010_build_env/>env</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0010_build_env/0010_pip_env/ title=py-env>py-env</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0010_build_env/0020_cuda_env/ title=cuda>cuda</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/>Internal</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0010_encode_mode/ title=字符编码>字符编码</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0020_basic_operator/ title=基础操作>基础操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0025_internal_module/ title=内置模块>内置模块</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0030_advance_operator/ title=进阶操作>进阶操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0040_error/ title=异常>异常</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0050_file/ title=文件读取>文件读取</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/>SDK</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0020_multiprocessing/ title=并行操作>并行操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0030_importlib/ title=importlib>importlib</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0040_ipdb/ title=ipdb>ipdb</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0050_re/ title=正则-re>正则-re</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0060_heapd/ title=堆-heapq>堆-heapq</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0070_request/ title=requests>requests</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0080_logging/ title=logging>logging</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0090_argparse/ title=argparse>argparse</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0100_pil/ title=PIL>PIL</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0110_opencv/ title=OpenCV>OpenCV</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0040_tf/>TensorFlow</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0040_tf/w0010_compat/>兼容1.x</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0040_tf/w0010_compat/0010_tf_compat_summary/ title=静态图>静态图</a></li><li><a href=/zh-cn/posts/00035_programming_language/0040_tf/w0010_compat/0020_tf_compat_train/ title=模型训练>模型训练</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/>PyTorch</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0010_torch_summary/ title=简介>简介</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0020_basic/ title=基础操作>基础操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0030_mathematical/ title=数学计算>数学计算</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0040_tensor/ title=Tensor和变量>Tensor和变量</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0050_train_model/ title=模型训练>模型训练</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0060_mxnet/>MxNet</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0060_mxnet/0010_ndarray/>NdArray</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0060_mxnet/0010_ndarray/0010_ndarray_summary/ title=NdArray使用>NdArray使用</a></li></ul></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/>CV</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0010_backbone/>基础</a><ul><li><a href=/zh-cn/posts/00100_cv/0010_backbone/d1_1_backbone_cnn/ title=CNN>CNN</a></li><li><a href=/zh-cn/posts/00100_cv/0010_backbone/d1_2_optimizer/ title=optimizer>optimizer</a></li><li><a href=/zh-cn/posts/00100_cv/0010_backbone/d1_3_backbone_net/ title="backbone net">backbone net</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0020_contrastive_learning/>对比学习</a><ul><li><a href=/zh-cn/posts/00100_cv/0020_contrastive_learning/contrastive_learning/ title="contrastive learning">contrastive learning</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0030_vision_transformer/>ViT</a><ul><li><a href=/zh-cn/posts/00100_cv/0030_vision_transformer/vision_transformer/ title="vision transformer">vision transformer</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0050_detect_object/>目标检测</a><ul><li><a href=/zh-cn/posts/00100_cv/0050_detect_object/object_detection_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0055_semantic_segmentation/>语义分割</a><ul><li><a href=/zh-cn/posts/00100_cv/0055_semantic_segmentation/object_detection_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0060_image-matting/>抠图</a><ul><li><a href=/zh-cn/posts/00100_cv/0060_image-matting/image-matting-summary/ title=抠图综述>抠图综述</a></li><li><a href=/zh-cn/posts/00100_cv/0060_image-matting/image-matting-animal/ title="animal matting">animal matting</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/>NLP</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0010_word_embedding/>Word Embedding</a><ul><li><a href=/zh-cn/posts/00200_nlp/0010_word_embedding/word_embedding_summary/ title="Word Embedding综述">Word Embedding综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0020_rnn/>RNN</a><ul><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/rnn_summary/ title=RNN综述>RNN综述</a></li><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/gru/ title=GRU网络>GRU网络</a></li><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/lstm/ title=LSTM网络>LSTM网络</a></li><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/encode_decode/ title=编解码架构>编解码架构</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0030_transformer/>Transformer</a><ul><li><a href=/zh-cn/posts/00200_nlp/0030_transformer/0001_attention/ title=Attention>Attention</a></li><li><a href=/zh-cn/posts/00200_nlp/0030_transformer/0010_transformer_summary/ title=Transformer>Transformer</a></li><li><a href=/zh-cn/posts/00200_nlp/0030_transformer/0030_position/ title=位置编码>位置编码</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0080_gpt/>GPT</a><ul><li><a href=/zh-cn/posts/00200_nlp/0080_gpt/0010_gpt_summary/ title=GPT综述>GPT综述</a></li><li><a href=/zh-cn/posts/00200_nlp/0080_gpt/0020_gpt1_detail/ title=GPT-1>GPT-1</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0100_bert/>Bert</a><ul><li><a href=/zh-cn/posts/00200_nlp/0100_bert/0001_bert_summary/ title=Bert综述>Bert综述</a></li><li><a href=/zh-cn/posts/00200_nlp/0100_bert/0020_bert_family/ title=Bert家族>Bert家族</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0120_t5/>T5</a><ul><li><a href=/zh-cn/posts/00200_nlp/0120_t5/0010_t5_summary/ title=T5综述>T5综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0150_bart/>BART</a><ul><li><a href=/zh-cn/posts/00200_nlp/0150_bart/bart_summary/ title=BART综述>BART综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0200_electra/>ELECTRA</a><ul><li><a href=/zh-cn/posts/00200_nlp/0200_electra/electra_summary/ title=ELECTRA综述>ELECTRA综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/1000_code/>CODE</a><ul><li><a href=/zh-cn/posts/00200_nlp/1000_code/bart_summary/ title=code解析>code解析</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/>AIGC</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/0005_summary/>AIGC综述</a><ul><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0001_aigc_summary/ title=综述>综述</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0002_aigc_data/ title=LLM-数据集>LLM-数据集</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0005_aigc_application/ title=模型应用策略>模型应用策略</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0010_aigc_train_p/ title=大模型训练框架>大模型训练框架</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0012_aigc_amp/ title=混合精度训练>混合精度训练</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0015_aigc_train_mini/ title=模型小型化>模型小型化</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0020_aigc_error/ title=生成式-问题>生成式-问题</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0025_aigc_eval/ title=生成模型-评估>生成模型-评估</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/0010_generate_text/>文本生成</a><ul><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0011_gpt/ title=GPT>GPT</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0015_llama/ title=LLaMa>LLaMa</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0020_palm/ title=PaLM>PaLM</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0025_chatglm/ title=ChatGLM>ChatGLM</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0030_claude/ title=Claude>Claude</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0035_cohere/ title=Cohere>Cohere</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0040_falcon/ title=Falcon>Falcon</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0045_vicuna/ title=Vicuna>Vicuna</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/0020_generate_image/>图像生成</a><ul><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1000_summary/ title=综述>综述</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1002_gan_summary/ title=GAN>GAN</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1005_can_summary/ title=CAN>CAN</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1010_dall_e/ title=DALL-E>DALL-E</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1015_vqgan/ title=VQGAN>VQGAN</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1020_diffusion/ title=Diffusion>Diffusion</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1025_midjourney/ title=Midjourney>Midjourney</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1030_imagen/ title=Imagen>Imagen</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00400_vlp/>多模态</a><ul><li><a href=/zh-cn/posts/00400_vlp/0001_vlp_summary/ title=简介>简介</a></li><li><a href=/zh-cn/posts/00400_vlp/0005_clip/ title=CLIP>CLIP</a></li><li><a href=/zh-cn/posts/00400_vlp/0010_mllm/ title=MLLM>MLLM</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00500_video/>视频理解</a><ul><li><a href=/zh-cn/posts/00500_video/vidio_summary/ title=简介>简介</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class="content container-fluid" id=content><div class="container-fluid post-card-holder" id=post-card-holder><div class=post-card><a href=/zh-cn/posts/00300_aigc/0005_summary/0020_aigc_error/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>生成式-问题</h5><p class="card-text post-summary">一、简介 问题1：在文本生成是，模型会一本正经的胡说八道，这种现象叫做模型的幻觉。 问题2：训练一个大模型，需要多少数据量呢？ 问题3：数据预处理，怎么过滤、去重 问题4：模型大小 与 数据大小 的关系？ 二、模型问题 1、Calibration 问题1：在文本生成是，模型会一本正经的胡说八道，这种现象叫做模型的幻觉。
产生幻觉的原因： LLM缺乏相关知识，或者内化了错误的知识 LLM有时高估了自己的能力 问题对齐过程误导LLM进入幻觉：在对齐过程中接受针对它们在预训练阶段尚未获得的知识的指示时，实际上是一种不对齐过程，鼓励LLMs产生幻觉。 LLMs采用的生成策略存在潜在风险：LLMs有时会过分坚持早期的错误，即使它们意识到这是不正确的。换句话说，LLMs可能更喜欢为了自身一致性而堆积幻觉，而不是从错误中恢复。 减轻幻觉的方案：
整理训练集：在预训练期间减轻幻觉主要集中在预训练语料库的策划上 SFT：监督训练，构建训练数据是减轻幻觉的一种方法 RLHF：人类监督强化学习。让模型学习到：诚实性、 在推理阶段： 设计解码策略 利用外部知识来减轻LLMs中的幻觉 《Language Models (Mostly) Know What They Know》 这篇论文发现：模型够大后，说谎才会心虚。
对于大模型，模型输出是正确的概率 VS 模型的自信度，这两个是相关的。当模型比较自信时，输出的结果是正确的概率就比较大。 对于小模型，模型输出是正确的概率 VS 模型的自信度，这两个是不相关的 其中，横轴：模型输出时的自信程度；纵轴：模型输出是正确的概率。黄色表示最大模型，自身表示最小模型。 三、数据问题 问题2：训练一个大模型，需要多少数据量呢？
训练一个大模型，需要多少数据量呢？《When Do You Need Billions of Words of Pretraining Data?》 问题3：数据预处理，怎么过滤、去重?
数据预处理：《Scaling Language Models: Methods, Analysis & Insights from Training Gopher》 过滤有害的内容，通过Google的审核接口 去掉一些 HTML 前端的一些tag 规则过滤，去掉低质量的文本。 去重 剔除测试数据 问题4：模型大小 与 数据大小 的关系？ 《Training Compute-Optimal Large Language Models》 这篇文章发现：</p></div><div class=card-footer><span class=float-left>August 5, 2023</span>
<a href=/zh-cn/posts/00300_aigc/0005_summary/0020_aigc_error/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div><div class=post-card><a href=/zh-cn/posts/00300_aigc/0005_summary/0025_aigc_eval/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>生成模型-评估</h5><p class="card-text post-summary">一、 测评指标：
1、BLEU(Bilingual evaluation understudy) $$ BLEU = BP \times e^{\sum^n_{n=1}W_n \times \log P_n}, BP = e^{1-\frac{lr}{lc}} 如果 lc &lt;= lr $$
2、PPL(perplexity) 3、ROUGE(Recall-Oriented Understudy for Gisting Evaluation) 4、Distance-base Metrics Edit Dist
二、</p></div><div class=card-footer><span class=float-left>August 5, 2023</span>
<a href=/zh-cn/posts/00300_aigc/0005_summary/0025_aigc_eval/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div><div class=post-card><a href=/zh-cn/posts/00033_reinforce/0001_reinforce_summary/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>综述</h5><p class="card-text post-summary">一、简介 graph LR; A(动态规划) -- B(Monte Carlo) B -- C(TD) C -- D(Q学习) D -- E(SARSA) E -- F(DQN) F -- G(PPO) G -- H(AC/A2C/A3C) H -- I(DDPG) I -- J(SAC) 二、</p></div><div class=card-footer><span class=float-left>August 5, 2023</span>
<a href=/zh-cn/posts/00033_reinforce/0001_reinforce_summary/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div><div class=post-card><a href=/zh-cn/posts/00300_aigc/0005_summary/0001_aigc_summary/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>综述</h5><p class="card-text post-summary">AIGC 的技术分类按照处理的模态来看，可以分为一下几类：
文本类：
主要包括：文章生成、文本风格转换、问答对话等生成或者编辑文本内容的AIGC技术。 音频类：
包括：文本转音频、语音转换、语音属性编辑等生成或者编辑语音内容的AIGC技术；以及音乐合成、场景声音编辑等生成或者编辑非语言内容的AIGC技术。例如：智能配音主播、虚拟歌手演唱、自动配乐、歌曲生成等 图像视频类：
包括：人脸生成、人脸替换、人物属性编辑、人类操控、姿态操控等AIGC技术；以及编辑图像、视频内容、图像生成、图像增强、图像修复等AIGC技术 虚拟空间类：
主要包括：三维重建、数字仿真等AIGC技术，以及编辑数字任务、虚拟场景相关的AIGC技术，例如：元宇宙、数字孪生、游戏引擎、3D建模、VR等。 在大语言模型的训练中，如果增大数据量，相应的应该减少学习率，这个跟原来的经验相反。
模型大小与模型效果 《Emergent Abilities of Large Language Models》 这篇文章指出：随着模型大小的增大，模型效果先不会有明显提升；增加到一定程度，模型有个突然顿悟时刻。
为什么需要预训练 《Visualizing and Understanding the Effectiveness of BERT》 这篇文章指出:
首先，预训练能在下游任务中达到一个良好的初始点，与从头开始训练相比，预训练能带来更宽的最优点，更容易优化。尽管 BERT 对下游任务的参数设置过高，但微调程序对过拟合具有很强的鲁棒性。 其次，可视化结果表明，由于最佳值平坦且宽广，以及训练损失面和泛化误差面之间的一致性，微调 BERT 趋向于更好地泛化。 第三，在微调过程中，BERT 的低层更具不变性，这表明靠近输入的层学习到了更多可迁移的语言表征。 一、文本生成 1、GPT 参考
GPT-4: 参数量1800B，训练集：1.3T token 2、PaLM 《PaLM: Scaling Language Modeling with Pathways》 PaLM才是真正的“大”模型。它是迄今为止训练的最大的密集语言模型，参数为 540B，需要 6144 个 TPU 来训练（这是 3 个完整的 TPU pod，每个包含 2048 个 TPU）。这太贵了！可能只有谷歌拥有资源+基础设施来做到这一点。使用的Token高达7800亿。PaLM是使用Google新一代PathWay分布式训练框架训练出来。
与GPT-3相比的变化：
多查询注意力（Multi-query attention）：在每个注意力头中共享K/V（Key/Value）嵌入，但使用独立的Q（Query）嵌入。这样做可以在推理阶段显著提高模型的速度。 并行Transformer块：使用并行的Transformer块来提高训练时间，相较于传统的串行设计，可以减少约15%的训练时间。 SwiGLU激活函数：与GPT-3使用的GELU激活函数不同，这里采用了SwiGLU激活函数。 旋转位置编码RoPE嵌入：使用RoPE（Relative Positional Encodings）嵌入代替学习得到的嵌入方式，在长文本上具有更好的性能 。 输入-输出嵌入共享：输入和输出embedding矩阵是共享的。 无偏置向量：在mlp、normlayer等算法中，都不使用bias，对于大模型，可以提高训练稳定性。 SentencePiece与256k标记：使用SentencePiece进行分词处理，标记数量为256k。 所以，有很多变化！同样，其中很多都是常见的，例如使用 GPT-3 的学习嵌入向量已经非常过时了，现在几乎没有人这样做。</p></div><div class=card-footer><span class=float-left>August 5, 2023</span>
<a href=/zh-cn/posts/00300_aigc/0005_summary/0001_aigc_summary/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div><div class=post-card><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1000_summary/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>综述</h5><p class="card-text post-summary">扩散模型 扩散模型通过向原始数据逐步加入噪声以破坏原始信息，然后在逆转这一过程来生成样本。相较于以往的深度生成模型，扩散模型生产的数据质量更高、更具多样性，并且扩散模型的结构更灵活。
用一个物理过程来通俗地解释扩散模型 把真实数据比作空气中的一团分子，它们相互交织，形成了具有特定结构的整体。由于分子团过于复杂，我们无法直接了解其结构。我们可以从无规则运动的某种粒子(服从标准高斯分布)出发，不断变换这些粒子的相对位置，将这些粒子的状态变换为我们想要的复杂的分子形态。也就是：从噪声开始，进行很多小的”噪声“变换，逐渐地将噪声的分布转换为数据的分布。这样就可以利用得到的数据分布进行采样，以便得到新的数据。
扩散模型发展历史 扩散模型：是一类生成式模型，用于高维复杂数据的概率分布的建模，核心思想：基于扩散过程描述数据的生成过程，通过逆向扩散过程从后验概率逐步推断出先验概率分布，从而实现对高维复杂数据的建模。该模型的发展历史：
郎之万动力学（Langevin Dynamics）：扩散模型最初的灵感来自郎之万动力学。郎之万动力学是一种用于模拟随机过程的方法，其中加入了随机噪声，类似与布朗运动。 去噪分数匹配（Denoising Score Matching）：2010年，Roux提出了一种名为 ”去噪分数匹配“ 的算法，利用郎之万动力学建立一个基于梯度的概率模型。这种方法利用加噪声的样本和其周围样本之间的梯度来训练模型，从而建立一个对高维数据建模的框架。 扩散过程（Diffusion Process）：2015年，Sohl-Dickstein等人提出了扩散模型，通过将郎之万动力学与扩散过程结合，建立了一个能够描述高维数据生成的模型。该模型使用扩散过程描述数据的生成过程，并通过逆向扩散过程推断出先验分布。 无参数扩散（Non-Parametric Diffusion）：2019年，Song等人提出了一种基于无参数扩散过程的生成模型，它将扩散模型过程嵌入流模型中，从而实现了对高维数据的建模。 扩散模型：2019年至今，深度学习快速发展，扩散模型先后出现了：DDPM、SGM、SDE等新的范式，大大提高了模型的生成效果。 得益于扩散模型的强大性能，目前实际生成中已经出现利用扩散模型进行创造性内容生成。
图像生成的应用包括：Stable Diffusion、DALL-E2、Midjourney等，这些模型，基于输入的引导生成符合条件的内容。这种引导可以是自然语句、部分图像，也可以用低分辨率的图像做为引导生成高分辨率的图像。 图像生成 1、GAN 2014年
2、CAN 2017年
3、DALL-E 2021年2月
根据文本描述绘画，绘画水平一般。
4、CLIP+VQGAN 2021年4月
根据文本描述绘画，绘画水平一般。
5、Disco Diffusion 2022年2月
根据文本描述绘画，具有原创性，图片精美，渲染时间长。
6、Midjourney 2022年3月
根据文本描述绘画，适合人像，细节突出
7、DALL-E2 2022年4月，OpenAI发布DALL-E 2，命名来源于著名画家Dali和机器人总动员Wall-E，是DALL-E的升级版，其分辨率是之前版本的4倍。
DALL-E 2 由三个模型组成：CLIP模型、先验模型、扩散模型。
CLIP模型主要是用来对齐文本和图像特征：获取文本编码 先验模型主要是将文本表征映射为图片表征：将文本编码映射为图片编码 扩散模型是根据图片表征来完成完整的图像：用图片编码生成完整的图片。 根据文本描述绘画，限制较多，对复杂文字理解准确，渲染快
8、Stable Diffusion 2022年8月，慕尼黑大学的Robin Rombach和Patrick Esser的团队提出的文本生成图像模型，交互简单，生成速度快。Stable Diffusion主要由三部分组成，分别是 VAE、U-Net、CLIP文本编码器：
首先使用CLIP模型将文本转换为表征形式 然后引导扩散模型U-Net在低维表征上进行扩散 最后将扩散后的低维表征送入VAE中的解码器，从而生成图像。 在GAN和CLIP的基础上，Stable Diffusion模型开源，直接推动了AIGC技术的突破性发展。
Stable Diffusion 扩散模型的原理是：先添加噪声后降噪。即：给现有的图像逐步添加噪声，直到图像被完全破坏，然后根据给定的高斯噪声，逆向逐步还原出原图。在模型训练完毕后，只需要输入一段随机的高斯噪声，就能生成一张图像。</p></div><div class=card-footer><span class=float-left>August 5, 2023</span>
<a href=/zh-cn/posts/00300_aigc/0020_generate_image/1000_summary/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div><div class=post-card><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0050_parameter_estimation/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>假设检验</h5><p class="card-text post-summary">一、基本概念 二、</p></div><div class=card-footer><span class=float-left>August 1, 2023</span>
<a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0050_parameter_estimation/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div><div class=post-card><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0010_basic-conception/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>基本概念</h5><p class="card-text post-summary">一、基本概念 随机实验：$E$ 样本空间：记为 $S$。随机实验 $E$ 的所有可能结果组成的集合，称为随机实验 $E$ 的样本空间。
样本点：样本空间的元素，即：随机实验 $E$ 的每个结果。
随机事件：随机实验 $E$ 的样本空间$S$的子集，称为 $E$ 的随机事件。
基本事件：由单个样本点组成的单点集，成为基本事件。
必然事件：样本空间 $S$ 集合，成为必然事件。
不可能事件：空集 $\varnothing$。 概率：随机实验 $E$，样本空间为 $S$。对于 $E$ 的每一件事 $A$，概率 记为 $P(A)$ 条件概率：事件 $A$ 已经发生的条件下，事件 $B$ 发生的概率。
划分：随机实验 $E$，样本空间为 $S$。$B_1, B_2, &mldr;, B_n$ 为 $E$ 的一组事件，若
$B_iB_j= \varnothing , i \ne j, i,j=1,2,&mldr;,n$ $B_1 \cup B_2 \cup &mldr; \cup B_n = S$ 则，称 $B_1, B_2, &mldr;, B_n$ 为样本空间 $S$ 的一个划分。</p></div><div class=card-footer><span class=float-left>August 1, 2023</span>
<a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0010_basic-conception/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div><div class=post-card><a href=/zh-cn/posts/00025_math_knowledge/0020_math_convex_optimization_theory/0010_basic_conception/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>基本概念</h5><p class="card-text post-summary">一、基本概念</p></div><div class=card-footer><span class=float-left>August 1, 2023</span>
<a href=/zh-cn/posts/00025_math_knowledge/0020_math_convex_optimization_theory/0010_basic_conception/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div><div class=post-card><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0030_law_of_large_numbers/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>大数定律</h5><p class="card-text post-summary">一、基本概念 二、</p></div><div class=card-footer><span class=float-left>August 1, 2023</span>
<a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0030_law_of_large_numbers/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div><div class=post-card><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0090_stationary_stochastic_process/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>平稳随机过程</h5><p class="card-text post-summary">一、基本概念 二、</p></div><div class=card-footer><span class=float-left>August 1, 2023</span>
<a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0090_stationary_stochastic_process/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div><div class=post-card><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0060_analysis_variance_regression/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>方差分析及回归分析</h5><p class="card-text post-summary">一、基本概念 二、</p></div><div class=card-footer><span class=float-left>August 1, 2023</span>
<a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0060_analysis_variance_regression/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div><div class=post-card><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0040_sample_distribution/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>样本及抽样分布</h5><p class="card-text post-summary">一、基本概念 二、</p></div><div class=card-footer><span class=float-left>August 1, 2023</span>
<a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0040_sample_distribution/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div></div><div class=paginator><ul class=pagination><li class=page-item><a href=/zh-cn/posts/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/zh-cn/posts/page/2/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/zh-cn/posts/>1</a></li><li class=page-item><a class=page-link href=/zh-cn/posts/page/2/>2</a></li><li class="page-item active"><a class=page-link href=/zh-cn/posts/page/3/>3</a></li><li class=page-item><a class=page-link href=/zh-cn/posts/page/4/>4</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/zh-cn/posts/page/9/>9</a></li><li class=page-item><a href=/zh-cn/posts/page/4/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/zh-cn/posts/page/9/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>导航</h5><ul><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#about>关于</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#skills>技能</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#experiences>经验</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#projects>项目</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#education>教育</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#recent-posts>最新博客</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#accomplishments>造诣</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#achievements>业绩</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>联系方式:</h5><ul><li><span>电话:</span> <span>+0123456789</span></li><li><span>邮箱:</span> <span>bsgshyn8@163.com</span></li></ul></div></div></div><hr><div class=container><p id=disclaimer><strong>免责声明:</strong> 这个主题是MIT许可的。因此，您可以将其用于非商业、商业或者私人用途。您可以修改或分发主题，而无须经 作者任何许可。但是，主题作者不对主题的任何问题提供任何保证或承担任何责任。</p></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=/js/list.js></script></body></html>