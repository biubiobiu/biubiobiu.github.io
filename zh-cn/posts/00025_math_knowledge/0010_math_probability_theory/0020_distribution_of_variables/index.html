<!doctype html><html><head><title>随机变量及其分布</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0d88336a9cbd3e68c7714efae786b0a9_38044_42x0_resize_box_2.png><meta property="og:title" content="随机变量及其分布"><meta property="og:description" content="一、一维随机变量  如何引入一个法则，将随机试验的每个结果（即：$S$ 中的每个元素 $e$）与实数 $x$ 对应起来 从而引入了随机变量的概念，即：定义域是：样本空间$S$，值域是：实数。
定义：设随机试验的样本空间为 $S=\{e\}, X=X(e)$ 是定义在样本空间 $S$ 上的实值单值函数。称 $X = X(e)$ 为随机变量。
例如：以 $X$ 记录三次投掷硬币得到正面的次数。$P(X=2) = 3/8$ 就表示：随机变量 $X=2$ 的概率，就是 $A=\{HHT,HTH,THH\}$ 这个事件的概率。
 随机变量的引入，使得我们能用随机变量来描述各种随机现象，并能利用数学分析的方法对随机试验的结果进行深入广泛的研究和讨论。
1、离散型随机变量 离散型随机变量：随机变量，它全部可能取到的值是有限个或者可列无限多个。 可以用 分布律 来描述。
常见离散型随机变量：
  (0-1)分布 期望：p，方差：p(1-p) 随机变量 $X$ 只能取 0 与 1 两个值。
  伯努利实验、二项分布，记：$ \textcolor{#f00000} {X \sim b(n, p)，期望：np，方差：np(1-p)}$
设实验 $E$ 只有两个可能结果：$A$ 和 $\bar A$ ，则称 $E$ 为伯努利实验。
将实验 $E$ 独立重复地进行 $n$ 次，则称这一串重复的独立实验为 $n$ 重伯努利实验。"><meta property="og:type" content="article"><meta property="og:url" content="https://biubiobiu.github.io/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0020_distribution_of_variables/"><meta property="article:published_time" content="2023-08-01T06:00:20+08:00"><meta property="article:modified_time" content="2023-08-01T06:00:20+08:00"><meta name=description content="随机变量及其分布"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/zh-cn><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png alt=Logo>
biubiobiu's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu547c3206082786c1d32d36034e2a655a_40863_42x0_resize_box_2.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/zh-cn/search><input type=text name=keyword placeholder=搜索 data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/zh-cn/posts data-filter=all>博文</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00020_toha-tutorial/>Toha教程</a><ul><li><a href=/zh-cn/posts/00020_toha-tutorial/0010_toha-config/ title=Toha的配置>Toha的配置</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0011_write-blogs/ title=撰写文章>撰写文章</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0013_markdown-tutorial/ title=MarkDown入门>MarkDown入门</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0015_latax_formula/ title=Katex公式>Katex公式</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0020_shortcodes_samples/ title=区域块-实例>区域块-实例</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/posts/00025_math_knowledge/>数学知识</a><ul class=active><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/>概率论</a><ul class=active><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0010_basic-conception/ title=基本概念>基本概念</a></li><li><a class=active href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0020_distribution_of_variables/ title=随机变量及其分布>随机变量及其分布</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0030_law_of_large_numbers/ title=大数定律>大数定律</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0040_sample_distribution/ title=样本及抽样分布>样本及抽样分布</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0050_parameter_estimation/ title=假设检验>假设检验</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0060_analysis_variance_regression/ title=方差分析及回归分析>方差分析及回归分析</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0070_stochastic_process/ title=随机过程>随机过程</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0080_markov_process/ title=马尔科夫链>马尔科夫链</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0090_stationary_stochastic_process/ title=平稳随机过程>平稳随机过程</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00025_math_knowledge/0020_math_convex_optimization_theory/>凸优化</a><ul><li><a href=/zh-cn/posts/00025_math_knowledge/0020_math_convex_optimization_theory/0010_basic_conception/ title=基本概念>基本概念</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00030_deeplearning_summary/>深度学习</a><ul><li><a href=/zh-cn/posts/00030_deeplearning_summary/0005_deeplearning_summary/ title=深度学习开篇>深度学习开篇</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0010_deeplearning_problem/ title=深度学习-结构>深度学习-结构</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0015_deeplearning_norm/ title=归一化>归一化</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0020_deeplearing_init/ title=初始化>初始化</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0100_draw_map_for_dl/ title=神经网络画图篇>神经网络画图篇</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0200_cam/ title=CAM>CAM</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00033_reinforce/>强化学习</a><ul><li><a href=/zh-cn/posts/00033_reinforce/0001_reinforce_summary/ title=综述>综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/>编程语言</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/>Python</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/0010_build_env/>env</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0010_build_env/0010_pip_env/ title=py-env>py-env</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0010_build_env/0020_cuda_env/ title=cuda>cuda</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/>Internal</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0010_encode_mode/ title=字符编码>字符编码</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0020_basic_operator/ title=基础操作>基础操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0025_internal_module/ title=内置模块>内置模块</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0030_advance_operator/ title=进阶操作>进阶操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0040_error/ title=异常>异常</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0050_file/ title=文件读取>文件读取</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/>SDK</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0020_multiprocessing/ title=并行操作>并行操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0030_importlib/ title=importlib>importlib</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0040_ipdb/ title=ipdb>ipdb</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0050_re/ title=正则-re>正则-re</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0060_heapd/ title=堆-heapq>堆-heapq</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0070_request/ title=requests>requests</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0080_logging/ title=logging>logging</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0090_argparse/ title=argparse>argparse</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0100_pil/ title=PIL>PIL</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0110_opencv/ title=OpenCV>OpenCV</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0040_tf/>TensorFlow</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0040_tf/w0010_compat/>兼容1.x</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0040_tf/w0010_compat/0010_tf_compat_summary/ title=静态图>静态图</a></li><li><a href=/zh-cn/posts/00035_programming_language/0040_tf/w0010_compat/0020_tf_compat_train/ title=模型训练>模型训练</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/>PyTorch</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0010_torch_summary/ title=简介>简介</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0020_basic/ title=基础操作>基础操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0030_mathematical/ title=数学计算>数学计算</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0040_tensor/ title=Tensor和变量>Tensor和变量</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0050_train_model/ title=模型训练>模型训练</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0060_mxnet/>MxNet</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0060_mxnet/0010_ndarray/>NdArray</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0060_mxnet/0010_ndarray/0010_ndarray_summary/ title=NdArray使用>NdArray使用</a></li></ul></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/>CV</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0010_backbone/>基础</a><ul><li><a href=/zh-cn/posts/00100_cv/0010_backbone/d1_1_backbone_cnn/ title=CNN>CNN</a></li><li><a href=/zh-cn/posts/00100_cv/0010_backbone/d1_2_optimizer/ title=optimizer>optimizer</a></li><li><a href=/zh-cn/posts/00100_cv/0010_backbone/d1_3_backbone_net/ title="backbone net">backbone net</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0020_contrastive_learning/>对比学习</a><ul><li><a href=/zh-cn/posts/00100_cv/0020_contrastive_learning/contrastive_learning/ title="contrastive learning">contrastive learning</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0030_vision_transformer/>ViT</a><ul><li><a href=/zh-cn/posts/00100_cv/0030_vision_transformer/vision_transformer/ title="vision transformer">vision transformer</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0050_detect_object/>目标检测</a><ul><li><a href=/zh-cn/posts/00100_cv/0050_detect_object/object_detection_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0055_semantic_segmentation/>语义分割</a><ul><li><a href=/zh-cn/posts/00100_cv/0055_semantic_segmentation/object_detection_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0060_image-matting/>抠图</a><ul><li><a href=/zh-cn/posts/00100_cv/0060_image-matting/image-matting-summary/ title=抠图综述>抠图综述</a></li><li><a href=/zh-cn/posts/00100_cv/0060_image-matting/image-matting-animal/ title="animal matting">animal matting</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/>NLP</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0010_word_embedding/>Word Embedding</a><ul><li><a href=/zh-cn/posts/00200_nlp/0010_word_embedding/word_embedding_summary/ title="Word Embedding综述">Word Embedding综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0020_rnn/>RNN</a><ul><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/rnn_summary/ title=RNN综述>RNN综述</a></li><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/gru/ title=GRU网络>GRU网络</a></li><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/lstm/ title=LSTM网络>LSTM网络</a></li><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/encode_decode/ title=编解码架构>编解码架构</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0030_transformer/>Transformer</a><ul><li><a href=/zh-cn/posts/00200_nlp/0030_transformer/0001_attention/ title=Attention>Attention</a></li><li><a href=/zh-cn/posts/00200_nlp/0030_transformer/0010_transformer_summary/ title=Transformer>Transformer</a></li><li><a href=/zh-cn/posts/00200_nlp/0030_transformer/0030_position/ title=位置编码>位置编码</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0080_gpt/>GPT</a><ul><li><a href=/zh-cn/posts/00200_nlp/0080_gpt/0010_gpt_summary/ title=GPT综述>GPT综述</a></li><li><a href=/zh-cn/posts/00200_nlp/0080_gpt/0020_gpt1_detail/ title=GPT-1>GPT-1</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0100_bert/>Bert</a><ul><li><a href=/zh-cn/posts/00200_nlp/0100_bert/0001_bert_summary/ title=Bert综述>Bert综述</a></li><li><a href=/zh-cn/posts/00200_nlp/0100_bert/0020_bert_family/ title=Bert家族>Bert家族</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0120_t5/>T5</a><ul><li><a href=/zh-cn/posts/00200_nlp/0120_t5/0010_t5_summary/ title=T5综述>T5综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0150_bart/>BART</a><ul><li><a href=/zh-cn/posts/00200_nlp/0150_bart/bart_summary/ title=BART综述>BART综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0200_electra/>ELECTRA</a><ul><li><a href=/zh-cn/posts/00200_nlp/0200_electra/electra_summary/ title=ELECTRA综述>ELECTRA综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/1000_code/>CODE</a><ul><li><a href=/zh-cn/posts/00200_nlp/1000_code/bart_summary/ title=code解析>code解析</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/>AIGC</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/0005_summary/>AIGC综述</a><ul><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0001_aigc_summary/ title=综述>综述</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0002_aigc_data/ title=LLM-数据集>LLM-数据集</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0005_aigc_application/ title=模型应用策略>模型应用策略</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0010_aigc_train_p/ title=大模型训练框架>大模型训练框架</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0012_aigc_amp/ title=混合精度训练>混合精度训练</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0015_aigc_train_mini/ title=模型小型化>模型小型化</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0020_aigc_error/ title=生成式-问题>生成式-问题</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0025_aigc_eval/ title=生成模型-评估>生成模型-评估</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/0010_generate_text/>文本生成</a><ul><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0011_gpt/ title=GPT>GPT</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0015_llama/ title=LLaMa>LLaMa</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0020_palm/ title=PaLM>PaLM</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0025_chatglm/ title=ChatGLM>ChatGLM</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0030_claude/ title=Claude>Claude</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0035_cohere/ title=Cohere>Cohere</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0040_falcon/ title=Falcon>Falcon</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0045_vicuna/ title=Vicuna>Vicuna</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/0020_generate_image/>图像生成</a><ul><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1000_summary/ title=综述>综述</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1002_gan_summary/ title=GAN>GAN</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1005_can_summary/ title=CAN>CAN</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1010_dall_e/ title=DALL-E>DALL-E</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1015_vqgan/ title=VQGAN>VQGAN</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1020_diffusion/ title=Diffusion>Diffusion</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1025_midjourney/ title=Midjourney>Midjourney</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1030_imagen/ title=Imagen>Imagen</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00400_vlp/>多模态</a><ul><li><a href=/zh-cn/posts/00400_vlp/0001_vlp_summary/ title=简介>简介</a></li><li><a href=/zh-cn/posts/00400_vlp/0005_clip/ title=CLIP>CLIP</a></li><li><a href=/zh-cn/posts/00400_vlp/0010_mllm/ title=MLLM>MLLM</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00500_video/>视频理解</a><ul><li><a href=/zh-cn/posts/00500_video/vidio_summary/ title=简介>简介</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/john_hu7f9991f5b5d471ecdfc6cff3db1a9fe6_6397_120x120_fit_box_2.png alt="Author Image"><h5 class=author-name>biubiobiu</h5><p>August 1, 2023</p></div><div class=title><h1>随机变量及其分布</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/zh-cn/tags/%E6%A6%82%E7%8E%87%E8%AE%BA class="btn, btn-sm">概率论</a></li><li class=rounded><a href=/zh-cn/tags/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F class="btn, btn-sm">随机变量</a></li><li class=rounded><a href=/zh-cn/tags/%E5%88%86%E5%B8%83 class="btn, btn-sm">分布</a></li></ul></div><div class=post-content id=post-content><h2 id=一一维随机变量>一、一维随机变量</h2><blockquote><p>如何引入一个法则，将随机试验的每个结果（即：$S$ 中的每个元素 $e$）与实数 $x$ 对应起来<br>从而引入了<font color=#f00000>随机变量</font>的概念，即：定义域是：样本空间$S$，值域是：实数。<br><strong>定义</strong>：设随机试验的样本空间为 $S=\{e\}, X=X(e)$ 是定义在样本空间 $S$ 上的实值单值函数。称 $X = X(e)$ 为随机变量。<br><strong>例如</strong>：以 $X$ 记录三次投掷硬币得到正面的次数。$P(X=2) = 3/8$ 就表示：随机变量 $X=2$ 的概率，就是 $A=\{HHT,HTH,THH\}$ 这个事件的概率。<br></p></blockquote><p>随机变量的引入，使得我们能用随机变量来描述各种随机现象，并能利用数学分析的方法对随机试验的结果进行深入广泛的研究和讨论。</p><h3 id=1离散型随机变量>1、离散型随机变量</h3><div class="alert alert-info"><strong><p><strong>离散型随机变量</strong>：随机变量，它全部可能取到的值是有限个或者可列无限多个。 可以用 <font color=#f00000>分布律</font> 来描述。<br></p><p><strong>常见离散型随机变量</strong>：<br></p><ol><li><p>(0-1)分布 <font color=#f00000>期望：p，方差：p(1-p)</font><br>随机变量 $X$ 只能取 0 与 1 两个值。</p></li><li><p>伯努利实验、二项分布，记：$ \textcolor{#f00000} {X \sim b(n, p)，期望：np，方差：np(1-p)}$<br>设实验 $E$ 只有两个可能结果：$A$ 和 $\bar A$ ，则称 $E$ 为伯努利实验。<br>将实验 $E$ 独立重复地进行 $n$ 次，则称这一串重复的独立实验为 $n$ 重伯努利实验。<br>设：$P(A) = p, P(\bar A) = 1 - p = q$ ，显然：<br>$$P(X=k) = C^k_n p^kq^{n-k}$$<br>$ \sum^n_{k=0} P(X=k) = \sum^n_{k=0} C^k_n p^k q^{n-k} = (p+q)^n = 1$</p></li><li><p>泊松分布，记：$\textcolor{#f00000} {X \sim \pi(\lambda)，期望：\lambda，方差：\lambda}$<br>设：随机变量 $X$ 所有可能取的值为 $0, 1, 2, &mldr;$ ，而取各个值的概率为：<br>$$P(X=k) = \frac{\lambda^k e^{-\lambda}}{k!}$$<br>其中 $\lambda > 0$ 是常数，则称 $X$ 服从参数为 $\lambda$ 的泊松分布，记为 $X \sim \pi(\lambda)$<br>由于：
$$
\sum^{\infty}_{k=0} \frac{\lambda^k}{k!} = e^{\lambda}
$$
服从泊松分布的，例如：一本书一页中的印刷错误数、某医院在一天内的急诊病人数、某地区一个时间间隔内发生交通事故的次数。</p></li></ol></strong></div><hr><div class="alert alert-success"><strong><strong>泊松定理</strong>：设 $\lambda > 0$ 是一个常数，$n$ 是任意正整数，设 $np_n = \lambda$，则对于任一固定的非负整数 $k$，有
$$
lim_{x \to \infty} C^k_n p^k_n(1-p_n)^{n-k} = \frac{\lambda^k e^{-\lambda}}{k!}
$$
定理的条件：$np_n = \lambda$，意味着当 $n$ 很大时（$\lambda$ 是常数），$p_n$ 必定很小。因此，上述定理表明当 $n$ 很大，$p$ 很小时，有以下近似式
$$
C^k_n p^k (1-p)^{n-k} \approx \frac{\lambda^k e^{-\lambda}}{k!}
$$
也就是说以 $n, p$ 为参数的二项分布的概率值可以由参数为 $\lambda = np$ 的泊松分布的概率值近似。</strong></div><h3 id=2非离散型>2、非离散型</h3><p>对于非离散型随机变量 $X$ ，由于其值不能一一列举出来，因而不能用分布律来描述它。<font color=#f00000>另外，通常所遇到的非离散型随机变量取任一指定的实数值的概率都是0。再者，我们不会对 $X$ 等于具体的值的概率感兴趣，而是对 随机变量 $X$ 落在一个区间 $(x_1, x_2)$ 的概率 感兴趣。</font><br>所以，我们只需要知道 $P(X &lt;= x_2)$ 和 $P(X &lt;= x_1)$ 就可以了，即：<font color=#f00000>分布函数</font>。</p><div class="alert alert-info"><strong><strong>分布函数</strong>：设 $X$ 是一个随机变量，$x$ 是任意实数，函数 $F(x) = P(X &lt;= x), -\infty &lt; x &lt; \infty$ 称为 $X$ 的分布函数。</strong></div><h3 id=3连续型随机变量>3、连续型随机变量</h3><div class="alert alert-info"><strong><p><strong>概率密度</strong>：对于随机变量 $X$ 的分布函数 $F(x)$，若存在非负函数 $f(x)$，使得对于任意实数 $x$ 有
$$
F(x) = \int^x_{-\infty} f(x) dt
$$
则称 $X$ 为连续型随机变量，其中函数 $f(x)$ 称为 $X$ 的概率密度函数，简称 概率密度。<br></p><p>给定 $X$ 的概率密度 $f(x)$ 就能确定 $F(x)$，由于 $f(x)$ 位于积分号之内，故改变 $f(x)$ 在个别点上的函数值并不改变 $F(x)$ 的值，因此，改变 $f(x)$ 在个别点上的值，是无关紧要的。</p><p><strong>常见连续型随机分布</strong><br></p><ol><li><p>均匀分布 $ \textcolor{#f00000} {X \sim U(a, b)，期望：\frac{a+b}{2}，方差：\frac{(b-a)^2}{12}}$<br>$
f(x) = \begin{cases}
\frac{1}{b-a} &a&lt;x&lt;b \<br>0 & 其他
\end{cases}
$</p></li><li><p>指数分布 <font color=#f00000>期望：$\theta，方差：\theta^2$</font><br>若连续型随机变量 $X$ 的概率密度为：<br>$
f(x) = \begin{cases}
\frac{1}{\theta} e^{-x/\theta} & x > 0 \<br>0 & 其他
\end{cases}
$<br>其中，$\theta > 0$ 为常数，则称 $X$ 服从参数为 $\theta$ 的指数分布。<br><font color=#f00000>属性</font>：无记忆性。$P(X > s+t | X > s) = P(X > t)$，即：$X$ 表示一元件的寿命。已知元件已经使用 s 小时，它总共能使用至少 s+t 小时的条件概率，与从开始使用时算起它至少能使用 t 小时的概率相等。也就是说，元件对它已使用过 s 小时没有记忆。</p></li><li><p>正态分布 $ \textcolor{#f00000} {X \sim N(\mu, \sigma^2)}$<br>若连续型随机变量 $X$ 的概率密度为：<br>$$
f(x) = \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}, -\infty &lt; x &lt; \infty
$$
其中，$\mu, \sigma > 0$ 为常数，则称 $X$ 服从参数为 $\mu, \sigma$ 的正态分布或者高斯分布，记为 $X \sim N(\mu, \sigma^2)$<br><font color=#f00000>属性</font>：</p><ul><li>曲线关于 $x=\mu$ 对称</li><li>当 $x=\mu$ 时，取到最大值，$x$ 离 $\mu$ 越远，$f(x)$ 的值越小。</li><li>如果固定 $\mu$，改变 $\sigma$ ，当 $\sigma$ 越小时，图形变得越尖，因而 $X$ 落在 $\mu$ 附近的概率越大。</li></ul></li></ol><p align=center><img src=/datasets/posts/maths/gass_n.png width=100% height=100%></p><ol start=4><li>$\Gamma$ 分布 $\textcolor{#f00000} {X \sim \Gamma(\alpha, \theta)}$<br>$
f(x) = \begin{cases}
\frac{1}{\theta^\alpha \Gamma(\alpha)} x^{\alpha-1} e^{-x/\theta} & x > 0, \alpha > 0, \theta > 0 \<br>0 & 其他
\end{cases}
$</li></ol></strong></div><h2 id=二一个随机变量的函数>二、一个随机变量的函数</h2><p>在实际中，我们感兴趣的随机变量不能直接测量得到，而它却是某个能直接测量的随机变量的函数。比如：我们不能直接测量圆的面积，我们可以直接测量直径 $d$，面积可以由直径计算得到：$A = \frac{1}{4} \pi d^2$。<br>所以，需要研究如何由已知的随机变量 $X$ 的概率分布去求得它的函数 $Y = g(X)$ 的概率分布。</p><p>设 $y = g(x), x = h(y)$<br>需要计算：$F_Y(y) = P(Y &lt;= y) = P(g(x) &lt;= y)$</p><div class="alert alert-info"><strong><strong>定理</strong>：<br>设随机变量 $X$ 具有概率密度 $f_x(x), -\infty &lt; x &lt; \infty$，又设函数 $y = g(x)$ 处处可导且<font color=#f00000>恒有 $g'(x) > 0$ 或者 恒有 $g'(x) &lt; 0$，</font>则 $Y = g(X)$ 是连续型随机变量，其概率密度为<br>$
f_y(y) = \begin{cases}
f_x[h(y)]|h'(y)| & a &lt; y &lt; b> \<br>0 & 其他
\end{cases}
$
其中，$a = min(g(-\infty), g(\infty)), b = max(g(-\infty), g(\infty))$，$h(y)$ 是 $g(x)$ 的反函数。</strong></div><h2 id=三多维随机变量及其分布>三、多维随机变量及其分布</h2><p>比如炮弹弹着点的位置需要由它的横坐标，纵坐标来确定，而横坐标和纵坐标是定义在同一个样本空间的两个随机变量。<br></p><p>二维随机变量 $(X, Y)$ 的性质不仅与 $X$、$Y$ 有关，而且还依赖于这两个随机变量的相互关系，因此，逐个来研究 $X$、$Y$ 的性质是不够的，还需要将 $(X, Y)$ 作为一个整体来进行研究。</p><div class="alert alert-info"><strong><p><strong>定义</strong>：设 $(X, Y)$ 是二维随机变量，对于任意实数 $x, y$ 二元函数：
$$
F(x, y) = P[(X&lt;=x) \cap (Y&lt;=y)] \overset{\underset{\mathrm{记作}}{}}{==} P(X&lt;=x, Y&lt;=y)
$$
称为二维随机变量 $(X, Y)$ 的分布函数，或称为随机变量 $X$ 和 $Y$ 的联合分布函数。<br></p><p>二维随机变量 $(X, Y)$ 作为一个整体，具有分布函数 $F(x, y)$，而 $X$ 和 $Y$ 都是随机变量，各自也有分布函数，将它们分别记为 $F_X(x), F_Y(y)$，依次称为二维随机变量 $(X, Y)$ 关于 $X$ 和 $Y$ 的 <font color=#f00000>边缘分布函数</font>，边缘分布函数可以由 $(X, Y)$ 的分布函数 $F(x, y)$ 所确定
$$
F_X(x) = P(X&lt;=x) = P(X&lt;=x, Y &lt; \infty) = F(x, \infty) \<br>f_X(x) = \int^\infty_{-\infty} f(x, y) dy \<br>f_Y(y) = \int^\infty_{-\infty} f(x, y) dx
$$
分别称 $f_X(x), f_Y(y)$ 为 $(X, Y)$ 关于 $X$ 和 $Y$ 的边缘概率密度</p><p><strong>条件分布</strong><br></p><ol><li><p>离散型 - 条件分布律<br>设 $(X, Y)$ 是二维离散型随机变量，对于固定的 $j$，若 $P(Y = y_j)$ > 0，则称
$$
P(X=x_i | Y=y_j) = \frac{P(X=x_i, Y=y_j)}{P(Y = y_j)} = \frac{p_{ij}}{p_{·j}}
$$
为在 $Y = y_j$ 条件下随机变量 $X$ 的条件分布律</p></li><li><p>连续型 - 条件概率密度<br>设二维随机变量 $(X, Y)$ 的概率密度为 $f(x, y)$，$(X, Y)$ 关于 $Y$ 的边缘概率密度为 $f_Y(y)$，若对于固定的 $y$，$f_Y(y) > 0$，则称 $\frac{f(x, y)}{f_Y(y)}$ 为在 $Y=y$ 的条件下 $X$ 的条件概率密度，记为：
$$
f_{X|Y}(x|y) = \frac{f(x, y)}{f_Y(y)}
$$
称 $\int^x_{-\infty} f_{X|Y}(x|y) dx = \int^x_{-\infty} \frac{f(x, y)}{f_Y(y)} dx$ 为在 $Y=y$ 的条件下 $X$ 的条件分布函数，记为 $P(X&lt;=x | Y=y)$ 或 $F_{X|Y}(x|y)$</p></li></ol><p><strong>常见二维随机变量</strong>：<br></p><ol><li>二维正态分布 $\textcolor{#f00000} {(X, Y) \sim N(\mu_1, \mu_2, \sigma^2_1, \sigma^2_2, \rho)}$<br>$$
f(x, y) = \frac{1}{2\pi\sigma_1 \sigma_2 \sqrt{1-\rho^2}} e ^{-\frac{1}{2(1-\rho^2)}[\frac{(x-\mu_1)^2}{\sigma^2_1}-2\rho \frac{(x-\mu_1)(y-\mu_2)}{\sigma_1 \sigma_2} + \frac{(y-\mu_2)^2}{\sigma^2_2}]}
$$
其中，$\mu_1, \mu_2, \sigma_1, \sigma_2, \rho$ 都是常数，且 $\sigma_1 > 0, \sigma_2 > 0, -1 &lt; \rho &lt; 1$。<br>x 的边缘概率密度：
$$
f_X(x) = \frac{1}{\sqrt{2\pi} \sigma_1} e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}}, -\infty &lt; x &lt; \infty
$$
y 的边缘概率密度：
$$
f_Y(y) = \frac{1}{\sqrt{2\pi \sigma_2}} e^{-\frac{(y-\mu_2)^2}{2\sigma_2^2}}, -\infty &lt; x &lt; \infty
$$
<font color=#f00000>可以看到二维正态分布的两个边缘分布都是一维正态分布，并且都不依赖于参数 $\rho$。</font><br>对于给定的$\mu_1, \mu_2, \sigma_1, \sigma_2$，不同的 $\rho$ 对应不同的二维正态分布，它们的边缘分布却都一样。这说明：<font color=#f00000>单由 $X$ 和 $Y$ 的边缘分布，一般来说不能确定随机变量 $X$ 和 $Y$ 的联合分布。</font></li></ol></strong></div><h2 id=四两个随机变量的函数>四、两个随机变量的函数</h2><p>两个随机变量组成的函数，是否也是个随机变量呢？</p><h3 id=1zxy-的分布>1、$Z=X+Y$ 的分布</h3><p>设 $(X, Y)$ 是二维连续型随机变量，它具有概率密度 $f(x, y)$，则 $Z=X+Y$ 仍然为连续型随机变量，其概率密度为：
$$
f_{X+Y}(z) = \int^\infty_{-\infty} f(z-y, y) dy \<br>f_{X+Y}(z) = \int^\infty_{-\infty} f(x, z-x) dy
$$
如果 $X, Y$ 相互独立，设 $(X, Y)$ 关于 $X, Y$ 的边缘密度分别为 $f_X(x), f_Y(y)$，则
$$
f_{X+Y}(z) = \int^\infty_{-\infty} f_X(z-y) f_Y(y) dy \<br>f_{X+Y}(z) = \int^\infty_{-\infty} f_X(x)f_Y(z-x) dx
$$
这两个公式称为 $f_X$ 和 $f_Y$ 的卷积公式，记为 $f_X * f_Y$ 即：
$$
f_X * f_Y = \int^\infty_{-\infty} f_X(z-y) f_Y(y) dy = \int^\infty_{-\infty} f_X(x)f_Y(z-x) dx
$$
<font color=#f00000>一般，设 $X, Y$ 相互独立，且 $X \sim N(\mu_1, \sigma_1^2), Y \sim N(mu_2, \sigma_2^2)$。由于 $Z=X+Y$ 仍然服从正态分布，且有 $Z \sim N(\mu_1+\mu_2, \sigma_1^2+\sigma_2^2)$。这个结论还能推广到 n 个独立正态随机变量之和的情况，即：若 $X_i \sim N(\mu_i, \sigma_i^2)$ 且它们相互独立，则 它们的和 $Z = X_1 + X_2 + &mldr; + X_n$ 仍然服从正态分布，且有 $Z \sim N(\mu_1+\mu_2+&mldr;+\mu_n, \sigma_1^2+\sigma_1^2+&mldr;+\sigma_n^2)$</font><br><font color=#a020f0>更一般：有限个相互独立的正态随机变量的线性组合，仍然服从正态分布。</font><br></p><p><font color=#f00000>如果是，$X, Y$ 都是 $\Gamma(\alpha, \theta)$ 分布，且相互独立</font>：<br>$X$ 的分布<br>$
f(x) = \begin{cases}
\frac{1}{\theta^\alpha \Gamma(\alpha)} x^{\alpha-1} e^{-x/\theta} & x > 0, \alpha > 0, \theta > 0 \<br>0 & 其他
\end{cases}
$<br>$Y$ 的分布<br>$
f(y) = \begin{cases}
\frac{1}{\theta^\beta \Gamma(\beta)} y^{\beta-1} e^{-y/\theta} & x > 0, \beta > 0, \theta > 0 \<br>0 & 其他
\end{cases}
$<br>有结论：$Z = X + Y$ 服从参数为 $\alpha + \beta, \theta$ 的 $\Gamma$ 分布，即 $X+Y \sim \Gamma(\alpha+\beta, \theta)$<br>这个结论还能推广到 $n$ 个相互独立的 $\Gamma$ 分布变量之和的情况，这一性质称为 $\Gamma$ 分布的可加性。</p><h3 id=2zfracyx-zxy的分布>2、$Z=\frac{Y}{X}, Z=XY$的分布</h3><p>设 $(X, Y)$ 是二维连续型随机变量，它具有概率密度 $f(x, y)$，则 $Z = \frac{Y}{X}$，$Z = YX$ 仍为连续型随机变量，其概率密度分别为：
$$
f_{Y/X} = \int^\infty_{-\infty} |x| f(x, xz) dx \<br>f_{XY} = \int^\infty_{-\infty} \frac{1}{|x|} f(x, \frac{z}{x}) dx
$$</p><h3 id=3mmaxx-y-nminx-y的分布>3、$M=max(X, Y); N=min(X, Y)$的分布</h3><p>设 $X, Y$ 是两个相互独立的随机变量，它们的分布函数分别为 $F_X(x)$ 和 $F_Y(y)$，现在来求 $M=max(X, Y); N=min(X, Y)$ 的分布函数。<br>$$
P(M &lt;= z) = P(X &lt;= z, Y &lt;= z) \overset{\mathrm{相互独立}}{===} P(X &lt;= z) P(Y &lt;= z) \<br>即: F_{max}(z) = F_X(z)F_Y(z)
$$
类似的：
$$
F_{min}(z) = p(N&lt;=z) = 1-P(N>z) = 1 - [1-F_X(z)][1-F_Y(z)]
$$</p><div class=row><div class="col col-sm-12 col-lg-6"><p align=center><img src=/datasets/posts/maths/dim2_fun.png width=60% height=60%></div><div class="col col-sm-12 col-lg-6">情况1：串联的情况： $Z = min(X, Y)$<br>情况2：并联的情况： $Z = max(X, Y)$<br>情况3：备用的情况： $Z = X+Y$</div></div><h2 id=五随机变量的数字特征>五、随机变量的数字特征</h2><p>虽然有分布函数、概率密度 可以完整地描述随机变量，但是某些实际的问题中，人们感兴趣于某些能描述随机变量某一种特征的常数，比如：运动员的平均身高、棉花纤维的平均长度、以及偏离程度。<br>所以，要介绍几个重要的数字特征：<font color=#f00000>数学期望、方差、相关系数、矩</font><br></p><h3 id=1数学期望>1、数学期望</h3><blockquote><p><strong>离散型</strong>：<br><strong>定义</strong>：设离散型随机变量 $X$ 的分布律 为
$$
P(X = x_k) = p_k, k = 1, 2, &mldr;
$$
若级数 $\sum^\infty_{k=1} x_k p_k$ 绝对收敛，则称级数 $\sum^\infty_{k=1} x_k p_k$ 的和为随机变量 $X$ 的 数学期望，记为 $E(X)$
$$
E(X) = \sum^\infty_{k=1} x_k p_k
$$<br>**连续型**：<br>设连续型随机变量 $X$ 的概率密度为 $f(x)$ 若积分 $\int^\infty_{-\infty} xf(x) dx$ 绝对收敛，则称积分 $\int^\infty_{-\infty} xf(x) dx$ 的值为随机变量 $X$ 的数学期望，记为 $E(X)$<br>$$
E(X) = \int^\infty_{-\infty} xf(x) dx
$$</p></blockquote><div class="alert alert-info"><strong><p><strong>随机变量的函数的期望</strong>:<br>定理：设 $Y$ 是随机变量 $X$ 的函数：$Y = g(X)$<br></p><ol><li><p>如果 $X$ 是离散型随机变量，则有
$$
E(Y) = E[g(X)] = \sum^\infty_{k=1} g(x_k)p_k
$$</p></li><li><p>如果 $X$ 是连续型随机变量，它的概率密度为 $f(x)$，则有
$$
E(Y) = E[g(X)] = \int^\infty_{-\infty} g(x)f(x)dx
$$</p></li></ol><p><font color=#f00000>定理的重要意义：当我们求 $E(Y)$ 时，不必计算 $Y$ 的分布律或者概率密度，而只需要利用 $X$ 的分布律或者概率密度就可以了。</font></p><p><strong>重要性质</strong>：<br></p><ol><li>设 $C$ 是常数，则有 $E(C) = C$</li><li>设 $X$ 是一个随机变量，$C$ 是常数，则有
$$
E(CX) = CE(X)
$$</li><li>设 $X, Y$ 是两个随机变量，则有
$$
E(X+Y) = E(X) + E(Y)
$$</li><li>设 $X, Y$ 是相互独立的随机变量，则有
$$
E(XY) = E(X) E(Y)
$$</li></ol></strong></div><h3 id=2方差>2、方差</h3><div class="alert alert-info"><strong><p>定义：设 $X$ 是一个随机变量，若 $E([X-E(X)]^2)$ 存在，则称 $E([X-E(X)]^2)$ 为 $X$ 的方差，记：$D(X) 或 Var(X)$，即：
$$
D(X) = Var(X) = E([X-E(X)]^2) = E(X^2) - [E(X)]^2
$$
来度量随机变量 $X$ 与 其均值 $E(X)$ 的偏离程度。 $D(X)$ 是刻画 $X$ 取值分散程度的一个量，它是衡量 $X$ 取值分散程度的一个尺度。<br></p><p>方差的重要性质：</p><ol><li>设 $C$ 为常数，则 $D(C) = 0$</li><li>设 $X$ 是随机变量，$C$ 是常数，则有
$$
D(CX) = C^2 D(X), D(X+C) = D(X)
$$</li><li>设 $X, Y$ 是两个随机变量，则有
$$
D(X+Y) = D(X) + D(Y) + 2E[(X-E(X))(Y-E(Y))]
$$
若 $X, Y$ 相互独立，则有
$$
D(X+Y) = D(X) + D(Y)
$$</li><li>$D(X) = 0$ 的充要条件是 $X$ 以概率1取常数 $E(X)$，即
$$
P(X=E(X)) = 1
$$</li></ol></strong></div><hr><div class="alert alert-success"><strong><p>有些分布，当拿到 期望、方差 这两个数字特征，可以完全确定整个分布：</p><ol><li>正态分布，完全可以由它的数学期望、方差确定</li><li>指数分布，$E(X) = \theta, D(X) = \theta^2$，完全可以根据 $\theta$ 完全确定整个分布</li><li>泊松分布，$E(X) = \lambda, D(X) = \lambda$，完全可以根据 $\lambda$ 完全确定整个分布</li></ol><p><font color=#f00000>切比雪夫(Chebyshev)不等式</font>：<br>设随机变量 $X$ 具有数学期望 $E(X) = \mu$，方差 $D(X) = \sigma^2$，则对于任意整数 $\varepsilon$，不等式成立：
$$
P[|X-\mu| \geqslant \varepsilon] \leqslant \frac{\sigma^2}{\varepsilon^2}
$$
切比雪夫不等式给出了在随机变量的<font color=#f00000>分布未知</font>，而只知道 $E(X), D(X)$ 的情况下，估计概率 $P[|X-\mu| \geqslant \varepsilon]$ 的界限。<br></p><p><strong>证明</strong>：<br>$$
P[|X-\mu| \geqslant \varepsilon] = \int_{|X-\mu| \geqslant \varepsilon} f(x) dx \leqslant \int_{|X-\mu| \geqslant \varepsilon} \frac{|x-\mu|^2}{\varepsilon^2} f(x) dx \leqslant \frac{1}{\varepsilon^2} \int^\infty_{-\infty} (x-\mu)^2 f(x)dx = \frac{\sigma^2}{\varepsilon^2}
$$</p></strong></div><h3 id=3相关系数>3、相关系数</h3><p>描述 $X, Y$ 之间相互关系的数字特征。前面计算方差时，如果相互独立，则 $E([X-E(X)][Y-E(Y)]) = 0$，如果不相互独立，则 $X, Y$ 之间存在一定的关系。</p><div class="alert alert-info"><strong><p><strong>定义</strong>： $E([X-E(X)][Y-E(Y)])$ 称为随机变量 $X, Y$ 的协方差，记为：$Cov(X, Y)$，即：
$$
Cov(X, Y) = E([X-E(X)][Y-E(Y)]) = E(XY) - E(X)E(Y)
$$
而
$$
\rho = \frac{Cov(X, Y)}{\sqrt{D(X)} \sqrt{D(Y)}}
$$
称为随机变量 $X, Y$ 的相关系数。<br></p><p><font color=#f00000>解释</font>：$|\rho|$ 较大时，表明 $X, Y$ （就线性关系来说）联系较紧密，特别当 $|\rho|=1$ 时，$X, Y$ 之间存在线性关系。于是 $\rho$ 是一个可以用来表征 $X, Y$ 之间线性关系紧密程度的量。<br></p><p><font color=#f00000>$\rho = 0$ 称 $X, Y$ 不相关。当$X, Y$相互独立时，有：$Cov(X, Y) = 0, \rho = 0$。反之，若 $\rho=0$，$X, Y$不相关，$X, Y$ 不一定相互独立。<br>因为：相关性 只是从线性关系来说的，而互相独立 是就更一般的关系而言。</font></p><p><font color=#a020f0>对于正态分布而言，不相关 与 相互独立 是等价的。</font></p><p>根据定义，可以推理出：</p><ol><li>$Cov(X, Y) = Cov(Y, X), Cov(X, X) = D(X)$</li><li>$D(X+Y) = D(X) + D(Y) + 2Cov(X, Y)$</li><li>$Cov(aX, bY) = ab Cov(X, Y)$</li><li>$Cov(X_1+X_2, Y) = Cov(X_1, Y) + Cov(X_2, Y)$</li></ol><p><strong>实例</strong>：<br></p><ol><li>正态分布<br>设 $X \sim N(\mu_1, \sigma^2_1), Y \sim N(\mu_2, \sigma^2_2)$，故
$$
Cov(X, Y) = \rho \sigma_1 \sigma_2
$$
这就是说，二维正态随机变量 $(X, Y)$ 的概率密度中的参数 $\rho$ 就是 $X, Y$ 的相关系数，因而二维正态随机变量的分布完全可由 $X, Y$ 各自的数学期望、方差以及它们的相关系数锁确定。<br><font color=#a020f0>若 $X, Y$ 服从二维正态分布，那么 $X, Y$ 相互独立的充要条件是：$\rho = 0$。</font></li></ol></strong></div><h3 id=4矩>4、矩</h3><div class="alert alert-info"><strong><p>设 $X, Y$ 是二维随机变量<br><strong>定义</strong>：设 $X, Y$ 是随机变量，若
$$
E(X^k), k = 1, 2, &mldr;
$$
存在，称它为 $X$ 的 <font color=#f00000>$k$ 阶原点矩</font>，简称 $k$ 阶矩。若
$$
E([X-E(X)]^k), k = 2, 3, &mldr;
$$
存在，称它为 $X$ 的 <font color=#f00000>$k$ 阶中心矩</font>。若
$$
E([X-E(X)]^k [Y-E(Y)]^l), k,l = 1, 2, 3, &mldr;
$$
存在，称它为 $X, Y$ 的 <font color=#f00000>$k+l$ 阶混合中心矩</font>。<br></p><p>显然，$X$ 的数学期望 $E(X)$ 是 $X$ 的一阶原点矩，方差 $D(X)$ 是 $X$ 的二阶中心距，协方差 $Cov(X, Y)$ 是 $X, Y$ 的二阶混合中心距。<br></p><p>二维随机变量 $(X_1, X_2)$ 有四个二阶中心矩，设它们都存在，分别记为：<br>$c_{11} = E([X_1 - E(X_1)]^2)$<br>$c_{12} = E([X_1 - E(X_1)][X_2 - E(X_2)])$<br>$c_{21} = E([X_2 - E(X_2)][X_1 - E(X_1)])$<br>$c_{22} = E([X_2 - E(X_2)]^2)$<br>将它们排成矩阵的形式：
$$
\begin{pmatrix}
c_{11} & c_{12} \<br>c_{21} & c_{22}
\end{pmatrix}
$$
这个矩阵称为随机变量 $(X_1, X_2)$ 的 <font color=#f00000>协方差矩阵</font>。<br></p><p>更为一般的 $n$ 维随机变量 $X_1, X_2, &mldr;, X_n$ 的协方差矩阵：
$$
\begin{pmatrix}
c_{11} & c_{12} & &mldr; & c_{1n} \<br>c_{21} & c_{22} & &mldr; & c_{2n} \<br>\vdots & \vdots & &mldr; & \vdots \<br>c_{n1} & c_{n2} & &mldr; & c_{nn}
\end{pmatrix}
$$
由于 $c_{ij} = c_{ji}, i \ne j$，所以 上述矩阵式一个对称矩阵。<br></p><p><font color=#c020f0>一般，$n$ 维随机变量的分布是不知道的，或者太复杂，以至在数学上不易处理，因此，在实际应用中协方差矩阵就显得重要了。</font><br>比如：$n$ 维正态随机变量的概率密度：<br>先看二维正态随机变量的概率密度，改写成另一种形式，以便将它推广到 $n$ 维随机变量的场合中区。
$$
f(x_1, x_2) = \frac{1}{2\pi\sigma_1\sigma_2 \sqrt{1-\rho^2}} exp\{ -\frac{\frac{(x_1-\mu_1)^2}{\sigma_1^2} - 2\rho \frac{(x_1 - \mu_1)(x_2 - \mu_2)}{\sigma_1 \sigma_2} + \frac{(x_2 - \mu_2)^2}{\sigma_2^2}}{2(1-\rho^2)} \}
$$
写成矩阵形式：
$$
\mathbf{X} = \begin{pmatrix}
x_1 \<br>x_2
\end{pmatrix}, \mathbf{\mu} = \begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix},
\mathbf{C} = \begin{pmatrix} c_{11} & c_{12} \\ c_{21} & c_{22} \end{pmatrix} = \begin{pmatrix} \sigma_1^2 & \rho \sigma_1 \sigma_2 \\ \rho \sigma_1 \sigma_2 & \sigma_2^2 \end{pmatrix}
$$
行列式 $det \mathbf{C} = \sigma_1^2 \sigma_2^2 (1-\rho^2)$，$\mathbf{C}$ 的逆矩阵：
$$
\mathbf{C^{-1}} = \frac{1}{det \mathbf{C}} \begin{pmatrix} \sigma_2^2 & -\rho \sigma_1 \sigma_2 \\ -\rho \sigma_1 \sigma_2 & \sigma_1^2 \end{pmatrix}
$$
开始变换矩阵形式：
$$
(\mathbf{X - \mu})^T \mathbf{C}^{-1} (\mathbf{X - \mu}) \<br>= \frac{1}{det \mathbf{C}} \begin{pmatrix} x_1 - \mu_1 & x_2 - \mu_2 \end{pmatrix} \begin{pmatrix} \sigma_2^2 & -\rho \sigma_1 \sigma_2 \\ -\rho \sigma_1 \sigma_2 & \sigma_1^2 \end{pmatrix} \begin{pmatrix} x_1 - \mu_1 \\ x_2 - \mu_2 \end{pmatrix} \<br>= \frac{1}{1-\rho^2} [\frac{(x_1-\mu_1)^2}{\sigma_1^2} - 2\rho \frac{(x_1 - \mu_1)(x_2 - \mu_2)}{\sigma_1 \sigma_2} + \frac{(x_2 - \mu_2)^2}{\sigma_2^2}]
$$
于是 $X_1, X_2$ 的概率密度可以写成：
$$
f(x_1, x_2) = \frac{1}{(2\pi)^{2/2} (det \mathbf{C})^{1/2}} exp\{ -\frac{1}{2} (\mathbf{X - \mu})^T \mathbf{C}^{-1} (\mathbf{X - \mu}) \}
$$</p><p><font color=#f00000>推广到 $n$ 维正态随机变量</font>：
$$
f(x_1, x_1, &mldr;, x_n) = \frac{1}{(2\pi)^{n/2} (det \mathbf{C})^{1/2}} exp\{ -\frac{1}{2} (\mathbf{X - \mu})^T \mathbf{C}^{-1} (\mathbf{X - \mu}) \}
$$
其中
$$
\mathbf{X} = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix},
\mathbf{\mu} = \begin{pmatrix} \mu_1 \\ \mu_2 \\ \vdots \\ \mu_n \end{pmatrix},
\mathbf{C} = \begin{pmatrix}
c_{11} & c_{12} & &mldr; & c_{1n} \<br>c_{21} & c_{22} & &mldr; & c_{2n} \<br>\vdots & \vdots & &mldr; & \vdots \<br>c_{n1} & c_{n2} & &mldr; & c_{nn}
\end{pmatrix}
$$</p></strong></div><hr><div class="alert alert-success"><strong><p>$n$ 维正态随机变量具有以下四条重要性质：</p><ol><li><p>$n$ 维正态随机变量 $X_1, X_2, &mldr;, X_n$ 的每一个分量 $X_i$ 都是正态随机变量，反之，若 $X_1, X_2, &mldr;, X_n$ 都是正态随机变量，且相互独立，则$X_1, X_2, &mldr;, X_n$ 是 $n$ 维正态随机变量。</p></li><li><p>$n$ 维正态随机变量 $X_1, X_2, &mldr;, X_n$ 服从 $n$ 维正态分布的充要条件是 $X_1, X_2, &mldr;, X_n$ 的任意的线性组合
$$
l_1 X_1 + l_2 X_2 + &mldr; + l_n X_n
$$
服从一维正态分布（其中 $l_1, l_2, &mldr;, l_n$ 不全为零）</p></li><li><p>若 $X_1, X_2, &mldr;, X_n$ 服从 $n$ 维正态分布，设 $Y_1, Y_2, &mldr;, Y_k$ 是 $X_j (j = 1, 2, &mldr;, n)$ 的线性函数，则 $Y_1, Y_2, &mldr;, Y_k$ 也服从多维正态分布。 这一性质称为 <font color=#f00000>正态变量的线性变换不变性</font>。</p></li><li><p>若 $X_1, X_2, &mldr;, X_n$ 服从 $n$ 维正态分布，则 <font color=#a020c0>$X_1, X_2, &mldr;, X_n$ 相互独立</font> 与 <font color=#a020c0>$X_1, X_2, &mldr;, X_n$ 两两不相关</font> 是等价的。</p></li></ol></strong></div><h2 id=参考>参考</h2><h3 id=1极限>1、极限</h3><ol><li>e
$$
lim_{n \to \infty} (1-\frac{\lambda}{n})^n = e^{-\lambda}
$$</li></ol><h3 id=2积分>2、积分</h3><ol><li><p>根据泰勒公式证明
$$
\sum^{\infty}_{k=0} \frac{\lambda^k}{k!} = e^{\lambda}
$$</p></li><li><p>积分
$$
\int^\infty_{-\infty} e^{-x^2} dx = \sqrt{\pi}
$$</p></li><li><p>利用极坐标系 证明：$dxdy = rdrd\theta$ 即：
$$
\int^\infty_{-\infty} e^{- \frac{t^2}{2}} dt = \sqrt{2\pi}
$$</p></li><li><p>求积分
$$
\int^\infty_{-\infty} x^2 e^{-\frac{x^2}{2}} dx = \sqrt{2\pi}
$$</p></li><li><p>求积分
$$
\int^\infty_0 x e^{-x} dx = 1
$$</p></li></ol></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"><strong>分享:</strong>
<a class="btn btn-sm facebook-btn" href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00025_math_knowledge%2f0010_math_probability_theory%2f0020_distribution_of_variables%2f" target=_blank><i class="fab fa-facebook"></i></a><a class="btn btn-sm twitter-btn" href="https://twitter.com/share?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00025_math_knowledge%2f0010_math_probability_theory%2f0020_distribution_of_variables%2f&text=%e9%9a%8f%e6%9c%ba%e5%8f%98%e9%87%8f%e5%8f%8a%e5%85%b6%e5%88%86%e5%b8%83&via=biubiobiu%27s%20Blog" target=_blank><i class="fab fa-twitter"></i></a><a class="btn btn-sm reddit-btn" href="https://reddit.com/submit?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00025_math_knowledge%2f0010_math_probability_theory%2f0020_distribution_of_variables%2f&title=%e9%9a%8f%e6%9c%ba%e5%8f%98%e9%87%8f%e5%8f%8a%e5%85%b6%e5%88%86%e5%b8%83" target=_blank><i class="fab fa-reddit"></i></a><a class="btn btn-sm linkedin-btn" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00025_math_knowledge%2f0010_math_probability_theory%2f0020_distribution_of_variables%2f&title=%e9%9a%8f%e6%9c%ba%e5%8f%98%e9%87%8f%e5%8f%8a%e5%85%b6%e5%88%86%e5%b8%83" target=_blank><i class="fab fa-linkedin"></i></a><a class="btn btn-sm whatsapp-btn" href="https://api.whatsapp.com/send?text=%e9%9a%8f%e6%9c%ba%e5%8f%98%e9%87%8f%e5%8f%8a%e5%85%b6%e5%88%86%e5%b8%83 https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00025_math_knowledge%2f0010_math_probability_theory%2f0020_distribution_of_variables%2f" target=_blank><i class="fab fa-whatsapp"></i></a><a class="btn btn-sm email-btn" href="mailto:?subject=%e9%9a%8f%e6%9c%ba%e5%8f%98%e9%87%8f%e5%8f%8a%e5%85%b6%e5%88%86%e5%b8%83&body=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00025_math_knowledge%2f0010_math_probability_theory%2f0020_distribution_of_variables%2f" target=_blank><i class="fas fa-envelope-open-text"></i></a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0010_basic-conception/ title=基本概念 class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i>上一篇</div><div class=next-prev-text>基本概念</div></a></div><div class="col-md-6 next-article"><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0030_law_of_large_numbers/ title=大数定律 class="btn btn-outline-info"><div>下一篇 <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>大数定律</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">目录</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#一一维随机变量>一、一维随机变量</a><ul><li><a href=#1离散型随机变量>1、离散型随机变量</a></li><li><a href=#2非离散型>2、非离散型</a></li><li><a href=#3连续型随机变量>3、连续型随机变量</a></li></ul></li><li><a href=#二一个随机变量的函数>二、一个随机变量的函数</a></li><li><a href=#三多维随机变量及其分布>三、多维随机变量及其分布</a></li><li><a href=#四两个随机变量的函数>四、两个随机变量的函数</a><ul><li><a href=#1zxy-的分布>1、$Z=X+Y$ 的分布</a></li><li><a href=#2zfracyx-zxy的分布>2、$Z=\frac{Y}{X}, Z=XY$的分布</a></li><li><a href=#3mmaxx-y-nminx-y的分布>3、$M=max(X, Y); N=min(X, Y)$的分布</a></li></ul></li><li><a href=#五随机变量的数字特征>五、随机变量的数字特征</a><ul><li><a href=#1数学期望>1、数学期望</a></li><li><a href=#2方差>2、方差</a></li><li><a href=#3相关系数>3、相关系数</a></li><li><a href=#4矩>4、矩</a></li></ul></li><li><a href=#参考>参考</a><ul><li><a href=#1极限>1、极限</a></li><li><a href=#2积分>2、积分</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>导航</h5><ul><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#about>关于</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#skills>技能</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#experiences>经验</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#projects>项目</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#education>教育</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#recent-posts>最新博客</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#accomplishments>造诣</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#achievements>业绩</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>联系方式:</h5><ul><li><span>电话:</span> <span>+0123456789</span></li><li><span>邮箱:</span> <span>bsgshyn8@163.com</span></li></ul></div></div></div><hr><div class=container><p id=disclaimer><strong>免责声明:</strong> 这个主题是MIT许可的。因此，您可以将其用于非商业、商业或者私人用途。您可以修改或分发主题，而无须经 作者任何许可。但是，主题作者不对主题的任何问题提供任何保证或承担任何责任。</p></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=stylesheet href=/katex/katex.min.css><script type=text/javascript defer src=/katex/katex.min.js></script><script type=text/javascript defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body);></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false},{left:'\\(',right:'\\)',display:false},{left:'\\[',right:'\\]',display:true}],throwOnError:false});});</script><script src=/js/mermaid-8.14.0.min.js></script><script>mermaid.initialize({startOnLoad:true});</script></body></html>