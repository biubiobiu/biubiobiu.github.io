<!doctype html><html><head><title>静态图</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0d88336a9cbd3e68c7714efae786b0a9_38044_42x0_resize_box_2.png><meta property="og:title" content="静态图"><meta property="og:description" content="在TensorFlow 2中使用兼容性模块，必须使用tf.compat.v1替换tf，并且在导入TensorFlow软件包后添加一行tf.compat.v1.disable_eager_execution()函数来关闭eager执行模式。
import tensorflow as tf tf.compat.v1.disable_eager_execution() 简介 数据流是一种编程模型，被广泛地应用于并行计算中。TF使用数据流图来表示计算中各个运算之间的关系，在数据流图中，节点：表示计算单元(即：操作tf.Operation)；边：表示被计算单元消费/生产的数据(即：tf.Tensor)。 数据流图，可以被导出成一个可移植的、编程语言不相关的表示(ProtoBuf)，这种表示可以被其他语言使用，来创建一个图并在会话中使用它。
def graph_demo(): a = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=tf.float32) b = tf.constant([[10, 0, 0], [0, 0.5, 0], [0, 0, 2]]) c = tf.constant([[1, -1, 3]], dtype=tf.float32) y = tf.add(tf.matmul(a, b), c, name='result') writer = tf.summary.FileWriter(os.path.join(root_dir, 'log/matmul'), tf.get_default_graph()) writer.close() return y # 在终端启动TensorBoard对图进行可视化 tensorboard --logdir log/matmul  上例中创建一个数据流图，然后用TensorBoard对这个图进行可视化。
 tf.summary.FileWriter 创建了一个tf.summary.SummaryWriter来保存一个图像化表示，这个writer对象创建时，初始化参数包括：a.该图像化表示的存储路径；b.一个tf.Graph对象，可以使用tf.get_default_graph函数返回默认图 tf.get_default_graph 函数，返回默认图。   在执行时，调用TF API创建数据流图，这个阶段并没有进行计算。"><meta property="og:type" content="article"><meta property="og:url" content="https://biubiobiu.github.io/zh-cn/posts/programming_language/tf/compat/tf_compat_summary/"><meta property="article:published_time" content="2021-09-08T06:00:20+06:00"><meta property="article:modified_time" content="2021-09-08T06:00:20+06:00"><meta name=description content="静态图"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/zh-cn><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png alt=Logo>
biubiobiu's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu547c3206082786c1d32d36034e2a655a_40863_42x0_resize_box_2.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/zh-cn/search><input type=text name=keyword placeholder=搜索 data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/zh-cn/posts data-filter=all>博文</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/toha-tutorial/>Toha教程</a><ul><li><a href=/zh-cn/posts/toha-tutorial/toha-config/ title=Toha的配置>Toha的配置</a></li><li><a href=/zh-cn/posts/toha-tutorial/write-blogs/ title=撰写文章>撰写文章</a></li><li><a href=/zh-cn/posts/toha-tutorial/markdown-tutorial/ title=MarkDown入门>MarkDown入门</a></li><li><a href=/zh-cn/posts/toha-tutorial/latax_formula/ title=Katex公式>Katex公式</a></li><li><a href=/zh-cn/posts/toha-tutorial/shortcodes_samples/ title=区域块-实例>区域块-实例</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/deeplearning_summary/>深度学习</a><ul><li><a href=/zh-cn/posts/deeplearning_summary/draw_map_for_dl/ title=神经网络画图篇>神经网络画图篇</a></li><li><a href=/zh-cn/posts/deeplearning_summary/deeplearning_start/ title=深度学习开篇>深度学习开篇</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/posts/programming_language/>编程语言</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/programming_language/python/>Python</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/programming_language/python/internal_lib/>Internal</a><ul><li><a href=/zh-cn/posts/programming_language/python/internal_lib/encode_mode/ title=字符编码>字符编码</a></li><li><a href=/zh-cn/posts/programming_language/python/internal_lib/basic_operator/ title=基础操作>基础操作</a></li><li><a href=/zh-cn/posts/programming_language/python/internal_lib/advance_operator/ title=进阶操作>进阶操作</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/programming_language/python/sdk_lib/>SDK</a><ul><li><a href=/zh-cn/posts/programming_language/python/sdk_lib/multiprocessing/ title=并行操作>并行操作</a></li><li><a href=/zh-cn/posts/programming_language/python/sdk_lib/importlib/ title=importlib>importlib</a></li></ul></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/posts/programming_language/tf/>TensorFlow</a><ul class=active><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/posts/programming_language/tf/compat/>兼容1.x</a><ul class=active><li><a class=active href=/zh-cn/posts/programming_language/tf/compat/tf_compat_summary/ title=静态图>静态图</a></li><li><a href=/zh-cn/posts/programming_language/tf/compat/tf_compat_train/ title=模型训练>模型训练</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/programming_language/pytorch/>PyTorch</a><ul><li><a href=/zh-cn/posts/programming_language/pytorch/torch_summary/ title=简介>简介</a></li><li><a href=/zh-cn/posts/programming_language/pytorch/basic/ title=基础操作>基础操作</a></li><li><a href=/zh-cn/posts/programming_language/pytorch/mathematical/ title=数学计算>数学计算</a></li><li><a href=/zh-cn/posts/programming_language/pytorch/tensor/ title=Tensor>Tensor</a></li><li><a href=/zh-cn/posts/programming_language/pytorch/train_model/ title=模型训练>模型训练</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/programming_language/mxnet/>MxNet</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/programming_language/mxnet/ndarray/>NdArray</a><ul><li><a href=/zh-cn/posts/programming_language/mxnet/ndarray/ndarray_summary/ title=NdArray使用>NdArray使用</a></li></ul></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/cv/>CV</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/cv/backbone/>基础</a><ul><li><a href=/zh-cn/posts/cv/backbone/backbone_cnn/ title=CNN>CNN</a></li><li><a href=/zh-cn/posts/cv/backbone/optimizer/ title=optimizer>optimizer</a></li><li><a href=/zh-cn/posts/cv/backbone/backbone_net/ title="backbone net">backbone net</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/cv/contrastive_learning/>对比学习</a><ul><li><a href=/zh-cn/posts/cv/contrastive_learning/contrastive_learning/ title="contrastive learning">contrastive learning</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/cv/vision_transformer/>ViT</a><ul><li><a href=/zh-cn/posts/cv/vision_transformer/vision_transformer/ title="vision transformer">vision transformer</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/cv/detect_object/>目标检测</a><ul><li><a href=/zh-cn/posts/cv/detect_object/object_detection_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/cv/semantic_segmentation/>语义分割</a><ul><li><a href=/zh-cn/posts/cv/semantic_segmentation/object_detection_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/cv/image-matting/>抠图</a><ul><li><a href=/zh-cn/posts/cv/image-matting/image-matting-summary/ title=抠图综述>抠图综述</a></li><li><a href=/zh-cn/posts/cv/image-matting/image-matting-animal/ title="animal matting">animal matting</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/nlp/>NLP</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/nlp/word_embedding/>Word Embedding</a><ul><li><a href=/zh-cn/posts/nlp/word_embedding/word_embedding_summary/ title="Word Embedding综述">Word Embedding综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/nlp/rnn/>RNN</a><ul><li><a href=/zh-cn/posts/nlp/rnn/rnn_summary/ title=RNN综述>RNN综述</a></li><li><a href=/zh-cn/posts/nlp/rnn/gru/ title=GRU网络>GRU网络</a></li><li><a href=/zh-cn/posts/nlp/rnn/lstm/ title=LSTM网络>LSTM网络</a></li><li><a href=/zh-cn/posts/nlp/rnn/encode_decode/ title=编解码架构>编解码架构</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/nlp/transformer/>Transformer</a><ul><li><a href=/zh-cn/posts/nlp/transformer/attention/ title=Attention>Attention</a></li><li><a href=/zh-cn/posts/nlp/transformer/transformer_summary/ title=Transformer>Transformer</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/nlp/gpt/>GPT</a><ul><li><a href=/zh-cn/posts/nlp/gpt/gpt_summary/ title=GPT综述>GPT综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/nlp/bert/>Bert</a><ul><li><a href=/zh-cn/posts/nlp/bert/bert_summary/ title=Bert综述>Bert综述</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/vlp/>多模态</a><ul><li><a href=/zh-cn/posts/vlp/vlp_summary/ title=简介>简介</a></li><li><a href=/zh-cn/posts/vlp/clip/ title=CLIP>CLIP</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/video/>视频理解</a><ul><li><a href=/zh-cn/posts/video/vidio_summary/ title=简介>简介</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/john_hu7f9991f5b5d471ecdfc6cff3db1a9fe6_6397_120x120_fit_box_2.png alt="Author Image"><h5 class=author-name>biubiobiu</h5><p>:date_full</p></div><div class=title><h1>静态图</h1></div><div class=taxonomy-terms><ul></ul></div><div class=post-content id=post-content><p>在TensorFlow 2中使用兼容性模块，必须使用<code>tf.compat.v1</code>替换<code>tf</code>，并且在导入TensorFlow软件包后添加一行<code>tf.compat.v1.disable_eager_execution()</code>函数来关闭eager执行模式。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> tensorflow <span style=color:#f92672>as</span> tf
tf<span style=color:#f92672>.</span>compat<span style=color:#f92672>.</span>v1<span style=color:#f92672>.</span>disable_eager_execution()
</code></pre></div><h3 id=简介>简介</h3><p>数据流是一种编程模型，被广泛地应用于并行计算中。TF使用数据流图来表示计算中各个运算之间的关系，在数据流图中，<code>节点</code>：表示计算单元(即：操作tf.Operation)；<code>边</code>：表示被计算单元消费/生产的数据(即：tf.Tensor)。</br><code>数据流图，可以被导出成一个可移植的、编程语言不相关的表示(ProtoBuf)，这种表示可以被其他语言使用，来创建一个图并在会话中使用它。</code></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>graph_demo</span>():
    a <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>constant([[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>], [<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>], [<span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>9</span>]], dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>float32)
    b <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>constant([[<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>], [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>0</span>], [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>]])
    c <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>constant([[<span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>]], dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>float32)

    y <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>matmul(a, b), c, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;result&#39;</span>)

    writer <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>summary<span style=color:#f92672>.</span>FileWriter(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(root_dir, <span style=color:#e6db74>&#39;log/matmul&#39;</span>), tf<span style=color:#f92672>.</span>get_default_graph())
    writer<span style=color:#f92672>.</span>close()
    <span style=color:#66d9ef>return</span> y

<span style=color:#75715e># 在终端启动TensorBoard对图进行可视化</span>
tensorboard <span style=color:#f92672>--</span>logdir log<span style=color:#f92672>/</span>matmul
</code></pre></div><blockquote><p>上例中创建一个数据流图，然后用TensorBoard对这个图进行可视化。</p><ol><li><code>tf.summary.FileWriter</code> 创建了一个tf.summary.SummaryWriter来保存一个图像化表示，这个writer对象创建时，初始化参数包括：a.该图像化表示的存储路径；b.一个tf.Graph对象，可以使用<code>tf.get_default_graph</code>函数返回默认图</li><li><code>tf.get_default_graph</code> 函数，返回默认图。</li></ol></blockquote><p>在执行时，调用TF API创建数据流图，这个阶段并没有进行计算。</p><h3 id=1图-tfgraph>1、图-tf.Graph</h3><p>TF是一个C++库，我们只是用python来用简单的方式来构造数据流图，python简化了数据流图的描述阶段，因为它无须特意显示定义一个图，而是会默认一个tf.Graph。</p><div class="alert alert-success"><strong><p>图的定义:</p><ol><li>隐式定义：在我们用tf.*搭建一个图时，TensorFlow总是会定义一个默认的<code>tf.Graph</code>，可以通过<code>tf.get_default_graph</code>访问。隐式定义限制了TF的表示能力，因为它被限制只能使用一个图。</li><li>显式定义：可以显式地定义一个计算图，因此每个应用可以有多个图。这种方式的表现能力更强，但并不常用，因为需要多个图的应用不常见。</br>TF通过<code>tf.Graph()</code>创建一个tf.Graph对象，并通过<code>as_default</code>方法创建一个上下文管理器，每个上下文中定义的运算都被放进相应的图中。实际上，<code>tf.Graph()</code>对象定义了一个它所包含的<code>tf.Operation</code>对象的命名空间。<div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> tensorflow <span style=color:#f92672>as</span> tf

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>graph_define</span>():
  g1 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>Graph()
  g2 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>Graph()

  <span style=color:#66d9ef>with</span> g1<span style=color:#f92672>.</span>as_default():
      a <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>constant([[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>], [<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>6</span>], [<span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>9</span>]], dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>float32)
      b <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>constant([[<span style=color:#ae81ff>9</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>], [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>], [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0.5</span>]])
      c <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>constant([[<span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>]], dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>float32)
      y <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>matmul(a, b), c, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;result&#39;</span>)

  <span style=color:#66d9ef>with</span> g2<span style=color:#f92672>.</span>as_default():
      <span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>name_scope(<span style=color:#e6db74>&#39;scope_2&#39;</span>):
          x <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>constant(<span style=color:#ae81ff>1</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;x&#39;</span>)
          <span style=color:#66d9ef>print</span>(x) <span style=color:#75715e># Tensor(&#34;scope_2/x:0&#34;, shape=(), dtype=int32)</span>
      <span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>name_scope(<span style=color:#e6db74>&#39;scope_3&#39;</span>):
          x <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>constant(<span style=color:#ae81ff>10</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;x&#39;</span>)
          <span style=color:#66d9ef>print</span>(x) <span style=color:#75715e># Tensor(&#34;scope_3/x:0&#34;, shape=(), dtype=int32)</span>
      y <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>constant(<span style=color:#ae81ff>12</span>)
      z <span style=color:#f92672>=</span> x<span style=color:#f92672>*</span>y

  writer <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>summary<span style=color:#f92672>.</span>FileWriter(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(root_dir, <span style=color:#e6db74>&#39;log/two_graphs/g1&#39;</span>), g1)
  writer <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>summary<span style=color:#f92672>.</span>FileWriter(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(root_dir, <span style=color:#e6db74>&#39;log/two_graphs/g2&#39;</span>), g2)
  writer<span style=color:#f92672>.</span>close()

</code></pre></div></li></ol><p>图的集合：<br>每个tf.Graph，用集合机制 来存储与图结构相关的元数据，一个集合由一个键值唯一标识，其内容是一个对象/运算的列表。使用者通常不需要关注集合是否存在，因为它们是TF为了正确定义一个图所使用的。</p><p>图中节点名：</p><ol><li>后缀：图中每个节点的名字都是唯一的，如果有重复，为了避免重复，TF会添加:id形式的后缀。</br>在定义时如果没有指定节点的name，TF就会用Operation（操作）的名字来命名，输出的tf.Tensor和其相关的tf.Operation名字相同，只是可能会加上后缀。</li><li>前缀：可以通过<code>tf.name_scope</code>函数定义一个上下文，为该上下文中所有的运算添加命名范围前缀。</li></ol><p>图中的计算：</p><ol><li>作为一个C++库，TF数据类型是严格的静态类型，这意味着在图定义阶段必须知道每个运算/张量的类型，且参与运算的数据类型必须相同。</li><li>可以使用运算符重载，来简化一些常用的数学运算。运算符重载使得图定义更便捷，并且与tf.*的API调用完全等价，只是有一点：不能给运算指定名字。<div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>y <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>matmul(A, x), b, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;result&#39;</span>)
<span style=color:#75715e># 等价</span>
y <span style=color:#f92672>=</span> A <span style=color:#960050;background-color:#1e0010>@</span> x <span style=color:#f92672>+</span> b
</code></pre></div><table><thead><tr><th style=text-align:left>运算符</th><th style=text-align:left>操作名</th><th style=text-align:left>运算符</th><th style=text-align:left>操作名</th><th style=text-align:left>运算符</th><th style=text-align:left>操作名</th><th style=text-align:left>运算符</th><th style=text-align:left>操作名</th></tr></thead><tbody><tr><td style=text-align:left><code>__neg__</code></td><td style=text-align:left>unary -</td><td style=text-align:left><code>__abs__</code></td><td style=text-align:left>abs()</td><td style=text-align:left><code>__invert__</code></td><td style=text-align:left>unary ~</td><td style=text-align:left><code>__add__</code></td><td style=text-align:left>binary +</td></tr><tr><td style=text-align:left><code>__sub__</code></td><td style=text-align:left>binary -</td><td style=text-align:left><code>__mul__</code></td><td style=text-align:left>binary 元素*</td><td style=text-align:left><code>__floordiv__</code></td><td style=text-align:left>binary //</td><td style=text-align:left><code>__truediv__</code></td><td style=text-align:left>binary /</td></tr><tr><td style=text-align:left><code>__mod__</code></td><td style=text-align:left>binary %</td><td style=text-align:left><code>__pow__</code></td><td style=text-align:left>binary **</td><td style=text-align:left><code>__and__</code></td><td style=text-align:left>binary &</td><td style=text-align:left><code>__or__</code></td><td style=text-align:left>binary |</td></tr><tr><td style=text-align:left><code>__xor__</code></td><td style=text-align:left>binary ^</td><td style=text-align:left><code>__le__</code></td><td style=text-align:left>binary &lt;</td><td style=text-align:left><code>__lt__</code></td><td style=text-align:left>binary &lt;=</td><td style=text-align:left><code>__gt__</code></td><td style=text-align:left>binary ></td></tr><tr><td style=text-align:left><code>__ge__</code></td><td style=text-align:left>binary >=</td><td style=text-align:left><code>__matmul__</code></td><td style=text-align:left>binary @</td><td></td><td></td><td></td><td></td></tr></tbody></table></li></ol></strong></div><h3 id=2图放置-tfdevice>2、图放置-tf.device</h3><p><code>tf.device</code>创建一个和设备相符的上下文管理器，这个函数运行使用者将同一个上下文下的所有运算放置在相同的设备上。tf.device指定的设备不仅仅是物理设备，它能指定远程服务器、远程设备、远程工作者、不同种类的物理设备(GPU、CPU、TPU)。</br></p><blockquote><p>格式：/job:&lt;JOB_NAME>/task:&lt;TASK_INDEX>/device:&lt;DEVICE_TYPE>:&lt;DEVICE_INDEX></p><ol><li>&lt;JOB_NAME>：是一个由字母和数字构成的字符串，首个字符不能是数字</li><li>&lt;TASK_INDEX>：是一个非负整数，代表在名为&lt;JOB_NAME>的工作中的任务编号</li><li>&lt;DEVICE_TYPE>：是一个已经注册过的设备类型(CPU或者GPU)</li><li>&lt;DEVICE_INDEX>：是一个非负整数，代表设备的索引号</li></ol></blockquote><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>device_demo</span>():
    <span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>device(<span style=color:#e6db74>&#39;/CPU:0&#39;</span>):
        a <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>constant([[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>], [<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>6</span>], [<span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>9</span>]], dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>float32)
        b <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>constant([[<span style=color:#ae81ff>9</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>], [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>], [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0.5</span>]])
        c <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>constant([[<span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>]], dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>float32)
    <span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>device(<span style=color:#e6db74>&#39;/GPU:0&#39;</span>):
        mul <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>matmul(a, b, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;mul_result&#39;</span>)

    y <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>add(mul, c, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;add_result&#39;</span>)

    writer <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>summary<span style=color:#f92672>.</span>FileWriter(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(root_dir, <span style=color:#e6db74>&#39;log/device&#39;</span>), tf<span style=color:#f92672>.</span>get_default_graph())
    writer<span style=color:#f92672>.</span>close()

</code></pre></div><h3 id=3图执行-tfsession>3、图执行-tf.Session</h3><p>静态图，图的定义与执行完全分离，在eager执行模式中不是这样。<code>tf.Session</code>：是一个TF提供的类，用来表示Python程序与C++运算库之间的联系，是唯一能直接与硬件通信、将运算放置到指定的设备上、使用本地和分布式TF运行库的类。它的主要目的：根据定义的图，具体地实现各个计算。</br><code>tf.Session</code>对象是高度优化过的，一旦被正确构建，它会将<code>tf.Graph</code>缓存起来以加速其执行，<code>tf.Session</code>作为物理资源的拥有者，必须以一个文件描述符的方式来做下面的工作：</p><ol><li>通过创建tf.Session来获取资源（等价于open操作系统调用）</li><li>使用这些资源（等价于在文件描述符上使用 读/写 操作）</li><li>使用tf.Session.close释放资源（通常会使用一个上下文管理器，不需要手动销毁释放资源）</li></ol><h4 id=1-tfsession的三个参数>1). <code>tf.Session</code>的三个参数</h4><p>Session(target='', graph=None, config=None)</p><ol><li><p><code>target</code>：配置执行引擎
常见的场景：</p><ol><li><p>使用当前的本地的硬件来执行图</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>Session() <span style=color:#66d9ef>as</span> sess:
    <span style=color:#75715e># 使用session去执行 某些操作</span>
    sess<span style=color:#f92672>.</span>run(<span style=color:#f92672>...</span>)
</code></pre></div></li><li><p>一些更复杂的场景：使用一个远程TensorFlow服务器，可以通过使用服务器的url(grpc://)来指定<code>tf.Session的target参数</code></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># TensorFlow服务器的 ip和port</span>
ip <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;192.168.1.90&#39;</span>
port <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;9877&#39;</span>
<span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>Session(target<span style=color:#f92672>=</span>f<span style=color:#e6db74>&#39;grpc://{ip}:{port}&#39;</span>) <span style=color:#66d9ef>as</span> sess:
    sess<span style=color:#f92672>.</span>run(<span style=color:#f92672>...</span>)
</code></pre></div></li></ol></li><li><p><code>graph</code>: 指定需要使用的图。tf.Session会使用默认的图对象，在需要运算多个图时，可以指定需要使用的图。tf.Session对象每次只能处理一个图。</p></li><li><p><code>config</code>: 硬件/网络配置，这个配置通过<code>tf.ConfigProto</code>对象来指定，用来控制Session的行为。<code>tf.ConfigProto</code>比较复杂，选项也比较多，最常用的选项有下面两个：</p><ol><li><code>allow_soft_placement</code>：当为True时：启动软设备安排，即：不是所有运算都会按照图定义的那样，被安排在指定的设备上。这是为了防止这种情况：比如GPU不存在，或者原来存在现在出了些问题，TensorFlow没有检测到该设备，就可以把指定给这个设备的运算，安排到其他正确的设备上。</li><li><code>gpu_options.allow_growth</code>：当为True时：会改变GPU显存分配器的工作方式。分配器默认的工作方式：tf.Session被创建时就会分配所有可用的GPU显存。当allow_growth=True时，分配器会以<code>逐步递增的方式</code>分配显存。这是为了适应这种情况：在研究环境下，GPU资源是共享的，当一个tf.Session执行时，不能占用所有资源，其他人也还在使用。</li><li><code>per_process_gpu_memory_fraction</code>：手动限定显存的使用量</li><li><code>log_device_placement</code>：当为True时，会获取Operations和Tensor被指派到的设备号，在终端会打印出各个操作是在那些设备上运行的。</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>    config <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>ConfigProto(allow_soft_placement<span style=color:#f92672>=</span>True, log_device_placement<span style=color:#f92672>=</span>True)
    config<span style=color:#f92672>.</span>gpu_options<span style=color:#f92672>.</span>allow_growth<span style=color:#f92672>=</span>True
    config<span style=color:#f92672>.</span>gpu_options<span style=color:#f92672>.</span>per_process_gpu_memory_fraction <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.4</span>  <span style=color:#75715e>#占用40%显存</span>
    <span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>Session(config<span style=color:#f92672>=</span>config) <span style=color:#66d9ef>as</span> sess:
        <span style=color:#75715e># 使用session去执行 某些操作</span>
        sess<span style=color:#f92672>.</span>run(<span style=color:#f92672>...</span>)

</code></pre></div></li></ol><h4 id=2-sessrun>2). sess.run()</h4><p><code>sess.run(y)</code>的工作方式如下：</p><ol><li>y是一个运算的输出节点，回溯y的输入</li><li>递归的回溯所有节点，直到无法找到父节点</li><li>评估输入</li><li>跟踪依赖图：分析各个节点的关系</li><li>执行计算</li></ol><p><code>feed_dict</code>：可以把外部的数据，注入计算图中，相当于重写计算图里的某个值。跟<code>tf.placeholder</code>配合使用，完成外部的数据流入计算图。
<code>tf.placeholder</code>：重写运算符。其创建的目的就是：当外面的值没有注入图中时，就会抛出一个错误。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>session_demo</span>():
    a <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>constant([[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>], [<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>6</span>], [<span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>9</span>]], dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>float32)
    b <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>constant([[<span style=color:#ae81ff>9</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>], [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>], [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0.5</span>]])
    c <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>constant([[<span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>]], dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>float32)
    y <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>matmul(a, b), c, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;result&#39;</span>)

    <span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>Session() <span style=color:#66d9ef>as</span> sess:
        a_value, b_value, c_value <span style=color:#f92672>=</span> sess<span style=color:#f92672>.</span>run([a, b, c])
        y_value <span style=color:#f92672>=</span> sess<span style=color:#f92672>.</span>run(y)

        <span style=color:#75715e># 重写</span>
        y_new <span style=color:#f92672>=</span> sess<span style=color:#f92672>.</span>run(y, feed_dict<span style=color:#f92672>=</span>{c: np<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>))})

    <span style=color:#66d9ef>print</span>(f<span style=color:#e6db74>&#39;a: {a_value}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>b: {b_value}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>c: {c_value}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>y: {y_value}&#39;</span>)
    <span style=color:#66d9ef>print</span>(f<span style=color:#e6db74>&#39;y_new: {y_new}&#39;</span>)
</code></pre></div><h3 id=4图中的变量>4、图中的变量</h3><p>一个变量是一个tf.Variable对象，用于维护图的状态，作为图中其他节点的输入。<code>tf.Tensor</code>和<code>tf.Variable</code>对象可以用相同的方式使用，不过<code>tf.Variable</code>拥有更多的属性：</p><ol><li>一个变量必须要被初始化</li><li>一个变量默认被加到<code>全局变量</code>和<code>可训练变量</code>集合中</li></ol><h4 id=1-变量声明>1. 变量声明</h4><p>声明变量的两种方式：需要(type, shape)</p><ol><li><p><code>tf.Variable</code>：是一个类，创建一个变量，同时它需要指定一个初始值。<br>变量的赋值，可以使用assign函数：比如：<code>w.assign(w+0.1)</code>等价于<code>w.assign_add(0.1)</code>。其实，变量的初始化操作，就是把初始值assign给每个变量。</p><blockquote><p>tf.Variable是一个类<br><code>__init__</code>(
initial_value=None,<br>trainable=True, # 是否可训练<br>collections=None,<br>validate_shape=True,<br>caching_device=None,<br>name=None,<br>variable_def=None,<br>dtype=None,<br>expected_shape=None,<br>import_scope=None,<br>constraint=None)</p></blockquote><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>size_in <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>
size_out <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>
<span style=color:#75715e># w的初始值是有tf.truncated_normal运算产生，服从正太分布N(0, 0.1)</span>
w <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>Variable(tf<span style=color:#f92672>.</span>truncated_normal([<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>5</span>, size_in, size_out], stddev<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>), name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;w&#39;</span>)
<span style=color:#75715e># b的初始值是有tf.constant运算产生的常量来初始化</span>
b <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>Variable(tf<span style=color:#f92672>.</span>constant(<span style=color:#ae81ff>0.1</span>, shape<span style=color:#f92672>=</span>[size_out]), name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;b&#39;</span>)

<span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>Session() <span style=color:#66d9ef>as</span> sess:
    <span style=color:#75715e># 变量的初始化</span>
    sess<span style=color:#f92672>.</span>run(w<span style=color:#f92672>.</span>initializer)

</code></pre></div></li><li><p><code>tf.get_variable</code>：更复杂，但拥有更强的表现能力。例如：如果我们需要<code>变量共享</code>，就不能使用tf.Variable定义，只能使用tf.get_variable。tf.get_variable和tf.variable_scope一起使用，通过它的reuse参数，实现tf.get_variable的变量共享能力。其中，tf.get_variable不受tf.name_scope的影响。</br>TensorFlow提供的<code>tf.layers</code>模块，包含了几乎所有常用的层，这些层内部都是用<code>tf.get_variable</code>来定义的，因此，这些层可以和<code>tf.variable_scope</code>一起使用来共享它们的变量。</p><blockquote><p>tf.get_variable是一个函数：<br>(<br>name,<br>shape=None,<br>dtype=None,<br>initializer=None,<br>regularizer=None,<br>trainable=True,<br>collections=None,<br>caching_device=None,<br>partitioner=None,<br>validate_shape=None,<br>use_resource=None,<br>custom_getter=None,<br>constraint=None<br>)</p></blockquote><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>variable_scope(<span style=color:#e6db74>&#39;scope&#39;</span>):
    a <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>get_variable(<span style=color:#e6db74>&#39;v&#39;</span>, [<span style=color:#ae81ff>1</span>])

<span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>variable_scope(<span style=color:#e6db74>&#39;scope&#39;</span>, reuse<span style=color:#f92672>=</span>True):
    b <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>get_variable(<span style=color:#e6db74>&#39;v&#39;</span>, [<span style=color:#ae81ff>1</span>])

<span style=color:#66d9ef>print</span>(a<span style=color:#f92672>.</span>name, b<span style=color:#f92672>.</span>name) <span style=color:#75715e># scope/v:0 scope/v:0</span>
</code></pre></div></li></ol><h4 id=2-变量初始化>2. 变量初始化</h4><p>TensorFlow变量随机初始化，例如：<code>w = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.1), name='w')</code></br>常见的随机函数：</p><table><thead><tr><th style=text-align:left>操作</th><th style=text-align:left>功能</th></tr></thead><tbody><tr><td style=text-align:left><code>tf.random_normal()</code></td><td style=text-align:left>正态分布，参数:(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)</td></tr><tr><td style=text-align:left><code>tf.truncated_normal</code></td><td style=text-align:left>正态分布，但如果随机出来的值偏离平均值超过了2个标准差，那么这个数将会被重新随机, 参数如上</td></tr><tr><td style=text-align:left><code>tf.random_uniform</code></td><td style=text-align:left>平均分布，参数：([m, n], 最小值, 最大值, 取值类型)</td></tr><tr><td style=text-align:left><code>tf.random.gamma</code></td><td style=text-align:left>Gamma分布，参数：([m, n], 形状参数 $\alpha$，尺度参数 $\beta$, 取数类型)</td></tr><tr><td style=text-align:left><code>常数函数</code></td><td></td></tr><tr><td style=text-align:left><code>tf.zeros()</code></td><td style=text-align:left>参数：(shape, dtype=tf.float32, name=None)， shape的格式: [m, n]</td></tr><tr><td style=text-align:left><code>tf.ones()</code></td><td style=text-align:left>参数：(shape, dtype=tf.float32, name=None), shape的格式: [m, n]</td></tr><tr><td style=text-align:left><code>tf.fill()</code></td><td style=text-align:left>参数：(shape, value, name=None), shape的格式: [m, n]</td></tr><tr><td style=text-align:left><code>tf.constant()</code></td><td style=text-align:left>参数：(value, dtype=None, shape=None, name=&lsquo;Const&rsquo;, verify_shape=False)</br>例如：tf.constant([1, 2, 3, 4, 5, 6, 7]) => [1 2 3 4 5 6 7]</br>tensor = tf.constant(-1.0, shape=[2, 3]) => [[-1. -1. -1.],[-1. -1. -1.]]</td></tr><tr><td style=text-align:left><code>tf.range()</code></td><td style=text-align:left>参数：tf.range(start, limit, delta)</td></tr><tr><td style=text-align:left><code>tf.linspace()</code></td><td style=text-align:left>参数：(start, stop, num) 功能： (stop - start)/(num - 1)</td></tr></tbody></table><ol><li><p>传入初始值</p><blockquote><p>在session中执行时，变量必须要初始化：<br>a. 全部变量初始化： <code>tf.global_variables_initializers():其实内部实现：=tf.variabels_initializer(tf.global_variables())</code><br>b. 部分变量初始化：<code>tf.variables_initializer([变量])</code><br>c. 检查变量是否初始化成功：<code>tf.is_variable_initialized</code>：检查变量是否初始化；<code>tf.report_uninitialized_variables</code>：获取未初始化的变量集合；<code>tf.assert_variables_initialized</code>：断言变量已经初始化。</p></blockquote><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>Session() <span style=color:#66d9ef>as</span> sess:
    sess<span style=color:#f92672>.</span>run(tf<span style=color:#f92672>.</span>global_variable_initializer()) <span style=color:#75715e># 初始化所有变量</span>
</code></pre></div></li><li><p>从checkpoint文件中恢复变量的值<br>当我们创建Saver实例时，它的构造方法会向当前的数据流图中添加一对操作：SaveOp和RestoreOp</p><ul><li><p>SaveOp负责向checkpoint文件中写入变量</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>saver <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>train<span style=color:#f92672>.</span>Saver()
saver<span style=color:#f92672>.</span>save(sess, <span style=color:#e6db74>&#39;/tmp/summary/test.ckpt&#39;</span>)
</code></pre></div></li><li><p>RestoreOp负责从checkpoint文件中恢复变量</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>saver <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>train<span style=color:#f92672>.</span>Saver()
saver<span style=color:#f92672>.</span>restore(sess, <span style=color:#e6db74>&#39;/tmp/summary/test.ckpt&#39;</span>)
</code></pre></div></li></ul></li></ol><h4 id=3-变量的访问>3. 变量的访问</h4><ol><li>通过在<code>tf.global_variable()</code>变量表中，根据变量名进行匹配查找</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>x <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>Variable(<span style=color:#ae81ff>1</span>,name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;x&#39;</span>)
y <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>get_variable(name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;y&#39;</span>,shape<span style=color:#f92672>=</span>[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>])
<span style=color:#66d9ef>for</span> var <span style=color:#f92672>in</span> tf<span style=color:#f92672>.</span>global_variables():              <span style=color:#75715e>#返回全部变量列表</span>
    <span style=color:#66d9ef>if</span> var<span style=color:#f92672>.</span>name <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;x:0&#39;</span>:
        <span style=color:#66d9ef>print</span>(var)
</code></pre></div><ol start=2><li>利用<code>tf.get_tensor_by_name</code>，在图中根据name查找</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> tensorflow <span style=color:#f92672>as</span> tf

x <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>Variable(<span style=color:#ae81ff>1</span>,name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;x&#39;</span>)
y <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>get_variable(name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;y&#39;</span>,shape<span style=color:#f92672>=</span>[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>])

graph <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>get_default_graph()
x1 <span style=color:#f92672>=</span> graph<span style=color:#f92672>.</span>get_tensor_by_name(<span style=color:#e6db74>&#34;x:0&#34;</span>)
y1 <span style=color:#f92672>=</span> graph<span style=color:#f92672>.</span>get_tensor_by_name(<span style=color:#e6db74>&#34;y:0&#34;</span>)

</code></pre></div></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"><strong>分享:</strong>
<a class="btn btn-sm facebook-btn" href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2fprogramming_language%2ftf%2fcompat%2ftf_compat_summary%2f" target=_blank><i class="fab fa-facebook"></i></a><a class="btn btn-sm twitter-btn" href="https://twitter.com/share?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2fprogramming_language%2ftf%2fcompat%2ftf_compat_summary%2f&text=%e9%9d%99%e6%80%81%e5%9b%be&via=biubiobiu%27s%20Blog" target=_blank><i class="fab fa-twitter"></i></a><a class="btn btn-sm reddit-btn" href="https://reddit.com/submit?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2fprogramming_language%2ftf%2fcompat%2ftf_compat_summary%2f&title=%e9%9d%99%e6%80%81%e5%9b%be" target=_blank><i class="fab fa-reddit"></i></a><a class="btn btn-sm linkedin-btn" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2fprogramming_language%2ftf%2fcompat%2ftf_compat_summary%2f&title=%e9%9d%99%e6%80%81%e5%9b%be" target=_blank><i class="fab fa-linkedin"></i></a><a class="btn btn-sm whatsapp-btn" href="https://api.whatsapp.com/send?text=%e9%9d%99%e6%80%81%e5%9b%be https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2fprogramming_language%2ftf%2fcompat%2ftf_compat_summary%2f" target=_blank><i class="fab fa-whatsapp"></i></a><a class="btn btn-sm email-btn" href="mailto:?subject=%e9%9d%99%e6%80%81%e5%9b%be&body=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2fprogramming_language%2ftf%2fcompat%2ftf_compat_summary%2f" target=_blank><i class="fas fa-envelope-open-text"></i></a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/zh-cn/posts/programming_language/python/sdk_lib/importlib/ title=importlib包 class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i>上一篇</div><div class=next-prev-text>importlib包</div></a></div><div class="col-md-6 next-article"><a href=/zh-cn/posts/programming_language/tf/compat/tf_compat_train/ title=模型训练 class="btn btn-outline-info"><div>下一篇 <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>模型训练</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">目录</h5><hr><div class=toc><nav id=TableOfContents><ul><li><ul><li><a href=#简介>简介</a></li><li><a href=#1图-tfgraph>1、图-tf.Graph</a></li><li><a href=#2图放置-tfdevice>2、图放置-tf.device</a></li><li><a href=#3图执行-tfsession>3、图执行-tf.Session</a><ul><li><a href=#1-tfsession的三个参数>1). <code>tf.Session</code>的三个参数</a></li><li><a href=#2-sessrun>2). sess.run()</a></li></ul></li><li><a href=#4图中的变量>4、图中的变量</a><ul><li><a href=#1-变量声明>1. 变量声明</a></li><li><a href=#2-变量初始化>2. 变量初始化</a></li><li><a href=#3-变量的访问>3. 变量的访问</a></li></ul></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>导航</h5><ul><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#about>关于</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#skills>技能</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#experiences>经验</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#projects>项目</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#education>教育</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#recent-posts>最新博客</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#accomplishments>造诣</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#achievements>业绩</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>联系方式:</h5><ul><li><span>电话:</span> <span>+0123456789</span></li><li><span>邮箱:</span> <span>bsgshyn8@163.com</span></li></ul></div></div></div><hr><div class=container><p id=disclaimer><strong>免责声明:</strong> 这个主题是MIT许可的。因此，您可以将其用于非商业、商业或者私人用途。您可以修改或分发主题，而无须经 作者任何许可。但是，主题作者不对主题的任何问题提供任何保证或承担任何责任。</p></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>