<!doctype html><html><head><title>contrastive learning</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0d88336a9cbd3e68c7714efae786b0a9_38044_42x0_resize_box_2.png><meta property="og:title" content="contrastive learning"><meta property="og:description" content="从2019年中~2020年中，对比学习火了一段时间，到ViT出来后，大量的研究这才投身于ViT。
一、简介 什么是对比学习？
简单来说就是，只要模型把相似的数据跟其他不相似的数据区分开就可以。比如：$A_1, A_2, &mldr;$ 是狗，$B_1, B_2, &mldr;$ 是猫，只要模型能把这两批数据区分开就行。
所以，训练集中不需要明确的标签，只要能区分出那些数据之间是相似的，那些是与它们不相似的。
所以，训练集中不必人为标注，只需要设计一些规则生产出这种类型的训练集就行。
看下Hinton老爷子的《Self-organizing neural network that discovers surfaces in random-dot stereograms》 和 LeCun的《Dimensionality reduction by learning an invariant mapping》 对比学习为啥在cv领域被认为是无监督呢？：
 通过设计一些巧妙的代理任务，就是pretext task：人为的定义一些规则，这些规则可以用来定义那些图片是相似的，那些图片是不相似的。
例如：instance discrimination：如果有N张图片的数据集，随机一张图片$x_i$，对这个图片随机裁剪+数据增广，从同一张图片中通过裁剪+增广产生的数据，虽然有差异但是语义信息是一样的，所以是正样本(它们之间是相似的)，负样本就是除了图$x_i$之外的所有样本。  1、代理任务 代理任务(pretext task)的目的: 生成一个自监督的信号，从而充当ground truth这个标签信息
有监督学习：训练时比较输出 $\hat{Y}$ 和 groud truth $Y$；
自监督学习：因为缺少groud truth，所以需要代理任务自己创建类似groud truth的信号。
2、对比学习的loss 1)、InfoNCE loss noise contrastive estimation loss：其实就是一个交叉熵 $$ L_q = -log\frac{exp(q\cdot k_+ / \tau)}{\sum_{i=0}^{K} exp(q\cdot k_i / \tau)} $$ 分母：一个正样本，K个负样本；$\tau$：温度超参数，值越大分布就越平缓，表示对每种的关注度越相似；值越小分布就越陡峭，表示比较关注比较困难的case，不容易收敛。"><meta property="og:type" content="article"><meta property="og:url" content="https://biubiobiu.github.io/zh-cn/posts/w00100_cv/w0020_contrastive_learning/contrastive_learning/"><meta property="article:published_time" content="2022-05-09T06:00:20+06:00"><meta property="article:modified_time" content="2022-05-09T06:00:20+06:00"><meta name=description content="contrastive learning"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/zh-cn><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png alt=Logo>
biubiobiu's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu547c3206082786c1d32d36034e2a655a_40863_42x0_resize_box_2.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/zh-cn/search><input type=text name=keyword placeholder=搜索 data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/zh-cn/posts data-filter=all>博文</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00020_toha-tutorial/>Toha教程</a><ul><li><a href=/zh-cn/posts/w00020_toha-tutorial/w0010_toha-config/ title=Toha的配置>Toha的配置</a></li><li><a href=/zh-cn/posts/w00020_toha-tutorial/w0011_write-blogs/ title=撰写文章>撰写文章</a></li><li><a href=/zh-cn/posts/w00020_toha-tutorial/w0013_markdown-tutorial/ title=MarkDown入门>MarkDown入门</a></li><li><a href=/zh-cn/posts/w00020_toha-tutorial/w0015_latax_formula/ title=Katex公式>Katex公式</a></li><li><a href=/zh-cn/posts/w00020_toha-tutorial/w0020_shortcodes_samples/ title=区域块-实例>区域块-实例</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00030_deeplearning_summary/>深度学习</a><ul><li><a href=/zh-cn/posts/w00030_deeplearning_summary/w0005_deeplearning_start/ title=深度学习开篇>深度学习开篇</a></li><li><a href=/zh-cn/posts/w00030_deeplearning_summary/w0010_draw_map_for_dl/ title=神经网络画图篇>神经网络画图篇</a></li><li><a href=/zh-cn/posts/w00030_deeplearning_summary/w0020_cam/ title=CAM>CAM</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00035_programming_language/>编程语言</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00035_programming_language/w0035_python/>Python</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00035_programming_language/w0035_python/w0020_internal_lib/>Internal</a><ul><li><a href=/zh-cn/posts/w00035_programming_language/w0035_python/w0020_internal_lib/encode_mode/ title=字符编码>字符编码</a></li><li><a href=/zh-cn/posts/w00035_programming_language/w0035_python/w0020_internal_lib/basic_operator/ title=基础操作>基础操作</a></li><li><a href=/zh-cn/posts/w00035_programming_language/w0035_python/w0020_internal_lib/advance_operator/ title=进阶操作>进阶操作</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00035_programming_language/w0035_python/w0050_sdk_lib/>SDK</a><ul><li><a href=/zh-cn/posts/w00035_programming_language/w0035_python/w0050_sdk_lib/multiprocessing/ title=并行操作>并行操作</a></li><li><a href=/zh-cn/posts/w00035_programming_language/w0035_python/w0050_sdk_lib/importlib/ title=importlib>importlib</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00035_programming_language/w0040_tf/>TensorFlow</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00035_programming_language/w0040_tf/w0010_compat/>兼容1.x</a><ul><li><a href=/zh-cn/posts/w00035_programming_language/w0040_tf/w0010_compat/tf_compat_summary/ title=静态图>静态图</a></li><li><a href=/zh-cn/posts/w00035_programming_language/w0040_tf/w0010_compat/tf_compat_train/ title=模型训练>模型训练</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00035_programming_language/w0050_pytorch/>PyTorch</a><ul><li><a href=/zh-cn/posts/w00035_programming_language/w0050_pytorch/torch_summary/ title=简介>简介</a></li><li><a href=/zh-cn/posts/w00035_programming_language/w0050_pytorch/basic/ title=基础操作>基础操作</a></li><li><a href=/zh-cn/posts/w00035_programming_language/w0050_pytorch/mathematical/ title=数学计算>数学计算</a></li><li><a href=/zh-cn/posts/w00035_programming_language/w0050_pytorch/tensor/ title=Tensor>Tensor</a></li><li><a href=/zh-cn/posts/w00035_programming_language/w0050_pytorch/train_model/ title=模型训练>模型训练</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00035_programming_language/w0060_mxnet/>MxNet</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00035_programming_language/w0060_mxnet/w0010_ndarray/>NdArray</a><ul><li><a href=/zh-cn/posts/w00035_programming_language/w0060_mxnet/w0010_ndarray/ndarray_summary/ title=NdArray使用>NdArray使用</a></li></ul></li></ul></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/posts/w00100_cv/>CV</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00100_cv/w0010_backbone/>基础</a><ul><li><a href=/zh-cn/posts/w00100_cv/w0010_backbone/d1_1_backbone_cnn/ title=CNN>CNN</a></li><li><a href=/zh-cn/posts/w00100_cv/w0010_backbone/d1_2_optimizer/ title=optimizer>optimizer</a></li><li><a href=/zh-cn/posts/w00100_cv/w0010_backbone/d1_3_backbone_net/ title="backbone net">backbone net</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/posts/w00100_cv/w0020_contrastive_learning/>对比学习</a><ul class=active><li><a class=active href=/zh-cn/posts/w00100_cv/w0020_contrastive_learning/contrastive_learning/ title="contrastive learning">contrastive learning</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00100_cv/w0030_vision_transformer/>ViT</a><ul><li><a href=/zh-cn/posts/w00100_cv/w0030_vision_transformer/vision_transformer/ title="vision transformer">vision transformer</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00100_cv/w0050_detect_object/>目标检测</a><ul><li><a href=/zh-cn/posts/w00100_cv/w0050_detect_object/object_detection_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00100_cv/w0055_semantic_segmentation/>语义分割</a><ul><li><a href=/zh-cn/posts/w00100_cv/w0055_semantic_segmentation/object_detection_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00100_cv/w0060_image-matting/>抠图</a><ul><li><a href=/zh-cn/posts/w00100_cv/w0060_image-matting/image-matting-summary/ title=抠图综述>抠图综述</a></li><li><a href=/zh-cn/posts/w00100_cv/w0060_image-matting/image-matting-animal/ title="animal matting">animal matting</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00200_nlp/>NLP</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00200_nlp/w0010_word_embedding/>Word Embedding</a><ul><li><a href=/zh-cn/posts/w00200_nlp/w0010_word_embedding/word_embedding_summary/ title="Word Embedding综述">Word Embedding综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00200_nlp/w0020_rnn/>RNN</a><ul><li><a href=/zh-cn/posts/w00200_nlp/w0020_rnn/rnn_summary/ title=RNN综述>RNN综述</a></li><li><a href=/zh-cn/posts/w00200_nlp/w0020_rnn/gru/ title=GRU网络>GRU网络</a></li><li><a href=/zh-cn/posts/w00200_nlp/w0020_rnn/lstm/ title=LSTM网络>LSTM网络</a></li><li><a href=/zh-cn/posts/w00200_nlp/w0020_rnn/encode_decode/ title=编解码架构>编解码架构</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00200_nlp/w0030_transformer/>Transformer</a><ul><li><a href=/zh-cn/posts/w00200_nlp/w0030_transformer/attention/ title=Attention>Attention</a></li><li><a href=/zh-cn/posts/w00200_nlp/w0030_transformer/transformer_summary/ title=Transformer>Transformer</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00200_nlp/w0080_gpt/>GPT</a><ul><li><a href=/zh-cn/posts/w00200_nlp/w0080_gpt/gpt_summary/ title=GPT综述>GPT综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00200_nlp/w0100_bert/>Bert</a><ul><li><a href=/zh-cn/posts/w00200_nlp/w0100_bert/bert_summary/ title=Bert综述>Bert综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00200_nlp/w0150_bart/>BART</a><ul><li><a href=/zh-cn/posts/w00200_nlp/w0150_bart/bart_summary/ title=BART综述>BART综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00200_nlp/w0200_electra/>ELECTRA</a><ul><li><a href=/zh-cn/posts/w00200_nlp/w0200_electra/electra_summary/ title=ELECTRA综述>ELECTRA综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00200_nlp/w1000_code/>CODE</a><ul><li><a href=/zh-cn/posts/w00200_nlp/w1000_code/bart_summary/ title=code解析>code解析</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00300_aigc/>AIGC</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00300_aigc/w0010_aigc_summary/>简介</a><ul><li><a href=/zh-cn/posts/w00300_aigc/w0010_aigc_summary/a_aigc_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00300_aigc/w0020_diffusion_model/>Diffusion Model</a><ul><li><a href=/zh-cn/posts/w00300_aigc/w0020_diffusion_model/w10_diffusion_summary_/ title=模型介绍>模型介绍</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00400_vlp/>多模态</a><ul><li><a href=/zh-cn/posts/w00400_vlp/vlp_summary/ title=简介>简介</a></li><li><a href=/zh-cn/posts/w00400_vlp/clip/ title=CLIP>CLIP</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00500_video/>视频理解</a><ul><li><a href=/zh-cn/posts/w00500_video/vidio_summary/ title=简介>简介</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/john_hu7f9991f5b5d471ecdfc6cff3db1a9fe6_6397_120x120_fit_box_2.png alt="Author Image"><h5 class=author-name>biubiobiu</h5><p>May 9, 2022</p></div><div class=title><h1>contrastive learning</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/zh-cn/tags/backbone class="btn, btn-sm">backbone</a></li><li class=rounded><a href=/zh-cn/tags/contrastive-learning class="btn, btn-sm">contrastive learning</a></li></ul></div><div class=post-content id=post-content><p>从2019年中~2020年中，对比学习火了一段时间，到ViT出来后，大量的研究这才投身于ViT。</p><h2 id=一简介>一、简介</h2><p>什么是对比学习？<br>简单来说就是，只要模型把相似的数据跟其他不相似的数据区分开就可以。比如：$A_1, A_2, &mldr;$ 是狗，$B_1, B_2, &mldr;$ 是猫，只要模型能把这两批数据区分开就行。<br>所以，训练集中不需要明确的标签，只要能区分出那些数据之间是相似的，那些是与它们不相似的。<br>所以，训练集中不必人为标注，只需要设计一些规则生产出这种类型的训练集就行。<br></p><p>看下Hinton老爷子的<a href=http://www.cs.toronto.edu/~fritz/absps/naturebecker.pdf target=blank>《Self-organizing neural network that discovers surfaces in random-dot stereograms》</a> 和 LeCun的<a href=http://www.cs.toronto.edu/~hinton/csc2535/readings/hadsell-chopra-lecun-06-1.pdf target=blank>《Dimensionality reduction by learning an invariant mapping》</a><br></p><p>对比学习为啥在cv领域被认为是无监督呢？：</p><ol><li>通过设计一些巧妙的代理任务，就是pretext task：人为的定义一些规则，这些规则可以用来定义那些图片是相似的，那些图片是不相似的。<br>例如：instance discrimination：如果有N张图片的数据集，随机一张图片$x_i$，对这个图片随机裁剪+数据增广，从同一张图片中通过裁剪+增广产生的数据，虽然有差异但是语义信息是一样的，所以是正样本(它们之间是相似的)，负样本就是除了图$x_i$之外的所有样本。</li></ol><h3 id=1代理任务>1、代理任务</h3><p>代理任务(pretext task)的目的: 生成一个自监督的信号，从而充当ground truth这个标签信息<br>有监督学习：训练时比较输出 $\hat{Y}$ 和 groud truth $Y$；<br>自监督学习：因为缺少groud truth，所以需要代理任务自己创建类似groud truth的信号。</p><h3 id=2对比学习的loss>2、对比学习的loss</h3><h4 id=1infonce-loss>1)、InfoNCE loss</h4><p>noise contrastive estimation loss：其实就是一个交叉熵
$$
L_q = -log\frac{exp(q\cdot k_+ / \tau)}{\sum_{i=0}^{K} exp(q\cdot k_i / \tau)}
$$
分母：一个正样本，K个负样本；$\tau$：温度超参数，值越大分布就越平缓，表示对每种的关注度越相似；值越小分布就越陡峭，表示比较关注比较困难的case，不容易收敛。</p><h3 id=3数据>3、数据</h3><p>数据处理流程：</p><ol><li>图片$x_1$经过不同的变换分别生成了不同的图片$x_1^1, x_1^2$，一般$x_1^1$为锚点作为基准；$x_1^2$是$x_1^1$的正样本；剩余的$x_2, x_3, &mldr;, x_N$是$x_1^1$的负样本。</li><li>有了正负样本数据后，就是把这些数据丢进编码器提取特征；锚点的特征：$f_{11}$，正样本：$f_{12}$，负样本：$f_2, f_3, &mldr;, f_N$</li><li>对比学习的目的：在特征空间里，让锚点的特征$f_{11}$与正样本的特征$f_{12}$尽量靠近；与负样本的特征$f_2, f_3, &mldr;, f_N$尽量远离。</li></ol><p align=center><img src=/datasets/posts/cnn/contrast_data.jpg width=70% height=70% title=data alt=data></p><h2 id=二初代对比网络>二、初代对比网络</h2><h3 id=1instdisc>1、InstDisc</h3><p><a href=https://arxiv.org/abs/1805.01978 target=blank>《Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination》2018</a> 缺点是：字典特征不一致。</p><ol><li>提出了个体判别这个代理任务。</li><li>把ImageNet的每张图片，表示层128维特征，提前算好存起来(memory bank)</li><li>对memory bank的特征进行动态更新</li><li>在计算loss时使用动量，来弥补字典特征的不一致性</li></ol><h3 id=2invaspread>2、InvaSpread</h3><p><a href=https://arxiv.org/abs/1904.03436 target=blank>《Unsupervised Embedding Learning via Invariant and Spreading Instance Feature》2019</a>：Invariant：对于相似的图片，特征尽量不变；Spreading：对于不相似的图片，特征尽量分散。 缺点：字典太小。</p><ol><li>个体判别代理任务</li><li>在mni-batch内选择正样本和负样本：比如batch size = 256，正样本就是mini-batch内的每个样本，负样本就是除去该正样本后的所有样本和其数据增强后的样本。</li><li>目标函数为 NCE Loss的一个变体</li></ol><h3 id=3cpc>3、CPC</h3><p><a href=https://arxiv.org/abs/1807.03748 target=blank>《Representation Learning with Contrastive Predictive Coding》2018</a></p><ol><li>预测未来的代理任务(生成式)：未来输入，通过已训的网络后的输出特征作为正样本；其他任意输入通过已训的网络后的输出特征作为负样本。</li></ol><h3 id=4cmc>4、CMC</h3><p><a href=https://arxiv.org/abs/1906.05849 target=blank>《Contrastive Multiview Coding》2019</a></p><ol><li>多视角的代理任务：同一事物的不同视角的表征，是正样本；其他事物的表征，是负样本</li><li>不同视角会有不同的编码器</li><li>证明了多模态融合的可能性</li></ol><h2 id=三二代目对比网络>三、二代目对比网络</h2><h3 id=1moco>1、MoCo</h3><p><a href=https://arxiv.org/abs/1911.05722 target=blank>MoCo</a>(2020) 作为一个无监督的表征学习工作，不仅在分类领域逼近有监督的模型，还在检测、分割、人体关键点检测都超越了有监督的预训练模型，MoCo的出现证明了在视觉领域无监督训练是有前途的。<br></p><div class="alert alert-success"><strong><p>问题：为什么无监督学习在NLP领域表现较好，在视觉领域效果不好呢？<br>作者认为：NLP领域，每个token是一个独立的语义信息，其分布是一个离散的信号空间，由于token的独立性，在字典集合中，其就是一个分类任务，可以有类似标签的形式帮助训练；视觉是一个高维连续空间，不像token有很强的语义信息而且浓缩的比较好，导致视觉不能创建一个类似NLP的字典，没有这个字典就不容易建模，所以在视觉领域无监督学习还不如有监督学习。</p><p>问题：作者设计动机？<br>以往的工作，会受限于：<code>a. 字典的大小；b. 字典内的一致性</code>。</p><ol><li>训练一个encoder，从图像数据里抽样出特征，由这些特征组成一个动态字典；<ol><li>这个字典一定要大：字典越大，就可以表征更多的视觉信息。</li><li>在训练的时候，字典内要有一致性：各个key的尽量是通过相同的编码器产出的，不然query可能选择与自己的编码器相同的key，而不是真的和它含有相同语义信息的那个key。</li></ol></li><li>对比学习使得正样本间距离尽量小，负样本距离尽量大</li></ol></strong></div><div class=row><div class="col col-sm-12 col-lg-6"><p>作者怎么设计的：</p><ol><li>怎么构建大字典？所有数据集组成一个字典肯定不行，计算一次要花费很长时间，而且内存也不够。作者使用了队列这种数据结构，队列的大小就是字典的大小；队列可以很大(字典很大)，每次训练的mini-batch很小；队列中的元素不是每次都需要更新，每次更新mini-batch大小的数据。新的数据入队，最久的数据出队。字典大小(作者默认：65536)是一个超参数，可以是几千上万，训练时间都差不多。</li><li>怎么保持一致性呢？作者设计了动量编码器：例如：query的编码器为 $\theta_q$，动量编码器为：$\theta_k \gets m \theta_{k} + (1-m)\theta_q$。设计一个较大的动量参数m(e.g., m=0.999)，使得动量编码器更新的<code>比较缓慢</code>，不会因为 $\theta_q$引起太多的改变，所以近似保持一致性。只有 $\theta_q$ 参与训练，$\theta_k$ 是不参与训练的，是由0.999的上一个 $\theta_k$+0.001的当前 $\theta_q$组合而成。</li></ol></div><div class="col col-sm-12 col-lg-6"><p align=center><img src=/datasets/posts/cnn/moco.jpg width=90% height=90% title=moco alt=moco></div></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># f_q, f_k: encoder networks for query and key</span>
<span style=color:#75715e># queue: dictionary as a queue of K keys (CxK)</span>
<span style=color:#75715e># m: momentum</span>
<span style=color:#75715e># t: temperature</span>
f_k<span style=color:#f92672>.</span>params <span style=color:#f92672>=</span> f_q<span style=color:#f92672>.</span>params <span style=color:#75715e># initialize</span>
<span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> loader: <span style=color:#75715e># load a minibatch x with N samples</span>
    x_q <span style=color:#f92672>=</span> aug(x) <span style=color:#75715e># a randomly augmented version</span>
    x_k <span style=color:#f92672>=</span> aug(x) <span style=color:#75715e># another randomly augmented version</span>
    q <span style=color:#f92672>=</span> f_q<span style=color:#f92672>.</span>forward(x_q) <span style=color:#75715e># queries: NxC</span>
    k <span style=color:#f92672>=</span> f_k<span style=color:#f92672>.</span>forward(x_k) <span style=color:#75715e># keys: NxC</span>
    k <span style=color:#f92672>=</span> k<span style=color:#f92672>.</span>detach() <span style=color:#75715e># no gradient to keys</span>
    <span style=color:#75715e># positive logits: Nx1</span>
    l_pos <span style=color:#f92672>=</span> bmm(q<span style=color:#f92672>.</span>view(N,<span style=color:#ae81ff>1</span>,C), k<span style=color:#f92672>.</span>view(N,C,<span style=color:#ae81ff>1</span>))
    <span style=color:#75715e># negative logits: NxK</span>
    l_neg <span style=color:#f92672>=</span> mm(q<span style=color:#f92672>.</span>view(N,C), queue<span style=color:#f92672>.</span>view(C,K))
    <span style=color:#75715e># logits: Nx(1+K)</span>
    logits <span style=color:#f92672>=</span> cat([l_pos, l_neg], dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
    <span style=color:#75715e># contrastive loss, Eqn.(1)</span>
    labels <span style=color:#f92672>=</span> zeros(N) <span style=color:#75715e># positives are the 0-th</span>
    loss <span style=color:#f92672>=</span> CrossEntropyLoss(logits<span style=color:#f92672>/</span>t, labels)
    <span style=color:#75715e># SGD update: query network</span>
    loss<span style=color:#f92672>.</span>backward()
    update(f_q<span style=color:#f92672>.</span>params)
    <span style=color:#75715e># momentum update: key network</span>
    f_k<span style=color:#f92672>.</span>params <span style=color:#f92672>=</span> m<span style=color:#f92672>*</span>f_k<span style=color:#f92672>.</span>params<span style=color:#f92672>+</span>(<span style=color:#ae81ff>1</span><span style=color:#f92672>-</span>m)<span style=color:#f92672>*</span>f_q<span style=color:#f92672>.</span>params
    <span style=color:#75715e># update dictionary</span>
    enqueue(queue, k) <span style=color:#75715e># enqueue the current minibatch</span>
    dequeue(queue) <span style=color:#75715e># dequeue the earliest minibatch</span>

</code></pre></div><h3 id=2simclr>2、SimCLR</h3><p><a href=https://arxiv.org/abs/2002.05709 target=blank>《A simple framework for contrastive learning of visual representations》2020</a>很简单。缺点就是 字典比较小。</p><p align=center><img src=/datasets/posts/cnn/simclr_arch.png width=70% height=70% title=SimCLR alt=SimCLR></p><ol><li>在mni-batch内选择正样本和负样本：比如batch size = 256，正样本就是mini-batch内的每个样本，负样本就是除去该正样本后的所有样本和其数据增强后的样本。</li><li>训练时添加了一个mlp全连接层，就是那个 $g(\sdot)$，这个简单的操作，在ImageNet数据集上提升了10个点。</li><li>更丰富的数据增强</li></ol><h3 id=3moco-v2>3、MoCo v2</h3><p><a href=https://arxiv.org/abs/2003.04297 target=blank>MoCo V2</a> 基于初版MoCo和SimCLR的全连接层，做了一些优化，发现添加mlp全连接层真香</p><p align=center><img src=/datasets/posts/cnn/mocov2.jpg width=70% height=70% title=MoCoV2 alt=MoCoV2></p><ol><li>加了更丰富的数据增强 <code>提升3个点</code></li><li>加了MLP <code>提升了6个点</code></li><li>加了cosine 的学习率</li><li>训练更多个epoch <code>提升4个点</code></li></ol><h3 id=4simclr-v2>4、SimCLR v2</h3><p><a href=https://arxiv.org/abs/2003.04297 target=blank>《Big Self-Supervised Models are Strong Semi-Supervised Learners》</a></p><ol><li>采用更大的模型，152-ResNet</li><li>添加了2层mlp</li><li>使用动量编码器</li></ol><h3 id=5swavswap-assignment-views>5、SwAV(swap assignment views)</h3><p><a href=https://arxiv.org/abs/2006.09882 target=blank>《Unsupervised Learning of Visual Features by Contrasting Cluster Assignments》2020</a>：给定同样一张图片，生成不同的视角；希望可以用一个视角得到的特征去预测另外一个视角得到的特征。</p><ol><li>multi crop 技术：全局和局部的特征都需要关注</li></ol><h2 id=四三代目对比网络>四、三代目对比网络</h2><p>不使用负样本</p><h3 id=1byol>1、BYOL</h3><p><a href=https://arxiv.org/abs/2006.07733 target=blank>《Bootstrap your own latent: A new approach to self-supervised Learning》2020</a></p><p align=center><img src=/datasets/posts/cnn/byol.jpg width=70% height=70% title=byol alt=byol></p><p align=center><img src=/datasets/posts/cnn/byol_2.jpg width=70% height=70% title=byol alt=byol></p><ol><li>对输入图片x，锚点通过一系列的变换，最后是 $q_{\theta}(z_{\theta})$；正样本通过一些列的变换，最后是 $sg(z'_{\xi})$。这两个是输入图片的近似表示</li><li>让 $sg(z'_{\xi})$ 做target，计算这两个的MSE-loss</li><li>模型最后就训练了编码器 $f_{\theta}$，正样本的编码器 $f_{\xi}$ 只是 $f_{\theta}$ 的平均，也就是动量编码器。</li><li>模型没有使用负样本，只是用自己预测自己，为啥没有出现<code>模型坍塌</code>呢？<ol><li>参考 这个 <a href=https://generallyintelligent.ai/blog/2020-08-24-understanding-self-supervised-contrastive-learning/ target=blank>Blog</a> 博主，通过一系列的实验，得出他的结论：BYOL能够学到东西，主要是因为Batch normalization。通过BN的操作，用整个batch的样本计算 均值、方差，然后用在batch内的各个样本上；这个操作相当于存在信息泄露，一个样本在计算是也能看到整个batch的信息，相当于一个平均的信息作为负样本；即使没有刻意提供负样本，但通过BN的操作也有了负样本的作用。</li><li>BYOL的作者听到后就不同意了，他也做了一些列的实验 <a href=https://arxiv.org/abs/2010.10241 target=blank>《BYOL works even without batch statistics》</a>，发现BN确实很香；不过，他认为是BN只是使得模型能够稳定训练，真正起作用的是一个很好的初始化。</li></ol></li></ol><h3 id=2simsiam>2、SimSiam</h3><p><a href=https://arxiv.org/abs/2011.10566 target=blank>《Exploring Simple Siamese Representation Learning》2020</a></p><div class=row><div class="col col-sm-12 col-lg-6"><p>作者怎么设计的：</p><ol><li>不需要负样本</li><li>不需要大的batch size</li><li>不需要动量编码器</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># f: backbone + projection mlp</span>
<span style=color:#75715e># h: prediction mlp</span>
<span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> loader: <span style=color:#75715e># load a minibatch x with n samples</span>
    x1, x2 <span style=color:#f92672>=</span> aug(x), aug(x) <span style=color:#75715e># random augmentation</span>
    z1, z2 <span style=color:#f92672>=</span> f(x1), f(x2) <span style=color:#75715e># projections, n-by-d</span>
    p1, p2 <span style=color:#f92672>=</span> h(z1), h(z2) <span style=color:#75715e># predictions, n-by-d</span>
    L <span style=color:#f92672>=</span> D(p1, z2)<span style=color:#f92672>/</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>+</span> D(p2, z1)<span style=color:#f92672>/</span><span style=color:#ae81ff>2</span> <span style=color:#75715e># loss</span>
    L<span style=color:#f92672>.</span>backward() <span style=color:#75715e># back-propagate</span>
    update(f, h) <span style=color:#75715e># SGD update</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>D</span>(p, z): <span style=color:#75715e># negative cosine similarity</span>
    z <span style=color:#f92672>=</span> z<span style=color:#f92672>.</span>detach() <span style=color:#75715e># stop gradient</span>
    p <span style=color:#f92672>=</span> normalize(p, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>) <span style=color:#75715e># l2-normalize</span>
    z <span style=color:#f92672>=</span> normalize(z, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>) <span style=color:#75715e># l2-normalize</span>
    <span style=color:#66d9ef>return</span> <span style=color:#f92672>-</span>(p<span style=color:#f92672>*</span>z)<span style=color:#f92672>.</span>sum(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>mean()

</code></pre></div></div><div class="col col-sm-12 col-lg-6"><p align=center><img src=/datasets/posts/cnn/simSiam.jpg width=90% height=90% title=moco alt=moco></div></div><hr><p>结论：类似于EM(Expectation-Maximization)算法。</p><h2 id=五四代目对比网络>五、四代目对比网络</h2><p>Transformer在对比学习上的应用</p><h3 id=1moco-v3>1、MoCo V3</h3><p><a href=https://arxiv.org/abs/2104.02057 target=blank>《An Empirical Study of Training Self-Supervised Vision Transformers》2021</a> 相当于 MoCo V2 和SimSiam的合体</p><div class=row><div class="col col-sm-12 col-lg-6"><ol><li>不训练 patch projection层。</li><li>把backbone有ResNet换成ViT；在训练时作者发现，准确度会时不时的塌陷。这个原因是什么呢？<ol><li>作者通过观察回传梯度，在第一层梯度波动较大，说明这一层梯度不正常。作者设想：梯度既然不正常还不如直接不用梯度更新第一层，作者在第一层初始化后就直接冻住，不再更新第一层，实验后发现问题解决了。</li><li>所以，在ViT的第一层patch projection 还是有问题的，后续会被继续研究</li></ol></li></ol></div><div class="col col-sm-12 col-lg-6"><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># f_q: encoder: backbone + proj mlp + pred mlp</span>
<span style=color:#75715e># f_k: momentum encoder: backbone + proj mlp</span>
<span style=color:#75715e># m: momentum coefficient</span>
<span style=color:#75715e># tau: temperature</span>
<span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> loader: <span style=color:#75715e># load a minibatch x with N samples</span>
    x1, x2 <span style=color:#f92672>=</span> aug(x), aug(x) <span style=color:#75715e># augmentation</span>
    q1, q2 <span style=color:#f92672>=</span> f_q(x1), f_q(x2) <span style=color:#75715e># queries: [N, C] each</span>
    k1, k2 <span style=color:#f92672>=</span> f_k(x1), f_k(x2) <span style=color:#75715e># keys: [N, C] each</span>
    loss <span style=color:#f92672>=</span> ctr(q1, k2) <span style=color:#f92672>+</span> ctr(q2, k1) <span style=color:#75715e># symmetrized</span>
    loss<span style=color:#f92672>.</span>backward()
    update(f_q) <span style=color:#75715e># optimizer update: f_q</span>
    f_k <span style=color:#f92672>=</span> m<span style=color:#f92672>*</span>f_k <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span><span style=color:#f92672>-</span>m)<span style=color:#f92672>*</span>f_q <span style=color:#75715e># momentum update: f_k</span>
<span style=color:#75715e># contrastive loss</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>ctr</span>(q, k):
    logits <span style=color:#f92672>=</span> mm(q, k<span style=color:#f92672>.</span>t()) <span style=color:#75715e># [N, N] pairs</span>
    labels <span style=color:#f92672>=</span> range(N) <span style=color:#75715e># positives are in diagonal</span>
    loss <span style=color:#f92672>=</span> CrossEntropyLoss(logits<span style=color:#f92672>/</span>tau, labels)
    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> tau <span style=color:#f92672>*</span> loss

</code></pre></div></div></div><h3 id=2dino>2、DINO</h3><p><a href=https://arxiv.org/abs/2104.14294 target=blank>《Emerging Properties in Self-Supervised Vision Transformers》2021</a> 跟BYOL、SimSiam类似，也是预测型。</p><div class=row><div class="col col-sm-12 col-lg-6"><p>跟MoCo V3 非常类似<br></p><p>总结一下：</p><p align=center><img src=/datasets/posts/cnn/craster_all.jpg width=90% height=90% title=contrastive alt=contrastive></p></div><div class="col col-sm-12 col-lg-6"><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># gs, gt: student and teacher networks</span>
<span style=color:#75715e># C: center (K)</span>
<span style=color:#75715e># tps, tpt: student and teacher temperatures</span>
<span style=color:#75715e># l, m: network and center momentum rates</span>
gt<span style=color:#f92672>.</span>params <span style=color:#f92672>=</span> gs<span style=color:#f92672>.</span>params
<span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> loader: <span style=color:#75715e># load a minibatch x with n samples</span>
    x1, x2 <span style=color:#f92672>=</span> augment(x), augment(x) <span style=color:#75715e># random views</span>
    s1, s2 <span style=color:#f92672>=</span> gs(x1), gs(x2) <span style=color:#75715e># student output n-by-K</span>
    t1, t2 <span style=color:#f92672>=</span> gt(x1), gt(x2) <span style=color:#75715e># teacher output n-by-K</span>
    loss <span style=color:#f92672>=</span> H(t1, s2)<span style=color:#f92672>/</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>+</span> H(t2, s1)<span style=color:#f92672>/</span><span style=color:#ae81ff>2</span>
    loss<span style=color:#f92672>.</span>backward() <span style=color:#75715e># back-propagate</span>
    <span style=color:#75715e># student, teacher and center updates</span>
    update(gs) <span style=color:#75715e># SGD</span>
    gt<span style=color:#f92672>.</span>params <span style=color:#f92672>=</span> l<span style=color:#f92672>*</span>gt<span style=color:#f92672>.</span>params <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span><span style=color:#f92672>-</span>l)<span style=color:#f92672>*</span>gs<span style=color:#f92672>.</span>params
    C <span style=color:#f92672>=</span> m<span style=color:#f92672>*</span>C <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span><span style=color:#f92672>-</span>m)<span style=color:#f92672>*</span>cat([t1, t2])<span style=color:#f92672>.</span>mean(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>H</span>(t, s):
    t <span style=color:#f92672>=</span> t<span style=color:#f92672>.</span>detach() <span style=color:#75715e># stop gradient</span>
    s <span style=color:#f92672>=</span> softmax(s <span style=color:#f92672>/</span> tps, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
    t <span style=color:#f92672>=</span> softmax((t <span style=color:#f92672>-</span> C) <span style=color:#f92672>/</span> tpt, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>) <span style=color:#75715e># center + sharpen</span>
    <span style=color:#66d9ef>return</span> <span style=color:#f92672>-</span> (t <span style=color:#f92672>*</span> log(s))<span style=color:#f92672>.</span>sum(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>mean()


</code></pre></div></div></div></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"><strong>分享:</strong>
<a class="btn btn-sm facebook-btn" href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2fw00100_cv%2fw0020_contrastive_learning%2fcontrastive_learning%2f" target=_blank><i class="fab fa-facebook"></i></a><a class="btn btn-sm twitter-btn" href="https://twitter.com/share?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2fw00100_cv%2fw0020_contrastive_learning%2fcontrastive_learning%2f&text=contrastive%20learning&via=biubiobiu%27s%20Blog" target=_blank><i class="fab fa-twitter"></i></a><a class="btn btn-sm reddit-btn" href="https://reddit.com/submit?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2fw00100_cv%2fw0020_contrastive_learning%2fcontrastive_learning%2f&title=contrastive%20learning" target=_blank><i class="fab fa-reddit"></i></a><a class="btn btn-sm linkedin-btn" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2fw00100_cv%2fw0020_contrastive_learning%2fcontrastive_learning%2f&title=contrastive%20learning" target=_blank><i class="fab fa-linkedin"></i></a><a class="btn btn-sm whatsapp-btn" href="https://api.whatsapp.com/send?text=contrastive%20learning https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2fw00100_cv%2fw0020_contrastive_learning%2fcontrastive_learning%2f" target=_blank><i class="fab fa-whatsapp"></i></a><a class="btn btn-sm email-btn" href="mailto:?subject=contrastive%20learning&body=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2fw00100_cv%2fw0020_contrastive_learning%2fcontrastive_learning%2f" target=_blank><i class="fas fa-envelope-open-text"></i></a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/zh-cn/posts/w00100_cv/w0010_backbone/d1_3_backbone_net/ title="backbone net" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i>上一篇</div><div class=next-prev-text>backbone net</div></a></div><div class="col-md-6 next-article"><a href=/zh-cn/posts/w00100_cv/w0030_vision_transformer/vision_transformer/ title="vision transformer" class="btn btn-outline-info"><div>下一篇 <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>vision transformer</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">目录</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#一简介>一、简介</a><ul><li><a href=#1代理任务>1、代理任务</a></li><li><a href=#2对比学习的loss>2、对比学习的loss</a><ul><li><a href=#1infonce-loss>1)、InfoNCE loss</a></li></ul></li><li><a href=#3数据>3、数据</a></li></ul></li><li><a href=#二初代对比网络>二、初代对比网络</a><ul><li><a href=#1instdisc>1、InstDisc</a></li><li><a href=#2invaspread>2、InvaSpread</a></li><li><a href=#3cpc>3、CPC</a></li><li><a href=#4cmc>4、CMC</a></li></ul></li><li><a href=#三二代目对比网络>三、二代目对比网络</a><ul><li><a href=#1moco>1、MoCo</a></li><li><a href=#2simclr>2、SimCLR</a></li><li><a href=#3moco-v2>3、MoCo v2</a></li><li><a href=#4simclr-v2>4、SimCLR v2</a></li><li><a href=#5swavswap-assignment-views>5、SwAV(swap assignment views)</a></li></ul></li><li><a href=#四三代目对比网络>四、三代目对比网络</a><ul><li><a href=#1byol>1、BYOL</a></li><li><a href=#2simsiam>2、SimSiam</a></li></ul></li><li><a href=#五四代目对比网络>五、四代目对比网络</a><ul><li><a href=#1moco-v3>1、MoCo V3</a></li><li><a href=#2dino>2、DINO</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>导航</h5><ul><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#about>关于</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#skills>技能</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#experiences>经验</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#projects>项目</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#education>教育</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#recent-posts>最新博客</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#accomplishments>造诣</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#achievements>业绩</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>联系方式:</h5><ul><li><span>电话:</span> <span>+0123456789</span></li><li><span>邮箱:</span> <span>bsgshyn8@163.com</span></li></ul></div></div></div><hr><div class=container><p id=disclaimer><strong>免责声明:</strong> 这个主题是MIT许可的。因此，您可以将其用于非商业、商业或者私人用途。您可以修改或分发主题，而无须经 作者任何许可。但是，主题作者不对主题的任何问题提供任何保证或承担任何责任。</p></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=stylesheet href=/katex/katex.min.css><script type=text/javascript defer src=/katex/katex.min.js></script><script type=text/javascript defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body);></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false},{left:'\\(',right:'\\)',display:false},{left:'\\[',right:'\\]',display:true}],throwOnError:false});});</script></body></html>