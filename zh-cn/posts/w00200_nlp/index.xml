<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NLP on biubiobiu's Blog</title><link>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/</link><description>Recent content in NLP on biubiobiu's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Everything is mine</copyright><lastBuildDate>Wed, 08 Sep 2021 06:00:20 +0600</lastBuildDate><atom:link href="https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>Attention</title><link>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0030_transformer/attention/</link><pubDate>Wed, 08 Sep 2021 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0030_transformer/attention/</guid><description>一、Attention机制 如何有选择地引导注意力：
非自主性提示： 基于环境中物体的突出性和易见性。比如 《辛德勒的名单》中的镜头：黑白镜头中的穿红衣服的小女孩。
自主性提示： 选择受到 认知、意识的控制。
在不受自我意识控制的情况下，与环境差别最大的事物，就越显眼、易见。
在受到自我意识控制的情况下，意识偏向那个，就选择那个
查询(query)：自主性提示，类似于自我意识。
键(key)：非自主提示，类似于事物的突出性、易见性。
值(value)：感官输入，类似于具体的事物-值。
attention机制可以认为是一个这样的函数：
$$ f(\bold{q_j}) = \sum_{i=1}^m \alpha(\bold{q}_j, \bold{k}_i) \bold{v}_i$$ 由$ \bold{V}$ 的各个向量的加权平均，组成一个新的向量 $f(q_j)$。其中，权重的计算是通过 query向量和每个key向量 计算出来的，这个计算方式可以有多种，比如：加性注意力、缩放点积注意力
$\bold{Q} \in \R^{n \times q}$: 查询矩阵，是由N个向量组成，每个向量有q个元素
K-V: M个键值对集合。
$\bold{K} \in \R^{m \times k}$: M个键向量组成的矩阵，每个键向量(k维)：就是每个字的标签信息
$\bold{V} \in \R^{m \times v}$: M个值向量组成的矩阵，每个值向量(v维)：就是每个字的embeding
1、加性注意力 $$\alpha(\bold{q}_j, \bold{k}_i) = \bold{w}_v^T tanh(\bold{W}_q \bold{q}_j + \bold{W}_k \bold{k}_i)$$ 其中，$\bold{w}_v^T \in \R^h, \bold{W}_q \in \R^{h \times q}, \bold{W}_k \in \R^{h \times k}$ 是需要训练的。</description></item><item><title>BART综述</title><link>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0150_bart/bart_summary/</link><pubDate>Wed, 08 Sep 2021 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0150_bart/bart_summary/</guid><description>一、背景 二、BART BART的全称是
三、总结</description></item><item><title>code解析</title><link>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w1000_code/bart_summary/</link><pubDate>Wed, 08 Sep 2021 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w1000_code/bart_summary/</guid><description>一、transformers Hugging Face公司发布的transformers包，能够超级方便的引入训练模型：BERT、GPT2、&amp;hellip; transformers英文文档 transformers中文文档
二、Tokenizer from transformers import BertTokenizerFast, BertTokenizer from transformers import GPT2TokenizerFast, GPT2LMHeadModel # 初始化tokenizer tokenizer = BertTokenizerFast(vocab_file=args.vocab_path, sep_token=&amp;#34;[SEP]&amp;#34;, pad_token=&amp;#34;[PAD]&amp;#34;, cls_token=&amp;#34;[CLS]&amp;#34;) # 对比 tokenizer.encode() 与 tokenizer.tokenize() sentence = &amp;#34;Hello, my son is cuting.&amp;#34; input_ids_1 = tokenizer.encode(sentence, add_special_tokens=False) # add_special_tokens=True 将句子转换成对应模型的输入形式，默认开启。就是首尾加上[cls]、[sep]。即：tensor([ 101, 7592, 1010, 2026, 2365, 2003, 3013, 2075, 1012, 102]) # add_special_tokens=False 首尾先不加[cls]、[sep] input_tokens = tokenizer.tokenize(sentence) # [&amp;#39;hello&amp;#39;, &amp;#39;,&amp;#39;, &amp;#39;my&amp;#39;, &amp;#39;son&amp;#39;, &amp;#39;is&amp;#39;, &amp;#39;cut&amp;#39;, &amp;#39;##ing&amp;#39;, &amp;#39;.&amp;#39;] input_ids_2 = tokenizer.</description></item><item><title>ELECTRA综述</title><link>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0200_electra/electra_summary/</link><pubDate>Wed, 08 Sep 2021 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0200_electra/electra_summary/</guid><description>一、背景 二、ELECTRA ELECTRA的全称是Efficiently Learning an Encoder that Classifies Token Replacements Accurately。最主要的贡献是提出了新的预训练任务和框架，把生成式的Masked language model(MLM)预训练任务改成了判别式的Replaced token detection(RTD)任务，判断当前token是否被语言模型提换过。
三、总结</description></item><item><title>GPT综述</title><link>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0080_gpt/gpt_summary/</link><pubDate>Wed, 08 Sep 2021 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0080_gpt/gpt_summary/</guid><description>一、简介 参考
评估指标：
困惑度：困惑度（perplexity）的基本思想是：给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好，公式如下 $PP(W)=P(w_1w_2&amp;hellip;w_N)^{\frac{-1}{N}}$ 。由公式可知，句子概率越大，语言模型越好，迷惑度越小。困惑度p可以理解为，如果每个时间步都根据语言模型计算的概率分布随机挑词，那么平均情况下，挑多少个词才能挑到正确的那个 Prompt ranking accuracy：这个指标的定义和评价方法，来自《Hierarchical Neural Story Generation》。主要是关注引导语和生成的故事之间的相关性。具体做法是：在测试集中选择一对（p，g），p表示引导语，g表示生成的故事，在随机选取其他的引导语p1-p9，然后计算p和g的likelihood。条件一：（p，g）的相似性比（p1，g）的相似性大。 那么就取10000个测试集中的（p，g），满足条件一的部分占比，就称为Prompt ranking accuracy。 句子嵌入的相似度：计算引导语和生成的故事的句子嵌入（用GloVe取每个词的平均嵌入值）的余弦相似度。 评价连贯性：连贯性的评价方法，来自《Modeling local coherence: An entity-based approach》，主要思想是，在测试数据集中，对于一个故事s0，选择前面15个句子，打乱顺序，生成14个乱序的故事s1-s14。然后用语言模型计算s0-s14的可能性。对于s1-s14，如果可能性大于s0，就称为反例。 错误率定义为反例的占比。 评价单词的重复性和rareness 二、GPT GPT(2018-06)
三、GPT-2 GPT-2(2019-02)
GPT-2去掉了fine-tuning层：不再针对不同任务分别进行微调建模，而是不定义这个模型应该做什么任务，模型会自动识别出来需要做什么任务。这就好比一个人博览群书，你问他什么类型的问题，他都可以顺手拈来，GPT-2就是这样一个博览群书的模型。在Pretrain部分基本与GPT方法相同，在Fine-tune部分把第二阶段的Fine-tuning有监督训练具体NLP任务，换成了无监督训练具体任务，这样使得预训练和Fine-tuning的结构完全一致。当问题的输入和输出均为文字时，只需要用特定方法组织不同类型的有标注数据即可代入模型，如对于问答使用“问题+答案+文档”的组织形式，对于翻译使用“英文+法文”形式。用前文预测后文，而非使用标注数据调整模型参数。这样既使用了统一的结构做训练，又可适配不同类型的任务。虽然学习速度较慢，但也能达到相对不错的效果。 增加网络参数：GPT-2将Transformer堆叠的层数增加到48层，隐层的维度为1600，参数量更是达到了15亿。(Bert的参数量也才只有3亿)。base版-12层-117M，medium版-24层-345M，large版-36层-774M，xl版-48层-1558M。 调整transformer：将layer normalization放到每个sub-block之前，并在最后一个Self-attention后再增加一个layer normalization。 四、GPT-3 GPT-3(2020-05)
五、chatGPT chatGPT(2022-12)
BART(Bidirectional and Auto-Regressive Transformers，双向自回归转换器)
prompt
Google T5 (Text-to-Text Transfer Transformer)
Masked language model(MLM) Replaced token detection(RTD)
参考 GPT-chatbot</description></item><item><title>GRU网络</title><link>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0020_rnn/gru/</link><pubDate>Wed, 08 Sep 2021 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0020_rnn/gru/</guid><description>一、简介 RNNs中，需要的信息都放在隐藏层，当序列太长时，隐藏层累积了太多的信息，对前面太久的信息，就不容易获取到了。
另外，有些信息不太重要，有些词比较重要，所以，设计了：
更新门： $Z_t$ 有助于捕获序列中的长期依赖关系。当$Z_t = 0$时，并不是就没有$H_{t-1}$的信息了，而是$H_{t-1}$的信息通过正常的计算$H_t$的途径进来；而当$Z_t &amp;gt; 0$时，$H_{t-1}$的信息可以绕过正常的计算途径，直接添加到$H_t$中。
重置门： $R_t$ 有助于捕获序列中的短期依赖关系。$\tilde{H_t}$ 的计算跟RNNs计算相似，就是加了 $R_t$ 来限制 $H_{t-1}$，本来RNNs对太久的信息就不容易获取，所以 $R_t$ 的作用：是否忘掉历史没用的信息。
$$R_t = sigmoid(X_tW_{xr}+H_{t-1}W_{hr}+b_r)$$ $$Z_t = sigmoid(X_tW_{xz}+H_{t-1}W_{hz}+b_z)$$ $$\tilde{H_t} = tanh(X_tW_{xh} + (R_t \odot H_{t-1})W_{hh} + b_h)$$ $$H_t = Z_t \odot H_{t-1} + (1-Z_t)\odot \tilde{H_t}$$
其中，$R_t$ ：表示在更新候选隐状态时，需要多少历史隐状态信息，$Z_t$ ：表示在算真正的隐状态时，需要多少新输入的$X_t$的信息，这两个的维度与隐状态是一致的。</description></item><item><title>LSTM网络</title><link>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0020_rnn/lstm/</link><pubDate>Wed, 08 Sep 2021 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0020_rnn/lstm/</guid><description>一、简介 长短期记忆网络(LSTM)
忘记门：$F_t = sigmoid(X_tW_{xf}+H_{t-1}W_{hf}+b_f)$ 输入门：$I_t = sigmoid(X_tW_{xi}+H_{t-1}W_{hi}+b_i)$ 输出门：$O_t = sigmoid(X_tW_{xo}+H_{t-1}W_{ho}+b_o)$ 候选记忆单元：$\tilde{C_t} = tanh(X_tW_{xc} + (R_t \odot H_{t-1})W_{hc} + b_c)$ 记忆单元：$C_t = F_t \odot C_{t-1} + I_t\odot \tilde{C_t}$ 隐状态：$H_t = O_t \odot tanh(C_t)$ 其中，$F_t, I_t, O_t, C_t, H_t, \in \R^{n \times d}$</description></item><item><title>RNN综述</title><link>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0020_rnn/rnn_summary/</link><pubDate>Wed, 08 Sep 2021 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0020_rnn/rnn_summary/</guid><description>一、文本预处理 1、词元-token 英文：在训练文本模型时，模型输入最小单元：可以是词元维度，也可以是字符维度(这样的话，模型还得学习怎么用字符组合成单词)
中文：一般是字符维度；如果是词元维度，在模型之前需要进行分词，如果要使用词元维度，需要先分词，用空格间隔开。
特殊词元：未知词元 &amp;lt;unk&amp;gt;，填充词元&amp;lt;pad&amp;gt;，序列开始词元 &amp;lt;bos&amp;gt;，序列结束词元 &amp;lt;eos&amp;gt;
2、词表-vocabulary 把token映射到：一个从0开始的数字索引，也就是：
token &amp;ndash;&amp;gt; idx：token_to_idx {0:then, 1:token, &amp;hellip;.}
idx &amp;ndash;&amp;gt; token：idx_to_token: [the, token, &amp;hellip;.] 例如：
tokens: 例如：一篇文章
例如：[[一句话按照空格split后], [], [], ....]
vocab：词表，代码里可以写成一个类，其元素有：
self.idx_to_token ：['&amp;lt;unk&amp;gt;', &amp;lsquo;the&amp;rsquo;, &amp;hellip;] token的列表，按照token的个数降序排列
self.token_to_idx ：{'&amp;lt;unk&amp;gt;': 0, &amp;lsquo;the&amp;rsquo;: 1, &amp;hellip;.} token&amp;ndash;&amp;gt;idx 的映射
corpus：语料库，先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计， 得到的统计结果称之为语料
例如：[('&amp;lt;unk&amp;gt;', 1000), ('the', 900), ....]
二、深度循环神经网络 循环神经网络(Recurrent Netural Networks)：是具有隐状态的神经网络。
类似于MLP多层感知机，RNNs只是添加了时间轴信息。比如，MLP的表示如下：
$$ H = \phi(XW_{xh} + b_h) $$ $$O = HW_{hq} + b_q $$</description></item><item><title>Transformer</title><link>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0030_transformer/transformer_summary/</link><pubDate>Wed, 08 Sep 2021 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0030_transformer/transformer_summary/</guid><description>一、简介 谷歌大脑、谷歌研究院等团队于2017年联合发表文章《Attention Is All You Need》，提出了一种新的注意力 Seq2Deq 模型，以取代之前以RNN作为编/解码器实现的 Seq2Seq 模型。模型采用的也是编码器-解码器架构，但是在该模型中，编码器和解码器不再是 RNN结构，取而代之的是编码器栈（encoder stack）和解码器栈（decoder stack）（注：所谓的“栈”就是将同一结构重复多次，“stack”翻译为“堆叠”更为合适）。编码器栈和解码器栈中分别为连续N个具有相同结构的编码器和解码器。
编码器：由两部分组成（自注意力模块 + 前馈神经网络）
自注意力模块：具体来说是“Multi-Head Attention”，即“多头注意力”模块
全连接前馈网络 每个子网络都具有残差连接，其输出形式为 $LayerNorm(Sublayer(x)+x)$ ，其中 $Sublayer(x)$ 表示子网络对输入特征x进行的具体映射操作；$LayerNorm()$ 表示归一化操作。
解码器：由三部分组成（自注意力模块 + 编码-解码注意力模块 + 前馈神经网络）
解码器中多了一个编码-解码注意力模块，用来利用当前已有的输出，来匹配输入特征（即：attention操作），然后拿计算出的新特征来计算当前时间步的输出。解码器中的自注意力模块与编码器不同是：这里只能看到当前时间步之前的输入，而不是全部的输入，所以需要有mask的操作。
论文中图： 二、Transformer 输入：序列的embeding表示 + 位置编码
编码器：
多头注意力 + 残差连接(residual connection) &amp;ndash;&amp;gt; 层归一化(layer normalization) 基于位置的前馈网络(positionwise feed-forward network) + 残差连接(residual connection) &amp;ndash;&amp;gt; 层归一化(layer normalization) class PositionWiseFFN(nn.Module): &amp;#34;&amp;#34;&amp;#34;基于位置的前馈网络&amp;#34;&amp;#34;&amp;#34; def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs): super(PositionWiseFFN, self).</description></item><item><title>Word Embedding综述</title><link>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0010_word_embedding/word_embedding_summary/</link><pubDate>Wed, 08 Sep 2021 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0010_word_embedding/word_embedding_summary/</guid><description>一、简介 Word Embedding is coming soon.</description></item><item><title>编解码架构</title><link>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0020_rnn/encode_decode/</link><pubDate>Wed, 08 Sep 2021 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0020_rnn/encode_decode/</guid><description>一、编码器-解码器 架构 机器翻译：是把一个序列转换为另一个序列。为处理这种类型的输入和输出，设计这样的架构：
编码器：接受一个长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。
解码器：将固定形状的编码状态映射到长度可变的序列。
二、seq2seq Ilya Sutskever 等人设计的seq2seq：将编码器最后一时间步的state，作为解码器第一时间步的state使用。
Kyunghyun Cho 等人设计的seq2seq，将编码器最后一时间步的state，作为解码器每一个时间步的输入序列的一部分。</description></item><item><title>Bert综述</title><link>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0100_bert/bert_summary/</link><pubDate>Wed, 08 Sep 2021 06:00:20 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0100_bert/bert_summary/</guid><description>一、背景 在使用预训练模型，处理下游任务时，有两类策略：基于特征(feature-based)、基于微调(fine-tuning)
基于特征：比如：ELMo，在使用时，对每个下游任务，创建一个跟这个任务相关的神经网络；预训练作为额外的特征跟输入一起输入到模型，预训练的额外特征可能会对要训练的模型有指导作用。 基于微调：比如：GPT，预训练模型在下游使用时，不需要改动太多，类似于视觉模型的fine-tuning，预训练完成特征提取，预训练模型后面添加个简单的网络用于实现具体任务。 1、上下文敏感 在自然语言中，有丰富的多义现象，一个词到底是什么意思，需要参考上下文才能判断。流行的上下文敏感表示：
TagLM(language-model-augmented sequence tagger 语言模型增强的序列标记器) CoVe(Context Vectors 上下文向量) ELMo(Embeddings from Language Models 来自语言模型的嵌入) ELMo 将来自预训练LSTM的所有中间层表示组合为输出表示 ELMo的表示，将作为添加特征添加到下游任务的有监督模型中 2、从特定任务到通用任务 ELMo显著改进了自然语言任务，但每个解决方案仍然依赖于一个特定的任务架构。怎么设计一个模型，让各个自然语言任务通用呢？
GPT(Generative Pre Training 生成式预训练)：在Transformer的基础上，为上下文敏感设计了通用的模型。
预训练一个用于表示文本序列的语言模型 当将GPT应用于下游任务时，语言模型的后面接一个线性输出层，以预测任务的标签。GPT的下游任务的监督学习过程，只对预训练Transformer解码器中的所有参数做微调。 GPT只能从左到右 二、BERT BERT的全称是Bidirectional Encoder Representation from Transformers, 即双向Transformer的Encoder。Bert结合了ELMo和GPT的有点，其主要贡献：
双向的重要性 基于微调的掩码语言模型(Masked Language Modeling)：BERT随机遮掩词元，并使用来自双向上下文的词元以自监督的方式预测该遮掩词元。 1、构造输入 token embedding: 格式：&amp;lt;CLS&amp;gt;第一个文本序列&amp;lt;SEP&amp;gt;第二个文本序列&amp;lt;SEP&amp;gt;
segment embedding: 用来区分句子
position embedding: 在bert中 位置嵌入 是可学习的
def get_tokens_and_segments(tokens_a, tokens_b=None): &amp;#34;&amp;#34;&amp;#34;获取输入序列的词元及其片段索引&amp;#34;&amp;#34;&amp;#34; tokens = [&amp;#39;&amp;lt;cls&amp;gt;&amp;#39;] + tokens_a + [&amp;#39;&amp;lt;sep&amp;gt;&amp;#39;] # 0和1分别标记片段A和B segments = [0] * (len(tokens_a) + 2) if tokens_b is not None: tokens += tokens_b + [&amp;#39;&amp;lt;sep&amp;gt;&amp;#39;] segments += [1] * (len(tokens_b) + 1) return tokens, segments 2、MLM 词元维度</description></item></channel></rss>