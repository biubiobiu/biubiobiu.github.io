<!doctype html><html><head><title>Bert</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0d88336a9cbd3e68c7714efae786b0a9_38044_42x0_resize_box_2.png><meta property="og:title" content="Bert"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://biubiobiu.github.io/zh-cn/posts/w00200_nlp/w0100_bert/"><meta property="og:updated_time" content="2021-09-08T06:00:20+08:00"><link rel=stylesheet href=/css/layouts/list.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/zh-cn><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png alt=Logo>
biubiobiu's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu547c3206082786c1d32d36034e2a655a_40863_42x0_resize_box_2.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/zh-cn/search><input type=text name=keyword placeholder=搜索 data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/zh-cn/posts data-filter=all>博文</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00020_toha-tutorial/>Toha教程</a><ul><li><a href=/zh-cn/posts/w00020_toha-tutorial/w0010_toha-config/ title=Toha的配置>Toha的配置</a></li><li><a href=/zh-cn/posts/w00020_toha-tutorial/w0011_write-blogs/ title=撰写文章>撰写文章</a></li><li><a href=/zh-cn/posts/w00020_toha-tutorial/w0013_markdown-tutorial/ title=MarkDown入门>MarkDown入门</a></li><li><a href=/zh-cn/posts/w00020_toha-tutorial/w0015_latax_formula/ title=Katex公式>Katex公式</a></li><li><a href=/zh-cn/posts/w00020_toha-tutorial/w0020_shortcodes_samples/ title=区域块-实例>区域块-实例</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00025_math_knowledge/>数学知识</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00025_math_knowledge/w0010_math_probability_theory/>概率论</a><ul><li><a href=/zh-cn/posts/w00025_math_knowledge/w0010_math_probability_theory/w0010_basic-conception/ title=基本概念>基本概念</a></li><li><a href=/zh-cn/posts/w00025_math_knowledge/w0010_math_probability_theory/w0020_distribution_of_variables/ title=随机变量及其分布>随机变量及其分布</a></li><li><a href=/zh-cn/posts/w00025_math_knowledge/w0010_math_probability_theory/w0030_law_of_large_numbers/ title=大数定律>大数定律</a></li><li><a href=/zh-cn/posts/w00025_math_knowledge/w0010_math_probability_theory/w0040_sample_distribution/ title=样本及抽样分布>样本及抽样分布</a></li><li><a href=/zh-cn/posts/w00025_math_knowledge/w0010_math_probability_theory/w0050_parameter_estimation/ title=假设检验>假设检验</a></li><li><a href=/zh-cn/posts/w00025_math_knowledge/w0010_math_probability_theory/w0060_analysis_variance_regression/ title=方差分析及回归分析>方差分析及回归分析</a></li><li><a href=/zh-cn/posts/w00025_math_knowledge/w0010_math_probability_theory/w0070_stochastic_process/ title=随机过程>随机过程</a></li><li><a href=/zh-cn/posts/w00025_math_knowledge/w0010_math_probability_theory/w0080_markov_process/ title=马尔科夫链>马尔科夫链</a></li><li><a href=/zh-cn/posts/w00025_math_knowledge/w0010_math_probability_theory/w0090_stationary_stochastic_process/ title=平稳随机过程>平稳随机过程</a></li></ul></li><li><a href=/zh-cn/posts/w00025_math_knowledge/w0020_math_convex_optimization_theory/ title=凸优化>凸优化</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00030_deeplearning_summary/>深度学习</a><ul><li><a href=/zh-cn/posts/w00030_deeplearning_summary/w0005_deeplearning_start/ title=深度学习开篇>深度学习开篇</a></li><li><a href=/zh-cn/posts/w00030_deeplearning_summary/w0010_draw_map_for_dl/ title=神经网络画图篇>神经网络画图篇</a></li><li><a href=/zh-cn/posts/w00030_deeplearning_summary/w0020_cam/ title=CAM>CAM</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00035_programming_language/>编程语言</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00035_programming_language/w0035_python/>Python</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00035_programming_language/w0035_python/w0020_internal_lib/>Internal</a><ul><li><a href=/zh-cn/posts/w00035_programming_language/w0035_python/w0020_internal_lib/w0010_encode_mode/ title=字符编码>字符编码</a></li><li><a href=/zh-cn/posts/w00035_programming_language/w0035_python/w0020_internal_lib/w0020_basic_operator/ title=基础操作>基础操作</a></li><li><a href=/zh-cn/posts/w00035_programming_language/w0035_python/w0020_internal_lib/w0030_advance_operator/ title=进阶操作>进阶操作</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00035_programming_language/w0035_python/w0050_sdk_lib/>SDK</a><ul><li><a href=/zh-cn/posts/w00035_programming_language/w0035_python/w0050_sdk_lib/w0020_multiprocessing/ title=并行操作>并行操作</a></li><li><a href=/zh-cn/posts/w00035_programming_language/w0035_python/w0050_sdk_lib/w0030_importlib/ title=importlib>importlib</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00035_programming_language/w0040_tf/>TensorFlow</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00035_programming_language/w0040_tf/w0010_compat/>兼容1.x</a><ul><li><a href=/zh-cn/posts/w00035_programming_language/w0040_tf/w0010_compat/w0010_tf_compat_summary/ title=静态图>静态图</a></li><li><a href=/zh-cn/posts/w00035_programming_language/w0040_tf/w0010_compat/w0020_tf_compat_train/ title=模型训练>模型训练</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00035_programming_language/w0050_pytorch/>PyTorch</a><ul><li><a href=/zh-cn/posts/w00035_programming_language/w0050_pytorch/w0010_torch_summary/ title=简介>简介</a></li><li><a href=/zh-cn/posts/w00035_programming_language/w0050_pytorch/w0020_basic/ title=基础操作>基础操作</a></li><li><a href=/zh-cn/posts/w00035_programming_language/w0050_pytorch/w0030_mathematical/ title=数学计算>数学计算</a></li><li><a href=/zh-cn/posts/w00035_programming_language/w0050_pytorch/w0040_tensor/ title=Tensor>Tensor</a></li><li><a href=/zh-cn/posts/w00035_programming_language/w0050_pytorch/w0050_train_model/ title=模型训练>模型训练</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00035_programming_language/w0060_mxnet/>MxNet</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00035_programming_language/w0060_mxnet/w0010_ndarray/>NdArray</a><ul><li><a href=/zh-cn/posts/w00035_programming_language/w0060_mxnet/w0010_ndarray/w0010_ndarray_summary/ title=NdArray使用>NdArray使用</a></li></ul></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00100_cv/>CV</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00100_cv/w0010_backbone/>基础</a><ul><li><a href=/zh-cn/posts/w00100_cv/w0010_backbone/d1_1_backbone_cnn/ title=CNN>CNN</a></li><li><a href=/zh-cn/posts/w00100_cv/w0010_backbone/d1_2_optimizer/ title=optimizer>optimizer</a></li><li><a href=/zh-cn/posts/w00100_cv/w0010_backbone/d1_3_backbone_net/ title="backbone net">backbone net</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00100_cv/w0020_contrastive_learning/>对比学习</a><ul><li><a href=/zh-cn/posts/w00100_cv/w0020_contrastive_learning/contrastive_learning/ title="contrastive learning">contrastive learning</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00100_cv/w0030_vision_transformer/>ViT</a><ul><li><a href=/zh-cn/posts/w00100_cv/w0030_vision_transformer/vision_transformer/ title="vision transformer">vision transformer</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00100_cv/w0050_detect_object/>目标检测</a><ul><li><a href=/zh-cn/posts/w00100_cv/w0050_detect_object/object_detection_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00100_cv/w0055_semantic_segmentation/>语义分割</a><ul><li><a href=/zh-cn/posts/w00100_cv/w0055_semantic_segmentation/object_detection_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00100_cv/w0060_image-matting/>抠图</a><ul><li><a href=/zh-cn/posts/w00100_cv/w0060_image-matting/image-matting-summary/ title=抠图综述>抠图综述</a></li><li><a href=/zh-cn/posts/w00100_cv/w0060_image-matting/image-matting-animal/ title="animal matting">animal matting</a></li></ul></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/posts/w00200_nlp/>NLP</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00200_nlp/w0010_word_embedding/>Word Embedding</a><ul><li><a href=/zh-cn/posts/w00200_nlp/w0010_word_embedding/word_embedding_summary/ title="Word Embedding综述">Word Embedding综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00200_nlp/w0020_rnn/>RNN</a><ul><li><a href=/zh-cn/posts/w00200_nlp/w0020_rnn/rnn_summary/ title=RNN综述>RNN综述</a></li><li><a href=/zh-cn/posts/w00200_nlp/w0020_rnn/gru/ title=GRU网络>GRU网络</a></li><li><a href=/zh-cn/posts/w00200_nlp/w0020_rnn/lstm/ title=LSTM网络>LSTM网络</a></li><li><a href=/zh-cn/posts/w00200_nlp/w0020_rnn/encode_decode/ title=编解码架构>编解码架构</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00200_nlp/w0030_transformer/>Transformer</a><ul><li><a href=/zh-cn/posts/w00200_nlp/w0030_transformer/attention/ title=Attention>Attention</a></li><li><a href=/zh-cn/posts/w00200_nlp/w0030_transformer/transformer_summary/ title=Transformer>Transformer</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00200_nlp/w0080_gpt/>GPT</a><ul><li><a href=/zh-cn/posts/w00200_nlp/w0080_gpt/gpt_summary/ title=GPT综述>GPT综述</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/posts/w00200_nlp/w0100_bert/>Bert</a><ul class=active><li><a href=/zh-cn/posts/w00200_nlp/w0100_bert/bert_summary/ title=Bert综述>Bert综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00200_nlp/w0150_bart/>BART</a><ul><li><a href=/zh-cn/posts/w00200_nlp/w0150_bart/bart_summary/ title=BART综述>BART综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00200_nlp/w0200_electra/>ELECTRA</a><ul><li><a href=/zh-cn/posts/w00200_nlp/w0200_electra/electra_summary/ title=ELECTRA综述>ELECTRA综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00200_nlp/w1000_code/>CODE</a><ul><li><a href=/zh-cn/posts/w00200_nlp/w1000_code/bart_summary/ title=code解析>code解析</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00300_aigc/>AIGC</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00300_aigc/w0010_aigc_summary/>简介</a><ul><li><a href=/zh-cn/posts/w00300_aigc/w0010_aigc_summary/a_aigc_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00300_aigc/w0020_diffusion_model/>Diffusion Model</a><ul><li><a href=/zh-cn/posts/w00300_aigc/w0020_diffusion_model/w10_diffusion_summary_/ title=模型介绍>模型介绍</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00400_vlp/>多模态</a><ul><li><a href=/zh-cn/posts/w00400_vlp/vlp_summary/ title=简介>简介</a></li><li><a href=/zh-cn/posts/w00400_vlp/clip/ title=CLIP>CLIP</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/w00500_video/>视频理解</a><ul><li><a href=/zh-cn/posts/w00500_video/vidio_summary/ title=简介>简介</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class="content container-fluid" id=content><div class="container-fluid post-card-holder" id=post-card-holder><div class=post-card><a href=/zh-cn/posts/w00200_nlp/w0100_bert/bert_summary/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>Bert综述</h5><p class="card-text post-summary">一、背景 在使用预训练模型，处理下游任务时，有两类策略：基于特征(feature-based)、基于微调(fine-tuning)
基于特征：比如：ELMo，在使用时，对每个下游任务，创建一个跟这个任务相关的神经网络；预训练作为额外的特征跟输入一起输入到模型，预训练的额外特征可能会对要训练的模型有指导作用。 基于微调：比如：GPT，预训练模型在下游使用时，不需要改动太多，类似于视觉模型的fine-tuning，预训练完成特征提取，预训练模型后面添加个简单的网络用于实现具体任务。 1、上下文敏感 在自然语言中，有丰富的多义现象，一个词到底是什么意思，需要参考上下文才能判断。流行的上下文敏感表示：
TagLM(language-model-augmented sequence tagger 语言模型增强的序列标记器) CoVe(Context Vectors 上下文向量) ELMo(Embeddings from Language Models 来自语言模型的嵌入) ELMo 将来自预训练LSTM的所有中间层表示组合为输出表示 ELMo的表示，将作为添加特征添加到下游任务的有监督模型中 2、从特定任务到通用任务 ELMo显著改进了自然语言任务，但每个解决方案仍然依赖于一个特定的任务架构。怎么设计一个模型，让各个自然语言任务通用呢？
GPT(Generative Pre Training 生成式预训练)：在Transformer的基础上，为上下文敏感设计了通用的模型。
预训练一个用于表示文本序列的语言模型 当将GPT应用于下游任务时，语言模型的后面接一个线性输出层，以预测任务的标签。GPT的下游任务的监督学习过程，只对预训练Transformer解码器中的所有参数做微调。 GPT只能从左到右 二、BERT BERT的全称是Bidirectional Encoder Representation from Transformers, 即双向Transformer的Encoder。Bert结合了ELMo和GPT的有点，其主要贡献：
双向的重要性 基于微调的掩码语言模型(Masked Language Modeling)：BERT随机遮掩词元，并使用来自双向上下文的词元以自监督的方式预测该遮掩词元。 1、构造输入 token embedding: 格式：&lt;CLS>第一个文本序列&lt;SEP>第二个文本序列&lt;SEP>
segment embedding: 用来区分句子
position embedding: 在bert中 位置嵌入 是可学习的
def get_tokens_and_segments(tokens_a, tokens_b=None): """获取输入序列的词元及其片段索引""" tokens = ['&lt;cls>'] + tokens_a + ['&lt;sep>'] # 0和1分别标记片段A和B segments = [0] * (len(tokens_a) + 2) if tokens_b is not None: tokens += tokens_b + ['&lt;sep>'] segments += [1] * (len(tokens_b) + 1) return tokens, segments 2、MLM 词元维度</p></div><div class=card-footer><span class=float-left>September 8, 2021</span>
<a href=/zh-cn/posts/w00200_nlp/w0100_bert/bert_summary/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div></div><div class=paginator></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>导航</h5><ul><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#about>关于</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#skills>技能</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#experiences>经验</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#projects>项目</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#education>教育</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#recent-posts>最新博客</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#accomplishments>造诣</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#achievements>业绩</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>联系方式:</h5><ul><li><span>电话:</span> <span>+0123456789</span></li><li><span>邮箱:</span> <span>bsgshyn8@163.com</span></li></ul></div></div></div><hr><div class=container><p id=disclaimer><strong>免责声明:</strong> 这个主题是MIT许可的。因此，您可以将其用于非商业、商业或者私人用途。您可以修改或分发主题，而无须经 作者任何许可。但是，主题作者不对主题的任何问题提供任何保证或承担任何责任。</p></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=/js/list.js></script></body></html>