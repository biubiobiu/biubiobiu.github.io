<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>深度学习 on biubiobiu's Blog</title><link>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/</link><description>Recent content in 深度学习 on biubiobiu's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Everything is mine</copyright><lastBuildDate>Sat, 05 Aug 2023 12:30:40 +0800</lastBuildDate><atom:link href="https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/index.xml" rel="self" type="application/rss+xml"/><item><title>初始化</title><link>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0020_deeplearing_init/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0020_deeplearing_init/</guid><description>一、初始化 1、Xavier初始化 在全连接层的Xavier初始化：用 $N(0, 1/m)$ 的随机分布初始化。
2、NTK参数化 除了直接用这种方式初始化外，还可以使用 参数化的方式：用 $N(0, 1)$ 的随机分布来初始化，但需要将输出结果除以 $\sqrt{m}$，即： $$ y_j = b_j + \frac{1}{\sqrt{m}} \sum_i{x_i w_{ij}} $$
这个高斯过程被称为 &amp;ldquo;NTK参数化&amp;rdquo;，可以参考 《Neural Tangent Kernel: Convergence and Generalization in Neural Networks》，《On the infinite width limit of neural networks with a standard parameterization》。利用NTK参数化后，所有参数都可以用方差为1的分布初始化，这意味着每个参数的尺度大致是一个级别，这样的话我们就可以设置较大的学习率，加快收敛。</description></item><item><title>CAM</title><link>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0200_cam/</link><pubDate>Fri, 09 Sep 2022 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0200_cam/</guid><description>一、简介 二、模型 1、gradient-based 1. GAP 《Learning Deep Features for Discriminative Localizatiion》
# 代码非常简单， 提取到特征图和目标类别全连接的权重，直接加权求和，再经过relu操作去除负值，最后归一化获取CAM，具体如下: # 获取全连接层的权重 self._fc_weights = self.model._modules.get(fc_layer).weight.data # 获取目标类别的权重作为特征权重 weights=self._fc_weights[class_idx, :] # 这里self.hook_a为最后一层特征图的输出 batch_cams = (weights.unsqueeze(-1).unsqueeze(-1) * self.hook_a.squeeze(0)).sum(dim=0) # relu操作,去除负值 batch_cams = F.relu(batch_cams, inplace=True) # 归一化操作 batch_cams = self._normalize(batch_cams) 2. Grad-CAM 《Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization》
2、gradient-free</description></item><item><title>神经网络画图篇</title><link>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0100_draw_map_for_dl/</link><pubDate>Thu, 09 Sep 2021 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0100_draw_map_for_dl/</guid><description>一、简介 一图抵万言！本篇介绍神经网络的可视化工具和绘图软件。
二、示意图 1、NN SVG 提供三种典型的神经网络绘图风格，个性化参数多；交互式绘图。 NN-SVG是由麻省理工学院弗兰克尔生物工程实验室开发的。可以绘制的图包括以节点形式展示的FCNN style，这个特别适合传统的全连接神经网络的绘制。
Github
Demo
2、PlotNeuralNet 底层基于latex的宏指令绘制，上层提供基于python的描述框架，绘制脚本简单。可以绘制复杂的网络结构。
PlotNeuralNet 是由萨尔大学计算机科学专业的一个学生开发的，目前主要支持的是卷积神经网络，其中卷积层、池化层、bottleneck、skip-connection、up-conv、Softmax等常规的层在代码中都有定义，但缺少RNN相关的可视化层展示。
Github
三、计算图 1、Netron Netron是一个神经网络可视化包，支持绝大多数神经网络操作。该功能包可以为不同节点显示不同的颜色，卷积层用蓝色显示，池化层和归一化层用绿色显示，数学操作用黑色显示。在使用方面，可以直接访问网页端，上传模型文件，就可以看到网络结构图，并可以进一步利用pip安装并引入到程序中通过浏览器查看模型的变化。
Github
Demo</description></item><item><title>归一化</title><link>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0015_deeplearning_norm/</link><pubDate>Thu, 05 Aug 2021 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0015_deeplearning_norm/</guid><description>一、Normlization介绍 一般而言，样本特征由于来源及度量单位不同，其尺度往往差异很大。如果尺度差异很大，神经网络就比较难训练。为了提高训练效率，对输入特征做归一化，把不同的尺度压缩到一定范围内，尺度统一后，大部分位置的梯度方向近似于最优解搜索方向。这样，在用梯度下降法进行求解时，每一步梯度的方向都基本上指向最小值，训练效率会大大提高。
归一化：泛指把数据特征转换为相同尺度的方法，比如：
把数据特征映射到 [0, 1] 或者 [-1, 1] 区间 映射为服从 N(0, 1) 的标准正态分布 1、逐层归一化 逐层归一化可以有效提高训练效率的原因：
更好的尺度不变形
深度神经网路中，一个神经层的输入是之前神经层的输出。给定一个神经层 $l$，它之前的神经层 $1, 2, &amp;hellip;, l-1$，的参数变化会导致其输入的分布发生很大的变化。当使用随机梯度下降法训练网络时，每次参数更新都会导致该神经层的输入分布发生变化，层数越高，其输入分布会改变得越明显。
为了缓解这个问题，可以对每个神经层的输入进行归一化，使其分布保持稳定。不管底层的参数如何变化，高层的输入相对稳定。另外，尺度不变性，可以使我们更加高效地进行参数初始化以及超参数选择。
更平滑的优化地形
逐层归一化，一方面可以是大部分神经层的输入处于不饱和区域，从而让梯度变大，避免梯度消失问题；另一方面可以使得神经网络的优化地形（Optimization Landscape）更加平滑，并使梯度变得更加稳定，从而允许使用更高的学习率，并加快收敛速度。
1. 批量归一化 批量归一化（Batch Normalization）对神经网络中的任意中间层进行归一化。
$$ a^{(l)} = f(z^{(l)}) = f(Wa^{(l-1)}+b) $$ $f(·)$ 是激活函数，$W$ 和 $b$ 是可学习的参数。
为了提高优化效率，就要使净输入 $z^l$ 的分布一致，比如：都归一化为标准正态分布。虽然归一化操作可以应用在输入 $a^{(l-1)}$ 上，但归一化 $z^l$ 更加有利于优化。因此，在实践中，归一化操作一般应用在仿射变换之后，激活函数之前。
2. 层归一化 层归一化（Layer Normalization）是和批量归一化非常类似的方法，与批量归一化不同的是，层归一化是对一个中间层的所有神经元进行归一化。
二、Norm的位置 在目前大模型中 Normalization 的位置：
pre Norm 的状态： $x_{t+1} = x_t + F_t(Norm(x_t))$</description></item><item><title>深度学习-结构</title><link>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0010_deeplearning_problem/</link><pubDate>Thu, 05 Aug 2021 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0010_deeplearning_problem/</guid><description>一、激活函数 1、Sigmoid函数 logistic函数
$$ \varphi(v) = \frac{1}{1+e^{-av}} $$
tanh函数
$$ \varphi(v) = tanh(v) = \frac{1-e^{-v}}{1+e^{-v}} $$
分段线性函数
$ \varphi(v) = \begin{cases} 1 &amp;amp;\text{if } v \geqslant \theta \\ kv &amp;amp;\text{if } - \theta &amp;lt; v &amp;lt; \theta \\ 0 &amp;amp;\text{if } v \leqslant 0 \end{cases}$
概率型函数
$$ P(1) = \frac{1}{1+e^{-\frac{x}{T}}} $$
2、ReLU函数 relu函数有助于梯度收敛，收敛速度快了6倍。但仍然有缺陷：
在x&amp;lt;0是，梯度为0，一旦变成负将无法影响训练，这种现象叫做死区。如果学习率较大，会发现40%的死区。如果有一个合适的学习率，死区会大大减少。
$ ReLU(x) = max(0, x) = \begin{cases} x &amp;amp;\text{if } x \geqslant 0 \\ 0 &amp;amp;\text{if } x &amp;lt; 0 \end{cases}$</description></item><item><title>深度学习开篇</title><link>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0005_deeplearning_summary/</link><pubDate>Thu, 05 Aug 2021 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0005_deeplearning_summary/</guid><description>论文入口
一、机器学习 目前，人工智能研究领域主要体现在一下几个方面：
智能感知：通过模拟人的感知能力（视觉、听觉、嗅觉）对外部信息进行感知和识别，并能够对信息进行加工和处理，从而做出反应。
智能学习：学习是人工智能的主要标志和获取知识的重要手段，研究机器通过模拟人的学习能力，如何从小样本、大数据中学习，主要有：
监督学习：（Supervised Learning）表示机器学习的数据是带有标记的，这些标记可以包括：数据类别、数据属性、特征点位置等。这些标记作为预期效果，不断修正机器的预测结果。常见的监督学习有分类、回归、结构化学习。 半监督学习：（Semi-Supervised Learning）利用少量标注数据和大量无标注数据进行学习的方式。常用的半监督学习算法有：自训练、协同训练 非监督学习：（Unsupervised Learning）表示机器学习的数据是没有标记的。常见的无监督学习有：聚类、降维 强化学习：（Reinforcement Learning）通过智能体和环境的交互，不断学习并调整策略的机器学习算法。这种算法带有一种激励机制，如果智能体根据环境做出一个正确的动作，则施予一定的“正激励”；如果是错误的动作，则给与一定的“负激励”。通过不断地累加激励，以获取激励最大化的回报。做火热的应用就是 AlphaGo Zero 认知推理：模拟人的认知能力，主要研究知识表示、推理、规划、决策等，主要有自然语言处理、脑科学。
二、表征学习 表征：为了提高机器学习系统的准确率，需要将输入信息转化为有效的特征，或者更一般性地称为 表征（Representation） 表征学习：如果有一种算法可以自动地学习有效的特征，并提高最终机器学习模型的性能，那么这种学习就可以叫做 表征学习。 表征学习的关键是解决语义鸿沟（Semantic Gap）问题。即：输入数据的底层特征与高层语义信息之间的不一致性和差异性。
机器学习中经常用两种方式表示特征：局部表示(Local Representation)、分布式表示(Distributed Representation)。
比如：颜色的表示。
局部表示：也称为离散表示或者符号表示，比如：one-hot向量的形式。假设所有颜色 构成一个词表 $\bold V$，此时，可以用一个 $|\bold V|$ 维的one-hot向量来表示一中颜色。但是，one-hot向量的维数很高，且不能扩展，如果有一种新的颜色，就需要增加一维来表示。不同颜色之间的相似度都为0，无法直到“红色”和“中国红”的相似度要高于“红色”和“黑色”的相似度。 分布式表示：另一种表示颜色的方法是用RGB值来表示颜色，不同颜色对应RGB三维空间中的一个点。分布式表示通常可以表示低维的稠密向量。 嵌入：神经网络将高维的局部表示空间 $\R^{|\bold V|}$，映射到一个非常低维的分布式表示空间 $\R^{D}$。在这个低维空间中，每个特征不再是坐标轴上的点，而是分散在整个低维空间中，在机器学习中，这个过程也成为嵌入（Embedding）。比如：自然语言中词的分布式表示也经常叫做词嵌入。
要学习到一种好的高层次语义表示（一般为分布式表示），通常只有从底层特征开始，经过多步非线性转换才能得到。深层结构的优点是可以提高特征的重用性，从而指数级增强表示能力。因此，表示学习的关键是构建具有一定深度的多层次特征表示。
三、深度学习 深度学习是机器学习的一个重要的、新的研究领域，源于对神经网络的进一步研究，通常采用包含多个隐藏层的神经网络结构，目的是建立、模拟人脑学习过程。
在描述深度学习之前，先回顾下机器学习和深度学习的关系。
机器学习：研究如何使用计算机系统利用经验改善性能。在机器学习的众多研究方向中，表征学习关注如何自动找出表示数据的合适方式，以便更好地将输入变换为正确的输出。
深度学习：是具有多级表示的表征学习方法。在每一级，深度学习通过简单的函数将该级的表示变换为更高级的表示。因此，深度学习模型也可以看作是由许多简单函数复合而成的函数。当这些复合函数足够多时，就可以表达非常复杂的变换。
作为表征学习的一种，深度学习将自动找出每一级表示数据的合适方式。逐级表示越来越抽象的概念或模式。高层特征是由底层特征通过推演归纳得到。
深度学习可通过学习一种深层非线性网络结构来表征输入数据，实现复杂函数逼近，具有很强的从少数样本集中学习数据集本质特征的能力。深度学习的主要思想：通过自学习的方法，学习到训练数据的结构，并在该结构上进行有监督训练微调。 以图像为例，它的输入是一堆原始像素值，模型中逐级表示为： graph LR; A(特定位置和角度的边缘) -- B(由边缘组合得出的花纹) B -- C(由多种花纹进一步汇合得到的特定部位) C -- D(由特定部位组合得到的整个目标) 1、神经元 神经元模型：</description></item></channel></rss>