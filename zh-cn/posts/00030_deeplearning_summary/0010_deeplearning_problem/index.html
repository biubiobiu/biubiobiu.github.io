<!doctype html><html><head><title>深度学习-结构</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0d88336a9cbd3e68c7714efae786b0a9_38044_42x0_resize_box_2.png><meta property="og:title" content="深度学习-结构"><meta property="og:description" content="Markdown rendering samples"><meta property="og:type" content="article"><meta property="og:url" content="https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0010_deeplearning_problem/"><meta property="article:published_time" content="2021-08-05T12:30:40+08:00"><meta property="article:modified_time" content="2021-08-05T12:30:40+08:00"><meta name=description content="Markdown rendering samples"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/zh-cn><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png alt=Logo>
biubiobiu's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu547c3206082786c1d32d36034e2a655a_40863_42x0_resize_box_2.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/zh-cn/search><input type=text name=keyword placeholder=搜索 data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/zh-cn/posts data-filter=all>博文</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00020_toha-tutorial/>Toha教程</a><ul><li><a href=/zh-cn/posts/00020_toha-tutorial/0010_toha-config/ title=Toha的配置>Toha的配置</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0011_write-blogs/ title=撰写文章>撰写文章</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0013_markdown-tutorial/ title=MarkDown入门>MarkDown入门</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0015_latax_formula/ title=Katex公式>Katex公式</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0020_shortcodes_samples/ title=区域块-实例>区域块-实例</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00025_math_knowledge/>数学知识</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/>概率论</a><ul><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0010_basic-conception/ title=基本概念>基本概念</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0020_distribution_of_variables/ title=随机变量及其分布>随机变量及其分布</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0030_law_of_large_numbers/ title=大数定律>大数定律</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0040_sample_distribution/ title=样本及抽样分布>样本及抽样分布</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0050_parameter_estimation/ title=假设检验>假设检验</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0060_analysis_variance_regression/ title=方差分析及回归分析>方差分析及回归分析</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0070_stochastic_process/ title=随机过程>随机过程</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0080_markov_process/ title=马尔科夫链>马尔科夫链</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0090_stationary_stochastic_process/ title=平稳随机过程>平稳随机过程</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00025_math_knowledge/0020_math_convex_optimization_theory/>凸优化</a><ul><li><a href=/zh-cn/posts/00025_math_knowledge/0020_math_convex_optimization_theory/0010_basic_conception/ title=基本概念>基本概念</a></li></ul></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/posts/00030_deeplearning_summary/>深度学习</a><ul class=active><li><a href=/zh-cn/posts/00030_deeplearning_summary/0005_deeplearning_summary/ title=深度学习开篇>深度学习开篇</a></li><li><a class=active href=/zh-cn/posts/00030_deeplearning_summary/0010_deeplearning_problem/ title=深度学习-结构>深度学习-结构</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0015_deeplearning_norm/ title=归一化>归一化</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0020_deeplearing_init/ title=初始化>初始化</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0100_draw_map_for_dl/ title=神经网络画图篇>神经网络画图篇</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0200_cam/ title=CAM>CAM</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00033_reinforce/>强化学习</a><ul><li><a href=/zh-cn/posts/00033_reinforce/0001_reinforce_summary/ title=综述>综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/>编程语言</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/>Python</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/0010_build_env/>env</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0010_build_env/0010_pip_env/ title=py-env>py-env</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0010_build_env/0020_cuda_env/ title=cuda>cuda</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/>Internal</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0010_encode_mode/ title=字符编码>字符编码</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0020_basic_operator/ title=基础操作>基础操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0025_internal_module/ title=内置模块>内置模块</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0030_advance_operator/ title=进阶操作>进阶操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0040_error/ title=异常>异常</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0050_file/ title=文件读取>文件读取</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/>SDK</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0020_multiprocessing/ title=并行操作>并行操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0030_importlib/ title=importlib>importlib</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0040_ipdb/ title=ipdb>ipdb</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0050_re/ title=正则-re>正则-re</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0060_heapd/ title=堆-heapq>堆-heapq</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0070_request/ title=requests>requests</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0080_logging/ title=logging>logging</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0090_argparse/ title=argparse>argparse</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0100_pil/ title=PIL>PIL</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0110_opencv/ title=OpenCV>OpenCV</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0040_tf/>TensorFlow</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0040_tf/w0010_compat/>兼容1.x</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0040_tf/w0010_compat/0010_tf_compat_summary/ title=静态图>静态图</a></li><li><a href=/zh-cn/posts/00035_programming_language/0040_tf/w0010_compat/0020_tf_compat_train/ title=模型训练>模型训练</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/>PyTorch</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0010_torch_summary/ title=简介>简介</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0020_basic/ title=基础操作>基础操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0030_mathematical/ title=数学计算>数学计算</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0040_tensor/ title=Tensor和变量>Tensor和变量</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0050_train_model/ title=模型训练>模型训练</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0060_mxnet/>MxNet</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0060_mxnet/0010_ndarray/>NdArray</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0060_mxnet/0010_ndarray/0010_ndarray_summary/ title=NdArray使用>NdArray使用</a></li></ul></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/>CV</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0010_backbone/>基础</a><ul><li><a href=/zh-cn/posts/00100_cv/0010_backbone/d1_1_backbone_cnn/ title=CNN>CNN</a></li><li><a href=/zh-cn/posts/00100_cv/0010_backbone/d1_2_optimizer/ title=optimizer>optimizer</a></li><li><a href=/zh-cn/posts/00100_cv/0010_backbone/d1_3_backbone_net/ title="backbone net">backbone net</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0020_contrastive_learning/>对比学习</a><ul><li><a href=/zh-cn/posts/00100_cv/0020_contrastive_learning/contrastive_learning/ title="contrastive learning">contrastive learning</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0030_vision_transformer/>ViT</a><ul><li><a href=/zh-cn/posts/00100_cv/0030_vision_transformer/vision_transformer/ title="vision transformer">vision transformer</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0050_detect_object/>目标检测</a><ul><li><a href=/zh-cn/posts/00100_cv/0050_detect_object/object_detection_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0055_semantic_segmentation/>语义分割</a><ul><li><a href=/zh-cn/posts/00100_cv/0055_semantic_segmentation/object_detection_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0060_image-matting/>抠图</a><ul><li><a href=/zh-cn/posts/00100_cv/0060_image-matting/image-matting-summary/ title=抠图综述>抠图综述</a></li><li><a href=/zh-cn/posts/00100_cv/0060_image-matting/image-matting-animal/ title="animal matting">animal matting</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/>NLP</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0010_word_embedding/>Word Embedding</a><ul><li><a href=/zh-cn/posts/00200_nlp/0010_word_embedding/word_embedding_summary/ title="Word Embedding综述">Word Embedding综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0020_rnn/>RNN</a><ul><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/rnn_summary/ title=RNN综述>RNN综述</a></li><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/gru/ title=GRU网络>GRU网络</a></li><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/lstm/ title=LSTM网络>LSTM网络</a></li><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/encode_decode/ title=编解码架构>编解码架构</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0030_transformer/>Transformer</a><ul><li><a href=/zh-cn/posts/00200_nlp/0030_transformer/0001_attention/ title=Attention>Attention</a></li><li><a href=/zh-cn/posts/00200_nlp/0030_transformer/0010_transformer_summary/ title=Transformer>Transformer</a></li><li><a href=/zh-cn/posts/00200_nlp/0030_transformer/0030_position/ title=位置编码>位置编码</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0080_gpt/>GPT</a><ul><li><a href=/zh-cn/posts/00200_nlp/0080_gpt/0010_gpt_summary/ title=GPT综述>GPT综述</a></li><li><a href=/zh-cn/posts/00200_nlp/0080_gpt/0020_gpt1_detail/ title=GPT-1>GPT-1</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0100_bert/>Bert</a><ul><li><a href=/zh-cn/posts/00200_nlp/0100_bert/0001_bert_summary/ title=Bert综述>Bert综述</a></li><li><a href=/zh-cn/posts/00200_nlp/0100_bert/0020_bert_family/ title=Bert家族>Bert家族</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0120_t5/>T5</a><ul><li><a href=/zh-cn/posts/00200_nlp/0120_t5/0010_t5_summary/ title=T5综述>T5综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0150_bart/>BART</a><ul><li><a href=/zh-cn/posts/00200_nlp/0150_bart/bart_summary/ title=BART综述>BART综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0200_electra/>ELECTRA</a><ul><li><a href=/zh-cn/posts/00200_nlp/0200_electra/electra_summary/ title=ELECTRA综述>ELECTRA综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/1000_code/>CODE</a><ul><li><a href=/zh-cn/posts/00200_nlp/1000_code/bart_summary/ title=code解析>code解析</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/>AIGC</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/0005_summary/>AIGC综述</a><ul><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0001_aigc_summary/ title=综述>综述</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0002_aigc_data/ title=LLM-数据集>LLM-数据集</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0005_aigc_application/ title=模型应用策略>模型应用策略</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0010_aigc_train_p/ title=大模型训练框架>大模型训练框架</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0012_aigc_amp/ title=混合精度训练>混合精度训练</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0015_aigc_train_mini/ title=模型小型化>模型小型化</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0020_aigc_error/ title=生成式-问题>生成式-问题</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/0010_generate_text/>文本生成</a><ul><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0011_gpt/ title=GPT>GPT</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0015_llama/ title=LLaMa>LLaMa</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0020_palm/ title=PaLM>PaLM</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0025_chatglm/ title=ChatGLM>ChatGLM</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0030_claude/ title=Claude>Claude</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0035_cohere/ title=Cohere>Cohere</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0040_falcon/ title=Falcon>Falcon</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0045_vicuna/ title=Vicuna>Vicuna</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/0020_generate_image/>图像生成</a><ul><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1000_summary/ title=综述>综述</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1002_gan_summary/ title=GAN>GAN</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1005_can_summary/ title=CAN>CAN</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1010_dall_e/ title=DALL-E>DALL-E</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1015_vqgan/ title=VQGAN>VQGAN</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1020_diffusion/ title=Diffusion>Diffusion</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1025_midjourney/ title=Midjourney>Midjourney</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1030_imagen/ title=Imagen>Imagen</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00400_vlp/>多模态</a><ul><li><a href=/zh-cn/posts/00400_vlp/0001_vlp_summary/ title=简介>简介</a></li><li><a href=/zh-cn/posts/00400_vlp/0005_clip/ title=CLIP>CLIP</a></li><li><a href=/zh-cn/posts/00400_vlp/0010_mllm/ title=MLLM>MLLM</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00500_video/>视频理解</a><ul><li><a href=/zh-cn/posts/00500_video/vidio_summary/ title=简介>简介</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/john_hu7f9991f5b5d471ecdfc6cff3db1a9fe6_6397_120x120_fit_box_2.png alt="Author Image"><h5 class=author-name>biubiobiu</h5><p>August 5, 2021</p></div><div class=title><h1>深度学习-结构</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/zh-cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0 class="btn, btn-sm">机器学习</a></li><li class=rounded><a href=/zh-cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0 class="btn, btn-sm">深度学习</a></li><li class=rounded><a href=/zh-cn/tags/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84 class="btn, btn-sm">网络结构</a></li></ul></div><div class=post-content id=post-content><h2 id=一激活函数>一、激活函数</h2><blockquote><p>为什么需要激活函数？<br>例如：两个感知机。 $h_1 = W_1 x + b_1, h_2 = W_2 h_1 + b_2$<br>如果没有激活函数这个 非线性变换，由于感知机的计算时线性变换，可以转换为：$h_2 = W_2 W_1 x + W_2 b_1 + b_2$<br>就是说：<font color=#f00000>如果没有激活函数，模型就做不了太深。两层的权重完全可以用一层的权重来表示。</font></p></blockquote><h3 id=1sigmoid函数>1、Sigmoid函数</h3><p><strong>logistic函数</strong><br>$$
\varphi(v) = \frac{1}{1+e^{-av}}
$$</p><p><strong>tanh函数</strong><br>$$
\varphi(v) = tanh(v) = \frac{1-e^{-v}}{1+e^{-v}}
$$</p><p><strong>分段线性函数</strong><br>$ \varphi(v) = \begin{cases} 1 &\text{if } v \geqslant \theta \\ kv &\text{if } - \theta &lt; v &lt; \theta \\ 0 &\text{if } v \leqslant 0 \end{cases}$</p><p><strong>概率型函数</strong><br>$$
P(1) = \frac{1}{1+e^{-\frac{x}{T}}}
$$</p><h3 id=2relu函数>2、ReLU函数</h3><p>relu函数有助于梯度收敛，收敛速度快了6倍。但仍然有缺陷：<br>在x&lt;0是，梯度为0，一旦变成负将无法影响训练，这种现象叫做死区。如果学习率较大，会发现40%的死区。如果有一个合适的学习率，死区会大大减少。<br></p><p>$ ReLU(x) = max(0, x) = \begin{cases} x &\text{if } x \geqslant 0 \\ 0 &\text{if } x &lt; 0 \end{cases}$</p><p><strong>带滞漏的ReLU</strong><br>$ LeakyReLU(x) = \begin{cases} x &\text{if } x \geqslant 0 \\ \gamma x &\text{if } x &lt; 0 \end{cases}$</p><p>缓解了死区，不过 $\gamma$ 是个超参，人为设定的不准，调参影响较大。<br>$\gamma = 0.01$ ，当神经元处于非激活状态时，也能有一个非零的梯度可以更新参数，避免永远不能被激活。</p><p><strong>带参数的ReLU</strong><br>$ PReLU(x) = \begin{cases} x &\text{if } x \geqslant 0 \\ \gamma_i x &\text{if } x &lt; 0 \end{cases}$</p><p>引入一个可学习的参数 $\gamma_i$ ，不同神经元可以有不同的参数。</p><p><strong>ELU函数</strong>：Exponential Linear Unit 指数线性单元<br></p><p>在小于0的部分使用指数，具备relu的优点，同时ELU也解决了relu函数自身死区的问题。不过ELU函数指数操作稍稍增大了工作量<br></p><p>优点：</p><ol><li>在所有点上都是连续的可微的</li><li>与ReLU不同，它没有死区问题</li><li>与ReLU相比，实现了更高的准确性</li></ol><p>缺点：</p><ol><li>计算速度慢，由于负输入设计非线性</li></ol><p>$ ELU(x) = \begin{cases} x &\text{if } x \geqslant 0 \\ \gamma(e^x-1) &\text{if } x &lt; 0 \end{cases}$</p><p align=center><img src=/datasets/posts/nlp/elu_0.png width=50% height=50%></p><p><strong>Softplus函数</strong><br>$$
Softplus(x) = log(1+e^x)
$$
Softplus函数可以看做ReLU函数的平滑版本，其导数刚好是logistic函数。Softplus函数虽然也具有单侧抑制、宽兴奋边界的特性，但没有稀疏激活性。</p><p><strong>swish</strong><br>该函数是google大脑提出的一个新的激活函数，从图像上来看，与relu差不多，唯一区别较大的是接近0的负半轴区域。
$$
swish(x) = \frac{x}{1+e^{-x}}
$$</p><p align=center><img src=/datasets/posts/nlp/swish_0.png width=50% height=50%></p><p><strong>GELU</strong><br>高斯误差线性单元（Gaussian Error Linear Unit）激活函数，公式如下：
$$
GELU(x) = 0.5 x (1+tanh(\sqrt{\frac{2}{\pi}} (x + 0.044715 x^3)))
$$</p><p align=center><img src=/datasets/posts/nlp/gelu_0.png width=50% height=50%></p>在大于0时，输出基本上是x。GELU激活函数的微分：<p align=center><img src=/datasets/posts/nlp/gelu_1.png width=50% height=50%></p><p>优点：</p><ol><li>平滑性：所有点上都可导，没有梯度截断问题。这使得梯度优化更加稳定，有助于提高神经网路的训练效率</li><li>近似性：对于较大的输入，GELU 接近线性函数 x，这使得它在这些情况下起到线性激活函数的作用</li><li>高斯分布：GELU 在0附近，近似微高斯分布，这有助于提高网络的泛化能力，使得模型更容易适应不同的数据分布。</li></ol><p>缺点：</p><ol><li>计算量大</li></ol><h3 id=3glu函数>3、GLU函数</h3><p><a href=https://arxiv.org/pdf/1612.08083.pdf target=bland>GLU（Gated Linear Unit）</a>激活函数是2017年提出的，其实不算是一种激活函数，而是一种神经网络层。它是一个线性变换后面接门控机制的结构。其中门控机制是一个sigmoid函数用来控制信息能够通过多少。<br>它的门控机制，可以帮助网络更好地捕捉序列数据中的长期依赖关系。GLU激活函数最初在自然语言处理（NLP）任务中提出，并在机器翻译、语音识别等领域取得了良好的效果。<br></p><p>$$
GLU(x) = (Vx+c) \otimes \sigma{(Wx+b)}
$$
其中，$x$：表示输入向量；$\otimes$：表示逐元素相乘；$\sigma()$：表示Sigmoid函数；$W, V, b, c$：是需要学习的参数。<br></p><p>理解GLU激活函数的关键在于它的门控机制。门控机制使得GLU能够选择性地过滤输入向量的某些部分，并根据输入的上下文来调整输出。门控部分的作用是将输入进行二分类，决定哪些部分应该被保留，哪些部分应该被抑制。例如，在语言模型中，GLU激活函数可以帮助网络根据上下文选择性地关注某些单词或短语，从而更好地理解句子的语义。门控机制可以有效地减少噪声和不相关信息的影响，提高网络的表达能力和泛化能力。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>GluLayer</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#66d9ef>def</span> __init__(self, input_size, output_size):
        super()<span style=color:#f92672>.</span>__init__()
        <span style=color:#75715e># 第一个线性层</span>
        self<span style=color:#f92672>.</span>fc1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(input_size, output_size)
        <span style=color:#75715e># 第二个线性层</span>
        self<span style=color:#f92672>.</span>fc2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(input_size, output_size)
        <span style=color:#75715e># pytorch的GLU层</span>
        self<span style=color:#f92672>.</span>glu <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>GLU()
    
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
        <span style=color:#75715e># 先计算第一个线性层结果</span>
        a <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>fc1(x)
        <span style=color:#75715e># 再计算第二个线性层结果</span>
        b <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>fc2(x)
        <span style=color:#75715e># 拼接a和b，水平扩展的方式拼接</span>
        <span style=color:#75715e># 然后把拼接的结果传给glu</span>
        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>glu(torch<span style=color:#f92672>.</span>cat((a, b), dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)) 
</code></pre></div><h3 id=1glu的变体>1、GLU的变体</h3><p>可以把 $\sigma()$ 换成其他的激活函数，例如：Relu、swish、tanh、GELU等等。<br></p><p>替换为tanh：$ GTU(x, W, V, b, c) = tanh(xW + b) \otimes \sigma{(xV+c)}$<br>替换为ReLU：$ReGLU(x, W, V, b, c) = ReLU(xW + b) \otimes \sigma{(xV+c)}$<br>替换为Swish：$SwiGLU(x, W, V, b, c) = Swish(xW + b) \otimes \sigma{(xV+c)}$<br>替换为GELU：$GEGLU(x, W, V, b, c) = GELU(xW + b) \otimes \sigma{(xV+c)}$<br>替换为Bilinear：$Bilinear(x, W, V, b, c) = (xW + b) \otimes \sigma{(xV+c)}$<br></p><h2 id=二损失函数>二、损失函数</h2><p>损失函数（Loss Function），是用来评价模型的预测值和真实值不一样的程度。常见的损失函数有：</p><ol><li>均方差损失函数
$$
MSE = \frac{1}{2N}\sum^{N}_{i=1}(t_i-y_i)^2
$$</li><li>平均绝对误差损失函数
$$
MAE = \frac{1}{N}\sum^{N}_{i=1}|t_i-y_i|
$$</li><li>交叉熵损失函数（Cross Entropy Loss Function）<br>二分类任务：
$$
E = - \frac{1}{N}\sum^N_{i=1}(t_i \log y_i + (1-t_i) \log (1-y_i))
$$
$N$：表示样本数量；$y_i$：表示样本 $i$ 预测为正类的概率；$t_i$：表示样本 $i$ 的目标值。<br>在多分类任务时，可以看成二分类的扩展：
$$
E = - \frac{1}{N}\sum_i^N\sum^{M}_{c=1}(t_i \log y_i)
$$
$N$：表示样本数量；$M$：表示类别的数量；$y_i$：表示样本 $i$ 属于类别 $c$ 的概率；$t_i$：表示样本 $i$ 的目标值(只有目标值为1，其余都是0)。<br>**熵**：表征的是期望的稳定性，其值越小越稳定；熵越大，表示该事件发生的可能性越小，风险度会越大。<br>**交叉熵**：主要应用于度量两个概率分布之间的差异性。<ul><li>当两个分布完全相同时，交叉熵的取值最小</li><li>交叉熵的值是非负的，并且预测值于目标值越接近，交叉熵的值就越小</li><li>对于损失函数而言，如果模型输出预测值于目标值相同，那么损失函数就越小。</li></ul></li></ol><h2 id=三学习规则>三、学习规则</h2><p>神经网络的学习规则：希望模型的损失函数最小，即：风险最小化准则。<br></p><p>对于训练的评价准确：有一个较小的期望风险。但是由于不知道真实的数据分布和映射函数，所以实际上无法计算模型的期望风险：$R(\bold \theta)$。给定一个训练集 $\bold D$，可以计算的是 <font color=#a00000>经验风险（Empirical Risk）</font> ，即：在训练集上的平均损失。根据<font color=#a00000>大数定理</font> 当训练集大小趋于无穷大时，经验风险就趋于期望风险。然而，在通常情况下，我们无法获取无限的训练样本，并且训练样本往往是真实数据的一个很小的子集（或许包含了一定的噪声数据）。因此，最优的学习规则：就是能够找到一组参数 $\hat{\bold \theta}$，使得经验风险最小，这就是经验风险最小化准则(Empirical Risk Minimization)。</p><h3 id=1极大似然估计>1、极大似然估计</h3><p>估计类条件概率（似然）的一种常用策略是：先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。<br>似然函数（Likelihood Function）：是统计模型中参数的函数。当给定 联合样本值 $\bold x$ 时，关于参数 $\bold \theta$ 的似然函数 $L(\bold \theta | \bold x)$，在数值上等于给定参数 $\bold \theta$ 后变量 $\bold x$ 的概率。
$$
L(\bold \theta | \bold x) = P(\bold X = \bold x|\bold \theta)
$$
实际上，概率模型的训练过程就是参数估计过程。对于参数估计，统计学界的两个学派分别提供了不同的解决方案：</p><ol><li>频率主义学派（Frequentist）：认为参数虽然未知，但客观存在固定值。因此，可以通过优化似然函数等准则来确定参数值。</li><li>贝叶斯学派（Batesian）：认为参数是为观测到的随机变量，其本身也可有分布，因此，可先假定参数服从一个先验分布，然后基于观测到的数据计算参数的后验分布。</li></ol><h3 id=2欠过拟合>2、欠/过拟合</h3><h4 id=1-误差>1. 误差</h4><p>训练误差(training error): 训练模型在训练数据集(training set)上表现出的误差。</p>泛化误差(generalization error)：模型在任意一个测试数据集(test set)上表现出的误差的期望。</p>训练集(training set)：用来产出模型参数。</p>验证集(validation set)：由于无法从训练误差评估泛化误差，因此从训练集中预留一部分数据作为验证集，主要用来选择模型。</p>测试集(test set)：在模型参数选定后，实际使用。</p></p><h4 id=2-欠过拟合>2. 欠/过拟合</h4><p>欠拟合(underfitting)：<code>模型的表现能力不足。</code></p></p><ol><li>训练样本足够，模型参数不足</li></ol><p>过拟合(overfitting)：<code>模型的表现能力过剩。</code></p></p><ol><li>训练样本不足，模型参数足够：样本不足导致特征较少，相当于模型足够表征数据的特征，产生过拟合现象。</li></ol><h4 id=3-优化过拟合>3. 优化过拟合</h4><p>增大训练集可能会减轻过拟合，但是获取训练数据往往代价很高。可以在模型方面优化一下，减轻过拟合现象。</p><ol><li><p>权重衰减(weight decay)：
对模型参数计算 $L_2$ 范数正则化。即：在原Loss中添加对模型参数的惩罚。使得模型学到的权重参数较接近于0。<code>权重衰减</code>通过惩罚绝对值较大的模型参数，为需要学习的模型增加了限制。这可能对过拟合有效。</p></li><li><p>丢弃法(dropout)：针对隐藏层中的各个神经元，以概率<em>p</em>随机丢弃，有可能该成神经元被全部清零。这样，下一层的计算无法过渡依赖该层的任意一个神经元，从而在训练中可以用来对付过拟合。在测试中，就不需要丢弃了。</p>例如：对隐藏层使用丢弃法，丢弃概率: <em>p</em>，那么h<sub>i</sub> 有<em>p</em>的概率被清零；不丢弃概率: 1-<em>p</em>，为了保证隐藏层的期望值不变<em>E(p')=E(p)</em>，需要对不丢弃的神经元做拉伸，即：$$h'_i = \frac{\xi_i} {1-p} h_i$$ 其中：随机变量<em>ξ<sub>i<sub></em> 为0和1的概率分别为<em>p</em>和1-<em>p</em>
<img src=/datasets/posts/dp_summary/mlp_dropout.jpg alt=DropOut></p></li><li><p>权重衰减（Weight Decay）：在每次参数更新时，引入一个衰减系数：
$$ \theta_t \gets (1-\beta)\theta_{t-1}-\eta g_t $$
其中 $\beta$ 为权重衰减系数，一般取值比较小，比如：0.0005。在标准的随机梯度下降中，权重衰减正则化和 $L_2$ 正则化的效果相同；但是在较为复杂的优化方法(比如：Adam)中，权重衰减正则化和 $L_2$ 正则化并不等价。</p></li><li><p>数据增强（Data Augmentation）：可以减轻网络的过拟合现象。通过对训练数据进行交换可以得到泛化能力更强的网络。</p></li></ol><h2 id=四优化方法>四、优化方法</h2><p>在深度学习中，通过最小化损失函数使得训练误差最小化，由于损失函数一般都会比较复杂，很难直接求解析解，而是需要基于数值方法的优化算法找到近似解，即：数值解。在局域数值方法的优化算法中，损失函数就是目标函数(Objective Function)，</p><h3 id=1-梯度下降法>1. 梯度下降法</h3><p>梯度下降(gradient descent)的工作原理，以一维为例：
假设连续可导的函数 $f:\Reals \to \Reals$ 的输入和输出都是标量，给定绝对值足够小的数 $\epsilon$ ，根据泰勒展开式，近似：
$$
f(x+\epsilon) \approx f(x) + \epsilon f'(x)
$$
其中 $f'(x)$ 表示函数在x处的梯度。找到一个常数 $\eta > 0$，使得 $\lvert \eta f'(x) \rvert$ 足够小，那么可以将 $\epsilon$ 提换为 $-\eta f'(x)$，得到：
$$
f(x-\eta f'(x)) \approx f(x) - \eta f'(x)^{2}
$$
所以
$$
f(x-\eta f'(x)) \lesssim f(x)
$$
这就意味着，可以通过 $x \gets x-\eta f'(x)$ 来迭代x，函数 $f(x)$ 的值可能会降低。在梯度下降中，先取一个初始值 $x_0$ 和学习率 $\eta>0$，然后不断通过上式迭代x，直到停止条件。学习率 $\eta$ 是一个超参数，需要人工设定，如果学习率过小：会导致x更新缓慢从而需要更多的迭代次数；如果学习率过大，泰勒展开式不再成立，可能会出现振荡，无法保证会迭代出近似最优解。</p><p>在每次迭代中，由于训练集较大，不可能把所有样本都加载到内存中，通常是随机均匀采样多个样本组成一个小批量，然后使用这个小批量来计算梯度，完成一次迭代，即：小批量随机梯度下降(batch gradient descent)。<br>设：目标函数 $f(x): \Reals^{d} \to \Reals$<br>　　小批量数据集 $\text{\ss}$<br>梯度计算：
$$
g_t \gets \nabla f_{\text{\ss}_{t}}
$$</p><p>$$
= \frac {1} {\lvert \text{\ss} \rvert} \sum_{i \in \text{\ss}_t} \nabla f_i(x_t)
$$</p><p>$$
x_t \gets x_{t-1} - \eta_t g_t
$$</p><p>其中，$ \lvert \text{\ss} \rvert $ 表示批量大小，$\eta_t$ 表示学习率，这两个都是超参数。</p><h3 id=2-动量法>2. 动量法</h3><p><strong>问题</strong>：自变量的梯度代表了目标函数在当前位置下降最快的方向，沿着该方向更新自变量，可能还是会有一些问题。例如：类似峡谷的函数，在有些方向上的梯度比较缓慢，在有些方向上梯度比较陡峭，在相同的学习率下，容易导致在梯度缓慢的方向收敛太慢；如果调大学习率，容易导致在梯度陡峭的方向上振荡。如下图，梯度在水平方向上为正，而在竖直方向上时上时下：</p><p align=center><img src=/datasets/posts/cnn/sgd_failed.png width=30% height=30% alt></p><p><strong>动量法</strong>：动量法在迭代自变量时，不仅仅是利用当前的梯度值，而是利用过去一段时间的梯度值的平均。新的梯度更迭方向，不再是指下降最陡峭的方向，而是指向过去梯度的加权平均值的方向，越靠近当前时刻权重越重。</p><p>$$
\upsilon_t \gets \gamma \upsilon_{t-1} + \eta_t g_t
$$</p><p>$$
x_t \gets x_{t-1} - \upsilon_t
$$</p><p>其中，$0 \leqslant \gamma &lt; 1$，当$\gamma = 0$时，动量法等价于小批量随机梯度下降法。</p><p><strong>证明</strong>：<br>我们先解释<strong>指数加权移动平均</strong>(exponentially weighted moving average)，然后在类比到动量法。<br>$$
y_t = \gamma y_{t-1} + (1-\gamma)x_t
$$
其中，$0 \leqslant \gamma &lt; 1$，在当前时间步$t$的变量$y_t$可以展开（类似信号系统中的激励与响应）：</p><p>$$
\begin{array}{cc}
y_t & = (1-\gamma)x_t + \gamma y_{t-1} \\ & = (1-\gamma)x_t + (1-\gamma)\gamma x_{t-1} + \gamma^2 y_{t-2} \\ & = (1-\gamma)x_t + (1-\gamma)\gamma x_{t-1} + \dots + (1-\gamma)\gamma^{t-1} x_{1} + \gamma^t y_{0}
\end{array}
$$</p><p>令$n=\frac {1} {1-\gamma}$，那么$(1-\frac {1} {n})^n = \gamma^{\frac {1} {1-\gamma}}$。<br>有极限：$\lim\limits_{n \to \infty} (1-\frac {1} {n})^n =\lim\limits_{\gamma \to 1} \gamma^{\frac {1} {1-\gamma}}= exp(-1) \approx 0.3679$</p><p>对于$y_t$，可以看做是对最近$\frac {1} {1-\gamma}$个时间步的加权平均；忽略含有$\gamma^{\frac {1} {1-\gamma}}$和比$\gamma^{\frac {1} {1-\gamma}}$更高阶系数的项，即：当$\gamma=0.95$时，可以看成对最近20时间步的$x_i$值的加权平均<br>$$
y_t \approx 0.05\displaystyle\sum_{i=0}^{19} 0.95^i x_{t-i}
$$</p><p><strong>类比向量法</strong></p><p>$$
\upsilon_t \gets \gamma \upsilon_{t-1} + (1-\gamma)\frac {\eta_t} {1-\gamma} g_t
$$</p><p>$$
x_t \gets x_{t-1} - \upsilon_t
$$</p><blockquote><p>所以：向量$\upsilon_t$实际上是对序列$\frac {\eta_{t-i}} {1-\gamma} g_{t-i}$做指数加权移动平均；也就是说：动量法在每个时间步的自变量更新量近似于将最近的$\frac {1} {1-\gamma}$个时间步的更新量做指数加权移动平均。<mark>动量法中，自变量在各个方向上的移动幅度，不仅取决于当前梯度，还取决于历史各个梯度在各个方向上是否一致。</mark>如果在某个方向上时正时负，说明在该方向上有振荡，通过动量的向量相加，对于该情况会降低每次的更新量，使得梯度在该方向上不发散。</p></blockquote><h3 id=3-adagrad算法>3. AdaGrad算法</h3><p><strong>问题</strong>：在统一学习率的情况下，梯度值较大的维度可能会振荡，梯度值较小的维度收敛可能会过慢。</p><p><strong>AdaGrad算法</strong>：根据自变量在每个维度的梯度值大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题。</p><p>$$
s_t \gets s_{t-1} + g_t \odot g_t
$$</p><p>$$
x_t \gets x_{t-1} - \frac {\eta} {\sqrt{s_t + \epsilon}} \odot g_t
$$</p><p>其中，$\odot$表示按元素相乘，$\eta$表示学习率。目标函数自变量中每个元素的学习率通过按元素运算重新调整一下，每个元素都分别拥有自己的学习率。由于$s_t$一直在累加，所以每个元素的学习率在迭代过程中一直在降低，当学习率在迭代早期降得比较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。</p><h3 id=4-rmsprop算法>4. RMSProp算法</h3><p><strong>问题</strong>：AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。为了解决这一问题，RMSProp算法对AdaGrad算法做了一些修改。</p><p><strong>RMSProp算法</strong>：只是在AdaGrad算法中添加了 指数加权移动平均。</p><p>$$
s_t \gets \gamma s_{t-1} + (1-\gamma)g_t \odot g_t
$$</p><p>$$
x_t \gets x_{t-1} - \frac {\eta} {\sqrt{s_t + \epsilon}} \odot g_t
$$</p><p>其中，$\eta$是学习率，RMSProp算法的状态变量是对平方项$g_t \odot g_t$的指数加权移动平均，所以可以看作最近$\frac {1} {1-\gamma}$个时间步的加权平均。如此一来，自变量每个元素的学习率在迭代过程中就不再一直降低。</p><h3 id=5-adadelta算法>5. AdaDelta算法</h3><p><strong>AdaDelta算法</strong>：是另一个针对AdaGrad算法优化的算法，不过没有学习率这个超参数。</p><p>$$
s_t \gets \gamma s_{t-1} + (1-\gamma)g_t \odot g_t
$$</p><p>$$
g_t' \gets \sqrt{\frac {\Delta x_{t-1} + \epsilon} {s_t + \epsilon}} \odot g_t
$$</p><p>$$
\Delta x_t \gets \gamma \Delta x_{t-1} + (1-\gamma)g_t' \odot g_t'
$$</p><p>RMSProp算法，还维护一个额外的状态变量$\Delta x_t$，用来记录自变量变化量$g_t'$按元素平方的指数加权移动平均。</p><p>$$
x_t \gets x_{t-1} - g_t'
$$</p><h3 id=6-adam算法>6. Adam算法</h3><p><strong>Adam算法</strong>：结合了动量变量$\upsilon_t$ 和 RMSProp算法的梯度按元素平方和的指数加权移动平均。</p><p>$$
\upsilon_t \gets \beta_1 \upsilon_{t-1} + (1-\beta_1) g_t
$$</p><p>其中，$0 \leqslant \beta_1 &lt; 1$（建议0.9），$\upsilon_0$初始化为0，则：$\upsilon_t = (1-\beta_1)\displaystyle\sum_{i=1}^t \beta_1^{t-i} g_i$<br>将过去各时间步小批量随机梯度的<mark>权值</mark>相加：$(1-\beta_1)\displaystyle\sum_{i=1}^t \beta_1^{t-i}=1-\beta_1^t$，当t较小时，过去各时间步梯度权值之和会较小，为了消除这样的影响，对任意时间步t，可以将向量$\upsilon_t$再除以$1-\beta_1^t$ ：</p><p>$$
\hat{\upsilon}_t \gets \frac {\upsilon_t} {1-\beta_1^t}
$$</p><p>$$
s_t \gets \beta_2 s_{t-1} + (1-\beta_2)g_t \odot g_t
$$</p><p>$$
\hat{s}_t \gets \frac {s_t} {1-\beta_2^t}
$$</p><p>其中，$0 \leqslant \beta_2 &lt; 1$（建议0.999），$s_0$初始化为0</p><p>Adam算法使用修正后的变量$\hat{\upsilon}_t, \hat{s}_t$，将模型参数中每个元素的学习率通过按元素运算重新调整。</p><p>$$
g_t' \gets \frac {\eta \hat{\upsilon}_t} {\sqrt{\hat{s}_t + \epsilon}}
$$
其中，$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，例如$10^{-8}$。<mark>分子：是动量，可以在方向上消除发散；分母：在幅度上修改每个元素的学习率。</mark></p><p>$$
x_t \gets x_{t-1} - g_t'
$$</p><h2 id=五模型评估>五、模型评估</h2><p>在深度学习中，用来衡量模型的好坏标准有很多，比如：混淆矩阵、准确率、精确率、召回率、F值等</p><h3 id=1混淆矩阵>1、混淆矩阵</h3><table><thead><tr><th style=text-align:left></th><th style=text-align:left>预测类别</th></tr></thead><tbody><tr><td style=text-align:left>真实类别</td><td></td></tr></tbody></table><table><thead><tr><th style=text-align:left></th><th style=text-align:left>P</th><th style=text-align:left>N</th></tr></thead><tbody><tr><td style=text-align:left>T</td><td style=text-align:left>TP</td><td style=text-align:left>TN</td></tr><tr><td style=text-align:left>F</td><td style=text-align:left>FP</td><td style=text-align:left>FN</td></tr></tbody></table><h3 id=2准确率精确率召回率>2、准确率、精确率、召回率</h3><ol><li><p>准确率（Accuracy）：正确预测的各个类别的数量 / 总数
$$
A_c = \frac {TP + FN} {TP + TN + FP + FN}
$$</p></li><li><p>精确率（Precision）：单个类别的正确预测数量 / 所有预测为这类的数量
$$
P_c = \frac {TP} {TP + FP}
$$</p></li><li><p>召回率（Recall）：单个类别的正确预测数量 / 所有这类真实值的数量
$$
R_c = \frac {TP} {TP + TN}
$$</p></li><li><p>F值（F Measure）：是一个综合指标，为精确率和召回率的调和平均
$$
F = \frac {(1+\beta^2)P_c R_c} {\beta^2 P_c + R_c}
$$
$\beta$：用于平衡精确率和召回率的重要性，一般取值为 1。$\beta = 1$时，$F$ 值称为 $F_1$ 值，是精确率和召回率的调和平均，表示：<font color=#a00000>精确率和召回率同等重要</font>。$\beta &lt; 1$ 表示：精确率更重要一些; $\beta > 1$ 表示：召回率更重要一些。</p></li></ol><h3 id=3rocaucpr曲线>3、ROC/AUC/PR曲线</h3><ol><li><p>ROC曲线（Receiver Operating Characteristic）称为接受者操作特征曲线。</p><ul><li>以 “真正例”（TPR）为y轴；以“假正例”（FPR）为x轴</li><li>(0, 1): 表示所有的样本都正确分类</li><li>(1, 0): 表示避开了所有正确的答案</li><li>(0, 0): 表示分类器把每个样本都预测为负例</li><li>(1, 1): 表示分类器把所有样本都预测为正例</li><li>ROC曲线越靠近左上角，模型的准确性就越高，一般ROC曲线是光滑的，那么基本上可以判断模型没有太大的过拟合。</li></ul></li><li><p>AUC曲线（Area Under Curve）的值为ROC曲线下面的面积，AUC的值越大，表示模型的性能越好。若分类器的性能极好，则AUC=1。但是在现实中，没有如此完美的模型，一般 $AUC \in (0.5, 1)$</p><ul><li>AUC=1：完美预测</li><li>$0.5 &lt; AUC &lt; 1$：优于随机猜测</li><li>$AUC = 0.5$：与随机猜测一样</li><li>$AUC &lt; 0.5$：比随机猜测还差</li></ul></li><li><p>PR曲线（Precision Recall）：表示精确率和召回率的曲线</p><ul><li>以精确率为y轴；以召回率为x轴。</li></ul></li></ol></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"><strong>分享:</strong>
<a class="btn btn-sm facebook-btn" href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00030_deeplearning_summary%2f0010_deeplearning_problem%2f" target=_blank><i class="fab fa-facebook"></i></a><a class="btn btn-sm twitter-btn" href="https://twitter.com/share?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00030_deeplearning_summary%2f0010_deeplearning_problem%2f&text=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0-%e7%bb%93%e6%9e%84&via=biubiobiu%27s%20Blog" target=_blank><i class="fab fa-twitter"></i></a><a class="btn btn-sm reddit-btn" href="https://reddit.com/submit?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00030_deeplearning_summary%2f0010_deeplearning_problem%2f&title=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0-%e7%bb%93%e6%9e%84" target=_blank><i class="fab fa-reddit"></i></a><a class="btn btn-sm linkedin-btn" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00030_deeplearning_summary%2f0010_deeplearning_problem%2f&title=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0-%e7%bb%93%e6%9e%84" target=_blank><i class="fab fa-linkedin"></i></a><a class="btn btn-sm whatsapp-btn" href="https://api.whatsapp.com/send?text=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0-%e7%bb%93%e6%9e%84 https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00030_deeplearning_summary%2f0010_deeplearning_problem%2f" target=_blank><i class="fab fa-whatsapp"></i></a><a class="btn btn-sm email-btn" href="mailto:?subject=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0-%e7%bb%93%e6%9e%84&body=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00030_deeplearning_summary%2f0010_deeplearning_problem%2f" target=_blank><i class="fas fa-envelope-open-text"></i></a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/zh-cn/posts/00030_deeplearning_summary/0005_deeplearning_summary/ title=深度学习开篇 class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i>上一篇</div><div class=next-prev-text>深度学习开篇</div></a></div><div class="col-md-6 next-article"><a href=/zh-cn/posts/00030_deeplearning_summary/0015_deeplearning_norm/ title=归一化 class="btn btn-outline-info"><div>下一篇 <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>归一化</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">目录</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#一激活函数>一、激活函数</a><ul><li><a href=#1sigmoid函数>1、Sigmoid函数</a></li><li><a href=#2relu函数>2、ReLU函数</a></li><li><a href=#3glu函数>3、GLU函数</a></li><li><a href=#1glu的变体>1、GLU的变体</a></li></ul></li><li><a href=#二损失函数>二、损失函数</a></li><li><a href=#三学习规则>三、学习规则</a><ul><li><a href=#1极大似然估计>1、极大似然估计</a></li><li><a href=#2欠过拟合>2、欠/过拟合</a><ul><li><a href=#1-误差>1. 误差</a></li><li><a href=#2-欠过拟合>2. 欠/过拟合</a></li><li><a href=#3-优化过拟合>3. 优化过拟合</a></li></ul></li></ul></li><li><a href=#四优化方法>四、优化方法</a><ul><li><a href=#1-梯度下降法>1. 梯度下降法</a></li><li><a href=#2-动量法>2. 动量法</a></li><li><a href=#3-adagrad算法>3. AdaGrad算法</a></li><li><a href=#4-rmsprop算法>4. RMSProp算法</a></li><li><a href=#5-adadelta算法>5. AdaDelta算法</a></li><li><a href=#6-adam算法>6. Adam算法</a></li></ul></li><li><a href=#五模型评估>五、模型评估</a><ul><li><a href=#1混淆矩阵>1、混淆矩阵</a></li><li><a href=#2准确率精确率召回率>2、准确率、精确率、召回率</a></li><li><a href=#3rocaucpr曲线>3、ROC/AUC/PR曲线</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>导航</h5><ul><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#about>关于</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#skills>技能</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#experiences>经验</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#projects>项目</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#education>教育</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#recent-posts>最新博客</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#accomplishments>造诣</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#achievements>业绩</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>联系方式:</h5><ul><li><span>电话:</span> <span>+0123456789</span></li><li><span>邮箱:</span> <span>bsgshyn8@163.com</span></li></ul></div></div></div><hr><div class=container><p id=disclaimer><strong>免责声明:</strong> 这个主题是MIT许可的。因此，您可以将其用于非商业、商业或者私人用途。您可以修改或分发主题，而无须经 作者任何许可。但是，主题作者不对主题的任何问题提供任何保证或承担任何责任。</p></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=stylesheet href=/katex/katex.min.css><script type=text/javascript defer src=/katex/katex.min.js></script><script type=text/javascript defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body);></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false},{left:'\\(',right:'\\)',display:false},{left:'\\[',right:'\\]',display:true}],throwOnError:false});});</script><script src=/js/mermaid-8.14.0.min.js></script><script>mermaid.initialize({startOnLoad:true});</script></body></html>