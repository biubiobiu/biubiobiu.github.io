<!doctype html><html><head><title>Transformer</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0d88336a9cbd3e68c7714efae786b0a9_38044_42x0_resize_box_2.png><meta property="og:title" content="Transformer"><meta property="og:description" content="一、简介 谷歌大脑、谷歌研究院等团队于2017年联合发表文章《Attention Is All You Need》，提出了一种新的注意力 Seq2Deq 模型，以取代之前以RNN作为编/解码器实现的 Seq2Seq 模型。模型采用的也是编码器-解码器架构，但是在该模型中，编码器和解码器不再是 RNN结构，取而代之的是编码器栈（encoder stack）和解码器栈（decoder stack）（注：所谓的“栈”就是将同一结构重复多次，“stack”翻译为“堆叠”更为合适）。编码器栈和解码器栈中分别为连续N个具有相同结构的编码器和解码器。
 编码器：由两部分组成（自注意力模块 + 前馈神经网络）
自注意力模块：具体来说是“Multi-Head Attention”，即“多头注意力”模块
全连接前馈网络 每个子网络都具有残差连接，其输出形式为 $LayerNorm(Sublayer(x)+x)$ ，其中 $Sublayer(x)$ 表示子网络对输入特征x进行的具体映射操作；$LayerNorm()$ 表示归一化操作。
  解码器：由三部分组成（自注意力模块 + 编码-解码注意力模块 + 前馈神经网络）
解码器中多了一个编码-解码注意力模块，用来利用当前已有的输出，来匹配输入特征（即：attention操作），然后拿计算出的新特征来计算当前时间步的输出。解码器中的自注意力模块与编码器不同是：这里只能看到当前时间步之前的输入，而不是全部的输入，所以需要有mask的操作。
 论文中图：    二、Transformer 输入：序列的embeding表示 + 位置编码
编码器：
 多头注意力 + 残差连接(residual connection) &ndash;> 层归一化(layer normalization) 基于位置的前馈网络(positionwise feed-forward network) + 残差连接(residual connection) &ndash;> 层归一化(layer normalization)  class PositionWiseFFN(nn.Module): &#34;&#34;&#34;基于位置的前馈网络&#34;&#34;&#34; def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs): super(PositionWiseFFN, self)."><meta property="og:type" content="article"><meta property="og:url" content="https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0030_transformer/0010_transformer_summary/"><meta property="article:published_time" content="2021-09-08T06:00:20+06:00"><meta property="article:modified_time" content="2021-09-08T06:00:20+06:00"><meta name=description content="Transformer"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/zh-cn><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png alt=Logo>
biubiobiu's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu547c3206082786c1d32d36034e2a655a_40863_42x0_resize_box_2.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/zh-cn/search><input type=text name=keyword placeholder=搜索 data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/zh-cn/posts data-filter=all>博文</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00020_toha-tutorial/>Toha教程</a><ul><li><a href=/zh-cn/posts/00020_toha-tutorial/0010_toha-config/ title=Toha的配置>Toha的配置</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0011_write-blogs/ title=撰写文章>撰写文章</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0013_markdown-tutorial/ title=MarkDown入门>MarkDown入门</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0015_latax_formula/ title=Katex公式>Katex公式</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0020_shortcodes_samples/ title=区域块-实例>区域块-实例</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00025_math_knowledge/>数学知识</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/>概率论</a><ul><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0010_basic-conception/ title=基本概念>基本概念</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0020_distribution_of_variables/ title=随机变量及其分布>随机变量及其分布</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0030_law_of_large_numbers/ title=大数定律>大数定律</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0040_sample_distribution/ title=样本及抽样分布>样本及抽样分布</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0050_parameter_estimation/ title=假设检验>假设检验</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0060_analysis_variance_regression/ title=方差分析及回归分析>方差分析及回归分析</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0070_stochastic_process/ title=随机过程>随机过程</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0080_markov_process/ title=马尔科夫链>马尔科夫链</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0090_stationary_stochastic_process/ title=平稳随机过程>平稳随机过程</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00025_math_knowledge/0020_math_convex_optimization_theory/>凸优化</a><ul><li><a href=/zh-cn/posts/00025_math_knowledge/0020_math_convex_optimization_theory/0010_basic_conception/ title=基本概念>基本概念</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00030_deeplearning_summary/>深度学习</a><ul><li><a href=/zh-cn/posts/00030_deeplearning_summary/0005_deeplearning_summary/ title=深度学习开篇>深度学习开篇</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0010_deeplearning_problem/ title=深度学习-结构>深度学习-结构</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0015_deeplearning_norm/ title=归一化>归一化</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0020_deeplearing_init/ title=初始化>初始化</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0100_draw_map_for_dl/ title=神经网络画图篇>神经网络画图篇</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0200_cam/ title=CAM>CAM</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00033_reinforce/>强化学习</a><ul><li><a href=/zh-cn/posts/00033_reinforce/0001_reinforce_summary/ title=综述>综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/>编程语言</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/>Python</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/0010_build_env/>env</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0010_build_env/0010_pip_env/ title=py-env>py-env</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0010_build_env/0020_cuda_env/ title=cuda>cuda</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/>Internal</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0010_encode_mode/ title=字符编码>字符编码</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0020_basic_operator/ title=基础操作>基础操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0025_internal_module/ title=内置模块>内置模块</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0030_advance_operator/ title=进阶操作>进阶操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0040_error/ title=异常>异常</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0050_file/ title=文件读取>文件读取</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/>SDK</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0020_multiprocessing/ title=并行操作>并行操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0030_importlib/ title=importlib>importlib</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0040_ipdb/ title=ipdb>ipdb</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0050_re/ title=正则-re>正则-re</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0060_heapd/ title=堆-heapq>堆-heapq</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0070_request/ title=requests>requests</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0080_logging/ title=logging>logging</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0090_argparse/ title=argparse>argparse</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0100_pil/ title=PIL>PIL</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0110_opencv/ title=OpenCV>OpenCV</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0040_tf/>TensorFlow</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0040_tf/w0010_compat/>兼容1.x</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0040_tf/w0010_compat/0010_tf_compat_summary/ title=静态图>静态图</a></li><li><a href=/zh-cn/posts/00035_programming_language/0040_tf/w0010_compat/0020_tf_compat_train/ title=模型训练>模型训练</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/>PyTorch</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0010_torch_summary/ title=简介>简介</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0020_basic/ title=基础操作>基础操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0030_mathematical/ title=数学计算>数学计算</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0040_tensor/ title=Tensor和变量>Tensor和变量</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0050_train_model/ title=模型训练>模型训练</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0060_mxnet/>MxNet</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0060_mxnet/0010_ndarray/>NdArray</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0060_mxnet/0010_ndarray/0010_ndarray_summary/ title=NdArray使用>NdArray使用</a></li></ul></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/>CV</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0010_backbone/>基础</a><ul><li><a href=/zh-cn/posts/00100_cv/0010_backbone/d1_1_backbone_cnn/ title=CNN>CNN</a></li><li><a href=/zh-cn/posts/00100_cv/0010_backbone/d1_2_optimizer/ title=optimizer>optimizer</a></li><li><a href=/zh-cn/posts/00100_cv/0010_backbone/d1_3_backbone_net/ title="backbone net">backbone net</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0020_contrastive_learning/>对比学习</a><ul><li><a href=/zh-cn/posts/00100_cv/0020_contrastive_learning/contrastive_learning/ title="contrastive learning">contrastive learning</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0030_vision_transformer/>ViT</a><ul><li><a href=/zh-cn/posts/00100_cv/0030_vision_transformer/vision_transformer/ title="vision transformer">vision transformer</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0050_detect_object/>目标检测</a><ul><li><a href=/zh-cn/posts/00100_cv/0050_detect_object/object_detection_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0055_semantic_segmentation/>语义分割</a><ul><li><a href=/zh-cn/posts/00100_cv/0055_semantic_segmentation/object_detection_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0060_image-matting/>抠图</a><ul><li><a href=/zh-cn/posts/00100_cv/0060_image-matting/image-matting-summary/ title=抠图综述>抠图综述</a></li><li><a href=/zh-cn/posts/00100_cv/0060_image-matting/image-matting-animal/ title="animal matting">animal matting</a></li></ul></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/posts/00200_nlp/>NLP</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0010_word_embedding/>Word Embedding</a><ul><li><a href=/zh-cn/posts/00200_nlp/0010_word_embedding/word_embedding_summary/ title="Word Embedding综述">Word Embedding综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0020_rnn/>RNN</a><ul><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/rnn_summary/ title=RNN综述>RNN综述</a></li><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/gru/ title=GRU网络>GRU网络</a></li><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/lstm/ title=LSTM网络>LSTM网络</a></li><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/encode_decode/ title=编解码架构>编解码架构</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/posts/00200_nlp/0030_transformer/>Transformer</a><ul class=active><li><a href=/zh-cn/posts/00200_nlp/0030_transformer/0001_attention/ title=Attention>Attention</a></li><li><a class=active href=/zh-cn/posts/00200_nlp/0030_transformer/0010_transformer_summary/ title=Transformer>Transformer</a></li><li><a href=/zh-cn/posts/00200_nlp/0030_transformer/0030_position/ title=位置编码>位置编码</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0080_gpt/>GPT</a><ul><li><a href=/zh-cn/posts/00200_nlp/0080_gpt/0010_gpt_summary/ title=GPT综述>GPT综述</a></li><li><a href=/zh-cn/posts/00200_nlp/0080_gpt/0020_gpt1_detail/ title=GPT-1>GPT-1</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0100_bert/>Bert</a><ul><li><a href=/zh-cn/posts/00200_nlp/0100_bert/0001_bert_summary/ title=Bert综述>Bert综述</a></li><li><a href=/zh-cn/posts/00200_nlp/0100_bert/0020_bert_family/ title=Bert家族>Bert家族</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0120_t5/>T5</a><ul><li><a href=/zh-cn/posts/00200_nlp/0120_t5/0010_t5_summary/ title=T5综述>T5综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0150_bart/>BART</a><ul><li><a href=/zh-cn/posts/00200_nlp/0150_bart/bart_summary/ title=BART综述>BART综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0200_electra/>ELECTRA</a><ul><li><a href=/zh-cn/posts/00200_nlp/0200_electra/electra_summary/ title=ELECTRA综述>ELECTRA综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/1000_code/>CODE</a><ul><li><a href=/zh-cn/posts/00200_nlp/1000_code/bart_summary/ title=code解析>code解析</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/>AIGC</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/0005_summary/>AIGC综述</a><ul><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0001_aigc_summary/ title=综述>综述</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0002_aigc_data/ title=LLM-数据集>LLM-数据集</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0005_aigc_application/ title=模型应用策略>模型应用策略</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0010_aigc_train_p/ title=大模型训练框架>大模型训练框架</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0012_aigc_amp/ title=混合精度训练>混合精度训练</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0015_aigc_train_mini/ title=模型小型化>模型小型化</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0020_aigc_error/ title=生成式-问题>生成式-问题</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/0010_generate_text/>文本生成</a><ul><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0011_gpt/ title=GPT>GPT</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0015_llama/ title=LLaMa>LLaMa</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0020_palm/ title=PaLM>PaLM</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0025_chatglm/ title=ChatGLM>ChatGLM</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0030_claude/ title=Claude>Claude</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0035_cohere/ title=Cohere>Cohere</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0040_falcon/ title=Falcon>Falcon</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0045_vicuna/ title=Vicuna>Vicuna</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/0020_generate_image/>图像生成</a><ul><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1000_summary/ title=综述>综述</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1002_gan_summary/ title=GAN>GAN</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1005_can_summary/ title=CAN>CAN</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1010_dall_e/ title=DALL-E>DALL-E</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1015_vqgan/ title=VQGAN>VQGAN</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1020_diffusion/ title=Diffusion>Diffusion</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1025_midjourney/ title=Midjourney>Midjourney</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1030_imagen/ title=Imagen>Imagen</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00400_vlp/>多模态</a><ul><li><a href=/zh-cn/posts/00400_vlp/0001_vlp_summary/ title=简介>简介</a></li><li><a href=/zh-cn/posts/00400_vlp/0005_clip/ title=CLIP>CLIP</a></li><li><a href=/zh-cn/posts/00400_vlp/0010_mllm/ title=MLLM>MLLM</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00500_video/>视频理解</a><ul><li><a href=/zh-cn/posts/00500_video/vidio_summary/ title=简介>简介</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/john_hu7f9991f5b5d471ecdfc6cff3db1a9fe6_6397_120x120_fit_box_2.png alt="Author Image"><h5 class=author-name>biubiobiu</h5><p>September 8, 2021</p></div><div class=title><h1>Transformer</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/zh-cn/tags/nlp class="btn, btn-sm">NLP</a></li><li class=rounded><a href=/zh-cn/tags/transformer class="btn, btn-sm">Transformer</a></li></ul></div><div class=post-content id=post-content><h2 id=一简介>一、简介</h2><p>谷歌大脑、谷歌研究院等团队于2017年联合发表文章<a href=https://arxiv.org/abs/1706.03762 target=blank>《Attention Is All You Need》</a>，提出了一种新的注意力 Seq2Deq 模型，以取代之前以RNN作为编/解码器实现的 Seq2Seq 模型。模型采用的也是编码器-解码器架构，但是在该模型中，编码器和解码器不再是 RNN结构，取而代之的是编码器栈（encoder stack）和解码器栈（decoder stack）（注：所谓的“栈”就是将同一结构重复多次，“stack”翻译为“堆叠”更为合适）。编码器栈和解码器栈中分别为连续N个具有相同结构的编码器和解码器。</p><blockquote><p><code>编码器：由两部分组成（自注意力模块 + 前馈神经网络）</code><br>自注意力模块：具体来说是“Multi-Head Attention”，即“多头注意力”模块<br>全连接前馈网络<br>每个子网络都具有残差连接，其输出形式为 $LayerNorm(Sublayer(x)+x)$ ，其中 $Sublayer(x)$ 表示子网络对输入特征x进行的具体映射操作；$LayerNorm()$ 表示归一化操作。</p></blockquote><blockquote><p><code>解码器：由三部分组成（自注意力模块 + 编码-解码注意力模块 + 前馈神经网络）</code><br>解码器中多了一个编码-解码注意力模块，用来利用当前已有的输出，来匹配输入特征（即：attention操作），然后拿计算出的新特征来计算当前时间步的输出。解码器中的自注意力模块与编码器不同是：这里只能看到当前时间步之前的输入，而不是全部的输入，所以需要有mask的操作。</p></blockquote><p>论文中图：<div class=row><div class="col col-sm-12 col-lg-6"><p align=center><img src=/datasets/posts/nlp/attention.png width=80% height=80% title=attention alt=attention></div><div class="col col-sm-12 col-lg-6"><p align=center><img src=https://s2.loli.net/2022/05/19/9nCzrTwESlBRfmy.jpg width=100% height=100% title=attention alt=attention></div></div></p><h2 id=二transformer>二、Transformer</h2><p>输入：序列的embeding表示 + 位置编码</p><p>编码器：</p><ol><li>多头注意力 + 残差连接(residual connection) &ndash;> 层归一化(layer normalization)</li><li>基于位置的前馈网络(positionwise feed-forward network) + 残差连接(residual connection) &ndash;> 层归一化(layer normalization)</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>PositionWiseFFN</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#e6db74>&#34;&#34;&#34;基于位置的前馈网络&#34;&#34;&#34;</span>
    <span style=color:#66d9ef>def</span> __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, <span style=color:#f92672>**</span>kwargs):
        super(PositionWiseFFN, self)<span style=color:#f92672>.</span>__init__(<span style=color:#f92672>**</span>kwargs)
        self<span style=color:#f92672>.</span>dense1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(ffn_num_input, ffn_num_hiddens)
        self<span style=color:#f92672>.</span>relu <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ReLU()
        self<span style=color:#f92672>.</span>dense2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(ffn_num_hiddens, ffn_num_outputs)

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, X):
        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>dense2(self<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>dense1(X)))
</code></pre></div><p>解码器：</p><ol><li>解码器自注意力：解码器中每个位置只能考虑该位置之前的所有位置，所以添加<code>掩码</code></li><li>编码器-解码器注意力：query：前一个解码器层的输出；k和v：整个编码器的输出。目的是捕获与当前解码最相关的编码状态，而不是所有的编码状态都是同等重要。</li><li>基于位置的前馈网络(positionwise feed-forward network) + 残差连接(residual connection) &ndash;> 层归一化(layer normalization)</li></ol><h3 id=1位置编码>1、位置编码</h3><p>与CNN/RNN不同，自注意力是<code>没有记录位置信息的</code>。可以回顾自注意力的计算过程，在$q_j$ 跟 $\bold{K}$、$\bold{V}$ 计算后，生成一个把$\bold{V}$各个向量加权后的向量，这里面是没有位置信息的，也就是说不管输入的 $\bold{V}$ 的向量顺序如何变化，自注意力的输出是不会改变的。<br>所以：<code>需要位置编码将位置信息注入到输入里。</code><br></p><p>输入：$\bold{X} \in \R^{n \times d}$ 包含一个序列中n个词元的d维嵌入表示。<br>位置编码：$\bold{P} \in \R^{n \times d}$, 矩阵第i行 偶数列、奇数列：用不同的频率、偏移来记录位置信息。
$$p_{i,2j} = sin(\frac{i}{10000^{\frac{2j}{d}}})$$
$$p_{i,2j+1} = cos(\frac{i}{10000^{\frac{2j}{d}}})$$</p><p>在 $\bold{X} + \bold{P}$ 时，当$\bold{X}$的幅度值比$\bold{P}$小或者差不多时，可以增大$\bold{X}$的幅度值，以保证$\bold{X}$的主导性。
$$
\bold{X} \times M + \bold{P}
$$</p><p align=center><img src=/datasets/posts/nlp/position_em.png width=50% height=50% title=position alt=position></p><h3 id=2层归一化>2、层归一化</h3><p><strong>层归一化</strong>：<a href=https://arxiv.org/abs/1607.06450 target=blank>《Layer Normalization》</a>，在一个输入序列中，做归一化。</p><ul><li>由于输入序列的长度是不确定的</li></ul><p><strong>批归一化</strong>：<a href=https://arxiv.org/abs/1502.03167 target=blank>《Batch normalization》</a>，在一个batch中，在通道维度 做归一化。</p><ul><li>避免梯度消失/爆炸：这是因为通过归一化(偏移、拉伸)，把原来可能波动较大的数据，限制在一定的范围内</li><li>为啥有效：有的解释是：通过(偏移、拉伸)，相当于添加了一个随机噪声，因为 均值、方差是在当前小批量样本上算出来的，包含了随机性。</li><li>批归一化，限制波动的范围，所以可以调大学习率，可以加速收敛。</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 批归一化</span>
mean <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>mean(dim<span style=color:#f92672>=</span>(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>), keepdim<span style=color:#f92672>=</span>True)
var <span style=color:#f92672>=</span> ((X <span style=color:#f92672>-</span> mean) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>mean(dim<span style=color:#f92672>=</span>(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>), keepdim<span style=color:#f92672>=</span>True)
X_hat <span style=color:#f92672>=</span> (X <span style=color:#f92672>-</span> mean) <span style=color:#f92672>/</span> torch<span style=color:#f92672>.</span>sqrt(var <span style=color:#f92672>+</span> eps)
</code></pre></div><div class="alert alert-info"><strong><p><strong>图像与文本</strong><br></p><ul><li>图像: (batch, c, h, w) ： (h, w: 图像的高和宽)、(c: 通道数)</li><li>文本: (batch, T, d) ：(T：序列长度)、(d: 每个词元embeding表示的维度)</li></ul><p>类比一下：</p><ul><li>每个图像中包含 $h \times w$ 个像素点，这个数目 类似 文本的长度。</li><li>每个像素点在channel方向是一个向量，这个向量 类似 词元的embeding</li></ul><p>也就是说：图像的每个像素点表示一个样本点，channel方向 表示该样本点的特征表示。这也就是为什么说 $1 \times 1$ 的卷积核的作用相当于全连接层。</p></strong></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>AddNorm</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#e6db74>&#34;&#34;&#34;残差连接后进行层规范化&#34;&#34;&#34;</span>
    <span style=color:#66d9ef>def</span> __init__(self, normalized_shape, dropout, <span style=color:#f92672>**</span>kwargs):
        super(AddNorm, self)<span style=color:#f92672>.</span>__init__(<span style=color:#f92672>**</span>kwargs)
        self<span style=color:#f92672>.</span>dropout <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dropout(dropout)
        self<span style=color:#f92672>.</span>ln <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(normalized_shape)

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, X, Y):
        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>ln(self<span style=color:#f92672>.</span>dropout(Y) <span style=color:#f92672>+</span> X)
</code></pre></div><p align=center><img src=https://s2.loli.net/2022/05/19/PGuYO7FWjdUTIeE.jpg width=50% height=50% title="layer NM" alt="layer NM"></p><h3 id=3基于位置的前馈网络>3、基于位置的前馈网络</h3><p>输入：(batch, 序列长度, embeding维度)<br>输出：(batch, 序列长度, 新特征维度)<br>作用：类似于卷积中的 $1 \times 1$ 卷积核，就是转换一下特征的维度，样本个数不变。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>PositionWiseFFN</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#e6db74>&#34;&#34;&#34;基于位置的前馈网络&#34;&#34;&#34;</span>
    <span style=color:#66d9ef>def</span> __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,
                 <span style=color:#f92672>**</span>kwargs):
        super(PositionWiseFFN, self)<span style=color:#f92672>.</span>__init__(<span style=color:#f92672>**</span>kwargs)
        self<span style=color:#f92672>.</span>dense1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(ffn_num_input, ffn_num_hiddens)
        self<span style=color:#f92672>.</span>relu <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ReLU()
        self<span style=color:#f92672>.</span>dense2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(ffn_num_hiddens, ffn_num_outputs)

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, X):
        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>dense2(self<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>dense1(X)))
</code></pre></div><h3 id=4编码器中的attention>4、编码器中的attention</h3><p>在编码器中使用VVV模式，该自注意力模块为 MHA 结构，其中<code>V</code>为上一个编码器对输入句子中的每个词的编码（这里的编码可以理解为 RNN 中的隐变量向量，即输入句子中每个词的内部表示。如果是第一个编码器这里的编码即每个词的嵌入向量）。编码器自注意力模块用来捕捉输入句子中词与词之间的关系。例如翻译句子“The dog is barking at the bird because it is angry”，这里的“it”到底说的是狗还是鸟？编码器自注意力模块就是为了在对“it”进行编码时，尽量使得 “dog”对其具有更高的影响力——即注意力权重。
下图为针对上述翻译问题的一个自注意力模块示意，其中 $x_1, \dotsb, x_{11}$ 分别代表句子中每个词的编码（为简化起见不考虑结束符等辅助符号），$y_1, \dotsb, y_{11}$ 分别为自注意力模块对每个词的输出， $e_{ij}$ 即为自注意力模型输出 时在输入 $x_i$ 上投射的注意力权重。下图以 $y_9$（即针对“it”一词）为例，示意了各个输入编码上的注意力权重分配。</p><p align=center><img src=/datasets/posts/nlp/encoder_vvv.jpg width=60% height=60% title=encoder alt=encoder></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>EncoderBlock</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#e6db74>&#34;&#34;&#34;transformer编码器块&#34;&#34;&#34;</span>
    <span style=color:#66d9ef>def</span> __init__(self, key_size, query_size, value_size, num_hiddens,
                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
                 dropout, use_bias<span style=color:#f92672>=</span>False, <span style=color:#f92672>**</span>kwargs):
        super(EncoderBlock, self)<span style=color:#f92672>.</span>__init__(<span style=color:#f92672>**</span>kwargs)
        self<span style=color:#f92672>.</span>attention <span style=color:#f92672>=</span> d2l<span style=color:#f92672>.</span>MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout,
            use_bias)
        self<span style=color:#f92672>.</span>addnorm1 <span style=color:#f92672>=</span> AddNorm(norm_shape, dropout)
        self<span style=color:#f92672>.</span>ffn <span style=color:#f92672>=</span> PositionWiseFFN(
            ffn_num_input, ffn_num_hiddens, num_hiddens)
        self<span style=color:#f92672>.</span>addnorm2 <span style=color:#f92672>=</span> AddNorm(norm_shape, dropout)

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, X, valid_lens):
        Y <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>addnorm1(X, self<span style=color:#f92672>.</span>attention(X, X, X, valid_lens))
        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>addnorm2(Y, self<span style=color:#f92672>.</span>ffn(Y))

<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TransformerEncoder</span>(d2l<span style=color:#f92672>.</span>Encoder):
    <span style=color:#e6db74>&#34;&#34;&#34;transformer编码器&#34;&#34;&#34;</span>
    <span style=color:#66d9ef>def</span> __init__(self, vocab_size, key_size, query_size, value_size,
                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,
                 num_heads, num_layers, dropout, use_bias<span style=color:#f92672>=</span>False, <span style=color:#f92672>**</span>kwargs):
        super(TransformerEncoder, self)<span style=color:#f92672>.</span>__init__(<span style=color:#f92672>**</span>kwargs)
        self<span style=color:#f92672>.</span>num_hiddens <span style=color:#f92672>=</span> num_hiddens
        self<span style=color:#f92672>.</span>embedding <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding(vocab_size, num_hiddens)
        self<span style=color:#f92672>.</span>pos_encoding <span style=color:#f92672>=</span> d2l<span style=color:#f92672>.</span>PositionalEncoding(num_hiddens, dropout)
        self<span style=color:#f92672>.</span>blks <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential()
        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(num_layers):
            self<span style=color:#f92672>.</span>blks<span style=color:#f92672>.</span>add_module(<span style=color:#e6db74>&#34;block&#34;</span><span style=color:#f92672>+</span>str(i),
                EncoderBlock(key_size, query_size, value_size, num_hiddens,
                             norm_shape, ffn_num_input, ffn_num_hiddens,
                             num_heads, dropout, use_bias))

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, X, valid_lens, <span style=color:#f92672>*</span>args):
        <span style=color:#75715e># 因为位置编码值在-1和1之间，</span>
        <span style=color:#75715e># 因此嵌入值乘以嵌入维度的平方根进行缩放，</span>
        <span style=color:#75715e># 然后再与位置编码相加。</span>
        X <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>pos_encoding(self<span style=color:#f92672>.</span>embedding(X) <span style=color:#f92672>*</span> math<span style=color:#f92672>.</span>sqrt(self<span style=color:#f92672>.</span>num_hiddens))
        self<span style=color:#f92672>.</span>attention_weights <span style=color:#f92672>=</span> [None] <span style=color:#f92672>*</span> len(self<span style=color:#f92672>.</span>blks)
        <span style=color:#66d9ef>for</span> i, blk <span style=color:#f92672>in</span> enumerate(self<span style=color:#f92672>.</span>blks):
            X <span style=color:#f92672>=</span> blk(X, valid_lens)
            self<span style=color:#f92672>.</span>attention_weights[
                i] <span style=color:#f92672>=</span> blk<span style=color:#f92672>.</span>attention<span style=color:#f92672>.</span>attention<span style=color:#f92672>.</span>attention_weights
        <span style=color:#66d9ef>return</span> X
</code></pre></div><h3 id=5解码器中的attention>5、解码器中的attention</h3><p>在解码器中先使用VVV模式，该自注意力模块与编码器自注意力模块的结构非常类似，唯一不同的是添加了掩膜机制，这是由于在解码器中，自注意力模块只被允许处理当前项之前的那些项，这一点与编码器需要“看到”所有项是不同的。上述目标的实现方式非常简单，只要在softmax 注意力权重概率化之前，用掩膜将当前处理项之后的所有项隐去即可，即将注意力的计算改为如下具有掩膜的形式</p><p align=center><img src=/datasets/posts/nlp/decoder_qvv.jpg width=60% height=60% title=decoder alt=decoder></p><p>解码器自注意力模块之后是编码-解码注意力模块：该模块也被构造为MHA结构、QVV模式。其中<code>Q</code>来自于上一个解码器的输出，而<code>V</code>来自于最后一个编码器输出（即也是编码器栈的最终输出）。该注意力模块能够使得解码器中的每个项都能够有重点的“看一遍”输入序列中的每一个词，这一点与基于 RNN结构的 Seq2Seq 结构类似。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>DecoderBlock</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#e6db74>&#34;&#34;&#34;解码器中第i个块&#34;&#34;&#34;</span>
    <span style=color:#66d9ef>def</span> __init__(self, key_size, query_size, value_size, num_hiddens,
                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
                 dropout, i, <span style=color:#f92672>**</span>kwargs):
        super(DecoderBlock, self)<span style=color:#f92672>.</span>__init__(<span style=color:#f92672>**</span>kwargs)
        self<span style=color:#f92672>.</span>i <span style=color:#f92672>=</span> i
        self<span style=color:#f92672>.</span>attention1 <span style=color:#f92672>=</span> d2l<span style=color:#f92672>.</span>MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout)
        self<span style=color:#f92672>.</span>addnorm1 <span style=color:#f92672>=</span> AddNorm(norm_shape, dropout)
        self<span style=color:#f92672>.</span>attention2 <span style=color:#f92672>=</span> d2l<span style=color:#f92672>.</span>MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout)
        self<span style=color:#f92672>.</span>addnorm2 <span style=color:#f92672>=</span> AddNorm(norm_shape, dropout)
        self<span style=color:#f92672>.</span>ffn <span style=color:#f92672>=</span> PositionWiseFFN(ffn_num_input, ffn_num_hiddens,
                                   num_hiddens)
        self<span style=color:#f92672>.</span>addnorm3 <span style=color:#f92672>=</span> AddNorm(norm_shape, dropout)

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, X, state):
        enc_outputs, enc_valid_lens <span style=color:#f92672>=</span> state[<span style=color:#ae81ff>0</span>], state[<span style=color:#ae81ff>1</span>]
        <span style=color:#75715e># 训练阶段，输出序列的所有词元都在同一时间处理，</span>
        <span style=color:#75715e># 因此state[2][self.i]初始化为None。</span>
        <span style=color:#75715e># 预测阶段，输出序列是通过词元一个接着一个解码的，</span>
        <span style=color:#75715e># 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示</span>
        <span style=color:#66d9ef>if</span> state[<span style=color:#ae81ff>2</span>][self<span style=color:#f92672>.</span>i] <span style=color:#f92672>is</span> None:
            key_values <span style=color:#f92672>=</span> X
        <span style=color:#66d9ef>else</span>:
            key_values <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat((state[<span style=color:#ae81ff>2</span>][self<span style=color:#f92672>.</span>i], X), axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
        state[<span style=color:#ae81ff>2</span>][self<span style=color:#f92672>.</span>i] <span style=color:#f92672>=</span> key_values
        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>training:
            batch_size, num_steps, _ <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>shape
            <span style=color:#75715e># dec_valid_lens的开头:(batch_size,num_steps),</span>
            <span style=color:#75715e># 其中每一行是[1,2,...,num_steps]</span>
            dec_valid_lens <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>arange(
                <span style=color:#ae81ff>1</span>, num_steps <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>, device<span style=color:#f92672>=</span>X<span style=color:#f92672>.</span>device)<span style=color:#f92672>.</span>repeat(batch_size, <span style=color:#ae81ff>1</span>)
        <span style=color:#66d9ef>else</span>:
            dec_valid_lens <span style=color:#f92672>=</span> None

        <span style=color:#75715e># 自注意力</span>
        X2 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>attention1(X, key_values, key_values, dec_valid_lens)
        Y <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>addnorm1(X, X2)
        <span style=color:#75715e># 编码器－解码器注意力。</span>
        <span style=color:#75715e># enc_outputs的开头:(batch_size,num_steps,num_hiddens)</span>
        Y2 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)
        Z <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>addnorm2(Y, Y2)
        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>addnorm3(Z, self<span style=color:#f92672>.</span>ffn(Z)), state

<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TransformerDecoder</span>(d2l<span style=color:#f92672>.</span>AttentionDecoder):
    <span style=color:#66d9ef>def</span> __init__(self, vocab_size, key_size, query_size, value_size,
                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,
                 num_heads, num_layers, dropout, <span style=color:#f92672>**</span>kwargs):
        super(TransformerDecoder, self)<span style=color:#f92672>.</span>__init__(<span style=color:#f92672>**</span>kwargs)
        self<span style=color:#f92672>.</span>num_hiddens <span style=color:#f92672>=</span> num_hiddens
        self<span style=color:#f92672>.</span>num_layers <span style=color:#f92672>=</span> num_layers
        self<span style=color:#f92672>.</span>embedding <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding(vocab_size, num_hiddens)
        self<span style=color:#f92672>.</span>pos_encoding <span style=color:#f92672>=</span> d2l<span style=color:#f92672>.</span>PositionalEncoding(num_hiddens, dropout)
        self<span style=color:#f92672>.</span>blks <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential()
        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(num_layers):
            self<span style=color:#f92672>.</span>blks<span style=color:#f92672>.</span>add_module(<span style=color:#e6db74>&#34;block&#34;</span><span style=color:#f92672>+</span>str(i),
                DecoderBlock(key_size, query_size, value_size, num_hiddens,
                             norm_shape, ffn_num_input, ffn_num_hiddens,
                             num_heads, dropout, i))
        self<span style=color:#f92672>.</span>dense <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(num_hiddens, vocab_size)

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>init_state</span>(self, enc_outputs, enc_valid_lens, <span style=color:#f92672>*</span>args):
        <span style=color:#66d9ef>return</span> [enc_outputs, enc_valid_lens, [None] <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>num_layers]

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, X, state):
        X <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>pos_encoding(self<span style=color:#f92672>.</span>embedding(X) <span style=color:#f92672>*</span> math<span style=color:#f92672>.</span>sqrt(self<span style=color:#f92672>.</span>num_hiddens))
        self<span style=color:#f92672>.</span>_attention_weights <span style=color:#f92672>=</span> [[None] <span style=color:#f92672>*</span> len(self<span style=color:#f92672>.</span>blks) <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range (<span style=color:#ae81ff>2</span>)]
        <span style=color:#66d9ef>for</span> i, blk <span style=color:#f92672>in</span> enumerate(self<span style=color:#f92672>.</span>blks):
            X, state <span style=color:#f92672>=</span> blk(X, state)
            <span style=color:#75715e># 解码器自注意力权重</span>
            self<span style=color:#f92672>.</span>_attention_weights[<span style=color:#ae81ff>0</span>][
                i] <span style=color:#f92672>=</span> blk<span style=color:#f92672>.</span>attention1<span style=color:#f92672>.</span>attention<span style=color:#f92672>.</span>attention_weights
            <span style=color:#75715e># “编码器－解码器”自注意力权重</span>
            self<span style=color:#f92672>.</span>_attention_weights[<span style=color:#ae81ff>1</span>][
                i] <span style=color:#f92672>=</span> blk<span style=color:#f92672>.</span>attention2<span style=color:#f92672>.</span>attention<span style=color:#f92672>.</span>attention_weights
        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>dense(X), state

    <span style=color:#a6e22e>@property</span>
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>attention_weights</span>(self):
        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>_attention_weights
</code></pre></div><h2 id=三参数量计算>三、参数量计算</h2><p>设：$V$ : 词表量；$H$ : embedding；$L$: Layers ；$S$：输入句子最大长度；$B$：batch size<br></p><p>$ \frak{L}$：损失函数<br></p><p>$ FLOPs $ ： 计算量</p><h3 id=1参数量>1、参数量</h3><ol><li>Embedding 层： $VH$</li><li>每层Transformer Block: $ VH + L(12H^2+13H)$<ul><li>两个layer normalization，总参数量为 : $4H$。<br>层归一化的公式：$y = w \odot \frac{x-\mu}{\sqrt{\sigma^2+\varepsilon}} + b$ ，即：每个LN有两个 $H$ 维度的参数。<br>需要计算3个梯度：$\frac{\partial\frak{L}}{\partial w}$，$\frac{\partial\frak{L}}{\partial b}$， $\frac{\partial\frak{L}}{\partial x}$</li><li>attention部分有 $Q, K, V$ 和输出的权重矩阵以及偏置，总参数量 $4H^2+4H$</li><li>MLP部分由两个线性层组成，分别从 $ H \rarr 4H$， $4H \rarr H$，总参数量为 $8H^2+5H$</li></ul></li></ol><h3 id=2计算量>2、计算量</h3><p>已知：对于形状为 $m \times k, k \times n$ 的两个矩阵相乘，其浮点数运算次数为：$2mkn$</p><ol><li><p>每个Tranformer Block的前向过程，有 $24BSH^2 + 4BS^2H$ 的$FLOPs$</p><ul><li>attention 中计算 $Q, K, V$，计算次数：$8BSH^2$</li><li>计算attention score时，$QK^T$，计算次数：$2BS^2H$</li><li>attention score 作用到 $V$ 上，计算次数：$2BS^2H$</li><li>MLP 的 两个全连接层，分别从 $ H \rarr 4H$， $4H \rarr H$，计算次数：$16BSH^2$</li></ul></li><li><p>每个Transformer Block的反向过程，有 $48BSH^2 + 8BS^2H$ 的 $FLOPs$</p><ul><li>简单来说，对于matmul和conv算子而言，它们额反向计算过程需要对两个输入求导，可以粗略估计反向的计算量就是前向的两倍。</li></ul></li></ol><h3 id=3通信量>3、通信量</h3><ol><li>数据并行（Data Parallel）
假设：<ul><li>把数据分成 $dps$ 份。模型就会copy $dps$ 份，每次更新模型参数时，需要把更新值合并到主节点中，计算完后，然后再分发给各个节点中。<br></li><li>模型大小为6B，且梯度为FP16模式，则模型的梯度大小一共为12GB。<br></li></ul></li></ol><p>一次all_reduce的通信量：$\frac{model}{dps} \times (dps - 1) \times 2$：<br></p><ul><li>收集：每个节点，receive $dps-1$ 次数据，每次receive数据的大小 $\frac{model}{dps}$</li><li>分发：每个节点，receive $dps-1$ 次数据，每次receive数据的大小 $\frac{model}{dps}$</li></ul><ol start=2><li>Pipeline Parallel</li></ol></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"><strong>分享:</strong>
<a class="btn btn-sm facebook-btn" href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00200_nlp%2f0030_transformer%2f0010_transformer_summary%2f" target=_blank><i class="fab fa-facebook"></i></a><a class="btn btn-sm twitter-btn" href="https://twitter.com/share?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00200_nlp%2f0030_transformer%2f0010_transformer_summary%2f&text=Transformer&via=biubiobiu%27s%20Blog" target=_blank><i class="fab fa-twitter"></i></a><a class="btn btn-sm reddit-btn" href="https://reddit.com/submit?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00200_nlp%2f0030_transformer%2f0010_transformer_summary%2f&title=Transformer" target=_blank><i class="fab fa-reddit"></i></a><a class="btn btn-sm linkedin-btn" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00200_nlp%2f0030_transformer%2f0010_transformer_summary%2f&title=Transformer" target=_blank><i class="fab fa-linkedin"></i></a><a class="btn btn-sm whatsapp-btn" href="https://api.whatsapp.com/send?text=Transformer https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00200_nlp%2f0030_transformer%2f0010_transformer_summary%2f" target=_blank><i class="fab fa-whatsapp"></i></a><a class="btn btn-sm email-btn" href="mailto:?subject=Transformer&body=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00200_nlp%2f0030_transformer%2f0010_transformer_summary%2f" target=_blank><i class="fas fa-envelope-open-text"></i></a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/zh-cn/posts/00200_nlp/0030_transformer/0001_attention/ title=Attention class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i>上一篇</div><div class=next-prev-text>Attention</div></a></div><div class="col-md-6 next-article"><a href=/zh-cn/posts/00200_nlp/0030_transformer/0030_position/ title=位置编码 class="btn btn-outline-info"><div>下一篇 <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>位置编码</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">目录</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#一简介>一、简介</a></li><li><a href=#二transformer>二、Transformer</a><ul><li><a href=#1位置编码>1、位置编码</a></li><li><a href=#2层归一化>2、层归一化</a></li><li><a href=#3基于位置的前馈网络>3、基于位置的前馈网络</a></li><li><a href=#4编码器中的attention>4、编码器中的attention</a></li><li><a href=#5解码器中的attention>5、解码器中的attention</a></li></ul></li><li><a href=#三参数量计算>三、参数量计算</a><ul><li><a href=#1参数量>1、参数量</a></li><li><a href=#2计算量>2、计算量</a></li><li><a href=#3通信量>3、通信量</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>导航</h5><ul><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#about>关于</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#skills>技能</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#experiences>经验</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#projects>项目</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#education>教育</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#recent-posts>最新博客</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#accomplishments>造诣</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#achievements>业绩</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>联系方式:</h5><ul><li><span>电话:</span> <span>+0123456789</span></li><li><span>邮箱:</span> <span>bsgshyn8@163.com</span></li></ul></div></div></div><hr><div class=container><p id=disclaimer><strong>免责声明:</strong> 这个主题是MIT许可的。因此，您可以将其用于非商业、商业或者私人用途。您可以修改或分发主题，而无须经 作者任何许可。但是，主题作者不对主题的任何问题提供任何保证或承担任何责任。</p></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=stylesheet href=/katex/katex.min.css><script type=text/javascript defer src=/katex/katex.min.js></script><script type=text/javascript defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body);></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false},{left:'\\(',right:'\\)',display:false},{left:'\\[',right:'\\]',display:true}],throwOnError:false});});</script></body></html>