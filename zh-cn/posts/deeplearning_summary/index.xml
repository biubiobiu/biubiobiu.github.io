<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>深度学习 on biubiobiu's Blog</title><link>https://biubiobiu.github.io/zh-cn/posts/deeplearning_summary/</link><description>Recent content in 深度学习 on biubiobiu's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Everything is mine</copyright><lastBuildDate>Fri, 09 Sep 2022 06:00:20 +0600</lastBuildDate><atom:link href="https://biubiobiu.github.io/zh-cn/posts/deeplearning_summary/index.xml" rel="self" type="application/rss+xml"/><item><title>CAM</title><link>https://biubiobiu.github.io/zh-cn/posts/deeplearning_summary/cam/</link><pubDate>Fri, 09 Sep 2022 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/deeplearning_summary/cam/</guid><description>一、简介 二、模型 1、gradient-based 1. GAP 《Learning Deep Features for Discriminative Localizatiion》
# 代码非常简单， 提取到特征图和目标类别全连接的权重，直接加权求和，再经过relu操作去除负值，最后归一化获取CAM，具体如下: # 获取全连接层的权重 self._fc_weights = self.model._modules.get(fc_layer).weight.data # 获取目标类别的权重作为特征权重 weights=self._fc_weights[class_idx, :] # 这里self.hook_a为最后一层特征图的输出 batch_cams = (weights.unsqueeze(-1).unsqueeze(-1) * self.hook_a.squeeze(0)).sum(dim=0) # relu操作,去除负值 batch_cams = F.relu(batch_cams, inplace=True) # 归一化操作 batch_cams = self._normalize(batch_cams) 2. Grad-CAM 《Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization》
2、gradient-free</description></item><item><title>神经网络画图篇</title><link>https://biubiobiu.github.io/zh-cn/posts/deeplearning_summary/draw_map_for_dl/</link><pubDate>Thu, 09 Sep 2021 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/deeplearning_summary/draw_map_for_dl/</guid><description>一、简介 一图抵万言！本篇介绍神经网络的可视化工具和绘图软件。
二、示意图 1、NN SVG 提供三种典型的神经网络绘图风格，个性化参数多；交互式绘图。 NN-SVG是由麻省理工学院弗兰克尔生物工程实验室开发的。可以绘制的图包括以节点形式展示的FCNN style，这个特别适合传统的全连接神经网络的绘制。
Github
Demo
2、PlotNeuralNet 底层基于latex的宏指令绘制，上层提供基于python的描述框架，绘制脚本简单。可以绘制复杂的网络结构。
PlotNeuralNet 是由萨尔大学计算机科学专业的一个学生开发的，目前主要支持的是卷积神经网络，其中卷积层、池化层、bottleneck、skip-connection、up-conv、Softmax等常规的层在代码中都有定义，但缺少RNN相关的可视化层展示。
Github
三、计算图 1、Netron Netron是一个神经网络可视化包，支持绝大多数神经网络操作。该功能包可以为不同节点显示不同的颜色，卷积层用蓝色显示，池化层和归一化层用绿色显示，数学操作用黑色显示。在使用方面，可以直接访问网页端，上传模型文件，就可以看到网络结构图，并可以进一步利用pip安装并引入到程序中通过浏览器查看模型的变化。
Github
Demo</description></item><item><title>深度学习开篇</title><link>https://biubiobiu.github.io/zh-cn/posts/deeplearning_summary/deeplearning_start/</link><pubDate>Fri, 01 Jan 2021 08:06:25 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/deeplearning_summary/deeplearning_start/</guid><description>论文入口
一、开篇 在描述深度学习之前，先回顾下机器学习和深度学习的关系。
机器学习：研究如何使用计算机系统利用经验改善性能。在机器学习的众多研究方向中，表征学习关注如何自动找出表示数据的合适方式，以便更好地将输入变换为正确的输出。
深度学习：是具有多级表示的表征学习方法。在每一级，深度学习通过简单的函数将该级的表示变换为更高级的表示。因此，深度学习模型也可以看作是由许多简单函数复合而成的函数。当这些复合函数足够多时，就可以表达非常复杂的变换。 作为表征学习的一种，深度学习将自动找出每一级表示数据的合适方式。 逐级表示越来越抽象的概念或模式。以图像为例，它的输入是一堆原始像素值，模型中逐级表示为：特定位置和角度的边缘 &amp;mdash;&amp;gt; 由边缘组合得出的花纹 &amp;mdash;&amp;gt; 由多种花纹进一步汇合得到的特定部位 &amp;mdash;&amp;gt; 由特定部位组合得到的整个目标。
二、简介 It&amp;rsquo;s coming soon.
三、欠/过拟合 1. 误差 训练误差(training error): 训练模型在训练数据集(training set)上表现出的误差。 泛化误差(generalization error)：模型在任意一个测试数据集(test set)上表现出的误差的期望。
训练集(training set)：用来产出模型参数。
验证集(validation set)：由于无法从训练误差评估泛化误差，因此从训练集中预留一部分数据作为验证集，主要用来选择模型。 测试集(test set)：在模型参数选定后，实际使用。
2. 欠/过拟合 欠拟合(underfitting)：模型的表现能力不足。
训练样本足够，模型参数不足 过拟合(overfitting)：模型的表现能力过剩。
训练样本不足，模型参数足够：样本不足导致特征较少，相当于模型足够表征数据的特征，产生过拟合现象。 3. 优化过拟合 增大训练集可能会减轻过拟合，但是获取训练数据往往代价很高。可以在模型方面优化一下，减轻过拟合现象。
权重衰减(weight decay)： 对模型参数计算L2范数正则化。即：在原Loss中添加对模型参数的惩罚。使得模型学到的权重参数较接近于0。权重衰减通过惩罚绝对值较大的模型参数，为需要学习的模型增加了限制。这可能对过拟合有效。
丢弃法(dropout)：针对隐藏层中的各个神经元，以概率p随机丢弃，有可能该成神经元被全部清零。这样，下一层的计算无法过渡依赖该层的任意一个神经元，从而在训练中可以用来对付过拟合。在测试中，就不需要丢弃了。
例如：对隐藏层使用丢弃法，丢弃概率: p，那么hi 有p的概率被清零；不丢弃概率: 1-p，为了保证隐藏层的期望值不变E(p')=E(p)，需要对不丢弃的神经元做拉伸，即：$$h'_i = \frac{\xi_i} {1-p} h_i$$ 其中：随机变量ξi 为0和1的概率分别为p和1-p</description></item></channel></rss>