<!doctype html><html><head><title>Bert综述</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0d88336a9cbd3e68c7714efae786b0a9_38044_42x0_resize_box_2.png><meta property="og:title" content="Bert综述"><meta property="og:description" content="一、背景 在使用预训练模型，处理下游任务时，有两类策略：基于特征(feature-based)、基于微调(fine-tuning)
 基于特征：比如：ELMo，在使用时，对每个下游任务，创建一个跟这个任务相关的神经网络；预训练作为额外的特征跟输入一起输入到模型，预训练的额外特征可能会对要训练的模型有指导作用。 基于微调：比如：GPT，预训练模型在下游使用时，不需要改动太多，类似于视觉模型的fine-tuning，预训练完成特征提取，预训练模型后面添加个简单的网络用于实现具体任务。  1、上下文敏感 在自然语言中，有丰富的多义现象，一个词到底是什么意思，需要参考上下文才能判断。流行的上下文敏感表示：
 TagLM(language-model-augmented sequence tagger 语言模型增强的序列标记器) CoVe(Context Vectors 上下文向量) ELMo(Embeddings from Language Models 来自语言模型的嵌入)  ELMo 将来自预训练LSTM的所有中间层表示组合为输出表示 ELMo的表示，将作为添加特征添加到下游任务的有监督模型中    2、从特定任务到通用任务 ELMo显著改进了自然语言任务，但每个解决方案仍然依赖于一个特定的任务架构。怎么设计一个模型，让各个自然语言任务通用呢？
GPT(Generative Pre Training 生成式预训练)：在Transformer的基础上，为上下文敏感设计了通用的模型。
 预训练一个用于表示文本序列的语言模型 当将GPT应用于下游任务时，语言模型的后面接一个线性输出层，以预测任务的标签。GPT的下游任务的监督学习过程，只对预训练Transformer解码器中的所有参数做微调。 GPT只能从左到右  二、BERT BERT的全称是Bidirectional Encoder Representation from Transformers, 即双向Transformer的Encoder。Bert结合了ELMo和GPT的有点，其主要贡献：
 双向的重要性 基于微调的掩码语言模型(Masked Language Modeling)：BERT随机遮掩词元，并使用来自双向上下文的词元以自监督的方式预测该遮掩词元。  1、构造输入 token embedding: 格式：<CLS>第一个文本序列<SEP>第二个文本序列<SEP>
segment embedding: 用来区分句子
position embedding: 在bert中 位置嵌入 是可学习的
def get_tokens_and_segments(tokens_a, tokens_b=None): &#34;&#34;&#34;获取输入序列的词元及其片段索引&#34;&#34;&#34; tokens = ['<cls>'] + tokens_a + ['<sep>'] # 0和1分别标记片段A和B segments = [0] * (len(tokens_a) + 2) if tokens_b is not None: tokens += tokens_b + ['<sep>'] segments += [1] * (len(tokens_b) + 1) return tokens, segments 2、MLM 词元维度"><meta property="og:type" content="article"><meta property="og:url" content="https://biubiobiu.github.io/zh-cn/posts/nlp/bert/bert_summary/"><meta property="article:published_time" content="2021-09-08T06:00:20+06:00"><meta property="article:modified_time" content="2021-09-08T06:00:20+06:00"><meta name=description content="Bert综述"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/zh-cn><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png alt=Logo>
biubiobiu's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu547c3206082786c1d32d36034e2a655a_40863_42x0_resize_box_2.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/zh-cn/search><input type=text name=keyword placeholder=搜索 data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/zh-cn/posts data-filter=all>博文</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/toha-tutorial/>Toha教程</a><ul><li><a href=/zh-cn/posts/toha-tutorial/toha-config/ title=Toha的配置>Toha的配置</a></li><li><a href=/zh-cn/posts/toha-tutorial/write-blogs/ title=撰写文章>撰写文章</a></li><li><a href=/zh-cn/posts/toha-tutorial/markdown-tutorial/ title=MarkDown入门>MarkDown入门</a></li><li><a href=/zh-cn/posts/toha-tutorial/latax_formula/ title=Katex公式>Katex公式</a></li><li><a href=/zh-cn/posts/toha-tutorial/shortcodes_samples/ title=区域块-实例>区域块-实例</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/deeplearning_summary/>深度学习</a><ul><li><a href=/zh-cn/posts/deeplearning_summary/draw_map_for_dl/ title=神经网络画图篇>神经网络画图篇</a></li><li><a href=/zh-cn/posts/deeplearning_summary/deeplearning_start/ title=深度学习开篇>深度学习开篇</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/programming_language/>编程语言</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/programming_language/python/>Python</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/programming_language/python/internal_lib/>Internal</a><ul><li><a href=/zh-cn/posts/programming_language/python/internal_lib/encode_mode/ title=字符编码>字符编码</a></li><li><a href=/zh-cn/posts/programming_language/python/internal_lib/basic_operator/ title=基础操作>基础操作</a></li><li><a href=/zh-cn/posts/programming_language/python/internal_lib/advance_operator/ title=进阶操作>进阶操作</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/programming_language/python/sdk_lib/>SDK</a><ul><li><a href=/zh-cn/posts/programming_language/python/sdk_lib/multiprocessing/ title=并行操作>并行操作</a></li><li><a href=/zh-cn/posts/programming_language/python/sdk_lib/importlib/ title=importlib>importlib</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/programming_language/tf/>TensorFlow</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/programming_language/tf/compat/>兼容1.x</a><ul><li><a href=/zh-cn/posts/programming_language/tf/compat/tf_compat_summary/ title=静态图>静态图</a></li><li><a href=/zh-cn/posts/programming_language/tf/compat/tf_compat_train/ title=模型训练>模型训练</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/programming_language/pytorch/>PyTorch</a><ul><li><a href=/zh-cn/posts/programming_language/pytorch/torch_summary/ title=简介>简介</a></li><li><a href=/zh-cn/posts/programming_language/pytorch/basic/ title=基础操作>基础操作</a></li><li><a href=/zh-cn/posts/programming_language/pytorch/mathematical/ title=数学计算>数学计算</a></li><li><a href=/zh-cn/posts/programming_language/pytorch/tensor/ title=Tensor>Tensor</a></li><li><a href=/zh-cn/posts/programming_language/pytorch/train_model/ title=模型训练>模型训练</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/programming_language/mxnet/>MxNet</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/programming_language/mxnet/ndarray/>NdArray</a><ul><li><a href=/zh-cn/posts/programming_language/mxnet/ndarray/ndarray_summary/ title=NdArray使用>NdArray使用</a></li></ul></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/cv/>CV</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/cv/backbone/>基础</a><ul><li><a href=/zh-cn/posts/cv/backbone/backbone_cnn/ title=CNN>CNN</a></li><li><a href=/zh-cn/posts/cv/backbone/optimizer/ title=optimizer>optimizer</a></li><li><a href=/zh-cn/posts/cv/backbone/backbone_net/ title="backbone net">backbone net</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/cv/contrastive_learning/>对比学习</a><ul><li><a href=/zh-cn/posts/cv/contrastive_learning/contrastive_learning/ title="contrastive learning">contrastive learning</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/cv/vision_transformer/>ViT</a><ul><li><a href=/zh-cn/posts/cv/vision_transformer/vision_transformer/ title="vision transformer">vision transformer</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/cv/detect_object/>目标检测</a><ul><li><a href=/zh-cn/posts/cv/detect_object/object_detection_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/cv/semantic_segmentation/>语义分割</a><ul><li><a href=/zh-cn/posts/cv/semantic_segmentation/object_detection_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/cv/image-matting/>抠图</a><ul><li><a href=/zh-cn/posts/cv/image-matting/image-matting-summary/ title=抠图综述>抠图综述</a></li><li><a href=/zh-cn/posts/cv/image-matting/image-matting-animal/ title="animal matting">animal matting</a></li></ul></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/posts/nlp/>NLP</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/nlp/word_embedding/>Word Embedding</a><ul><li><a href=/zh-cn/posts/nlp/word_embedding/word_embedding_summary/ title="Word Embedding综述">Word Embedding综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/nlp/rnn/>RNN</a><ul><li><a href=/zh-cn/posts/nlp/rnn/rnn_summary/ title=RNN综述>RNN综述</a></li><li><a href=/zh-cn/posts/nlp/rnn/gru/ title=GRU网络>GRU网络</a></li><li><a href=/zh-cn/posts/nlp/rnn/lstm/ title=LSTM网络>LSTM网络</a></li><li><a href=/zh-cn/posts/nlp/rnn/encode_decode/ title=编解码架构>编解码架构</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/nlp/transformer/>Transformer</a><ul><li><a href=/zh-cn/posts/nlp/transformer/attention/ title=Attention>Attention</a></li><li><a href=/zh-cn/posts/nlp/transformer/transformer_summary/ title=Transformer>Transformer</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/nlp/gpt/>GPT</a><ul><li><a href=/zh-cn/posts/nlp/gpt/gpt_summary/ title=GPT综述>GPT综述</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/posts/nlp/bert/>Bert</a><ul class=active><li><a class=active href=/zh-cn/posts/nlp/bert/bert_summary/ title=Bert综述>Bert综述</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/vlp/>多模态</a><ul><li><a href=/zh-cn/posts/vlp/vlp_summary/ title=简介>简介</a></li><li><a href=/zh-cn/posts/vlp/clip/ title=CLIP>CLIP</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/video/>视频理解</a><ul><li><a href=/zh-cn/posts/video/vidio_summary/ title=简介>简介</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/john_hu7f9991f5b5d471ecdfc6cff3db1a9fe6_6397_120x120_fit_box_2.png alt="Author Image"><h5 class=author-name>biubiobiu</h5><p>:date_full</p></div><div class=title><h1>Bert综述</h1></div><div class=taxonomy-terms><ul></ul></div><div class=post-content id=post-content><h2 id=一背景>一、背景</h2><p>在使用预训练模型，处理下游任务时，有两类策略：基于特征(feature-based)、基于微调(fine-tuning)</p><ul><li>基于特征：比如：ELMo，在使用时，对每个下游任务，创建一个跟这个任务相关的神经网络；预训练作为额外的特征跟输入一起输入到模型，预训练的额外特征可能会对要训练的模型有指导作用。</li><li>基于微调：比如：GPT，预训练模型在下游使用时，不需要改动太多，类似于视觉模型的fine-tuning，预训练完成特征提取，预训练模型后面添加个简单的网络用于实现具体任务。</li></ul><h3 id=1上下文敏感>1、上下文敏感</h3><p>在自然语言中，有丰富的多义现象，一个词到底是什么意思，需要参考上下文才能判断。流行的上下文敏感表示：</p><ul><li><a href=https://arxiv.org/abs/1705.00108 target=blank>TagLM</a>(language-model-augmented sequence tagger 语言模型增强的序列标记器)</li><li><a href=https://arxiv.org/abs/1708.00107 target=blank>CoVe</a>(Context Vectors 上下文向量)</li><li><a href=https://arxiv.org/abs/1802.05365 target=blank>ELMo</a>(Embeddings from Language Models 来自语言模型的嵌入)<ol><li>ELMo 将来自预训练LSTM的所有中间层表示组合为输出表示</li><li>ELMo的表示，将作为添加特征添加到下游任务的有监督模型中</li></ol></li></ul><h3 id=2从特定任务到通用任务>2、从特定任务到通用任务</h3><p>ELMo显著改进了自然语言任务，但每个解决方案仍然依赖于一个特定的任务架构。怎么设计一个模型，让各个自然语言任务通用呢？<br><a href=https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf target=blank>GPT</a>(Generative Pre Training 生成式预训练)：在Transformer的基础上，为上下文敏感设计了通用的模型。</p><ol><li>预训练一个用于表示文本序列的语言模型</li><li>当将GPT应用于下游任务时，语言模型的后面接一个线性输出层，以预测任务的标签。GPT的下游任务的监督学习过程，只对预训练Transformer解码器中的所有参数做微调。</li><li>GPT只能从左到右</li></ol><h2 id=二bert>二、BERT</h2><p><a href=https://arxiv.org/abs/1810.04805 target=blank>BERT</a>的全称是Bidirectional Encoder Representation from Transformers, 即双向Transformer的Encoder。Bert结合了ELMo和GPT的有点，其主要贡献：</p><ol><li>双向的重要性</li><li>基于微调的掩码语言模型(Masked Language Modeling)：BERT随机遮掩词元，并使用来自双向上下文的词元以自监督的方式预测该遮掩词元。</li></ol><h3 id=1构造输入>1、<strong>构造输入</strong></h3><p>token embedding: 格式：<code>&lt;CLS></code>第一个文本序列<code>&lt;SEP></code>第二个文本序列<code>&lt;SEP></code><br>segment embedding: 用来区分句子<br>position embedding: 在bert中 位置嵌入 是可学习的<br></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_tokens_and_segments</span>(tokens_a, tokens_b<span style=color:#f92672>=</span>None):
    <span style=color:#e6db74>&#34;&#34;&#34;获取输入序列的词元及其片段索引&#34;&#34;&#34;</span>
    tokens <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;&lt;cls&gt;&#39;</span>] <span style=color:#f92672>+</span> tokens_a <span style=color:#f92672>+</span> [<span style=color:#e6db74>&#39;&lt;sep&gt;&#39;</span>]
    <span style=color:#75715e># 0和1分别标记片段A和B</span>
    segments <span style=color:#f92672>=</span> [<span style=color:#ae81ff>0</span>] <span style=color:#f92672>*</span> (len(tokens_a) <span style=color:#f92672>+</span> <span style=color:#ae81ff>2</span>)
    <span style=color:#66d9ef>if</span> tokens_b <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> None:
        tokens <span style=color:#f92672>+=</span> tokens_b <span style=color:#f92672>+</span> [<span style=color:#e6db74>&#39;&lt;sep&gt;&#39;</span>]
        segments <span style=color:#f92672>+=</span> [<span style=color:#ae81ff>1</span>] <span style=color:#f92672>*</span> (len(tokens_b) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)
    <span style=color:#66d9ef>return</span> tokens, segments
</code></pre></div><p align=center><img src=https://s2.loli.net/2022/05/21/fzAiG9ZIhB3X1c4.jpg width=70% height=70% title=input alt=input></p><h3 id=2mlm>2、<strong>MLM</strong></h3><p><strong>词元维度</strong><br>在预训练任务中，随机选择15%的词元作为预测的遮掩词元。</p><ul><li>80%的概率 替换为特殊词元 <code>&lt;mask></code> （填词）</li><li>10%的概率 替换为 随机词元 （纠错）</li><li>10%的概率 不做任何处理 （作弊）</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MaskLM</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#e6db74>&#34;&#34;&#34;BERT的掩蔽语言模型任务&#34;&#34;&#34;</span>
    <span style=color:#66d9ef>def</span> __init__(self, vocab_size, num_hiddens, num_inputs<span style=color:#f92672>=</span><span style=color:#ae81ff>768</span>, <span style=color:#f92672>**</span>kwargs):
        super(MaskLM, self)<span style=color:#f92672>.</span>__init__(<span style=color:#f92672>**</span>kwargs)
        self<span style=color:#f92672>.</span>mlp <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(nn<span style=color:#f92672>.</span>Linear(num_inputs, num_hiddens),
                                 nn<span style=color:#f92672>.</span>ReLU(),
                                 nn<span style=color:#f92672>.</span>LayerNorm(num_hiddens),
                                 nn<span style=color:#f92672>.</span>Linear(num_hiddens, vocab_size))

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, X, pred_positions):
        num_pred_positions <span style=color:#f92672>=</span> pred_positions<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]
        pred_positions <span style=color:#f92672>=</span> pred_positions<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
        batch_size <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
        batch_idx <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, batch_size)
        <span style=color:#75715e># 假设batch_size=2，num_pred_positions=3</span>
        <span style=color:#75715e># 那么batch_idx是np.array（[0,0,0,1,1,1]）</span>
        <span style=color:#75715e># batch_idx: batch * 序列大小</span>
        batch_idx <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>repeat_interleave(batch_idx, num_pred_positions)
        masked_X <span style=color:#f92672>=</span> X[batch_idx, pred_positions]
        <span style=color:#75715e># masked_x的形状：（batch, 每个序列中被mask词的个数, 词元特征维度）</span>
        masked_X <span style=color:#f92672>=</span> masked_X<span style=color:#f92672>.</span>reshape((batch_size, num_pred_positions, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>))
        mlm_Y_hat <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>mlp(masked_X)
        <span style=color:#75715e># 输出mlm_Y_hat形状：（batch, 每个序列中被mask词的个数, vocab_size）</span>
        <span style=color:#66d9ef>return</span> mlm_Y_hat

</code></pre></div><h3 id=3预测下一句>3、预测下一句</h3><p><strong>句子维度</strong><br>尽管MLM能够使用上下文来表示词元，但它不能显式地建模文本对之间的逻辑关系，为了帮助理解两个文本序列之间的关系，BERT在预训练中考虑了一个二元分类：预测下一句。<br></p><ul><li>在为预训练构建句子对儿时，50%的概率 句子对儿是连续句子；50%的概率 句子对儿不是连续句子。</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>NextSentencePred</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#e6db74>&#34;&#34;&#34;BERT的下一句预测任务&#34;&#34;&#34;</span>
    <span style=color:#66d9ef>def</span> __init__(self, num_inputs, <span style=color:#f92672>**</span>kwargs):
        super(NextSentencePred, self)<span style=color:#f92672>.</span>__init__(<span style=color:#f92672>**</span>kwargs)
        self<span style=color:#f92672>.</span>output <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(num_inputs, <span style=color:#ae81ff>2</span>)

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, X):
        <span style=color:#75715e># X的形状：(batchsize,num_hiddens)</span>
        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>output(X)
</code></pre></div><h3 id=4bert模型>4、bert模型</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>
<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>BERTEncoder</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#e6db74>&#34;&#34;&#34;BERT编码器&#34;&#34;&#34;</span>
    <span style=color:#66d9ef>def</span> __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
                 ffn_num_hiddens, num_heads, num_layers, dropout,
                 max_len<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>, key_size<span style=color:#f92672>=</span><span style=color:#ae81ff>768</span>, query_size<span style=color:#f92672>=</span><span style=color:#ae81ff>768</span>, value_size<span style=color:#f92672>=</span><span style=color:#ae81ff>768</span>,
                 <span style=color:#f92672>**</span>kwargs):
        super(BERTEncoder, self)<span style=color:#f92672>.</span>__init__(<span style=color:#f92672>**</span>kwargs)
        self<span style=color:#f92672>.</span>token_embedding <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding(vocab_size, num_hiddens)
        self<span style=color:#f92672>.</span>segment_embedding <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding(<span style=color:#ae81ff>2</span>, num_hiddens) 
        self<span style=color:#f92672>.</span>blks <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential()
        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(num_layers):
            self<span style=color:#f92672>.</span>blks<span style=color:#f92672>.</span>add_module(f<span style=color:#e6db74>&#34;{i}&#34;</span>, 
                                 d2l<span style=color:#f92672>.</span>EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, 
                                                  ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))
        <span style=color:#75715e># 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数</span>
        self<span style=color:#f92672>.</span>pos_embedding <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>1</span>, max_len, num_hiddens))

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, tokens, segments, valid_lens):
        <span style=color:#75715e># 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）</span>
        X <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>token_embedding(tokens) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>segment_embedding(segments)
        X <span style=color:#f92672>=</span> X <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>pos_embedding<span style=color:#f92672>.</span>data[:, :X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>], :]
        <span style=color:#66d9ef>for</span> blk <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>blks:
            X <span style=color:#f92672>=</span> blk(X, valid_lens)
        <span style=color:#66d9ef>return</span> X

<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>BERTModel</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#e6db74>&#34;&#34;&#34;BERT模型&#34;&#34;&#34;</span>
    <span style=color:#66d9ef>def</span> __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
                 ffn_num_hiddens, num_heads, num_layers, dropout,
                 max_len<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>, key_size<span style=color:#f92672>=</span><span style=color:#ae81ff>768</span>, query_size<span style=color:#f92672>=</span><span style=color:#ae81ff>768</span>, value_size<span style=color:#f92672>=</span><span style=color:#ae81ff>768</span>,
                 hid_in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>768</span>, mlm_in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>768</span>, nsp_in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>768</span>):
        super(BERTModel, self)<span style=color:#f92672>.</span>__init__()
        self<span style=color:#f92672>.</span>encoder <span style=color:#f92672>=</span> BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers,
                    dropout, max_len<span style=color:#f92672>=</span>max_len, key_size<span style=color:#f92672>=</span>key_size, query_size<span style=color:#f92672>=</span>query_size, value_size<span style=color:#f92672>=</span>value_size)
        self<span style=color:#f92672>.</span>hidden <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(nn<span style=color:#f92672>.</span>Linear(hid_in_features, num_hiddens), 
                                    nn<span style=color:#f92672>.</span>Tanh())
        self<span style=color:#f92672>.</span>mlm <span style=color:#f92672>=</span> MaskLM(vocab_size, num_hiddens, mlm_in_features)
        self<span style=color:#f92672>.</span>nsp <span style=color:#f92672>=</span> NextSentencePred(nsp_in_features)

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, tokens, segments, valid_lens<span style=color:#f92672>=</span>None, pred_positions<span style=color:#f92672>=</span>None):
        encoded_X <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>encoder(tokens, segments, valid_lens)
        <span style=color:#75715e># encoded_X 的形状：（批量大小，最大序列长度，num_hiddens）</span>
        <span style=color:#66d9ef>if</span> pred_positions <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> None:
            mlm_Y_hat <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>mlm(encoded_X, pred_positions)
        <span style=color:#66d9ef>else</span>:
            mlm_Y_hat <span style=color:#f92672>=</span> None
        <span style=color:#75715e># 用于下一句预测的多层感知机分类器的隐藏层，0是“&lt;cls&gt;”标记的索引</span>
        nsp_Y_hat <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>nsp(self<span style=color:#f92672>.</span>hidden(encoded_X[:, <span style=color:#ae81ff>0</span>, :]))
        <span style=color:#66d9ef>return</span> encoded_X, mlm_Y_hat, nsp_Y_hat
</code></pre></div></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"><strong>分享:</strong>
<a class="btn btn-sm facebook-btn" href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2fnlp%2fbert%2fbert_summary%2f" target=_blank><i class="fab fa-facebook"></i></a><a class="btn btn-sm twitter-btn" href="https://twitter.com/share?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2fnlp%2fbert%2fbert_summary%2f&text=Bert%e7%bb%bc%e8%bf%b0&via=biubiobiu%27s%20Blog" target=_blank><i class="fab fa-twitter"></i></a><a class="btn btn-sm reddit-btn" href="https://reddit.com/submit?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2fnlp%2fbert%2fbert_summary%2f&title=Bert%e7%bb%bc%e8%bf%b0" target=_blank><i class="fab fa-reddit"></i></a><a class="btn btn-sm linkedin-btn" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2fnlp%2fbert%2fbert_summary%2f&title=Bert%e7%bb%bc%e8%bf%b0" target=_blank><i class="fab fa-linkedin"></i></a><a class="btn btn-sm whatsapp-btn" href="https://api.whatsapp.com/send?text=Bert%e7%bb%bc%e8%bf%b0 https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2fnlp%2fbert%2fbert_summary%2f" target=_blank><i class="fab fa-whatsapp"></i></a><a class="btn btn-sm email-btn" href="mailto:?subject=Bert%e7%bb%bc%e8%bf%b0&body=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2fnlp%2fbert%2fbert_summary%2f" target=_blank><i class="fas fa-envelope-open-text"></i></a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/zh-cn/posts/nlp/gpt/gpt_summary/ title=GPT综述 class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i>上一篇</div><div class=next-prev-text>GPT综述</div></a></div><div class="col-md-6 next-article"><a href=/zh-cn/posts/vlp/vlp_summary/ title=简介 class="btn btn-outline-info"><div>下一篇 <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>简介</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">目录</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#一背景>一、背景</a><ul><li><a href=#1上下文敏感>1、上下文敏感</a></li><li><a href=#2从特定任务到通用任务>2、从特定任务到通用任务</a></li></ul></li><li><a href=#二bert>二、BERT</a><ul><li><a href=#1构造输入>1、<strong>构造输入</strong></a></li><li><a href=#2mlm>2、<strong>MLM</strong></a></li><li><a href=#3预测下一句>3、预测下一句</a></li><li><a href=#4bert模型>4、bert模型</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>导航</h5><ul><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#about>关于</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#skills>技能</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#experiences>经验</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#projects>项目</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#education>教育</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#recent-posts>最新博客</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#accomplishments>造诣</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#achievements>业绩</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>联系方式:</h5><ul><li><span>电话:</span> <span>+0123456789</span></li><li><span>邮箱:</span> <span>bsgshyn8@163.com</span></li></ul></div></div></div><hr><div class=container><p id=disclaimer><strong>免责声明:</strong> 这个主题是MIT许可的。因此，您可以将其用于非商业、商业或者私人用途。您可以修改或分发主题，而无须经 作者任何许可。但是，主题作者不对主题的任何问题提供任何保证或承担任何责任。</p></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>