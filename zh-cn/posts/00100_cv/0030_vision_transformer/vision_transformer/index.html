<!doctype html><html><head><title>vision transformer</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0d88336a9cbd3e68c7714efae786b0a9_38044_42x0_resize_box_2.png><meta property="og:title" content="vision transformer"><meta property="og:description" content="一、简介 1、Transformer用在CV领域 在NLP中，Transformer的输入是一个时间步长为T的序列，比如：basic版bert，T=512，每个token embeding为768维特征。如何把二维图片转化为一维呢？
 $\bf \color{red} \times$ 如果把每个像素点看做是一个样本，铺平后是一维序列。但是，图片大小 224*224=50176，远远大于Transformer的最大序列长度。 $\bf \color{red} \times$ 卷积和Transformer一起用：《Non-local Neural Networks》(2018)、《End-to-End Object Detection with Transformers》(2020) 为了减小序列的长度，不直接使用输入图片，而是使用feature map 转换为序列。比如：ResNet50在最后的阶段的输出尺寸为 14x14，拉平后序列长度只有196。 $\bf \color{red} \times$ 抛弃卷积使用定制化的自注意力机制：《Stand-Alone Self-Attention in Vision Models》(2019) 采用的是 孤立自注意力。用一个局部的小窗口做自注意力； 《Stand-alone axial-attention for panoptic segmentation》(2020) 采用的是轴注意力。在高度的方向上做自注意力、在宽度方向做自注意力。由于这些自注意力机制比较定制化，还没有在硬件上大规模加速计算，所以网络做不大。 $\color{green} \checkmark$ 对图片做些预处理，直接使用Transformer：将图片切分成一个个patch，然后每个patch作为一个token输入到Transformer中。  $224 \times 224$ 的图片，切分成一个个 $16 \times 16$ 的patch，最终切分出196个patch；每个patch的大小是：$16 \times 16 \times 3=768$，刚好是basic版bert每个token的维度。 多头注意力机制，12个头，每个头的k、q、v对应的维度是64维    二、网络 1、ViT ViT(2021) 直接把Transformer应用到图像处理，尽量改动最少，所以只对图像做预处理，让其符合NLP的输入形式， 思路：
 图片尺寸 224x224，将图片切分成一个个patch，patch的大小16x16，每个patch作为一个token，即：14x14=196个patch，每个patch长16x16x3=768 学习一个线性矩阵$E$，尺寸为768x768，对每个patch做线性变换。多头注意力的话，basic版本12个头，所以12个196x64拼接起来，还是196x768。 位置编码：可学习的位置向量，尺寸为196x768 cls的输出作为提取的图片特征，用于后续的分类操作  实验结论："><meta property="og:type" content="article"><meta property="og:url" content="https://biubiobiu.github.io/zh-cn/posts/00100_cv/0030_vision_transformer/vision_transformer/"><meta property="article:published_time" content="2022-05-09T06:00:20+06:00"><meta property="article:modified_time" content="2022-05-09T06:00:20+06:00"><meta name=description content="vision transformer"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/zh-cn><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png alt=Logo>
biubiobiu's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu547c3206082786c1d32d36034e2a655a_40863_42x0_resize_box_2.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/zh-cn/search><input type=text name=keyword placeholder=搜索 data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/zh-cn/posts data-filter=all>博文</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00020_toha-tutorial/>Toha教程</a><ul><li><a href=/zh-cn/posts/00020_toha-tutorial/0010_toha-config/ title=Toha的配置>Toha的配置</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0011_write-blogs/ title=撰写文章>撰写文章</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0013_markdown-tutorial/ title=MarkDown入门>MarkDown入门</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0015_latax_formula/ title=Katex公式>Katex公式</a></li><li><a href=/zh-cn/posts/00020_toha-tutorial/0020_shortcodes_samples/ title=区域块-实例>区域块-实例</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00025_math_knowledge/>数学知识</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/>概率论</a><ul><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0010_basic-conception/ title=基本概念>基本概念</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0020_distribution_of_variables/ title=随机变量及其分布>随机变量及其分布</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0030_law_of_large_numbers/ title=大数定律>大数定律</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0040_sample_distribution/ title=样本及抽样分布>样本及抽样分布</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0050_parameter_estimation/ title=假设检验>假设检验</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0060_analysis_variance_regression/ title=方差分析及回归分析>方差分析及回归分析</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0070_stochastic_process/ title=随机过程>随机过程</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0080_markov_process/ title=马尔科夫链>马尔科夫链</a></li><li><a href=/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0090_stationary_stochastic_process/ title=平稳随机过程>平稳随机过程</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00025_math_knowledge/0020_math_convex_optimization_theory/>凸优化</a><ul><li><a href=/zh-cn/posts/00025_math_knowledge/0020_math_convex_optimization_theory/0010_basic_conception/ title=基本概念>基本概念</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00030_deeplearning_summary/>深度学习</a><ul><li><a href=/zh-cn/posts/00030_deeplearning_summary/0005_deeplearning_summary/ title=深度学习开篇>深度学习开篇</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0010_deeplearning_problem/ title=深度学习-结构>深度学习-结构</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0015_deeplearning_norm/ title=归一化>归一化</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0020_deeplearing_init/ title=初始化>初始化</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0100_draw_map_for_dl/ title=神经网络画图篇>神经网络画图篇</a></li><li><a href=/zh-cn/posts/00030_deeplearning_summary/0200_cam/ title=CAM>CAM</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00033_reinforce/>强化学习</a><ul><li><a href=/zh-cn/posts/00033_reinforce/0001_reinforce_summary/ title=综述>综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/>编程语言</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/>Python</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/0010_build_env/>env</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0010_build_env/0010_pip_env/ title=py-env>py-env</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0010_build_env/0020_cuda_env/ title=cuda>cuda</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/>Internal</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0010_encode_mode/ title=字符编码>字符编码</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0020_basic_operator/ title=基础操作>基础操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0025_internal_module/ title=内置模块>内置模块</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0030_advance_operator/ title=进阶操作>进阶操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0040_error/ title=异常>异常</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0050_file/ title=文件读取>文件读取</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/>SDK</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0020_multiprocessing/ title=并行操作>并行操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0030_importlib/ title=importlib>importlib</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0040_ipdb/ title=ipdb>ipdb</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0050_re/ title=正则-re>正则-re</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0060_heapd/ title=堆-heapd>堆-heapd</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0070_request/ title=requests>requests</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0080_logging/ title=logging>logging</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0090_argparse/ title=argparse>argparse</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0100_pil/ title=PIL>PIL</a></li><li><a href=/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0110_opencv/ title=OpenCV>OpenCV</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0040_tf/>TensorFlow</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0040_tf/w0010_compat/>兼容1.x</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0040_tf/w0010_compat/0010_tf_compat_summary/ title=静态图>静态图</a></li><li><a href=/zh-cn/posts/00035_programming_language/0040_tf/w0010_compat/0020_tf_compat_train/ title=模型训练>模型训练</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/>PyTorch</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0010_torch_summary/ title=简介>简介</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0020_basic/ title=基础操作>基础操作</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0030_mathematical/ title=数学计算>数学计算</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0040_tensor/ title=Tensor和变量>Tensor和变量</a></li><li><a href=/zh-cn/posts/00035_programming_language/0050_pytorch/0050_train_model/ title=模型训练>模型训练</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0060_mxnet/>MxNet</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00035_programming_language/0060_mxnet/0010_ndarray/>NdArray</a><ul><li><a href=/zh-cn/posts/00035_programming_language/0060_mxnet/0010_ndarray/0010_ndarray_summary/ title=NdArray使用>NdArray使用</a></li></ul></li></ul></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/posts/00100_cv/>CV</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0010_backbone/>基础</a><ul><li><a href=/zh-cn/posts/00100_cv/0010_backbone/d1_1_backbone_cnn/ title=CNN>CNN</a></li><li><a href=/zh-cn/posts/00100_cv/0010_backbone/d1_2_optimizer/ title=optimizer>optimizer</a></li><li><a href=/zh-cn/posts/00100_cv/0010_backbone/d1_3_backbone_net/ title="backbone net">backbone net</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0020_contrastive_learning/>对比学习</a><ul><li><a href=/zh-cn/posts/00100_cv/0020_contrastive_learning/contrastive_learning/ title="contrastive learning">contrastive learning</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/zh-cn/posts/00100_cv/0030_vision_transformer/>ViT</a><ul class=active><li><a class=active href=/zh-cn/posts/00100_cv/0030_vision_transformer/vision_transformer/ title="vision transformer">vision transformer</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0050_detect_object/>目标检测</a><ul><li><a href=/zh-cn/posts/00100_cv/0050_detect_object/object_detection_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0055_semantic_segmentation/>语义分割</a><ul><li><a href=/zh-cn/posts/00100_cv/0055_semantic_segmentation/object_detection_summary/ title=简介>简介</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00100_cv/0060_image-matting/>抠图</a><ul><li><a href=/zh-cn/posts/00100_cv/0060_image-matting/image-matting-summary/ title=抠图综述>抠图综述</a></li><li><a href=/zh-cn/posts/00100_cv/0060_image-matting/image-matting-animal/ title="animal matting">animal matting</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/>NLP</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0010_word_embedding/>Word Embedding</a><ul><li><a href=/zh-cn/posts/00200_nlp/0010_word_embedding/word_embedding_summary/ title="Word Embedding综述">Word Embedding综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0020_rnn/>RNN</a><ul><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/rnn_summary/ title=RNN综述>RNN综述</a></li><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/gru/ title=GRU网络>GRU网络</a></li><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/lstm/ title=LSTM网络>LSTM网络</a></li><li><a href=/zh-cn/posts/00200_nlp/0020_rnn/encode_decode/ title=编解码架构>编解码架构</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0030_transformer/>Transformer</a><ul><li><a href=/zh-cn/posts/00200_nlp/0030_transformer/0001_attention/ title=Attention>Attention</a></li><li><a href=/zh-cn/posts/00200_nlp/0030_transformer/0010_transformer_summary/ title=Transformer>Transformer</a></li><li><a href=/zh-cn/posts/00200_nlp/0030_transformer/0030_position/ title=位置编码>位置编码</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0080_gpt/>GPT</a><ul><li><a href=/zh-cn/posts/00200_nlp/0080_gpt/0010_gpt_summary/ title=GPT综述>GPT综述</a></li><li><a href=/zh-cn/posts/00200_nlp/0080_gpt/0020_gpt1_detail/ title=GPT-1>GPT-1</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0100_bert/>Bert</a><ul><li><a href=/zh-cn/posts/00200_nlp/0100_bert/0001_bert_summary/ title=Bert综述>Bert综述</a></li><li><a href=/zh-cn/posts/00200_nlp/0100_bert/0020_bert_family/ title=Bert家族>Bert家族</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0120_t5/>T5</a><ul><li><a href=/zh-cn/posts/00200_nlp/0120_t5/0010_t5_summary/ title=T5综述>T5综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0150_bart/>BART</a><ul><li><a href=/zh-cn/posts/00200_nlp/0150_bart/bart_summary/ title=BART综述>BART综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/0200_electra/>ELECTRA</a><ul><li><a href=/zh-cn/posts/00200_nlp/0200_electra/electra_summary/ title=ELECTRA综述>ELECTRA综述</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00200_nlp/1000_code/>CODE</a><ul><li><a href=/zh-cn/posts/00200_nlp/1000_code/bart_summary/ title=code解析>code解析</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/>AIGC</a><ul><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/0005_summary/>AIGC综述</a><ul><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0001_aigc_summary/ title=综述>综述</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0005_aigc_application/ title=模型应用策略>模型应用策略</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0010_aigc_train_p/ title=大模型训练框架>大模型训练框架</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0012_aigc_amp/ title=混合精度训练>混合精度训练</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0015_aigc_train_mini/ title=模型小型化>模型小型化</a></li><li><a href=/zh-cn/posts/00300_aigc/0005_summary/0020_aigc_error/ title=生成式-问题>生成式-问题</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/0010_generate_text/>文本生成</a><ul><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0011_gpt/ title=GPT>GPT</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0015_llama/ title=LLaMa>LLaMa</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0020_palm/ title=PaLM>PaLM</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0025_chatglm/ title=ChatGLM>ChatGLM</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0030_claude/ title=Claude>Claude</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0035_cohere/ title=Cohere>Cohere</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0040_falcon/ title=Falcon>Falcon</a></li><li><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0045_vicuna/ title=Vicuna>Vicuna</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00300_aigc/0020_generate_image/>图像生成</a><ul><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1000_gan_summary/ title=GAN>GAN</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1005_can_summary/ title=CAN>CAN</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1010_dall_e/ title=DALL-E>DALL-E</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1015_vqgan/ title=VQGAN>VQGAN</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1020_diffusion/ title=Diffusion>Diffusion</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1025_midjourney/ title=Midjourney>Midjourney</a></li><li><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1030_imagen/ title=Imagen>Imagen</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00400_vlp/>多模态</a><ul><li><a href=/zh-cn/posts/00400_vlp/0001_vlp_summary/ title=简介>简介</a></li><li><a href=/zh-cn/posts/00400_vlp/0005_clip/ title=CLIP>CLIP</a></li><li><a href=/zh-cn/posts/00400_vlp/0010_mllm/ title=MLLM>MLLM</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/zh-cn/posts/00500_video/>视频理解</a><ul><li><a href=/zh-cn/posts/00500_video/vidio_summary/ title=简介>简介</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/john_hu7f9991f5b5d471ecdfc6cff3db1a9fe6_6397_120x120_fit_box_2.png alt="Author Image"><h5 class=author-name>biubiobiu</h5><p>May 9, 2022</p></div><div class=title><h1>vision transformer</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/zh-cn/tags/backbone class="btn, btn-sm">backbone</a></li><li class=rounded><a href=/zh-cn/tags/vision-transformer class="btn, btn-sm">vision transformer</a></li></ul></div><div class=post-content id=post-content><h2 id=一简介>一、简介</h2><h3 id=1transformer用在cv领域>1、Transformer用在CV领域</h3><p>在NLP中，Transformer的输入是一个时间步长为T的序列，比如：basic版bert，T=512，每个token embeding为768维特征。如何把二维图片转化为一维呢？<br></p><ul><li>$\bf \color{red} \times$ 如果把每个像素点看做是一个样本，铺平后是一维序列。但是，图片大小 224*224=50176，远远大于Transformer的最大序列长度。</li><li>$\bf \color{red} \times$ 卷积和Transformer一起用：<a href=https://arxiv.org/abs/1711.07971 target=blank>《Non-local Neural Networks》</a>(2018)、<a href=https://arxiv.org/abs/2005.12872 target=blank>《End-to-End Object Detection with Transformers》</a>(2020) 为了减小序列的长度，不直接使用输入图片，而是使用feature map 转换为序列。比如：ResNet50在最后的阶段的输出尺寸为 14x14，拉平后序列长度只有196。</li><li>$\bf \color{red} \times$ 抛弃卷积使用定制化的自注意力机制：<a href=https://arxiv.org/abs/1906.05909 target=blank>《Stand-Alone Self-Attention in Vision Models》</a>(2019) 采用的是 孤立自注意力。用一个局部的小窗口做自注意力； <a href=https://arxiv.org/abs/2003.07853 target=blank>《Stand-alone axial-attention for panoptic segmentation》</a>(2020) 采用的是轴注意力。在高度的方向上做自注意力、在宽度方向做自注意力。由于这些自注意力机制比较定制化，还没有在硬件上大规模加速计算，所以网络做不大。</li><li>$\color{green} \checkmark$ 对图片做些预处理，直接使用Transformer：将图片切分成一个个patch，然后每个patch作为一个token输入到Transformer中。<ol><li>$224 \times 224$ 的图片，切分成一个个 $16 \times 16$ 的patch，最终切分出196个patch；每个patch的大小是：$16 \times 16 \times 3=768$，刚好是basic版bert每个token的维度。</li><li>多头注意力机制，12个头，每个头的k、q、v对应的维度是64维</li></ol></li></ul><h2 id=二网络>二、网络</h2><h3 id=1vit>1、ViT</h3><p><a href=https://arxiv.org/abs/2010.11929 target=blank>ViT</a>(2021) 直接把Transformer应用到图像处理，尽量改动最少，所以只对图像做预处理，让其符合NLP的输入形式， 思路：</p><ol><li>图片尺寸 224x224，将图片切分成一个个patch，patch的大小16x16，每个patch作为一个token，即：14x14=196个patch，每个patch长16x16x3=768</li><li>学习一个线性矩阵$E$，尺寸为768x768，对每个patch做线性变换。多头注意力的话，basic版本12个头，所以12个196x64拼接起来，还是196x768。</li><li>位置编码：可学习的位置向量，尺寸为196x768</li><li>cls的输出作为提取的图片特征，用于后续的分类操作</li></ol><p align=center><img src=/datasets/posts/cnn/vit.jpg width=70% height=70% title=ViT alt=ViT></p><p>实验结论：</p><ol><li>额外使用cls做分类，为啥不用GAP呢？作者对比了这两种方式，发现如果参数调整好的话效果是一样的。所以为了在使用Transformer时改动最少，所以继续使用cls作为分类。</li><li>位置编码信息，每个位置编码信息是一个768维度的向量。图像是二维的，所以是否需要在位置编码里体现二维特征呢？作者实验发现，即使是一维的768维度的向量，其中也会学习到二维特征，所以没有必要故意设计二维特征。</li><li>归纳偏置(inductive bias)，在cnn中位置信息是贯穿在所有卷积操作中的，卷积是线性操作，具有偏移不变性。在ViT中除了添加了位置编码信息，是没有其他的空间信息的。所以作者认为是这个原因导致ViT在小规模的数据集上效果不好，在中/大规模的数据集上效果较好。</li><li>Transformer有处理长文本的能力，所以ViT能够捕获图片的全局信息，而并不是像cnn只用到感受野区域。</li><li>作者还提出是否可以类比BERT，mask掉一些patch，然后自监督训练，修复出mask掉的区域。这就是后来何大神的MAE。</li><li>ViT可以说是打通了CV和NLP的鸿沟，对多模态具有重要的意义。</li></ol><p>可视化显示模型能学习的信息：<div class=row><div class="col col-sm-12 col-lg-6"><p align=center><img src=/datasets/posts/cnn/vit-1.jpg title=ViT alt=ViT></p>可学习的线性矩阵$\bf E$，具体学习的信息：跟CNN很像，可以学习到颜色、纹理、等底层信息。</div><div class="col col-sm-12 col-lg-6"><p align=center><img src=/datasets/posts/cnn/vit-2.jpg width=70% height=70% title=ViT alt=ViT></p>位置信息：发现可以学习到位置信息，同时也可以学习到行、列的信息；这就是为啥没有必要设计2维的位置信息。</div></div></p><div class=row><div class="col col-sm-12 col-lg-6"><p align=center><img src=/datasets/posts/cnn/vit-3.jpg width=80% height=80% title=ViT alt=ViT></p>图中，每列都有16个点，就是16个头的输出；纵轴：平均注意力的距离；横轴：网络的层数。图展示了ViT能不能注意到全局信息: a. 在前几层有相近的、距离远的，这说明，在刚开始模型就能注意到较远的像素，而不像CNN前几层因为感受野较小只能注意到相近的像素点；b. 在后几层距离都比较远，说明网络学习到的特征越来越具有语义信息。</div><div class="col col-sm-12 col-lg-6"><p align=center><img src=/datasets/posts/cnn/vit-4.jpg width=68% height=68% title=ViT alt=ViT></p>作者用网络的最后一层的输出，映射回输入图片上，发现模型是可以获取图像的高阶语义信息，是可以关注到用于分类的图像区域。</div></div><h3 id=2beit>2、BEiT</h3><p><a href=https://arxiv.org/abs/2106.08254 target=blank>《BEiT: BERT Pre-Training of Image Transformers》</a>(2021)</p><h3 id=3pvt>3、PVT</h3><p><a href=https://arxiv.org/abs/2102.12122 target=blank>《Pyramid Vision Transformer》</a>(2021)</p><h3 id=4swin>4、Swin</h3><p><a href=https://arxiv.org/abs/2103.14030 target=blank>Swin Transformer</a>(2021) 微软亚研究院发表在ICCV上的一篇文章，获取2021 best paper。
<a href=https://github.com/microsoft/Swin-Transformer target=blank>github</a></p><p>把Transformer从NLP应用到vision领域，有两个挑战：</p><ol><li>图中物体尺寸不一，比如一个行人、一辆骑车，在图中尺寸不同；就算是两个行人，也有大有小。NLP的同一个词，在图片中尺寸可能差别很大。</li><li>图像分辨率太大，如果以像素点为基本单位的话，则序列就非常大。为了减少序列长度，一些方法：使用feature</li></ol><p>作者为了解决这两个问题，提出了Swin：</p><ol><li>通过<code>移动窗口</code>学习出 序列特征作为Transformer的输入；</li><li>移动窗口(shifted window)能够使得相邻的两个窗口有了交互，变相的达到全局建模的能力。</li><li>层级结构(hierarchical architecture)，非常灵活，不仅可以提供<code>不同尺度</code>的特征信息；而且计算复杂度跟图片大小成线性关系</li></ol><hr><div class=row><div class="col col-sm-12 col-lg-6"><p align=center><img src=/datasets/posts/cnn/swin_0.jpg width=90% height=90% title=swin alt=swin></p><ol><li>ViT：每个token代表的尺寸都是一样的，每一层看到的token的尺寸都是一样的；虽说通过多注意力机制能够把握全局信息，但是在多尺寸特征上的把握是不够的。而在vision中多尺寸的的特征是很重要的。</li><li>ViT：多注意力机制是在全图上计算，所以它的计算复杂度跟图片尺寸成平方的关系；</li><li>Swin：在小窗口内计算多注意力，因为在视觉里有这样的先验：相邻的区域大概率是相似物体。对于视觉直接做全局注意力是浪费的</li><li>Swin：在卷积操作中，由于pooling操作提供了不同尺寸的特征；类比pooling，作者提出了patch merging：把相邻的小patch合并成一个大patch</li></ol></div><div class="col col-sm-12 col-lg-6"><p align=center><img src=/datasets/posts/cnn/swin_1.jpg width=90% height=90% title=swin alt=swin></p><p>怎么计算多注意力？滑动窗口的设计？<br>图中灰框：是 $4\times4$ 的小块；rgb三通道拉直后就是 $1\times48$<br>图中红框：是一个包含 $7\times7$ 个小块的窗口，即：$7\times7\times48$。多注意力机制就是在这些窗口中计算的。<br>滑动窗口：</p><p align=center><img src=/datasets/posts/cnn/swin_win.jpg width=90% height=90% title=swin alt=swin></p><ol><li>在窗口里做多头注意力计算，只能关注到这个窗口的信息；只这样操作，就违背了Transformer的初衷(把握上下文)，所以作者采用滑动窗口，下一层的窗口与上一层的多个窗口相交，这样多个窗口之间就有了联系。作者提出的patch merging 合并到Transformer最后几层时，就会看到大部分图片的信息了。</li><li>为了统一的标准化计算，采用循环移位；<ul><li>向右下移动两个位置；上面多出的部分循环移动到下面、左边多出的部分循环移动到右边、左上角多出的部分循环移动到右下角</li><li>在窗口内，循环移动的这些块之间 在图片中是不相邻的，所以不应该计算它们之间的联系(比如：上面是天空下面是地面)。作者就采用masked MSA</li><li>在计算完注意力后，把移动的块复原到原来的位置；保证信息的一致性</li></ul></li></ol></div></div><hr><p><strong>网络架构</strong></p><p align=center><img src=/datasets/posts/cnn/swin_net.jpg width=70% height=70% title="swin net" alt="swin net"></p><ol><li><p>输入图片：$224\times224\times3$，通过patch partition操作，把 $4\times4$ 的小patch拉直后，变成：$56\times56\times48$</p></li><li><p>通过4个Swin Transformer块，生成不同尺度的特征。</p><ol><li>第一个Swin Tranformer块 没有使用patch merging，尺寸信息为 $56\times56\times96$，其中 $C=96$；剩余的模块，都经过patch merging：让H、W减半，在channel上扩大2倍。即：$56\times56\times96 \rArr 28\times28\times192 \rArr 14\times14\times384 \rArr 7\times7\times768$ (768怎么这么熟悉^_^)</li><li>每个Swin Transformer块，都含有两个Transformer块，第一个采用W-MSA（窗口-多头自注意力），第二个相匹配的采用SW-MSA（滑动窗口-多头自注意力）。</li></ol></li><li><p>不同变体：</p><ol><li>Swin-T：$C=96, layer numbers = {2, 2, 6, 2}$ ，计算复杂度与ResNet50差不多</li><li>Swin-S：$C=96, layer numbers = {2, 2, 18, 2}$ ，计算复杂度与ResNet101差不多</li><li>Swin-B：$C=128, layer numbers = {2, 2, 18, 2}$</li><li>Swin-L：$C=192, layer numbers = {2, 2, 18, 2}$</li></ol></li></ol><div class=row><div class="col col-sm-12 col-lg-6"><p><strong>patch merging</strong></p><p>其中，patch merging 的操作如下图所示：</p><p align=center><img src=/datasets/posts/cnn/swin_merge.png width=90% height=90% title="patch merging" alt="patch merging"></p><ol><li>把相邻 $2*2$ 的patch块，拉伸到channel维度；使得H、W方向降维，C方向升维</li><li>拉伸后变成 $4C$，如果希望是 $2C$的话，后续接一个全连接层</li></ol></div><div class="col col-sm-12 col-lg-6"><p><strong>masked MSA</strong></p><p align=center><img src=/datasets/posts/cnn/swin_mask-msa.png width=90% height=90% title="masked MSA" alt="masked MSA"></p><ol><li>只要不是自己区域的向量相乘，就需要被mask掉</li><li>掩码矩阵：需要mask的区域为：-100，不需要mask掉的区域为：0。</li><li>softmax(计算好的自注意力矩阵（就是那个权重） + 掩码矩阵)，-100经过softmax后近似为0了。</li></ol></div></div><hr><div class=row><div class="col col-sm-12 col-lg-6"><p><strong>W-MSA</strong></p><p align=center><img src=/datasets/posts/cnn/swin-wmsa.png width=90% height=90% title=w-msa alt=w-msa></p><ol><li>在每个窗口中做多头自注意力计算，各个窗口之间是没有联系的</li><li>计算复杂度大概：$4hwC^2+2hwM^2C$，相比于全图片做自注意力计算($4hwC^2+2(hw)^2C$)，计算效率提升不少</li></ol></div><div class="col col-sm-12 col-lg-6"><p><strong>SW-MSA</strong></p><p align=center><img src=/datasets/posts/cnn/swin-sw-msa.png width=90% height=90% title=sw-msa alt=sw-msa></p><ol><li>如果只在窗口内各自计算注意力，那么就没有整个图片上下文；通过移动窗口来使得相邻的窗口之间有联系</li><li>窗口的大小固定为 $7\times7$，由于patch merging类似于pooling操作，所以随着层数的加深，窗口看到的感受野越来越大</li></ol></div></div><h3 id=5mae>5、MAE</h3><p><a href=https://arxiv.org/abs/2111.06377 target=blank>MAE</a>(2021)
主要思想：随机mask掉一些patch，然后重构这些patch里的所有像素。</p><ol><li>设计非对称的encoder-decoder架构<br>encoder：作用在非mask的patch；将这些观察到的信息，映射到一个潜表示（在语义空间上的表示）<br>decoder：是一个轻量级的解码器，从这个潜表示中重构原始信号；</li><li>模型架构就是ViT，不一样的是输入是非mask的patchs<ul><li>高比例的mask是比较有效的，因为低比例的mask可以通过简单的插值就能重组，使得模型学不到什么东西；高比例的mask迫使模型学习更有效的表征；</li><li>由于参与计算的是非mask的patch，高比例的mask使得计算速度加快好几倍。</li></ul></li><li>图片patch与文本token的区别：图像只是被记录下来的光，没有语义分解成视觉上的词。后续的工作可以是：mask掉 不能构建语义段的patchs，就是这些patchs没有主体，只是包含主体的一小部分。<ul><li>由于patch不是一个word，不是独立的语义，可能跟其他patch构成一个独立的语义word，这就说明图片相对于文本冗余信息太多，即使mask掉好多信息也可以重构出图片</li><li>模型学习到图片的全局信息，可以通过一些局部信息重构图片</li></ul></li></ol><p align=center><img src=/datasets/posts/cnn/mae.jpg width=50% height=50% title=MAE alt=MAE></p><hr><h3 id=6>6、</h3><p>微软研究院提出的<a href=https://arxiv.org/abs/2107.00641 target=blank>Focal Transformer</a>在分类/检测/分割任务上表现SOTA！在ADE20K 语义分割上高达55.4 mIoU<br></p><p>中科大、微软亚洲研究院提出的<a href=https://arxiv.org/abs/2107.00652 target=blank>CSWin</a>在ImageNet上高达87.5%准确率，在ADE20K上高达55.2mIoU
<a href=https://github.com/microsoft/CSWin-Transformer target=blank>github</a><br></p><p>北大提出的<a href=https://arxiv.org/abs/2107.00420 target=blank>CBNetV2</a>
<a href=https://github.com/VDIGPKU/CBNetV2 target=blank>github</a><br><br></p><h2 id=三cv各领域>三、CV各领域</h2><h3 id=1目标检测>1、目标检测</h3><h4 id=1-detrdetection-transformer>1). DETR(Detection Transformer)</h4><p><a href=https://arxiv.org/abs/2005.12872 target=blank>《End-to-End Object Detection with Transformers》</a>(2020)，是Transformer在目标检测领域的开山之作。</p><h3 id=2图像分割>2、图像分割</h3></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"><strong>分享:</strong>
<a class="btn btn-sm facebook-btn" href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00100_cv%2f0030_vision_transformer%2fvision_transformer%2f" target=_blank><i class="fab fa-facebook"></i></a><a class="btn btn-sm twitter-btn" href="https://twitter.com/share?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00100_cv%2f0030_vision_transformer%2fvision_transformer%2f&text=vision%20transformer&via=biubiobiu%27s%20Blog" target=_blank><i class="fab fa-twitter"></i></a><a class="btn btn-sm reddit-btn" href="https://reddit.com/submit?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00100_cv%2f0030_vision_transformer%2fvision_transformer%2f&title=vision%20transformer" target=_blank><i class="fab fa-reddit"></i></a><a class="btn btn-sm linkedin-btn" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00100_cv%2f0030_vision_transformer%2fvision_transformer%2f&title=vision%20transformer" target=_blank><i class="fab fa-linkedin"></i></a><a class="btn btn-sm whatsapp-btn" href="https://api.whatsapp.com/send?text=vision%20transformer https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00100_cv%2f0030_vision_transformer%2fvision_transformer%2f" target=_blank><i class="fab fa-whatsapp"></i></a><a class="btn btn-sm email-btn" href="mailto:?subject=vision%20transformer&body=https%3a%2f%2fbiubiobiu.github.io%2fzh-cn%2fposts%2f00100_cv%2f0030_vision_transformer%2fvision_transformer%2f" target=_blank><i class="fas fa-envelope-open-text"></i></a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/zh-cn/posts/00100_cv/0020_contrastive_learning/contrastive_learning/ title="contrastive learning" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i>上一篇</div><div class=next-prev-text>contrastive learning</div></a></div><div class="col-md-6 next-article"><a href=/zh-cn/posts/00100_cv/0050_detect_object/object_detection_summary/ title=简介 class="btn btn-outline-info"><div>下一篇 <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>简介</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">目录</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#一简介>一、简介</a><ul><li><a href=#1transformer用在cv领域>1、Transformer用在CV领域</a></li></ul></li><li><a href=#二网络>二、网络</a><ul><li><a href=#1vit>1、ViT</a></li><li><a href=#2beit>2、BEiT</a></li><li><a href=#3pvt>3、PVT</a></li><li><a href=#4swin>4、Swin</a></li><li><a href=#5mae>5、MAE</a></li><li><a href=#6>6、</a></li></ul></li><li><a href=#三cv各领域>三、CV各领域</a><ul><li><a href=#1目标检测>1、目标检测</a><ul><li><a href=#1-detrdetection-transformer>1). DETR(Detection Transformer)</a></li></ul></li><li><a href=#2图像分割>2、图像分割</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>导航</h5><ul><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#about>关于</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#skills>技能</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#experiences>经验</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#projects>项目</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#education>教育</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#recent-posts>最新博客</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#accomplishments>造诣</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#achievements>业绩</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>联系方式:</h5><ul><li><span>电话:</span> <span>+0123456789</span></li><li><span>邮箱:</span> <span>bsgshyn8@163.com</span></li></ul></div></div></div><hr><div class=container><p id=disclaimer><strong>免责声明:</strong> 这个主题是MIT许可的。因此，您可以将其用于非商业、商业或者私人用途。您可以修改或分发主题，而无须经 作者任何许可。但是，主题作者不对主题的任何问题提供任何保证或承担任何责任。</p></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=stylesheet href=/katex/katex.min.css><script type=text/javascript defer src=/katex/katex.min.js></script><script type=text/javascript defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body);></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false},{left:'\\(',right:'\\)',display:false},{left:'\\[',right:'\\]',display:true}],throwOnError:false});});</script></body></html>