[{"categories":null,"contents":"查阅文档 怎么查阅相关文档？ 官网\n1. 查阅模块里的所有函数和类 from mxnet import nd print(dir(nd.random))  __开头和结尾的函数 (python的特别对象) 可以忽略 _开头的函数 (一般为内部函数) 可以忽略 其余成员，可以根据名字 大致猜出是什么意思。  2. 查阅特定函数和类的使用 想了解某个函数或者类的具体用法，可以使用help函数。以NDArray中的ones_like函数为例。\nhelp(nd.ones_like) 注意：\n jupyter记事本里，使用?来将文档显示在另外一个窗口中。例如：nd.ones_like? 与 help(nd.ones_like)效果一样。nd.ones_like??会额外显示该函数实现的代码。      内存开销   原始操作 首先来个例子：Y = Y + X \u0026ndash;\u0026gt; 每个操作会新开内存来存储运算结果。 上例中，X，Y 变量首先存储在内存中，相加的计算结果会另外开辟内存来存储；然后变量Y在指向新的内存。 内存使用情况：\n内存id_x \u0026lt;\u0026ndash; X 内存id_y \u0026lt;\u0026ndash; Y 内存id_x+y \u0026lt;\u0026ndash; Y\n  Y[:] = X + Y 或者 Y += X 通过[:]把X+Y的结果写进Y对应的内存中。上述操作中，需要另外开辟内存来存储计算结果。 内存使用情况： 内存id_x \u0026lt;\u0026ndash; X 内存id_y \u0026lt;\u0026ndash; Y 内存id_x+y \u0026ndash;\u0026gt; 把内存id_x+y中数值复制到内存id_y中\n  使用运算符全名函数中的out参数 可以避免临时内存开销，使用运算符全名函数：nd.elemwise_add(X, Y, out=Y)。内存使用情况： 内存id_x \u0026lt;\u0026ndash; X 内存id_y \u0026lt;\u0026ndash; Y 内存id_y \u0026lt;\u0026ndash; 直接存放 X+Y 的计算结果\n      自动求梯度 MXNet提供的autograd模块，可以自动求梯度(gradient) from mxnet import autograd, nd # 1. 创建变量 x，并赋初值 x = nd.arrange(4).reshape((4, 1)) # 2. 为了求变量x的梯度，先调用attach_grad函数来申请存储梯度所需要的内存  x.attach_grad() # 3. 为了减少计算和内存开销，默认条件下MXNet是不会记录：求梯度的计算， # 需要调用record函数来要求MXNet记录与求梯度有关的计算。 print(autograd.is_training()) # False with autograd.record(): print(autograd.is_training()) # True y = 2*nd.dot(x.T, x) # 4. 调用backward函数自动求梯度。y必须是一个标量， # 如果y不是标量：MXNet会先对y中元素求和，然后对该和值求有关x的梯度 y.backward() 注意：\n 在调用record函数后，MXNet会记录并计算梯度； 默认情况下，autograd会改变运行模式：从预测模式转为训练模式。可以通过调用is_training函数来查看。      样例 from mxnet import nd     ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/notes/mxnet/ndarray/0010_ndarray_summary/","summary":"查阅文档 怎么查阅相关文档？ 官网\n1. 查阅模块里的所有函数和类 from mxnet import nd print(dir(nd.random))  __开头和结尾的函数 (python的特别对象) 可以忽略 _开头的函数 (一般为内部函数) 可以忽略 其余成员，可以根据名字 大致猜出是什么意思。  2. 查阅特定函数和类的使用 想了解某个函数或者类的具体用法，可以使用help函数。以NDArray中的ones_like函数为例。\nhelp(nd.ones_like) 注意：\n jupyter记事本里，使用?来将文档显示在另外一个窗口中。例如：nd.ones_like? 与 help(nd.ones_like)效果一样。nd.ones_like??会额外显示该函数实现的代码。      内存开销   原始操作 首先来个例子：Y = Y + X \u0026ndash;\u0026gt; 每个操作会新开内存来存储运算结果。 上例中，X，Y 变量首先存储在内存中，相加的计算结果会另外开辟内存来存储；然后变量Y在指向新的内存。 内存使用情况：\n内存id_x \u0026lt;\u0026ndash; X 内存id_y \u0026lt;\u0026ndash; Y 内存id_x+y \u0026lt;\u0026ndash; Y\n  Y[:] = X + Y 或者 Y += X 通过[:]把X+Y的结果写进Y对应的内存中。上述操作中，需要另外开辟内存来存储计算结果。 内存使用情况： 内存id_x \u0026lt;\u0026ndash; X 内存id_y \u0026lt;\u0026ndash; Y 内存id_x+y \u0026ndash;\u0026gt; 把内存id_x+y中数值复制到内存id_y中","tags":null,"title":"NdArray使用"},{"categories":null,"contents":"sum/mean等操作 - 保留原维度数 keepdims: 保留原维度数。例如：\nfrom mxnet import nd def softmax(X): X_exp = X.exp() # shape = (n, m) # shape = (n, 1) 而并不是 (n,) partition = X_exp.sum(axis=1, keepdims=True) return X_exp / partition # 这里应用了广播机制 X = nd.random.normal(shape=(2, 5)) X_prob = softmax(X)     B的值作为A的索引 - 取值 from mxnet import nd y_hat = nd.array([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]]) y = nd.array([0, 2], dtype=\u0026#39;int32\u0026#39;) nd.pick(y_hat, y) # 结果: [0.1, 0.5] # 应用的实例：交叉熵的实现 def cross_entropy(y_hat, y): return -nd.pick(y_hat, y).log()     样例 from mxnet import nd     ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/notes/mxnet/ndarray/0020_technic_gather/","summary":"sum/mean等操作 - 保留原维度数 keepdims: 保留原维度数。例如：\nfrom mxnet import nd def softmax(X): X_exp = X.exp() # shape = (n, m) # shape = (n, 1) 而并不是 (n,) partition = X_exp.sum(axis=1, keepdims=True) return X_exp / partition # 这里应用了广播机制 X = nd.random.normal(shape=(2, 5)) X_prob = softmax(X)     B的值作为A的索引 - 取值 from mxnet import nd y_hat = nd.array([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]]) y = nd.array([0, 2], dtype=\u0026#39;int32\u0026#39;) nd.pick(y_hat, y) # 结果: [0.","tags":null,"title":"NdArray技巧搜集"},{"categories":null,"contents":"模型基类-Block from mxnet.gluon import Block, nn from mxnet import ndarray as F class Model(Block): def __init__(self, **kwargs): super(Model, self).__init__(**kwargs) # use name_scope to give child Blocks appropriate names. with self.name_scope(): self.dense0 = nn.Dense(20) self.dense1 = nn.Dense(20) def forward(self, x): x = F.relu(self.dense0(x)) return F.relu(self.dense1(x)) model = Model() model.initialize(ctx=mx.cpu(0)) model(F.zeros((10, 10), ctx=mx.cpu(0))) class Block(builtins.object)\n网络的最基础的类，搭建网络时必须继承此Block类 —————————————————\nBlock的两个参数：\n prefix : str; 前缀的作用就像一个命名空间。在父模块的作用域下创建的子模块都有父模块的前缀(prefix). params : ParameterDict or None; 共享参数。\n例如：dense1共享dense0的参数。\ndense0 = nn.Dense(20)\ndense1 = nn.Dense(20, params=dense0.collect_params())  —————————————————\nBlock的方法：\n collect_params(self, select=None) 返回一个ParameterDict类。默认包含所有的参数；同时也可以正则匹配:\n例如：选出特定的参数 [\u0026lsquo;conv1_weight\u0026rsquo;, \u0026lsquo;conv1_bias\u0026rsquo;, \u0026lsquo;fc_weight\u0026rsquo;, \u0026lsquo;fc_bias\u0026rsquo;]\n即：model.collect_params(\u0026lsquo;conv1_weight|conv1_bias|fc_weight|fc_bias\u0026rsquo;)\nParameters：空 或者 正则表达式\nReturns: py:class:ParameterDict forward(self, *args) 完成前向计算，输入是NDArray列表\nParameters：*args : list of NDArray hybridize(self, active=True, **kwargs) 激活/不激活HybridBlock的递归\nParameters： bool, default True initialize(self, init=\u0026lt;mxnet.initializer.Uniform object\u0026gt;, ctx=None, verbose=False, force_reinit=False)\n对模型的参数初始化，默认是均匀分布。\n等价于：block.collect_params().initialize(\u0026hellip;)\nParameters：\ninit : Initializer 初始化方法\nctx : 设备 或者 设备列表。会把模型copy到所有指定的设备上\nverbose : bool, default False 是否在初始化时粗略地打印细节。\nforce_reinit : bool, default False 是否重新初始化，即使已经初始化 load_parameters(self, filename, ctx=None, allow_missing=False, ignore_extra=False, cast_dtype=False, dtype_source=\u0026lsquo;current\u0026rsquo;)\n加载模型参数从 用save_parameters保存的模型文件中。\nParameters：\nfilename : str 模型文件路径\nctx : 设备 或者设备列表。默认使用CPU\nallow_missing : bool, default False 是否默默跳过模型文件中不存\n在的模型参数。\nignore_extra : bool, default False 是否默默忽略模型中不存在的参\n数(模型文件中有，模型定义中没有)\ncast_dtype : bool, default False 从checkpointload模型时，是否根\n据传入转换NDArray的数据类型\ndtype_source : str, default \u0026lsquo;current\u0026rsquo; 枚举值：{\u0026lsquo;current\u0026rsquo;, \u0026lsquo;saved\u0026rsquo;}\n只有再cast_dtype=True时有效，指定模型参数的数据类型 name_scope(self) 返回一个命名空间，用来管理Block和参数names。\n必须在with语句中使用：\nwith self.name_scope():\nself.dense = nn.Dense(20) register_child(self, block, name=None) 将block注册为子节点，block的属\n性将自动注册。 save_parameters(self, filename) 保持模型参数到磁盘。该方法只保存模型\n参数的权重，不保存模型的结构。如果想要保存模型的结构，请使\n用:py:meth:HybridBlock.export.\nParameters：Path to file. summary(self, *inputs) 打印模型的输出和参数的摘要。模型必须被初始化  —————————————————\n数据描述：\n name :py:class:Block 的名字 params：返回一个参数字典（不包含子节点的参数） prefix：返回py:class:Block的前缀      模型参数-Parameter ctx = mx.gpu(0) x = mx.nd.zeros((16, 100), ctx=ctx) w = mx.gluon.Parameter(\u0026#39;fc_weight\u0026#39;, shape=(64, 100), init=mx.init.Xavier()) b = mx.gluon.Parameter(\u0026#39;fc_bias\u0026#39;, shape=(64,), init=mx.init.Zero()) w.initialize(ctx=ctx) b.initialize(ctx=ctx) out = mx.nd.FullyConnected(x, w.data(ctx), b.data(ctx), num_hidden=64) class:Parameter 一个存放Blocks的参数的权重的容器。初始化后Parameter.initialize(...)，会copy所有参数权重到每个设备上。如果grad_req不为null，在每个设备上，该容器会拥有一个梯度向量。\nParameter(name,\ngrad_req=\u0026lsquo;write\u0026rsquo;,\nshape=None,\ndtype=\u0026lt;class \u0026lsquo;numpy.float32\u0026rsquo;\u0026gt;,\nlr_mult=1.0,\nwd_mult=1.0,\ninit=None,\nallow_deferred_init=False,\ndifferentiable=True,\nstype=\u0026lsquo;default\u0026rsquo;,\ngrad_stype=\u0026lsquo;default\u0026rsquo;)\n形参：\n——————————\n name : str类型；参数的名字。 grad_req : 枚举值：{\u0026lsquo;write\u0026rsquo;, \u0026lsquo;add\u0026rsquo;, \u0026lsquo;null\u0026rsquo;}, 默认值：\u0026lsquo;write\u0026rsquo;。指定怎么更新梯度到梯度向量。\n'write':每次把梯度值写到 梯度向量中\n'add': 每次把计算的梯度值add到梯度向量中. 在每次迭代之前，\n你需要手动调用zero_grad()来清理梯度缓存。\n'null': 参数不需要计算梯度，不会分配梯度向量。 shape : int or tuple of int, default None. 参数的尺寸. dtype : numpy.dtype or str, default \u0026lsquo;float32\u0026rsquo;. 参数的数据类型 lr_mult : float, default 1.0, 学习率. wd_mult : float, default 1.0, 权重衰减率 L2 init : Initializer, default None. 参数的初始化，默认全局初始化 stype: 枚举值: {\u0026lsquo;default\u0026rsquo;, \u0026lsquo;row_sparse\u0026rsquo;, \u0026lsquo;csr\u0026rsquo;}, defaults to \u0026lsquo;default\u0026rsquo;. 参数的存储类型。 grad_stype: 枚举值: {\u0026lsquo;default\u0026rsquo;, \u0026lsquo;row_sparse\u0026rsquo;, \u0026lsquo;csr\u0026rsquo;}, defaults to \u0026lsquo;default\u0026rsquo;. 参数梯度的存储类型  属性:\n——————————\n grad_req : 枚举值:{\u0026lsquo;write\u0026rsquo;, \u0026lsquo;add\u0026rsquo;, \u0026lsquo;null\u0026rsquo;} 可以在初始化之前/之后设置。当不需要计算参数的梯度时，设置为null，以节省内存和计算量。 lr_mult : float 学习率 wd_mult : float 权重衰减率  定义的函数：\n——————————\n  cast(self, dtype) 转换参数的值/梯度的数据类型。\ndtype : str or numpy.dtype 新的数据类型\n  data(self, ctx=None) 获取这个参数在设备ctx上的值，参数必须已经初始化了。\nctx : 指定设备\nReturns：NDArray on ctx\n  grad(self, ctx=None) 获取这个参数在设备ctx上的梯度值。\nctx : 指定设备\n  initialize(self, init=None, ctx=None,\ndefault_init=\u0026lt;mxnet.initializer.Uniform\u0026gt;,\nforce_reinit=False) 初始化参数和梯度向量\ninit : Initializer 初始化参数的值\nctx : 设备/设备列表, 默认使用:py:meth:context.current_context().\ndefault_init : Initializer 当:py:func:init和:py:meth:Parameter.init都为none时，使用该默认的初始化.\nforce_reinit : bool, default False 当参数已经被初始化，是否再次初始化。\n  weight = mx.gluon.Parameter(\u0026#39;weight\u0026#39;, shape=(2, 2)) weight.initialize(ctx=mx.cpu(0)) weight.data() #　[[-0.01068833 0.01729892] #　[ 0.02042518 -0.01618656]] #　\u0026lt;NDArray 2x2 @cpu(0)\u0026gt; weight.grad() #　[[ 0. 0.] #　[ 0. 0.]] #　\u0026lt;NDArray 2x2 @cpu(0)\u0026gt; weight.initialize(ctx=[mx.gpu(0), mx.gpu(1)]) weight.data(mx.gpu(0)) #　[[-0.00873779 -0.02834515] #　[ 0.05484822 -0.06206018]] #　\u0026lt;NDArray 2x2 @gpu(0)\u0026gt; weight.data(mx.gpu(1)) #　[[-0.00873779 -0.02834515] #　[ 0.05484822 -0.06206018]] #　\u0026lt;NDArray 2x2 @gpu(1)\u0026gt;  list_ctx(self) 返回参数初始化在那些设备上 list_data(self) 按照顺序返回所有设备上的参数值 Returns: list of NDArrays list_grad(self) 按照顺序返回所有设备上的梯度值 list_row_sparse_data(self, row_id) 按照顺序返回所有设备上的 行稀疏的参数。\nrow_id: 指定看哪一行的数据\nReturns: list of NDArrays reset_ctx(self, ctx) 重新设定设备，把参数copy到该设备上\nctx : Context or list of Context, default context.current_context() row_sparse_data(self, row_id)\nrow_id: NDArray 指定看哪一行的数据\nReturns: NDArray on row_id\u0026rsquo;s context set_data(self, data) 在所有设备上，设置该参数的值。 var(self) 返回一个代表该参数的符号 zero_grad(self) 将所有设备上的梯度缓存清零  数据描述:\n——————————\n dtype 参数的数据类型 grad_req shape 参数的尺寸      模型参数-访问 ToTensor：将图像数据从uint8格式变换成32位浮点数格式，并除以255使得所有像素的数值均在0到1之间 transform_first函数：数据集的函数。将ToTensor的变换应用在每个数据样本（图像和标签）的第一个元素，即图像之上.\nfrom mxnet import nd from mxnet.gluon import nn     网络设计 from mxnet import nd from mxnet.gluon import nn     模型初始化 from mxnet import nd from mxnet.gluon import nn     模型初始化 from mxnet import nd from mxnet.gluon import nn     ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/notes/mxnet/gluon/0030_module_gluon_nn/","summary":"模型基类-Block from mxnet.gluon import Block, nn from mxnet import ndarray as F class Model(Block): def __init__(self, **kwargs): super(Model, self).__init__(**kwargs) # use name_scope to give child Blocks appropriate names. with self.name_scope(): self.dense0 = nn.Dense(20) self.dense1 = nn.Dense(20) def forward(self, x): x = F.relu(self.dense0(x)) return F.relu(self.dense1(x)) model = Model() model.initialize(ctx=mx.cpu(0)) model(F.zeros((10, 10), ctx=mx.cpu(0))) class Block(builtins.object)\n网络的最基础的类，搭建网络时必须继承此Block类 —————————————————\nBlock的两个参数：\n prefix : str; 前缀的作用就像一个命名空间。在父模块的作用域下创建的子模块都有父模块的前缀(prefix). params : ParameterDict or None; 共享参数。\n例如：dense1共享dense0的参数。\ndense0 = nn.","tags":null,"title":"Gluon-nn模块"},{"categories":null,"contents":"实例-单层感知机 模型：o = w1*x1 + w2*x2 + b 输出o作为线性回归的输出，输入层是2维特征；输入层不涉及计算，该神经网络只有输出层1层。\n神经元：输出层中负责计算o的单元。\n该神经元，依赖于输入层的全部特征，也就是说输出层中的神经元和输入层中各个输入完全连接，所以，这里的输出层又叫作全连接层(fully connected layer)或者稠密层(dense layer)\n    生成数据集 目标： o = 2x1 - 3.4x2 + 4.2 其中： 样本集：features: [w1, w2]， labels: [真实值+噪声]\nfrom IPython import display from matplotlib import pyplot as plt from mxnet import autograd, nd import random num_inputs = 2 num_examples = 1000 true_w = [2, -3.4] true_b = 4.2 features = nd.random.normal(scale=1, shape=(num_examples, num_inputs)) labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b labels += nd.random.normal(scale=0.01, shape=labels.shape)     读取数据集 - 从零实现 def data_iter(batch_size, features, labels): num_examples = len(features) indices = list(range(num_examples)) random.shuffle(indices) # 样本的读取顺序是随机的 for i in range(0, num_examples, batch_size): j = nd.array(indices[i: min(i + batch_size, num_examples)]) # take函数根据索引返回对应元素 yield features.take(j), labels.take(j)     读取数据集 - Gluon实现 DataLoader 返回一个迭代器，一次返回batch_size个样本\nfrom mxnet.gluon import data as gdata batch_size = 10 # 将训练数据的特征和标签组合 dataset = gdata.ArrayDataset(features, labels) # 随机读取小批量 data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True)     模型定义 - 从零实现 手动定义模型参数，一定要开辟存储梯度的内存。\n# 将权重初始化为：均值为0、标准差为0.01的正太随机数 w = nd.random.normal(scale=0.01, shape=(num_inputs, 1)) b = nd.zeros(shape=(1,)) # 开辟存储梯度的内存 w.attach_grad() b.attach_grad() # 定义线性回归的模型 def linreg(X, w, b): return nd.dot(X, w) + b     模型定义 - Gluon实现 Gluon模块：提供了大量预定义的层。nn模块(neural networks)的缩写，所以里面定义了大量神经网络 的层。 Sequential：可以看作是一个 串联各个层的容器，在构建模型时，在该容器中一次添加层。当给定输入数据时， 容器中的每一层的输出作为下一层的输入。\ninit模块：initializer的缩写。该模块提供了模型参数初始化的各种方法。\nfrom mxnet.gluon import nn from mxnet import init # 定义模型 net = nn.Sequential() net.add(nn.Dense(1)) # 初始化模型参数 net.initialize(init.Normal(sigma=0.01))     定义损失函数 - 从零实现 def squared_loss(y_hat, y): return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2 loss = squared_loss     定义损失函数 - Gluon实现 Gluon中的loss模块：定义了各种损失函数。\nfrom mxnet.gluon import loss as gloss loss = gloss.L2Loss() # 平方损失又称L2范数损失     定义优化算法 - 从零实现 param.grad自动求梯度模块计算得来的梯度是一个批量样本的梯度和。在迭代模型参数时，需要除以批量大小来得到平均值。\ndef sgd(params, lr, batch_size): for param in params: param[:] = param - lr * param.grad / batch_size     定义优化算法 - Gluon实现 Gluon模块中的Trainer类，用来迭代模型中的全部参数。这些参数可以通过collect_params函数获取。\nfrom mxnet.gluon import Trainer trainer = Trainer(net.collect_params(), \u0026#39;sgd\u0026#39;, {\u0026#39;learning_rate\u0026#39;: 0.03})     训练模型 - 从零实现 lr = 0.03 num_epochs = 3 net = linreg # 训练模型一共需要num_epochs个迭代周期 for epoch in range(num_epochs): # 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除） # x和y分别是小批量样本的特征和标签 for X, y in data_iter(batch_size, features, labels): with autograd.record(): l = loss(net(X, w, b), y) # l是有关小批量X和y的损失 l.backward() # 小批量的损失对模型参数求梯度 sgd([w, b], lr, batch_size) # 使用小批量随机梯度下降迭代模型参数 train_l = loss(net(features, w, b), labels) print(\u0026#39;epoch %d, loss %f\u0026#39; % (epoch + 1, train_l.mean().asnumpy()))     训练模型 - Gluon实现 通过Trainer实例的step函数来迭代模型参数。由于loss是长度为batch_size的向量，在执行l.backward()时， 等价于执行l.sum().backward()。所以要用batch_size做平均。\nnum_epochs = 3 # 训练模型一共需要num_epochs个迭代周期 for epoch in range(1, num_epochs + 1): # 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除） # x和y分别是小批量样本的特征和标签 for X, y in data_iter: with autograd.record(): l = loss(net(X), y) # l是有关小批量X和y的损失 l.backward() # 等价于l.sum().backward() trainer.step(batch_size) # 指定batch_size，从而对批量样本梯度求平均 l = loss(net(features), labels) print(\u0026#39;epoch %d, loss: %f\u0026#39; % (epoch, l.mean().asnumpy()))     ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/notes/mxnet/gluon/0010_gluon_summary/","summary":"实例-单层感知机 模型：o = w1*x1 + w2*x2 + b 输出o作为线性回归的输出，输入层是2维特征；输入层不涉及计算，该神经网络只有输出层1层。\n神经元：输出层中负责计算o的单元。\n该神经元，依赖于输入层的全部特征，也就是说输出层中的神经元和输入层中各个输入完全连接，所以，这里的输出层又叫作全连接层(fully connected layer)或者稠密层(dense layer)\n    生成数据集 目标： o = 2x1 - 3.4x2 + 4.2 其中： 样本集：features: [w1, w2]， labels: [真实值+噪声]\nfrom IPython import display from matplotlib import pyplot as plt from mxnet import autograd, nd import random num_inputs = 2 num_examples = 1000 true_w = [2, -3.4] true_b = 4.2 features = nd.random.normal(scale=1, shape=(num_examples, num_inputs)) labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b labels += nd.","tags":null,"title":"Gluon实例"},{"categories":null,"contents":"gluon模块-结构 路径.mxnet/gluon/下的树状结构:\n│　block.py 类：Block, HybridBlock\n│　loss.py 各种loss函数\n│　parameter.py 类：Parameter, Constant, ParameterDict\n│　trainer.py 类：Trainer\n│　utils.py 优化操作\n│　init.py\n│\n├─contrib\n│　│\n│　├─cnn\n│　│　└─ conv_layers.py\n│　├─data\n│　│　└─ sampler.py\n│　│\n│　├─estimator\n│　│　│　estimator.py\n│　│　└─ event_handler.py\n│　│\n│　├─nn\n│　│　└─ basic_layers.py\n│　│\n│　└─rnn\n│　│　conv_rnn_cell.py\n│　└─ rnn_cell.py\n│\n├─data 主要是数据处理操作\n│　│　dataloader.py 类：DataLoader\n│　│　dataset.py 常用类: ArrayDataset\n│　│　sampler.py\n│　│\n│　└─vision\n│　│　datasets.py 可用的数据集-各个类\n│　└─ transforms.py 数据预处理-各个类\n│\n├─model_zoo\n│　│　model_store.py\n│　│\n│　└─vision\n│　│　alexnet.py\n│　│　densenet.py\n│　│　inception.py\n│　│　mobilenet.py\n│　│　resnet.py\n│　│　squeezenet.py\n│　└─ vgg.py\n├─nn 网络结构\n│　│　activations.py 定义了各种激活层\n│　│　basic_layers.py 定义了网络的基础层,例如：BN,Dropout等\n│　└─ conv_layers.py 定义了各种卷积池化层等\n│\n└─rnn\n│　rnn_cell.py\n└─ rnn_layer.py    gluon模块-导入 # data from mxnet.gluon.data import ArrayDataset, DataLoader from mxnet.gluon.data.vision.transforms import ToTensor, Normalize # nn from mxnet.gluon.nn import Block, HybridBlock, Sequential, HybridSequential, Dropout, BatchNorm, Dense, PReLU, Conv2D # 模型参数 from mxnet.gluon.parameter import Parameter, Constant, ParameterDict # 训练 from mxnet.gluon.trainer import Trainer # 损失函数 from mxnet.gluon. import loss # 损失函数 [\u0026#39;Loss\u0026#39;, \u0026#39;L2Loss\u0026#39;, \u0026#39;L1Loss\u0026#39;, \u0026#39;SigmoidBinaryCrossEntropyLoss\u0026#39;, \u0026#39;SigmoidBCELoss\u0026#39;, \u0026#39;SoftmaxCrossEntropyLoss\u0026#39;, \u0026#39;SoftmaxCELoss\u0026#39;, \u0026#39;KLDivLoss\u0026#39;, \u0026#39;CTCLoss\u0026#39;, \u0026#39;HuberLoss\u0026#39;, \u0026#39;HingeLoss\u0026#39;, \u0026#39;SquaredHingeLoss\u0026#39;, \u0026#39;LogisticLoss\u0026#39;, \u0026#39;TripletLoss\u0026#39;, \u0026#39;PoissonNLLLoss\u0026#39;, \u0026#39;CosineEmbeddingLoss\u0026#39;]     数据集 - data ToTensor：将图像数据从uint8格式变换成32位浮点数格式，并除以255使得所有像素的数值均在0到1之间 transform_first函数：数据集的函数。将ToTensor的变换应用在每个数据样本（图像和标签）的第一个元素，即图像之上.\nfrom mxnet.gluon import data as gdata batch_size = 256 transformer = gdata.vision.transforms.ToTensor() if sys.platform.startswith(\u0026#39;win\u0026#39;): num_workers = 0 # 0表示不用额外的进程来加速读取数据 else: num_workers = 4 train_iter = gdata.DataLoader(mnist_train.transform_first(transformer), batch_size, shuffle=True, num_workers=num_workers) test_iter = gdata.DataLoader(mnist_test.transform_first(transformer), batch_size, shuffle=False, num_workers=num_workers)     模型初始化 - init from mxnet import init     损失函数 - loss from mxnet.gluon import loss as gloss # 平方损失又称L2范数损失 loss = gloss.L2Loss() # 包含了softmax运算和交叉熵损失运算 loss = gloss.SoftmaxCrossEntropyLoss()     优化算法 - Trainer from mxnet.gluon import Trainer     ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/notes/mxnet/gluon/0020_module_gather/","summary":"gluon模块-结构 路径.mxnet/gluon/下的树状结构:\n│　block.py 类：Block, HybridBlock\n│　loss.py 各种loss函数\n│　parameter.py 类：Parameter, Constant, ParameterDict\n│　trainer.py 类：Trainer\n│　utils.py 优化操作\n│　init.py\n│\n├─contrib\n│　│\n│　├─cnn\n│　│　└─ conv_layers.py\n│　├─data\n│　│　└─ sampler.py\n│　│\n│　├─estimator\n│　│　│　estimator.py\n│　│　└─ event_handler.py\n│　│\n│　├─nn\n│　│　└─ basic_layers.py\n│　│\n│　└─rnn\n│　│　conv_rnn_cell.py\n│　└─ rnn_cell.py\n│\n├─data 主要是数据处理操作","tags":null,"title":"Gluon模块简介"},{"categories":null,"contents":"标题     -- 数组  134. 加油站  关键点：每个加油站的 剩余=添加-消耗。 累积每个加油站的剩余量，剩余累积量达到最小值时（升高的拐点，注意累积量保持最小值不变的情况。），下一个加油站就是起点。   238. 除自身外数组的乘积  关键点：从左到右累积相乘，从右到左累积相乘。这样就不用除法，且避免了重复的计算。\n  189. 轮转数组  关键点：\n 方法一：环状替换：替换到下一个位置，直到回到原位置，完成一轮。如果有没有遍历的元素，偏移一个位置，继续环状替换操作。   方法二：多次翻转 \u0026ndash;达到\u0026ndash;\u0026gt; 旋转的效果        图   127. 单词接龙  关键点：单词与单词之间用“中间单词”连接。这样的设计是 降低了计算复杂度。\n 每个单词mask掉一个字母，单词与单词之间没有连接，是通过中间的单词相互连接 通过广度优先遍历，从起始单词开始，直到结束单词。由于路径中有一半的量是“中间单词”，所以总的步数N，应该缩小：N//2+1    class Solution(object): def __init__(self): self.nodeNum = 0 def ladderLength(self, beginWord, endWord, wordList): \u0026#34;\u0026#34;\u0026#34; :type beginWord: str :type endWord: str :type wordList: List[str] :rtype: int \u0026#34;\u0026#34;\u0026#34; def addWord(word): if word not in wordId: wordId[word] = self.nodeNum self.nodeNum += 1 def addEdge(word): addWord(word) id1 = wordId[word] chars = list(word) for i in range(len(chars)): tmp = chars[i] chars[i] = \u0026#34;*\u0026#34; newWord = \u0026#34;\u0026#34;.join(chars) addWord(newWord) id2 = wordId[newWord] edge[id1].append(id2) edge[id2].append(id1) chars[i] = tmp wordId = dict() edge = collections.defaultdict(list) for word in wordList: addEdge(word) addEdge(beginWord) if endWord not in wordId: return 0 dis = [float(\u0026#34;inf\u0026#34;)] * self.nodeNum beginId, endId = wordId[beginWord], wordId[endWord] dis[beginId] = 0 que = collections.deque([beginId]) while que: x = que.popleft() if x == endId: return dis[endId] // 2 + 1 for it in edge[x]: if dis[it] == float(\u0026#34;inf\u0026#34;): dis[it] = dis[x] + 1 que.append(it) return 0     ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/notes/computer_algorithm/0450_leedcode_classic.zh-ch/","summary":"标题     -- 数组  134. 加油站  关键点：每个加油站的 剩余=添加-消耗。 累积每个加油站的剩余量，剩余累积量达到最小值时（升高的拐点，注意累积量保持最小值不变的情况。），下一个加油站就是起点。   238. 除自身外数组的乘积  关键点：从左到右累积相乘，从右到左累积相乘。这样就不用除法，且避免了重复的计算。\n  189. 轮转数组  关键点：\n 方法一：环状替换：替换到下一个位置，直到回到原位置，完成一轮。如果有没有遍历的元素，偏移一个位置，继续环状替换操作。   方法二：多次翻转 \u0026ndash;达到\u0026ndash;\u0026gt; 旋转的效果        图   127. 单词接龙  关键点：单词与单词之间用“中间单词”连接。这样的设计是 降低了计算复杂度。\n 每个单词mask掉一个字母，单词与单词之间没有连接，是通过中间的单词相互连接 通过广度优先遍历，从起始单词开始，直到结束单词。由于路径中有一半的量是“中间单词”，所以总的步数N，应该缩小：N//2+1    class Solution(object): def __init__(self): self.nodeNum = 0 def ladderLength(self, beginWord, endWord, wordList): \u0026#34;\u0026#34;\u0026#34; :type beginWord: str :type endWord: str :type wordList: List[str] :rtype: int \u0026#34;\u0026#34;\u0026#34; def addWord(word): if word not in wordId: wordId[word] = self.","tags":null,"title":"LeedCode-经典"},{"categories":null,"contents":"参考\n第一周：链表、栈、队列  链表的基础知识：单链表 反转链表（ LeetCode 206  ） 相交链表（ LeetCode 160  ） 合并两个有序链表 （ LeetCode 21  ） 分隔链表 （ LeetCode 86  ） 环形链表 II （ LeetCode 142  ） 反转链表 II （ LeetCode 92  ） 复制带随机指针的链表（ LeetCode 138  ） 栈的基础知识 有效的括号（ LeetCode 20  ） 基本计算器（ LeetCode 224  ） 最小栈（ LeetCode 155  ） 验证栈序列（ LeetCode 946  ） 每日温度（ LeetCode 739  ） 接雨水（ LeetCode 42  ） 队列的基础知识 用栈实现队列 （ LeetCode 232  ） 滑动窗口最大值（ LeetCode 239  ） 设计循环双端队列（ LeetCode 641  ） 移除链表元素（ LeetCode 203  ） K 个一组翻转链表（ LeetCode 25  ） 回文链表（ LeetCode 234  ） 奇偶链表（ LeetCode 328  ） 从尾到头打印链表（ 剑指Offer 06  ） 链表中倒数第 k 个节点（ 剑指Offer 22  ）      第二周，递归、排序、贪心  递归基础知识 冒泡排序基础知识 选择排序基础知识 插入排序基础知识 快速排序基础知识 计数排序基础知识 归并排序 桶排序（了解即 可） 堆排序 基数排序（了解即 可） 希尔排序（了解即 可） 合并两个有序数组( LeetCode 88  ) 颜色分类( LeetCode 75  ) 部分排序 （面试题 16） 计算右侧小于当前元素的个数 ( LeetCode 315  ) 合并 K 个升序链表（LeetCode 23 ） 有序数组的平方( LeetCode 977  ) 盛最多水的容器 ( LeetCode 11  ) 两数之和（LeetCode 1 ） 二叉堆基础知识 分发饼干（ LeetCode 455  ） 柠檬水找零（ LeetCode 860  ） 用最少数量的箭引爆气球（ LeetCode 452  ） 移掉 K 位数字（ LeetCode 402  ） 跳跃游戏（ LeetCode 55  ） 摆动序列（ LeetCode 376  ） 买卖股票的最佳时机 II（ LeetCode 122  ） 三数之和（LeetCode 15 ） 最接近三数之和（LeetCode 16 ） 加油站（ LeetCode 134  ） 合并区间（ LeetCode 56  ）      第三周，搜索算法、回溯算法、位运算、二分查找  二分查找基础知识 二分查找（ LeetCode 704  ） 搜索插入位置（ LeetCode 35  ） 在排序数组中查找元素的第一个和最后一个位置（ LeetCode 34  ） 搜索旋转排序数组（ LeetCode 33  ） 搜索二维矩阵（ LeetCode 74  ） 寻找两个正序数组的中位数（ LeetCode 4  ） 有效三角形的个数（ LeetCode 611  ） 剑指 Offer 53 – II. 0～n-1中缺失的数字 剑指 Offer 53 – I. 在排序数组中查找数字 I 剑指 Offer 51. 数组中的逆序对 寻找峰值（ LeetCode 162  ） 第一个错误的版本（ LeetCode 278  ） 山脉数组的峰顶索引（ LeetCode 852  ） 有效的完全平方数（ LeetCode 367  ） 位运算基础知识 丢失的数字（ LeetCode 268  ） 2 的幂（ LeetCode 231  ） 比特位计数（ LeetCode 338  ） 位 1 的个数（ LeetCode 191  ） 只出现一次的数字 II（ LeetCode 137  ） 只出现一次的数字 III（ LeetCode 260  ） 最大单词长度乘积（ LeetCode 318  ） 汉明距离（ LeetCode 461  ） 回溯基础知识 岛屿数量（ LeetCode 200  ） N 皇后（ LeetCode 51  ） 子集（ LeetCode 78  ） 组合总和 II（ LeetCode 40  ） 括号生成（ LeetCode 22  ） 火柴拼正方形（ LeetCode 437  ） 接雨水 II（ LeetCode 407  ） 组合（ LeetCode 77  ） 组合总和 II（ LeetCode 216  ） 分割回文串（ LeetCode 131  ） 全排列（ LeetCode 46  ）      第四周，二叉树  二叉树基础知识 二叉树的前序遍历（ LeetCode 144  ） 二叉树的中序遍历（ LeetCode 94  ） 二叉树的后序遍历（ LeetCode 145  ） 二叉树的层序遍历（ LeetCode 102  ） 二叉树的锯齿形层序遍历（ LeetCode 103  ） 从前序与中序遍历序列构造二叉树（ LeetCode 105  ） 路径总和 II（ LeetCode 113  ） 二叉树的最近公共祖先（ LeetCode 236  ） 二叉树的右视图（ LeetCode 199  ） 二叉树展开为链表（ LeetCode 114  ） 将有序数组转换为二叉搜索树（ LeetCode 108  ） 把二叉搜索树转换为累加树（ LeetCode 538  ） 删除二叉搜索树中的节点（ LeetCode 450  ） 二叉树的序列化与反序列化（ LeetCode 297  ） 完全二叉树的节点个数（ LeetCode 222  ） 二叉树的最大深度（ LeetCode 104  ） 二叉树的最小深度（ LeetCode 111  ） 二叉树的所有路径（ LeetCode 257  ） 平衡二叉树（ LeetCode 110  ） 左叶子之和（ LeetCode 404  ） 找树左下角的值（ LeetCode 513  ） 修剪二叉搜索树（ LeetCode 669  ） 二叉搜索树的最近公共祖先（ LeetCode 235  ） 二叉搜索树的最小绝对差（ LeetCode 530  ） 最大二叉树（ LeetCode 654  ）      第五周，动态规划、背包问题  动态规划基础知识和解题步骤 爬楼梯（ LeetCode 70  ） 斐波那契数（ LeetCode 509  ） 最大子序和（ LeetCode 53  ） 零钱兑换（ LeetCode 322  ） 零钱兑换 II（ LeetCode 518  ） 最小路径和（ LeetCode 64  ） 编辑距离（ LeetCode 72  ） 买卖股票的最佳时机（ LeetCode 121  ） 买卖股票的最佳时机II（ LeetCode 122  ） 买卖股票的最佳时机III（ LeetCode 123  ） 买卖股票的最佳时机IV（ LeetCode 188  ） 最佳买卖股票时机含冷冻期(LeetCode 309  ) 买卖股票的最佳时机含手续费(LeetCode 714  ) 完全平方数（ LeetCode 279  ） 三角形最小路径和（ LeetCode 120  ） 不同路径（ LeetCode 62  ） 不同路径II（ LeetCode 63  ） 整数拆分（ LeetCode 343  ） 不同的二叉搜索树（ LeetCode 96  ） 地下城游戏（ LeetCode 174  ） 打家劫舍（ LeetCode 198  ） 打家劫舍II（ LeetCode 213  ） 打家劫舍III（ LeetCode 337  ） 最长递增子序列（ LeetCode 300  ） 最长连续递增序列（ LeetCode 674  ） 分割等和子集（ LeetCode 416  ） 最长重复子数组（ LeetCode 718  ） 最长公共子序列（ LeetCode 1143  ） 最长回文子序列（ LeetCode 516  ） 最长回文子串（ LeetCode 5  ） 01 背包问题 目标和（ LeetCode 494  ） 最后一块石头的重量 II（ LeetCode 1049  ）      第六周，剑指offer  剑指 Offer 03. 数组中重复的数字 剑指 Offer 04. 二维数组中的查找 剑指 Offer 05. 替换空格 剑指 Offer 06. 从尾到头打印链表 剑指 Offer 09. 用两个栈实现队列 剑指 Offer 11. 旋转数组的最小数字 剑指 Offer 12. 矩阵中的路径 剑指 Offer 18. 删除链表的节点 剑指 Offer 21. 调整数组顺序使奇数位于偶数前面 剑指 Offer 22. 链表中倒数第k个节点 剑指 Offer 24. 反转链表 剑指 Offer 25. 合并两个排序的链表 剑指 Offer 26. 树的子结构 剑指 Offer 30. 包含min函数的栈 剑指 Offer 32 - I. 从上到下打印二叉树 剑指 Offer 32 - II. 从上到下打印二叉树 II 剑指 Offer 32 - III. 从上到下打印二叉树 III 剑指 Offer 33. 二叉搜索树的后序遍历序列 剑指 Offer 41. 数据流中的中位数 剑指 Offer 42. 连续子数组的最大和 剑指 Offer 45. 把数组排成最小的数 剑指 Offer 46. 把数字翻译成字符串 剑指 Offer 47. 礼物的最大价值 剑指 Offer 50. 第一个只出现一次的字符 剑指 Offer 51. 数组中的逆序对 剑指 Offer 52. 两个链表的第一个公共节点 剑指 Offer 53 - I. 在排序数组中查找数字 I 剑指 Offer 53 - II. 0～n-1中缺失的数字 剑指 Offer 54. 二叉搜索树的第k大节点 剑指 Offer 55 - I. 二叉树的深度 剑指 Offer 57. 和为s的两个数字 剑指 Offer 58 - II. 左旋转字符串 剑指 Offer 61. 扑克牌中的顺子 剑指 Offer 66. 构建乘积数组      标题     -- ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/notes/computer_algorithm/0500_leedcode_list/","summary":"参考\n第一周：链表、栈、队列  链表的基础知识：单链表 反转链表（ LeetCode 206  ） 相交链表（ LeetCode 160  ） 合并两个有序链表 （ LeetCode 21  ） 分隔链表 （ LeetCode 86  ） 环形链表 II （ LeetCode 142  ） 反转链表 II （ LeetCode 92  ） 复制带随机指针的链表（ LeetCode 138  ） 栈的基础知识 有效的括号（ LeetCode 20  ） 基本计算器（ LeetCode 224  ） 最小栈（ LeetCode 155  ） 验证栈序列（ LeetCode 946  ） 每日温度（ LeetCode 739  ） 接雨水（ LeetCode 42  ） 队列的基础知识 用栈实现队列 （ LeetCode 232  ） 滑动窗口最大值（ LeetCode 239  ） 设计循环双端队列（ LeetCode 641  ） 移除链表元素（ LeetCode 203  ） K 个一组翻转链表（ LeetCode 25  ） 回文链表（ LeetCode 234  ） 奇偶链表（ LeetCode 328  ） 从尾到头打印链表（ 剑指Offer 06  ） 链表中倒数第 k 个节点（ 剑指Offer 22  ）      第二周，递归、排序、贪心  递归基础知识 冒泡排序基础知识 选择排序基础知识 插入排序基础知识 快速排序基础知识 计数排序基础知识 归并排序 桶排序（了解即 可） 堆排序 基数排序（了解即 可） 希尔排序（了解即 可） 合并两个有序数组( LeetCode 88  ) 颜色分类( LeetCode 75  ) 部分排序 （面试题 16） 计算右侧小于当前元素的个数 ( LeetCode 315  ) 合并 K 个升序链表（LeetCode 23 ） 有序数组的平方( LeetCode 977  ) 盛最多水的容器 ( LeetCode 11  ) 两数之和（LeetCode 1 ） 二叉堆基础知识 分发饼干（ LeetCode 455  ） 柠檬水找零（ LeetCode 860  ） 用最少数量的箭引爆气球（ LeetCode 452  ） 移掉 K 位数字（ LeetCode 402  ） 跳跃游戏（ LeetCode 55  ） 摆动序列（ LeetCode 376  ） 买卖股票的最佳时机 II（ LeetCode 122  ） 三数之和（LeetCode 15 ） 最接近三数之和（LeetCode 16 ） 加油站（ LeetCode 134  ） 合并区间（ LeetCode 56  ）      第三周，搜索算法、回溯算法、位运算、二分查找  二分查找基础知识 二分查找（ LeetCode 704  ） 搜索插入位置（ LeetCode 35  ） 在排序数组中查找元素的第一个和最后一个位置（ LeetCode 34  ） 搜索旋转排序数组（ LeetCode 33  ） 搜索二维矩阵（ LeetCode 74  ） 寻找两个正序数组的中位数（ LeetCode 4  ） 有效三角形的个数（ LeetCode 611  ） 剑指 Offer 53 – II.","tags":null,"title":"LeedCode刷库记录"},{"categories":null,"contents":"Depth First Search(DFS)遍历  深度优先遍历：\n 使用递归，代码比较简单 如果不用递归，可以利用栈这种数据结构   # -*- coding: utf-8 -*- class TreeNode: def __init__(self, value): self.value = value self.left = None self.right = None class Tree_Method: def DFS(self, root): \u0026#39;\u0026#39;\u0026#39; 深度优先遍历，即先访问根节点，然后遍历左子树接着遍历右子树。 主要利用栈的特点，先将右子树压栈，再将左子树压栈，这样左子树就位于栈顶， 可以结点的左子树先与右子树被遍历。 \u0026#39;\u0026#39;\u0026#39; if root == None: return None stack = [] \u0026#39;\u0026#39;\u0026#39;用列表模仿入栈\u0026#39;\u0026#39;\u0026#39; stack.append(root) while stack: \u0026#39;\u0026#39;\u0026#39;将栈顶元素出栈\u0026#39;\u0026#39;\u0026#39; current_node = stack.pop() print(current_node.value, end=\u0026#39; \u0026#39;) \u0026#39;\u0026#39;\u0026#39;判断该节点是否有右孩子，有就入栈\u0026#39;\u0026#39;\u0026#39; if current_node.right: stack.append(current_node.right) \u0026#39;\u0026#39;\u0026#39;判断该节点是否有左孩子，有就入栈\u0026#39;\u0026#39;\u0026#39; if current_node.left: stack.append(current_node.left) def preOrder(self, root): \u0026#39;\u0026#39;\u0026#39;先序遍历\u0026#39;\u0026#39;\u0026#39; if root == None: return None print(root.value) self.preOrder(root.left) self.preOrder(root.right) # 先序打印二叉树（非递归） def preOrderTravese(node): stack = [node] while len(stack) \u0026gt; 0: print(node.val) if node.right is not None: stack.append(node.right) if node.left is not None: stack.append(node.left) node = stack.pop() def minOrder(self, root): \u0026#39;\u0026#39;\u0026#39;中序遍历\u0026#39;\u0026#39;\u0026#39; if root == None: return None self.minOrder(root.left) print(root.value) self.minOrder(root.right) # 中序打印二叉树（非递归） def inOrderTraverse(node): stack = [] pos = node while pos or stack: # 当前节点不为null，先入栈，然后继续检测其左子节点 if pos: stack.append(pos) pos = pos.left # 当前节点为null，表示上一个节点的左子节点为null， # 1. 打印上一节点， # 2. 然后检测上一节点的右子节点 else: pos = stack.pop() print(pos.val) pos = pos.right def postOrder(self, root): \u0026#39;\u0026#39;\u0026#39;后序遍历\u0026#39;\u0026#39;\u0026#39; if root == None: return None self.postOrder(root.left) self.postOrder(root.right) print(root.value) # 后序打印二叉树（非递归） # 使用两个栈结构 # 第一个栈进栈顺序：左节点-\u0026gt;右节点-\u0026gt;跟节点 # 第一个栈弹出顺序： 跟节点-\u0026gt;右节点-\u0026gt;左节点(先序遍历栈弹出顺序：跟-\u0026gt;左-\u0026gt;右) # 第二个栈存储为第一个栈的每个弹出依次进栈 # 最后第二个栈依次出栈 def postOrderTraverse(node): stack = [node] stack2 = [] while stack: node = stack.pop() stack2.append(node) if node.left: stack.append(node.left) if node.right: stack.append(node.right) while stack2: print(stack2.pop().val)     Breadth First Search(BFS)遍历  广度优先遍历：\n 一般借助 队列，每层入队，遍历完后再把下一层copy到队列中。   # -*- coding: utf-8 -*- class TreeNode: def __init__(self, value): self.value = value self.left = None self.right = None class Tree_Method: def create_tree(self, arr): \u0026#39;\u0026#39;\u0026#39; 利用二叉树的三个组成部分：根节点-左子树-右子树； 传入的arr是一个多维列表，每一维最大为3， 每一维中的内容依次表示根节点-左子树-右子树。然后递归的进行构建 \u0026#39;\u0026#39;\u0026#39; length = len(arr) #计算每一维的大小 root = TreeNode(arr[0]) #获取每一维的根节点 if length \u0026gt;= 2: #判断是否有左子树 root.left = self.create_tree(arr[1]) if length \u0026gt;= 3: #判断是否有右子树 root.right = self.create_tree(arr[2]) return root def BFS(self, root): \u0026#39;\u0026#39;\u0026#39; 广度优先遍历，即从上到下，从左到右遍历。 主要利用队列先进先出的特性，入队的时候，是按根左右的顺序，那么只要按照这个顺序出队就可以了 \u0026#39;\u0026#39;\u0026#39; if root == None: return None queue = [] \u0026#39;\u0026#39;\u0026#39;用列表模仿入队\u0026#39;\u0026#39;\u0026#39; queue.append(root) while queue: \u0026#39;\u0026#39;\u0026#39;将队首元素出栈\u0026#39;\u0026#39;\u0026#39; current_node = queue.pop(0) print(current_node.value, end=\u0026#39; \u0026#39;) \u0026#39;\u0026#39;\u0026#39;判断该节点是否有左孩子，有就入队\u0026#39;\u0026#39;\u0026#39; if current_node.left: queue.append(current_node.left) \u0026#39;\u0026#39;\u0026#39;判断该节点是否有右孩子，有就入队\u0026#39;\u0026#39;\u0026#39; if current_node.right: queue.append(current_node.right)      堆排序 利用堆排序，时间复杂度可以是 $O(n log_2 n)$ 堆排序还可以应用到：比如：从N个数中，取最大的k个值。\n思路：构建一个k维的小顶堆，这样堆顶就是这个k个数的最小值。从N个数中逐一取数，\n 如果该数小于堆顶值，则丢弃 如果该数大于堆顶值，则该数大小目前在前k。用该数替换堆顶，然后维护堆。  # encoding: utf-8 # 大顶堆 def big_heap(array, start, end): root = start # 左孩子的索引 child = root * 2 + 1 while child \u0026lt;= end: # 节点有右子节点，并且右子节点的值大于左子节点，则将child变为右子节点的索引 if child + 1 \u0026lt;= end and array[child] \u0026lt; array[child + 1]: child += 1 if array[root] \u0026lt; array[child]: # 交换节点与子节点中较大者的值 array[root], array[child] = array[child], array[root] # 交换值后，如果存在孙节点，则将root设置为子节点，继续与孙节点进行比较 root = child child = root * 2 + 1 else: break # 小顶堆 def little_heap(array, start, end): root = start # 左孩子的索引 child = root * 2 + 1 while child \u0026lt;= end: # 节点有右子节点，并且右子节点的值小于左子节点，则将child变为右子节点的索引 if child + 1 \u0026lt;= end and array[child] \u0026gt; array[child + 1]: child += 1 if array[root] \u0026gt; array[child]: # 交换节点与子节点中较小者的值 array[root], array[child] = array[child], array[root] # 交换值后，如果存在孙节点，则将root设置为子节点，继续与孙节点进行比较 root = child child = root * 2 + 1 else: break # 正序：使用大顶堆 def heap_sort(array): first = len(array) // 2 - 1 # 1.构建大顶堆：从下到上，从右到左对每个非叶节点进行调整，循环构建成大顶堆 for start in range(first, -1, -1): big_heap(array, start, len(array) - 1) # 2.排序 for end in range(len(array) - 1, 0, -1): # 交换堆顶和堆尾的数据 array[0], array[end] = array[end], array[0] # 重新调整完全二叉树，构造成大顶堆 big_heap(array, 0, end - 1) return array # 倒序：使用小顶堆 def heap_sort_reverse(array): first = len(array) // 2 - 1 # 1.构建小顶堆：从下到上，从右到左对每个非叶节点进行调整，循环构建成大顶堆 for start in range(first, -1, -1): little_heap(array, start, len(array) - 1) # 2.排序 for end in range(len(array) - 1, 0, -1): # 交换堆顶和堆尾的数据 array[0], array[end] = array[end], array[0] # 重新调整完全二叉树，构造成大顶堆 little_heap(array, 0, end - 1) return array def main(): print(\u0026#39;#===run a program with a main function===#\u0026#39;) array = [10, 17, 50, 7, 30, 24, 27, 45, 15, 5, 36, 21] rst = heap_sort(array) print(rst)     torch模块-样例     -- ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/notes/computer_algorithm/0020_tree_search/","summary":"Depth First Search(DFS)遍历  深度优先遍历：\n 使用递归，代码比较简单 如果不用递归，可以利用栈这种数据结构   # -*- coding: utf-8 -*- class TreeNode: def __init__(self, value): self.value = value self.left = None self.right = None class Tree_Method: def DFS(self, root): \u0026#39;\u0026#39;\u0026#39; 深度优先遍历，即先访问根节点，然后遍历左子树接着遍历右子树。 主要利用栈的特点，先将右子树压栈，再将左子树压栈，这样左子树就位于栈顶， 可以结点的左子树先与右子树被遍历。 \u0026#39;\u0026#39;\u0026#39; if root == None: return None stack = [] \u0026#39;\u0026#39;\u0026#39;用列表模仿入栈\u0026#39;\u0026#39;\u0026#39; stack.append(root) while stack: \u0026#39;\u0026#39;\u0026#39;将栈顶元素出栈\u0026#39;\u0026#39;\u0026#39; current_node = stack.pop() print(current_node.value, end=\u0026#39; \u0026#39;) \u0026#39;\u0026#39;\u0026#39;判断该节点是否有右孩子，有就入栈\u0026#39;\u0026#39;\u0026#39; if current_node.right: stack.append(current_node.right) \u0026#39;\u0026#39;\u0026#39;判断该节点是否有左孩子，有就入栈\u0026#39;\u0026#39;\u0026#39; if current_node.left: stack.append(current_node.left) def preOrder(self, root): \u0026#39;\u0026#39;\u0026#39;先序遍历\u0026#39;\u0026#39;\u0026#39; if root == None: return None print(root.","tags":null,"title":"二叉树-遍历"},{"categories":null,"contents":" 分治法 分治法(divide and conquer)的工作原理：\n 找出简单的基线条件。 确定如何缩小问题的规模，使其符合基线条件。  分治法：并非可用于解决问题的算法，而是一种解决问题的思路。\n 待解决复杂问题，能够简化为若干个小规模相同的问题，各个子问题独立存在，并且与原问题形式相同； 递归地解决各个子问题； 将各个子问题的解合并，得到原问题的解。  实例1：\nN和M的最大公约数（把一块农田均分成方块，求方块最大值）\n实例2:\n快速排序：\n 基线条件：空数组或者只有一个元素的数组，直接返回 选中一个基准值后，小于基准值的放在左边，大于基准值的放在右边。  def quick_sort(nums): if len(nums) \u0026lt; 2: return nums else: pivot = nums[0] lesser, greater = [], [] for item in nums[1:]: if item \u0026lt;= pivot: lesser.append(item) else: greater.append(item) return quick_sort(lesser) + [pivot] + quick_sort(greater) 实例3：\n归并排序\n实例4：\n数组中最大值\n 普通的做法：设置个变量记录当前的最大值，变量数组中的每个值，最终找到数组的最大值。这中做法的时间复杂度 $O(n)$ 分治法：把数组分成两半：分别找打这两个子数组的最大值，再从这两个值中选出最大值。以此类推。这种做法的时间复杂度 $O(\\log n)$       贪心算法  近似算法：approximation algorithm. 在获得精确解需要的时间太长时，可使用近似算法。判断近似算法优劣的标准如下：\n 速度有多快 得到的近似解与最优解的接近程度      NP完全问题：就是以难解著称的问题。很多非常聪明的人都认为，根本不可能编写出可快速解决这些问题的算法。\n 集合覆盖问题：有n个广播站，每个广播站可能覆盖几个省(覆盖有重复)，想要覆盖全国，最少需要选那几个广播站。每个广播站覆盖的范围：是一个集合。想要全集：选最少个集合，并集是全集。 旅行商问题：旅行商打算旅行n个城市，找出前往这n个城市的最短路径。如果要找最优解：有n!种可能。    如何识别NP完全问题：如果能够判断是NP完全问题，这样就好了，就不用去寻找完美的解决方案，而是使用近似算法即可。\n 元素较少时算法的运行速度非常快，但随着元素数量的增加，速度会变得非常慢 涉及所有组合的问题，通常是NP完全问题 不能将问题分成小问题，必须考虑各种可能的情况，这可能是NP完全问题 如果问题涉及序列(比如：旅行商问题中的城市序列) 且难以解决，可能就是NP完全问题 如果问题涉及集合(比如：广播台集合) 且难以解决，可能就是NP完全问题 如果问题可转换为集合问题、旅行商问题，它肯定就是NP完全问题。      贪心算法：\n 贪心算法，是寻找局部最优解，企图以这种方式获得全局最优解 面对NP完全问题，还没有找到快速解决方案。最佳的做法是使用近似算法 贪心算法，是一种易于实现、运行速度快 的近似算法。    贪心算法解决的问题：\n 教室，安排课程 旅行商问题 序列全排列问题     贪心算法：在对问题求解时，总是做出在当前看来是做好的选择。即：当考虑做何种选择的时候，我们只考虑对当前问题最佳的选择而不考虑子问题的结果，这是贪心算法可行的第一个基本要素。不从整体最优上考虑，而是仅仅在某种意义上的局部最优解。贪心算法以迭代的方式作出相继的贪心选择，每做一次贪心选择就将问题简化为规模更小的子问题。\n何时采用贪心算法：对于一个具体问题，要确定它是否具有贪心选择性质，必须证明每一步所作的贪心选择最终导致问题的整体最优解。\n示例：\n完全背包问题、均分纸牌、最大整数\n实际上，贪心算法适用的情况很少。需要先证明：局部最优解会得出整体最优解，才可以使用。一旦证明能成立，它就是一种高效的算法。\n例如【0-1背包问题】：即：对于每个物品，要么装要么不装(0或1)\n有一个背包，背包容量是M=150。有7个物品，物品可以分割成任意大小。要求尽可能让装入背包中的物品总价值最大，但不能超过总容量。\n物品： A B C D E F G\n重量： 35 30 60 50 40 10 25\n价值： 10 40 30 50 35 40 30\n目标函数： ∑pi最大\n利用贪心算法，可以这样：\n 每次挑选价值最大的物品装入背包，（是否是最优解？） 每次选择重量最小的物品装入背包，（是否是最优解？） 每次选择单位重量价值最大的物品，（是否是最优解？）  上面的3中贪心策略，都无法成立，所以不能采用贪心算法。所以，贪心算法虽然简单高效，但是能证明可以使用该算法的场景比较少。\n     动态规划 与分治法的不同：\n动态规划与分治法相似，都是组合子问题的解来解决原问题，与分治法的不同在于：\n 分治法：将原问题划分为一个个不相交的子问题（比如：归并排序，将数组不断地划分为一个个的子数组进行排序，再将返回的两个有序数组进行合并排序） 动态规划：要解决的是子问题有重叠的问题，例如0-1背包问题。即：不同的子问题有公共的子子问题，这些重叠的子问题在动态规划中是不应该也不需要重新计算的，而是应该将其解以一定方式保存起来，提供给父问题使用。  哪些可以使用动态规划呢？\n 动态规划可以帮助你在给定约束条件下找到最优解。比如：在背包问题中，你必须在背包容量给定的情况下，偷取价值最高的商品。 在问题可以分解为彼此独立且离散的子问题时，可以使用动态规划来解决问题  怎么设计动态规划？：\n要设计出动态规划解决方案可能很难，下面给出一些小贴士：  每种动态规划解决方案都涉及网格\n 单元格的值通常就是你需要优化的值。 每个单元格都是一个子问题，因此你应考虑如何将问题分成子问题，这有助于找出网格的坐标轴。   个人理解：\n 在设计网格时，要考虑：限制条件、分解子问题。 每个单元格是一个需要优化的值，这个值是在 限制条件下的子问题的最优解。如果限制条件不一样，分解的子问题也会不一样。所以：限制条件要找准。   -- 例如:\n 最长公共子串(要求连续相同)： 在设计网格时，每个单元格是一个需要优化的值，含义是：公共字符串必须含有当前位置字符结尾的情况下的最长公共子串。所以才会有：如果结尾不相同，值=0；如果结尾相同，左上角 + 1。 最长公共子序列(不要求连续相同，只要顺序一致上 相同)：在设计网格时，每个单元格是当前状态下最长公共子序列的长度值。所以，在两个字母不同时会保持当前最大值。如果两个字母不同，max(上方，左边)；如果两个字母相同，左上方 + 1。 如果是 max(上边，左边) + 1，会是怎么呢？答案是：如果有多个相同的，会累积。  设计步骤：\n动态规划通常用来求解最优解问题，这类问题会有很多个解，每个解都对应一个值，而我们则希望在这些解中找到最优解（最大值或者最小值）。 通常四个步骤设计一个动态规划算法：\n 定义dp数组以及下标的含义； 推导出：递推公式 dp数组的初始化 遍历顺序 打印出dp数组  实现方法：\n 递归，属于自顶向下的计算方法：如果子问题有重复计算的情况下，需要一个备忘录来辅助实现，备忘录主要用来保存每一个子问题的解，当每个子问题只求一次，如果后续需要子问题的解，只需要查找备忘录中保存的结果，不必重复计算。 动态规划，属于自底向上的计算方法：此方法最常用，必须明确每个子问题规模的概念，使得任何子问题的求解都依赖于子子问题的解来进行求解。  示例：\n0-1背包问题 最长公共子串\n最长公共子序列\n编辑距离   股票问题 ： 所有股票问题都是要最大化手里持有的钱。 买股票手里的钱减少，卖股票手里的钱增加，无论什么时刻，我们要保证手里的钱最多。而且，本次买还是卖只跟上一次我们卖还是买的状态有关。\n buy和sell都代表操作之后手里的钱。 buy和sell：都有两个状态，操作、不操作。所以，当buy和sell 保持上一次的状态，就表示本次没有买卖操作；当buy和sell状态有变动，就表示本次有买卖操作。具体是否需要有操作，是优化条件决定的：保持手里的钱最多。  实例\n 只交易1次\n121. 买卖股票的最佳时机  class Solution: def maxProfit(self, prices: List[int]) -\u0026gt; int: buy, sell = -float(\u0026#34;inf\u0026#34;), 0 for p in prices: buy = max(buy, 0 - p) sell = max(sell, buy + p) return sell  交易无限次\n122. 买卖股票的最佳时机 II 这两个问题唯一的不同点在于我们是买一次还是买无穷多次，而代码就只有 0-p 和 sell-p 的区别。 因为如果买无穷多次，就需要上一次卖完的状态。如果只买一次，那么上一个状态一定是0。 class Solution: def maxProfit(self, prices: List[int]) -\u0026gt; int: buy, sell = -float(\u0026#34;inf\u0026#34;), 0 for p in prices: buy = max(buy, sell - p) sell = max(sell, buy + p) return sell  只交易2次\n123. 买卖股票的最佳时机 III class Solution: def maxProfit(self, prices: List[int]) -\u0026gt; int: b1, b2, s1, s2 = -float(\u0026#34;inf\u0026#34;), -float(\u0026#34;inf\u0026#34;), 0, 0 for p in prices: b1 = max(b1, 0 - p) s1 = max(s1, b1 + p) b2 = max(b2, s1 - p) s2 = max(s2, b2 + p) return s2  只交易k次\n188. 买卖股票的最佳时机 IV class Solution: def maxProfit(self, k: int, prices: List[int]) -\u0026gt; int: k = min(k, len(prices) // 2) buy = [-float(\u0026#34;inf\u0026#34;)] * (k+1) sell = [0] * (k+1) for p in prices: for i in range(1, k+1): buy[i] = max(buy[i], sell[i-1] - p) sell[i] = max(sell[i], buy[i] + p) return sell[-1]  可交易无限次 + 冷冻期\n309. 买卖股票的最佳时机含冷冻期 这道题只是第二题的变形，卖完要隔一天才能买，那么就多记录上一次卖的状态即可。 class Solution: def maxProfit(self, prices: List[int]) -\u0026gt; int: buy, sell_pre, sell = -float(\u0026#34;inf\u0026#34;), 0, 0 for p in prices: buy = max(buy, sell_pre - p) sell_pre, sell = sell, max(sell, buy + p) return sell  可交易无限次 + 手续费\n714. 买卖股票的最佳时机含手续费 每次买卖需要手续费，那么我们买的时候减掉手续费就行了。 class Solution: def maxProfit(self, prices: List[int], fee: int) -\u0026gt; int: buy, sell = -float(\u0026#34;inf\u0026#34;), 0 for p in prices: buy = max(buy, sell - p - fee) sell = max(sell, buy + p) return sell        回溯法 回溯法： 是一种类似枚举的搜索尝试过程，在搜索尝试过程中寻找问题的解，当发现已不满足条件时，就回溯返回，尝试别的路径。\n回溯法是一种选优搜索法，通常是创建一棵树，从根节点出发，按照深度优先搜索的策略进行搜索，到达某一节点后，搜索该节点是否包含该问题的解：\n 设计状态：表示求解问题的不同阶段，在回溯的时候，要有状态重置 如果包含，则进入下一个节点进行搜索； 如果不包含，则回溯到父节点选择其他支路进行搜索。  何时采用回溯算法： 必须有标志性操作——搜索时不满足条件就剪枝 + 所有解\n设计步骤:\n 针对所给的原问题，定义问题的解空间，设计状态，用于记录不同阶段 确定易于搜索的解空间结构； 以深度优先搜索解空间，并在搜索过程中用剪枝函数除去无效搜索。  示例：\n全排列、旅行商问题、八皇后问题\n例如：全排列\nclass Solution(object): def permute(self, nums): \u0026#34;\u0026#34;\u0026#34; :type nums: List[int] :rtype: List[List[int]] \u0026#34;\u0026#34;\u0026#34; visited = [] rst = [] n = len(nums) def combin(idx): if idx == n: rst.append(visited[:]) return for num in nums: if num not in visited: # 更新状态 visited.append(num) combin(idx+1) # 回溯 visited.pop() combin(0) return rst   对比这两种组合：\n 组合  $C_n^k$ : 排列组合，从n中选k个数，有多少种形式。 选中一个数的情况，在剩下的选k-1个：combin(idx+1, k-1) 不选中这个数的情况，在剩下的选k个：combin(idx+1, k)   和的组合  元素可以无限随便用，只要最终和为k，有多少种形式。 选中一个数 $a$ 的情况（累加 $a$），从这个数的下标开始，和 变成成了 $k-a$：combin(idx, k-a) 不用这个数 $a$ 的情况（不在累加 $a$ 了），从这个数的下一个数开始，和 还是 $k$: combin(idx+1, k) 可以理解为：每个数都会：累积1、2、3、4、。。。。次，直到触发终止条件。\n      实例  迷宫问题 N皇后问题 N 皇后问题源自国际象棋，所有棋子中权力最大的称为皇后，它可以直着走、横着走、斜着走（沿 45 度角），可以攻击移动途中遇到的任何棋子。\nN 皇后问题的具体内容是：如何将 N 个皇后摆放在 N*N 的棋盘中，使它们无法相互攻击。\n回溯算法解决N皇后问题的具体思路是：将 N 个皇后逐一放置在不同的行，以“回溯”的方式逐一测试出每行皇后所在行的具体位置，最终确定所有皇后的位置。  # 伪代码 输入 N // 输入皇后的个数 q[1...N] //存储每行的皇后的具体位置（列标） n_queens(k , n): // 确定第 k 行皇后的位置 if k \u0026gt; n: // 递归的出口 Print q // 输出各个皇后的位置 else: for j \u0026lt;- 1 to n: // 从第 k 行第 1 列开始，判断各个位置是否可行 if isSafe(k , j): // 如果可行，继续判断下一行 q[k] \u0026lt;- j // 将第 k 行皇后放置的位置 j 记录下来 n_queens(k+1 , n) // 继续判断下一行皇后的位置 # python代码 class Solution: def solveNQueens(self, n): \u0026#34;\u0026#34;\u0026#34; :type n: int :rtype: List[List[str]] \u0026#34;\u0026#34;\u0026#34; def generateBoard(): board = list() for i in range(n): row[queens[i]] = \u0026#34;Q\u0026#34; board.append(\u0026#34;\u0026#34;.join(row)) row[queens[i]] = \u0026#34;.\u0026#34; return board def backtrack(row): if row == n: board = generateBoard() solutions.append(board) else: for i in range(n): if i in columns or row-i in diagonal1 or row+i in diagonal2: continue queens[row] = i columns.add(i) diagonal1.add(row+i) diagonal2.add(row-i) backtrack(row+1) diagonal2.remove(row-i) diagonal1.remove(row+i) columns.remove(i) solutions = list() queens = [-1] * n # 列：记录在列方向上是否已经放置 columns = set() # 斜线1：从左上到右下方向：同一条斜线上的每个位置满足：行值 - 列值 是相等的。 # 因此：使用 行值 - 列值 表示一条方向的斜线。 diagonal1 = set() # 斜线2：从右上到左下方向：同一条斜线上的每个位置满足：行值 + 列值 是相等的。 # 因此：使用 行值 + 列值 表示一条方向的斜线 diagonal2 = set() row = [\u0026#34;.\u0026#34;] * n backtrack(0) return solutions      分支限界法 分支限界法(branch and bound method)： 和回溯法类似，也是一种搜索算法，与回溯法不同的是：\n 回溯法：找出问题的许多解；通常用深度优先的方式搜索解空间树； 分支限界法：找出原问题的一个解，或者 在满足约束条件的解中找出使某一目标函数的极大解/极小解。通常以广度优先或最小耗费优先的方式搜索解空间树。  在当前节点(扩展节点)处，生成其所有的子节点(分支)，然后再从当前节点的子节点表中选择下一个扩展节点。为了有效地选择下一个扩展节点，加速搜索的进程，在每个节点处，计算一个限界，从其子节点表中选择一个最有利的节点作为扩展节点，使搜索朝着解空间上最优解的分支推进。\n何时采用分支界限法： 必须有标志性操作——搜索时不满足限界就剪枝 + 最优解\n示例：\n0-1背包问题：限界就是背包的大小，一个节点的子节点表中，如果有超过限界的就直接剪枝。如下图所示：\n    ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/notes/computer_algorithm/0030_five_algorithms/","summary":"分治法 分治法(divide and conquer)的工作原理：\n 找出简单的基线条件。 确定如何缩小问题的规模，使其符合基线条件。  分治法：并非可用于解决问题的算法，而是一种解决问题的思路。\n 待解决复杂问题，能够简化为若干个小规模相同的问题，各个子问题独立存在，并且与原问题形式相同； 递归地解决各个子问题； 将各个子问题的解合并，得到原问题的解。  实例1：\nN和M的最大公约数（把一块农田均分成方块，求方块最大值）\n实例2:\n快速排序：\n 基线条件：空数组或者只有一个元素的数组，直接返回 选中一个基准值后，小于基准值的放在左边，大于基准值的放在右边。  def quick_sort(nums): if len(nums) \u0026lt; 2: return nums else: pivot = nums[0] lesser, greater = [], [] for item in nums[1:]: if item \u0026lt;= pivot: lesser.append(item) else: greater.append(item) return quick_sort(lesser) + [pivot] + quick_sort(greater) 实例3：\n归并排序\n实例4：\n数组中最大值\n 普通的做法：设置个变量记录当前的最大值，变量数组中的每个值，最终找到数组的最大值。这中做法的时间复杂度 $O(n)$ 分治法：把数组分成两半：分别找打这两个子数组的最大值，再从这两个值中选出最大值。以此类推。这种做法的时间复杂度 $O(\\log n)$       贪心算法  近似算法：approximation algorithm.","tags":null,"title":"五大常用算法"},{"categories":null,"contents":"标题     -- 狄克斯特拉算法(Dijkstra) 参考算法解析  广度优先搜索：可以回答两类问题，即：适合非加权图\n 从节点A出发，有往节点B的路径吗？ 从节点A出发，前往节点B的那条路径最短。      狄克斯特拉算法(Dijkstra)：适合 没有负权边的加权图。\n 狄克斯特拉算法，假设：对于处理过的节点，没有前往该节点的更短路径。这种假设仅在没有负权边时才成立。 狄克斯特拉算法，是典型最短路径算法，用于计算一个结点到其他结点的最短路径。 它的主要特点是以起始点为中心向外层层扩展(广度优先搜索思想)，直到扩展到终点为止。      贝尔曼-福德算法：适合 包含负权边的加权图\n   狄克斯特拉算法(Dijkstra)包括4个步骤\n  找出”最便宜“的节点，即：可在最短时间内到达的节点 更新该节点的邻居的开销，检查是否有前往它们的更短路径，如果有，就更新其开销。 重复这个过程，直到对图中的每个节点都这样做了 计算最终路径   例如：乐谱 -换-\u0026gt; 钢琴\n第一步：找出最便宜的节点。这里，换海报最便宜了，不需要支付额外的费用。\n第二步：计算前往该节点的各个邻居的开销。\n父节点：代表该节点的上一级最便宜节点。\n第三步：目前条件(未遍历：黑胶唱片、吉他、架子鼓；已遍历：海报)，在目前未遍历节点中找下一个最便宜的节点是 ”黑胶唱片“；更新 ”黑胶唱片“ 的各个邻居的开销。\n下一个最便宜的是 吉他，因此更新其邻居的开销： 下一个最便宜的是 架子鼓，因此更新其邻居的开销： 第四步：所有节点都已遍历完了，当前，我们直到最短路径的开销是35美元，但如何确定这条路径呢？为此，可以根据父节点寻找。\n    狄克斯特拉算法：python实例：乐谱 -换-\u0026gt; 钢琴 class Solution(object): def __init__(self): pass @staticmethod # 在未处理的节点中找出开销最小的节点 def find_lowest_cost_node(costs, processed): lowest_cost = float(\u0026#39;inf\u0026#39;) lowest_cost_node = None for node in costs: cost = costs[node] if cost \u0026lt; lowest_cost and node not in processed: lowest_cost = cost lowest_cost_node = node return lowest_cost_node def dikesi(self, graph): # 开销-散列表。未知节点的开销，先设置为无穷大 costs = { \u0026#39;A\u0026#39;: 0, \u0026#39;B\u0026#39;: 5, \u0026#39;C\u0026#39;: 0, \u0026#39;D\u0026#39;: float(\u0026#39;inf\u0026#39;), \u0026#39;E\u0026#39;: float(\u0026#39;inf\u0026#39;), \u0026#39;F\u0026#39;: float(\u0026#39;inf\u0026#39;) } # 父节点-散列表 parents = {\u0026#39;B\u0026#39;: \u0026#39;A\u0026#39;, \u0026#39;C\u0026#39;: \u0026#39;A\u0026#39;, \u0026#39;F\u0026#39;: None} # 已处理过的节点 processed = [] node = \u0026#39;A\u0026#39; while node is not None: cost = costs[node] neighbors = graph[node] # 遍历当前节点的所有邻居 for n in neighbors: new_cost = cost + neighbors[n] # 如果当前节点前往邻居更近，就更新该邻居的开销；同时更新该邻居的父节点 if costs[n] \u0026gt; new_cost: costs[n] = new_cost parents[n] = node processed.append(node) node = self.find_lowest_cost_node(costs, processed) return costs[\u0026#39;F\u0026#39;] # 图-散列表 graph = {} graph[\u0026#39;A\u0026#39;] = {\u0026#39;B\u0026#39;: 5, \u0026#39;C\u0026#39;: 0} graph[\u0026#39;B\u0026#39;] = {\u0026#39;D\u0026#39;: 15, \u0026#39;E\u0026#39;: 20} graph[\u0026#39;C\u0026#39;] = {\u0026#39;D\u0026#39;: 30, \u0026#39;E\u0026#39;: 35} graph[\u0026#39;D\u0026#39;] = {\u0026#39;F\u0026#39;: 20} graph[\u0026#39;E\u0026#39;] = {\u0026#39;F\u0026#39;: 10} graph[\u0026#39;F\u0026#39;] = {} alpha = Solution() rst = alpha.dikesi(graph)      弗洛伊德算法(Floyd)  和Dijkstra算法一样，弗洛伊德(Floyd)算法也是一种用于寻找给定的加权图中顶点间最短路径的算法。该算法名称以创始人之一、1978年图灵奖获得者、斯坦福大学计算机科学系教授罗伯特·弗洛伊德命名。\n 弗洛伊德算法(Floyd)计算图中各个顶点之间的最短路径 迪杰斯特拉算法用于计算图中某一个顶点到其他顶点的最短路径。 弗洛伊德算法 VS 迪杰斯特拉算法：  迪杰斯特拉算法通过选定的被访问顶点，求出从出发访问顶点到其他顶点的最短路径； 弗洛伊德算法中每一个顶点都是出发访问点，所以需要将每一个顶点看做被访问顶点，求出从每一个顶点到其他顶点的最短路径。        弗洛伊德算法(Floyd) 的步骤：\n 定义初始化矩阵：距离矩阵 $D_0$、节点序列矩阵 $S_0$。  $d_{ij}$：表示 节点i 到 节点j 的最短距离。 $s_{ij}$：表示 节点i 到 节点j 需要经过节点j。   一般经过k步，每步表示：如果包含节点k，从节点i 到 节点j 的最短距离是否会更短。如果更短就包含 节点k；否则 不包含。\n对于矩阵 $D_{k-1}$（上一步完成后的矩阵），如果满足条件： $d_{ik} + d_{kj} \u0026lt; d_{ij}, i \\ne k, j \\ne k, i \\ne j$，则进行下面的操作：  用 $d_{ik} + d_{kj}$ 替换矩阵 $D_{k-1}$ 中的元素 $d_{ij}$，从而得到矩阵 $D_k$ 用 $k$ 替换矩阵 $S_{k-1}$ 中的元素 $s_{ij}$，从而得到矩阵 $S_k$ 令 $k = k + 1$，如果 $k = n+1$，即：每个节点都遍历过了。停止，否则重复上面操作     例如：\n步骤1：初始化矩阵：距离矩阵 $D_0$、节点序列矩阵 $S_0$。\n迭代1：令k=1，表示：从 节点i 到 节点j，如果经过 节点1 中转，路径是否会缩短。比较：$d_{ik} + d_{kj}$ 与 $D_0$ 中[i][j]，经过比较，只有： $d_{23}, d_{32}$\n迭代2：修改了：$d_{14}, d_{41}$ 迭代3：修改了：$d_{15}, d_{25}$ 迭代4：修改了：$d_{15}, d_{23}, d_{25}, d_{32}, d_{35}, d_{51}, d_{52}, d_{53}$ 迭代5：没有修改\n最后得到的矩阵为 个人理解：\n 遍历 $k \\in (所有节点)$ ：每次以一个节点为桥梁，检测能不能连通两个节点（在已经遍历的节点基础上，可能会连接得更远）；或者检测是不是能使两个节点的路径变得更短。 遍历 $k \\in (所有节点)$ ：其实先遍历那个无所谓，比如 $ 1 \\rightarrow 2 \\rightarrow 4 \\rightarrow 5$ 与 $5 \\rightarrow 4 \\rightarrow 2 \\rightarrow 1$ 每什么区别。  这两个矩阵包含了网络中任意两个节点最短路径的所有信息。比如\n 从矩阵$D$中可以看出节点1到节点5的最短路径长度为12。 从矩阵$S$中发现，节点1到节点5的中间节点是4; 从节点1到节点4的中间节点是2；从节点1到节点2，没有中间节点。        弗洛伊德算法(Floyd)-实例 # encoding: utf-8 class Solution(object): def calcEquation(self, graph): arr = list(graph.keys()) # floyd 算法 for k in arr: for i in arr: for j in arr: if k == i or k == j or i == j: continue if k in graph[i] and j in graph[k]: if j in graph[i]: graph[i][j] = min(graph[i][j], graph[i][k] + graph[k][j]) else: graph[i][j] = graph[i][k] + graph[k][j] return graph def main(): print(\u0026#39;#===run a program with a main function===#\u0026#39;) differ = Solution() graph = { \u0026#34;1\u0026#34;: {\u0026#34;2\u0026#34;: 3, \u0026#34;3\u0026#34;: 10}, \u0026#34;2\u0026#34;: {\u0026#34;1\u0026#34;: 3, \u0026#34;4\u0026#34;: 5}, \u0026#34;3\u0026#34;: {\u0026#34;1\u0026#34;: 10, \u0026#34;4\u0026#34;: 6, \u0026#34;5\u0026#34;: 15}, \u0026#34;4\u0026#34;: {\u0026#34;2\u0026#34;: 5, \u0026#34;3\u0026#34;: 6, \u0026#34;5\u0026#34;: 4}, \u0026#34;5\u0026#34;: {\u0026#34;4\u0026#34;: 4} } rst = differ.calcEquation(graph) print(rst) if __name__ == \u0026#39;__main__\u0026#39;: main() # 结果 # { # \u0026#34;1\u0026#34;: {\u0026#34;2\u0026#34;: 3, \u0026#34;3\u0026#34;: 10, \u0026#34;4\u0026#34;: 8, \u0026#34;5\u0026#34;: 12},  # \u0026#34;2\u0026#34;: {\u0026#34;1\u0026#34;: 3, \u0026#34;3\u0026#34;: 11, \u0026#34;4\u0026#34;: 5, \u0026#34;5\u0026#34;: 9},  # \u0026#34;3\u0026#34;: {\u0026#34;1\u0026#34;: 10 \u0026#34;2\u0026#34;: 11, \u0026#34;4\u0026#34;: 6, \u0026#34;5\u0026#34;: 10,},  # \u0026#34;4\u0026#34;: {\u0026#34;1\u0026#34;: 8, \u0026#34;2\u0026#34;: 5, \u0026#34;3\u0026#34;: 6, \u0026#34;5\u0026#34;: 4},  # \u0026#34;5\u0026#34;: {\u0026#34;1\u0026#34;: 12, \u0026#34;2\u0026#34;: 9, \u0026#34;3\u0026#34;: 10, \u0026#34;4\u0026#34;: 4} # }     克鲁斯卡尔(Kruskal)算法 参考算法解析 最佳应用：修路问题：\n有北京有新增7个站点(A, B, C, D, E, F, G) ，现在需要修路把7个站点连通，各个站点的距离用边线表示(权) ，比如 A – B 距离 12公里，问：如何修路保证各个站点都能连通，并且总的修建公路总里程最短?\n克鲁斯卡尔(Kruskal)算法 ：用来求加权连通图的最小生成树的算法，采用了贪心算法\n 基本思想：按照权值，从小到大的顺序选择n-1条边，并保证这n-1条边不构成回路 具体做法：  将联通网中所有的边按照权值大小做升序排序，从权值最小的边开始选择，只要此边不构成回路，就可以选择它组成最小生成树 对N个顶点的联通网，挑选出 $N-1$ 条符合条件的边，这些边组成的生成树就是最小生成树。        普利姆(Prim)算法 参考算法解析 普利姆(Prim)算法：查找最小生成树的过程，采用了贪心算法的思想，对于包含N个顶点的连通网，买次从连通网中找出一个权值最小的边，这样的操作重复 $N-1$ 次，由 $N-1$ 条权值最小的边组成的生成树，就是最小生成树。\n最佳应用：修路问题：\n有北京有新增7个站点(A, B, C, D, E, F, G) ，现在需要修路把7个站点连通，各个站点的距离用边线表示(权) ，比如 A – B 距离 12公里，问：如何修路保证各个站点都能连通，并且总的修建公路总里程最短?\n思路\n 将连通网中的所有顶点分为两类（假设为：A类、B类）。初始状态下，所有顶点位于B类； 选择任意一个顶点，将其从B类移动到A类； 从B类的所有顶点出发，找出一条连接着A类中某个顶点且权值最小的边，将次边连接的B类中的顶点移动到A类; 重复执行第3步，直到B类中的所有顶点全部移动到A类，恰好可以找到 $N-1$ 条边。  例如：\n 初始化状态： $A = \\lbrace \\rbrace, B = \\lbrace A, B, C, D, S, T \\rbrace$ 随便选一个点，比如 $S$，从B类移动到A类：$A = \\lbrace S \\rbrace$ ，找与 $S$ 最近的点 是 $A$ 重复上面的操作，直到找到N-1个边      马踏棋盘算法  马踏棋盘算法也被称为骑士周游问题实际上是图的深度优先搜索(DFS)的应用 将马随机放在国际象棋的8×8棋盘Board[0～7][0～7]的某个方格中，马按走棋规则(马走日字)进行移动。要求每个方格只进入一次，走遍棋盘上全部64个方格      标题     -- ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/notes/computer_algorithm/0050_graph_search/","summary":"标题     -- 狄克斯特拉算法(Dijkstra) 参考算法解析  广度优先搜索：可以回答两类问题，即：适合非加权图\n 从节点A出发，有往节点B的路径吗？ 从节点A出发，前往节点B的那条路径最短。      狄克斯特拉算法(Dijkstra)：适合 没有负权边的加权图。\n 狄克斯特拉算法，假设：对于处理过的节点，没有前往该节点的更短路径。这种假设仅在没有负权边时才成立。 狄克斯特拉算法，是典型最短路径算法，用于计算一个结点到其他结点的最短路径。 它的主要特点是以起始点为中心向外层层扩展(广度优先搜索思想)，直到扩展到终点为止。      贝尔曼-福德算法：适合 包含负权边的加权图\n   狄克斯特拉算法(Dijkstra)包括4个步骤\n  找出”最便宜“的节点，即：可在最短时间内到达的节点 更新该节点的邻居的开销，检查是否有前往它们的更短路径，如果有，就更新其开销。 重复这个过程，直到对图中的每个节点都这样做了 计算最终路径   例如：乐谱 -换-\u0026gt; 钢琴\n第一步：找出最便宜的节点。这里，换海报最便宜了，不需要支付额外的费用。\n第二步：计算前往该节点的各个邻居的开销。\n父节点：代表该节点的上一级最便宜节点。\n第三步：目前条件(未遍历：黑胶唱片、吉他、架子鼓；已遍历：海报)，在目前未遍历节点中找下一个最便宜的节点是 ”黑胶唱片“；更新 ”黑胶唱片“ 的各个邻居的开销。\n下一个最便宜的是 吉他，因此更新其邻居的开销： 下一个最便宜的是 架子鼓，因此更新其邻居的开销： 第四步：所有节点都已遍历完了，当前，我们直到最短路径的开销是35美元，但如何确定这条路径呢？为此，可以根据父节点寻找。\n    狄克斯特拉算法：python实例：乐谱 -换-\u0026gt; 钢琴 class Solution(object): def __init__(self): pass @staticmethod # 在未处理的节点中找出开销最小的节点 def find_lowest_cost_node(costs, processed): lowest_cost = float(\u0026#39;inf\u0026#39;) lowest_cost_node = None for node in costs: cost = costs[node] if cost \u0026lt; lowest_cost and node not in processed: lowest_cost = cost lowest_cost_node = node return lowest_cost_node def dikesi(self, graph): # 开销-散列表。未知节点的开销，先设置为无穷大 costs = { \u0026#39;A\u0026#39;: 0, \u0026#39;B\u0026#39;: 5, \u0026#39;C\u0026#39;: 0, \u0026#39;D\u0026#39;: float(\u0026#39;inf\u0026#39;), \u0026#39;E\u0026#39;: float(\u0026#39;inf\u0026#39;), \u0026#39;F\u0026#39;: float(\u0026#39;inf\u0026#39;) } # 父节点-散列表 parents = {\u0026#39;B\u0026#39;: \u0026#39;A\u0026#39;, \u0026#39;C\u0026#39;: \u0026#39;A\u0026#39;, \u0026#39;F\u0026#39;: None} # 已处理过的节点 processed = [] node = \u0026#39;A\u0026#39; while node is not None: cost = costs[node] neighbors = graph[node] # 遍历当前节点的所有邻居 for n in neighbors: new_cost = cost + neighbors[n] # 如果当前节点前往邻居更近，就更新该邻居的开销；同时更新该邻居的父节点 if costs[n] \u0026gt; new_cost: costs[n] = new_cost parents[n] = node processed.","tags":null,"title":"图的搜索"},{"categories":null,"contents":"标题     --  递归 Leigh Caldwell在Stack Overflow上说的一句话: “如果使用循环，程序的性能可能更高;如果使用递归，程序可能更容易理解。如何选择要看什么对你来说更重要。”\n编写递归函数时，必须告诉它何时停止递归。正因为如此，每个递归函数都有两部分:\n 基线条件(base case)。指的是函数不再调用自己，从而避免形成无限循环。 递归条件(recursive case)。指的是函数调用自己。       二分查找 比如：从1~100的数字中，我认选一个，让你猜。我只会说：大了、小了、对了。需要猜多少次呢？\n二分查找：一半一半的猜，每次都排除一半。所以需要的次数是：log2N。（向上取整）\nclass BinarySearch(object): # 迭代 def search_iterative(self, nums, item): low = 0 high = len(nums) - 1 while low\u0026lt;=high: mid = (low + high) // 2 guess = nums[mid] if guess == item: return mid elif guess \u0026gt; item: high = mid - 1 else: low = mid + 1 return None # 递归 def search_recursive(self, nums, low, high, item): if high \u0026gt;= low: mid = (high + low) // 2 guess = nums[mid] if guess == item: return mid elif guess \u0026gt; item: return self.search_recursive(nums, low, mid-1, item) else: return self.search_recursive(nums, mid+1, high, item) else: return None   在排序数组中查找元素的第一个和最后一个位置  体会一下：二分查找法。如下代码中：nums为有序数组，ans 在 nums[mid] \u0026gt; target 中跟新，即：切分到最后，ans记录的是第一个 大于 target的值。\nclass Solution(object): def searchRange(self, nums, target): \u0026#34;\u0026#34;\u0026#34; :type nums: List[int] :type target: int :rtype: List[int] \u0026#34;\u0026#34;\u0026#34; n = len(nums) def binary_search(lower): left, right, ans = 0, n-1, n while left \u0026lt;= right: mid = (left + right)//2 if nums[mid] \u0026gt; target or (lower and nums[mid] \u0026gt;= target): right = mid-1 ans = mid else: left = mid + 1 return ans left_id = binary_search(True) right_id = binary_search(False)-1 if left_id \u0026lt;= right_id and right_id \u0026lt; n and nums[left_id] == target and nums[right_id]==target: return [left_id, right_id] return [-1, -1]   问题描述：给两个有序数组，nums1、nums2。返回 这两个有序数按照顺序组合并后的第k个值。\n这个可以用来解决：寻找两个正序数组的中位数 \nnums1 = [1, 2, 5, 9] nums2 = [3, 4, 6, 8, 13] m, n = len(nums1), len(nums2) def get_k(k): idx1, idx2 = 0, 0 while True: # 特殊情况：其中一个数组为空 if idx1 == m: return nums2[idx2+k-1] if idx2 == n: return nums1[idx1+k-1] if k == 1: return min(nums1[idx1], nums2[idx2]) # 从0开始，比较两个数组 第k//2位置的大小；然后调整 分割线：idx1, idx2和k的值 new_idx1 = min(idx1 + k // 2 - 1, m - 1) new_idx2 = min(idx2 + k // 2 - 1, n - 1) pivot1, pivot2 = nums1[new_idx1], nums2[new_idx2] # 如果nums1[new_idx1]位置的值比较小，说明这 new_idx1+1个数，肯定是较小的值排在前面 # 所以：起始位置 idx1 移动到new_idx1+1，k = k - (new_idx1-idx1+1)，即：删掉了new_idx1-idx1+1个元素 # 从寻找第k个值，变成：寻找第k-(new_idx1-idx1+1)个值 if pivot1 \u0026lt;= pivot2: k -= new_idx1 - idx1 + 1 idx1 = new_idx1 + 1 else: k -= new_idx2 - idx2 + 1 idx2 = new_idx2 + 1      最大公约数  原始：求两个数(N, M)的最大公约数。 变形1：假设你是农场主，有一小块土地。你要将这块地均匀地分成方块，且分出的方块要尽可能大。  伪代码-思路： 假设：N表示较小的数，M表示较大的数。 重复一下操作，直到 N=0 新N = M % N 新M = 原N      摩尔投票算法 问题描述：给定一个大小为n的数组，其中有一个元素，出现次数大于 n/2。只遍历一次数组，找出这个元素。\n 这个元素就是 “众数”。 投票：众数：票数+1，非众数：票数-1。 所以，用 票数(cnt) 和 相应的元素对应起来。如果cnt=0时，就更换元素。       数组的翻转-旋转 数组的旋转：移动k次，从1 2 3 4 5 6 \u0026ndash;移动k次-\u0026gt; 5 6 1 2 3 4 直接 环状替换:\n 从0位置开始，替换到下一个，下一个继续替换到下一个，直到回到0位置。 回到0位置后，是否遍历完全部的数据呢？可以用一个count变量来记录已遍历的个数，直到遍历完毕。  class Solution(object): def rotate(self, nums, k): \u0026#34;\u0026#34;\u0026#34; :type nums: List[int] :type k: int :rtype: None Do not return anything, modify nums in-place instead. \u0026#34;\u0026#34;\u0026#34; k = k%len(nums) if len(nums) \u0026lt; 2: return nums anchor, point =0, 0 anchor_value = nums[anchor] for _ in range(len(nums)): anchor = (anchor + k) % len(nums) anchor_value, nums[anchor] = nums[anchor], anchor_value if anchor == point: point += 1 anchor = point if anchor \u0026lt; len(nums): anchor_value = nums[anchor] return nums 数组的多次翻转==旋转\n   操作 结果     原始数组 1 2 3 4 5 6 7   翻转所有元素 7 6 5 4 3 2 1   翻转[0, k]区间 5 6 7 4 3 2 1   翻转[k, n]区间 5 6 7 1 2 3 4         图的搜索  广度优先搜索：可以回答两类问题，即：适合非加权图\n 从节点A出发，有往节点B的路径吗？ 从节点A出发，前往节点B的那条路径最短。      狄克斯特拉算法(Dijkstra)：适合 没有负权边的加权图。\n 狄克斯特拉算法，假设：对于处理过的节点，没有前往该节点的更短路径。这种假设仅在没有负权边时才成立。      贝尔曼-福德算法：适合 包含负权边的加权图\n   狄克斯特拉算法(Dijkstra)包括4个步骤\n  找出”最便宜“的节点，即：可在最短时间内到达的节点 更新该节点的邻居的开销，检查是否有前往它们的更短路径，如果有，就更新其开销。 重复这个过程，直到对图中的每个节点都这样做了 计算最终路径   例如：乐谱 -换-\u0026gt; 钢琴\n第一步：找出最便宜的节点。这里，换海报最便宜了，不需要支付额外的费用。\n第二步：计算前往该节点的各个邻居的开销。\n父节点：代表该节点的上一级最便宜节点。\n第三步：目前条件(未遍历：黑胶唱片、吉他、架子鼓；已遍历：海报)，在目前未遍历节点中找下一个最便宜的节点是 ”黑胶唱片“；更新 ”黑胶唱片“ 的各个邻居的开销。\n下一个最便宜的是 吉他，因此更新其邻居的开销： 下一个最便宜的是 架子鼓，因此更新其邻居的开销： 第四步：所有节点都已遍历完了，当前，我们直到最短路径的开销是35美元，但如何确定这条路径呢？为此，可以根据父节点寻找。\n    狄克斯特拉算法：python实例：乐谱 -换-\u0026gt; 钢琴 class Solution(object): def __init__(self): pass @staticmethod # 在未处理的节点中找出开销最小的节点 def find_lowest_cost_node(costs, processed): lowest_cost = float(\u0026#39;inf\u0026#39;) lowest_cost_node = None for node in costs: cost = costs[node] if cost \u0026lt; lowest_cost and node not in processed: lowest_cost = cost lowest_cost_node = node return lowest_cost_node def dikesi(self, graph): # 开销-散列表。未知节点的开销，先设置为无穷大 costs = { \u0026#39;A\u0026#39;: 0, \u0026#39;B\u0026#39;: 5, \u0026#39;C\u0026#39;: 0, \u0026#39;D\u0026#39;: float(\u0026#39;inf\u0026#39;), \u0026#39;E\u0026#39;: float(\u0026#39;inf\u0026#39;), \u0026#39;F\u0026#39;: float(\u0026#39;inf\u0026#39;) } # 父节点-散列表 parents = {\u0026#39;B\u0026#39;: \u0026#39;A\u0026#39;, \u0026#39;C\u0026#39;: \u0026#39;A\u0026#39;, \u0026#39;F\u0026#39;: None} # 已处理过的节点 processed = [] node = \u0026#39;A\u0026#39; while node is not None: cost = costs[node] neighbors = graph[node] # 遍历当前节点的所有邻居 for n in neighbors: new_cost = cost + neighbors[n] # 如果当前节点前往邻居更近，就更新该邻居的开销；同时更新该邻居的父节点 if costs[n] \u0026gt; new_cost: costs[n] = new_cost parents[n] = node processed.append(node) node = self.find_lowest_cost_node(costs, processed) return costs[\u0026#39;F\u0026#39;] # 图-散列表 graph = {} graph[\u0026#39;A\u0026#39;] = {\u0026#39;B\u0026#39;: 5, \u0026#39;C\u0026#39;: 0} graph[\u0026#39;B\u0026#39;] = {\u0026#39;D\u0026#39;: 15, \u0026#39;E\u0026#39;: 20} graph[\u0026#39;C\u0026#39;] = {\u0026#39;D\u0026#39;: 30, \u0026#39;E\u0026#39;: 35} graph[\u0026#39;D\u0026#39;] = {\u0026#39;F\u0026#39;: 20} graph[\u0026#39;E\u0026#39;] = {\u0026#39;F\u0026#39;: 10} graph[\u0026#39;F\u0026#39;] = {} alpha = Solution() rst = alpha.dikesi(graph)      双指针 示例：盛更多的水\n通过这个题目，来体会一下 双指针的优雅。\n 开始的状态：一个指针（left）指向 头，一个指针（right）指向 尾； 当 $h[left_0] \u0026lt; h[right_0]$时，则把 $left_0+=1$ 。\n这是因为：当 $h[left_0] \u0026lt; h[right_0]$ 时，如果 $left_0$ 指针不动，调整 $right_0$ 指针， $ right \\in (left_0, right_0]$ 这个系列的值对会小于 起始值，所以，相当于这个系列拿最大值比较，其他的就不用考虑了。 所以，每个调整小边。相当于每次过滤了一批可能项。对比两个for循环的话，所有可能项都会遍历。  class Solution(object): def maxArea(self, height): \u0026#34;\u0026#34;\u0026#34; :type height: List[int] :rtype: int \u0026#34;\u0026#34;\u0026#34; max_c = 0 n = len(height) i, j = 0, n-1 while i \u0026lt; j: if height[i] \u0026lt; height[j]: max_c = max(max_c, height[i] * (j-i)) i += 1 else: max_c = max(max_c, height[j] * (j - i)) j -= 1 return max_c      固定位数-比较 示例：字母异位词分组 关键点：26个字母是固定的，异位词：相同字母的个数是一样的，就是位置可能不一样。每个字符串 都可以用 [0]*26，累积每个位置字母的个数。用这个 元组作为比较的可以。 类似这个题目，有固定位数的情况，可以考虑 固定位数的数组（[0]*26）、固定位数的二进制表示。\n     KMP算法 KMP算法，是经典的字符匹配算法。\n在介绍KMP之前，先介绍 字符串的 前缀集合 和 后缀集合 比如：abab，前缀集合：a, ab, aba； 后缀集合：b, ab, bab。\nKMP算法的思路：\n 逐个 原字符串 和 匹配字符串。每次匹配失败时，就不用从0开始，而是从当前 前/后缀 的最长长度 开始 用next记录 匹配字符串 的每个位置的最长 前/后缀 的最长长度  在计算next数组时，当匹配失败时，为什么是：$ j = next[j-1] $ ？\n 此时，$p[j] \\ne p[i]$。$p[:j]$：表示目前最长的前缀 $next[j-1]$ ：表示 在 $p[:j]$ 字符串中，最长的 前/后缀长度，假设 $ k = next[j-1] $。 所以 $ p[:k] == p[i-k: i] == p[j-k: j] $，这三段字符串相等，所以，$ j = next[j-1] $，即：从 $ p[k] $ 开始 继续与 $p[i]$ 比较  def get_next(p): n = len(p) # i: 表示遍历的字符下标 # j: 最大前后缀长度 i, j = 1, 0 # 记录：每个当前的最大前后缀长度 next = [0] * n for i in range(1, n): # 如果相等，长度+1 if p[i] == p[j]: j += 1 next[i] = j # 如果不相等，更新长度的值，直到相等或者长度为0 else: while j \u0026gt; 0 and p[i] != p[j]: j = next[j-1] return next def kmp_search(string, patt): # 获取next数组 next = get_next(patt) i, j, n, m = 0, 0, len(string), len(patt) while i \u0026lt; n and j \u0026lt; m: if string[i] == patt[j]: i += 1 j += 1 elif j \u0026gt; 0: # 字符匹配失败，根据next跳过子串前面的一些字符 j = next[j-1] else: # 子串第一个字符就失配 i += 1 if j == m: return i-m return -1     标题     -- ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/notes/computer_algorithm/0015_comm_ideas/","summary":"标题     --  递归 Leigh Caldwell在Stack Overflow上说的一句话: “如果使用循环，程序的性能可能更高;如果使用递归，程序可能更容易理解。如何选择要看什么对你来说更重要。”\n编写递归函数时，必须告诉它何时停止递归。正因为如此，每个递归函数都有两部分:\n 基线条件(base case)。指的是函数不再调用自己，从而避免形成无限循环。 递归条件(recursive case)。指的是函数调用自己。       二分查找 比如：从1~100的数字中，我认选一个，让你猜。我只会说：大了、小了、对了。需要猜多少次呢？\n二分查找：一半一半的猜，每次都排除一半。所以需要的次数是：log2N。（向上取整）\nclass BinarySearch(object): # 迭代 def search_iterative(self, nums, item): low = 0 high = len(nums) - 1 while low\u0026lt;=high: mid = (low + high) // 2 guess = nums[mid] if guess == item: return mid elif guess \u0026gt; item: high = mid - 1 else: low = mid + 1 return None # 递归 def search_recursive(self, nums, low, high, item): if high \u0026gt;= low: mid = (high + low) // 2 guess = nums[mid] if guess == item: return mid elif guess \u0026gt; item: return self.","tags":null,"title":"常见算法思路"},{"categories":null,"contents":"数据结构 常见的数据结构可分为「线性数据结构」与「非线性数据结构」，具体为：「数组」、「链表」、「栈」、「队列」、「树」、「图」、「散列表」、「堆」。\n    数组与链表  数组： 在内存中是连续的一整块。\n 随机访问，数组在内存中是连续的一整块，所以支持随机访问。 增/删操作，费事。增加元素时，如果内存不够一整块，还得整体迁移      链表： 可以存储在内存的任何地方。\n 顺序访问，由于存在任何地方，每个元素都存储了下一个元素的地址，所以只能从头开始逐个查询。 增/删操作，不费事。只要修改一下 下一元素地址 就行。       栈 递归操作，就是使用的调用栈。即：把每个递归调用函数，都压入栈，完成一个弹出一个，直到空栈。\n 优先考虑用栈的 “信号”\n 有返回上一步的操作 成对匹配的问题，比如：（） 链表/list 的翻转问题，例如：K 个一组翻转链表         队列 队列(First In First Out)：先进先出的数据结构。\n  图的广度优先搜索，就是先把1级元素压入队列，然后在一个一个出队遍历时，把其邻居压入队列         树 字典树\n字典树 ( Trie 树 ) 又称单词查找树， 是一种用于在 字符串集合 中 高效地 存储 和 查找 字符串 的 树形 数据结构。\n实例： 实现 Trie (前缀树)\nclass Trie(object): def __init__(self): self.children = [None] * 26 self.is_end = False def searchPrefix(self, prefix): node = self for ch in prefix: ch = ord(ch) - ord(\u0026#39;a\u0026#39;) if not node.children[ch]: return None node = node.children[ch] return node def insert(self, word): \u0026#34;\u0026#34;\u0026#34; :type word: str :rtype: None \u0026#34;\u0026#34;\u0026#34; node = self for ch in word: ch = ord(ch) - ord(\u0026#39;a\u0026#39;) if not node.children[ch]: node.children[ch] = Trie() node = node.children[ch] node.is_end = True def search(self, word): \u0026#34;\u0026#34;\u0026#34; :type word: str :rtype: bool \u0026#34;\u0026#34;\u0026#34; node = self.searchPrefix(word) return node is not None and node.is_end def startsWith(self, prefix): \u0026#34;\u0026#34;\u0026#34; :type prefix: str :rtype: bool \u0026#34;\u0026#34;\u0026#34; node = self.searchPrefix(prefix) return node is not None      散列表 散列函数：将任何输入映射到数字。\n在pyhton中 散列表的实现为字典 dict()\n散列表是一种非线性数据结构，通过利用 Hash 函数将指定的「键 key」映射至对应的「值 value」，以实现高效的元素查找。\n比如：通过输入学号，在名字库里找到对应的名字。\n# 输入：学号 # 小力: 10001 # 小特: 10002 # 小扣: 10003 # 名字库 names = [ \u0026#34;小力\u0026#34;, \u0026#34;小特\u0026#34;, \u0026#34;小扣\u0026#34; ] # Hash函数的目的：把学号，映射为序号index， # 这个序号index就是 名字库names的名字对应序号     堆 堆是一种基于「完全二叉树」的数据结构，可使用数组实现。以堆为原理的排序算法称为「堆排序」，基于堆实现的数据结构为「优先队列」。堆分为「大顶堆」和「小顶堆」，大（小）顶堆：任意节点的值不大于（小于）其父节点的值。\n完全二叉树定义： 设二叉树深度为 k，若二叉树除第 k 层外的其它各层（第 1 至 k−1 层）的节点达到最大个数，且处于第 k 层的节点都连续集中在最左边，则称此二叉树为完全二叉树。\n小顶堆： $K_i \u0026lt;= K_{2i+1} \\ \\\u0026amp; \\ K_i \u0026lt;= K_{2i+2}$ **大顶堆**： $K_i \u0026gt;= K_{2i+1} \\ \\\u0026amp; \\ K_i \u0026gt;= K_{2i+2}$ 怎么构建一个「小顶堆」?\n 遍历二叉树的非叶子节点自下往上的构造小顶堆，针对每个非叶子节点，都跟它的左右子节点比较，把最大的值换到这个子树的父节点。  上图就是一个「小顶堆」，堆的操作：\n 搜索：$O(1)$，就是访问 堆顶的元素。 添加：就是要满足堆的定义：任意节点的值不大于（小于）其父节点的值。 删除：跟添加一样，就是要满足堆的定义：任意节点的值不大于（小于）其父节点的值。  # encoding: utf-8 # 大顶堆 def big_heap(array, start, end): root = start # 左孩子的索引 child = root * 2 + 1 while child \u0026lt;= end: # 节点有右子节点，并且右子节点的值大于左子节点，则将child变为右子节点的索引 if child + 1 \u0026lt;= end and array[child] \u0026lt; array[child + 1]: child += 1 if array[root] \u0026lt; array[child]: # 交换节点与子节点中较大者的值 array[root], array[child] = array[child], array[root] # 交换值后，如果存在孙节点，则将root设置为子节点，继续与孙节点进行比较 root = child child = root * 2 + 1 else: break # 小顶堆 def little_heap(array, start, end): root = start # 左孩子的索引 child = root * 2 + 1 while child \u0026lt;= end: # 节点有右子节点，并且右子节点的值小于左子节点，则将child变为右子节点的索引 if child + 1 \u0026lt;= end and array[child] \u0026gt; array[child + 1]: child += 1 if array[root] \u0026gt; array[child]: # 交换节点与子节点中较小者的值 array[root], array[child] = array[child], array[root] # 交换值后，如果存在孙节点，则将root设置为子节点，继续与孙节点进行比较 root = child child = root * 2 + 1 else: break # 正序：使用大顶堆 def heap_sort(array): first = len(array) // 2 - 1 # 1.构建大顶堆：从下到上，从右到左对每个非叶节点进行调整，循环构建成大顶堆 for start in range(first, -1, -1): big_heap(array, start, len(array) - 1) # 2.排序 for end in range(len(array) - 1, 0, -1): # 交换堆顶和堆尾的数据 array[0], array[end] = array[end], array[0] # 重新调整完全二叉树，构造成大顶堆 big_heap(array, 0, end - 1) return array # 倒序：使用小顶堆 def heap_sort_reverse(array): first = len(array) // 2 - 1 # 1.构建小顶堆：从下到上，从右到左对每个非叶节点进行调整，循环构建成大顶堆 for start in range(first, -1, -1): little_heap(array, start, len(array) - 1) # 2.排序 for end in range(len(array) - 1, 0, -1): # 交换堆顶和堆尾的数据 array[0], array[end] = array[end], array[0] # 重新调整完全二叉树，构造成大顶堆 little_heap(array, 0, end - 1) return array      图 连通图与非联通图\n 非连通图：比如图a中，找不到一条从a到c的路径 连通图：图b是一个连通图，因为从一个顶点到另一个顶点都至少存在一条通路  生成树\n所谓生成树，是指具备一下条件的连通图\n 包含图中所有顶点 任意顶点之间有且只有一条通路，比如上图就是一个连通图，对应的多种生成树。  最小生成树：就是上述生成树中，路径权值和最小的那个。具体问题比如：修路问题，n座城市之间修公路，要求两两互联，公里数最短。\n求解最小生成树的算法通常有两种：\n 普里姆算法 克鲁斯卡尔算法      堆     --  ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/notes/computer_algorithm/0010_data_struct/","summary":"数据结构 常见的数据结构可分为「线性数据结构」与「非线性数据结构」，具体为：「数组」、「链表」、「栈」、「队列」、「树」、「图」、「散列表」、「堆」。\n    数组与链表  数组： 在内存中是连续的一整块。\n 随机访问，数组在内存中是连续的一整块，所以支持随机访问。 增/删操作，费事。增加元素时，如果内存不够一整块，还得整体迁移      链表： 可以存储在内存的任何地方。\n 顺序访问，由于存在任何地方，每个元素都存储了下一个元素的地址，所以只能从头开始逐个查询。 增/删操作，不费事。只要修改一下 下一元素地址 就行。       栈 递归操作，就是使用的调用栈。即：把每个递归调用函数，都压入栈，完成一个弹出一个，直到空栈。\n 优先考虑用栈的 “信号”\n 有返回上一步的操作 成对匹配的问题，比如：（） 链表/list 的翻转问题，例如：K 个一组翻转链表         队列 队列(First In First Out)：先进先出的数据结构。\n  图的广度优先搜索，就是先把1级元素压入队列，然后在一个一个出队遍历时，把其邻居压入队列         树 字典树\n字典树 ( Trie 树 ) 又称单词查找树， 是一种用于在 字符串集合 中 高效地 存储 和 查找 字符串 的 树形 数据结构。","tags":null,"title":"数据结果"},{"categories":null,"contents":"滑动窗口算法 参考\n 个人理解，滑动窗口主要解决的问题特点：\n 连续性，一定是连续序列或者字符串的最长/最短 的问题。   滑动窗口算法：是在给定特定窗口大小的数组或字符串上执行要求的操作，该技术可以将一部分问题中的嵌套循环转变为一个单循环，可以减少时间复杂度。即：在一个特定大小的字符串/数组上进行操作，而不是在整个字符串/数组上操作，这样就降低了问题的复杂度。\n滑动：说明这个窗口是移动的；\n窗口：窗口大小并不是固定的，可以不断扩容直到满足一定的条件；也可以不断缩小，直到找到一个满足条件的最小窗口；也可以是固定大小。\n滑动窗口算法的思路：\n 我们在字符串 S 中使用双指针中的左右指针技巧，初始化 left = right = 0，把索引闭区间 [left, right] 称为一个「窗口」。 我们先不断地增加 right 指针扩大窗口 [left, right]，直到窗口中的字符串符合要求（包含了 T 中的所有字符）。 此时，我们停止增加 right，转而不断增加 left 指针缩小窗口 [left, right]，直到窗口中的字符串不再符合要求（不包含 T 中的所有字符了）。同时，每次增加 left，我们都要更新一轮结果。 重复第 2 和第 3 步，直到 right 到达字符串 S 的尽头。  对于固定窗口大小，框架总结如下：\n# 固定窗口大小为k # 在s中 寻找窗口大小为k时的所包含最大元音字母个数 right = 0 while right\u0026lt;len(s): window.append(s[right]) right += 1 # 如果符合要求，说明窗口构造完成 if right\u0026gt;=k: # 这已经是一个窗口了，根据条件做一些事情 ... 可以计算窗口最大值 # 最后不要忘记把 【right-k】位置元素从窗口里移除 对于不固定窗口大小，框架总结如下：\n# 在s中寻找 t 的 最小覆盖子串 left, right = 0, 0 while right\u0026lt;len(s): right += 1 # 如果符合要求，说明窗口构造完成，移动left缩小窗口 while \u0026#39;符合要求\u0026#39;: # 如果这个窗口的子串更短，则更新res res = minLen(res, windown) window.remove(left) left += 1 return res     ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/notes/computer_algorithm/0040_sliding_window/","summary":"滑动窗口算法 参考\n 个人理解，滑动窗口主要解决的问题特点：\n 连续性，一定是连续序列或者字符串的最长/最短 的问题。   滑动窗口算法：是在给定特定窗口大小的数组或字符串上执行要求的操作，该技术可以将一部分问题中的嵌套循环转变为一个单循环，可以减少时间复杂度。即：在一个特定大小的字符串/数组上进行操作，而不是在整个字符串/数组上操作，这样就降低了问题的复杂度。\n滑动：说明这个窗口是移动的；\n窗口：窗口大小并不是固定的，可以不断扩容直到满足一定的条件；也可以不断缩小，直到找到一个满足条件的最小窗口；也可以是固定大小。\n滑动窗口算法的思路：\n 我们在字符串 S 中使用双指针中的左右指针技巧，初始化 left = right = 0，把索引闭区间 [left, right] 称为一个「窗口」。 我们先不断地增加 right 指针扩大窗口 [left, right]，直到窗口中的字符串符合要求（包含了 T 中的所有字符）。 此时，我们停止增加 right，转而不断增加 left 指针缩小窗口 [left, right]，直到窗口中的字符串不再符合要求（不包含 T 中的所有字符了）。同时，每次增加 left，我们都要更新一轮结果。 重复第 2 和第 3 步，直到 right 到达字符串 S 的尽头。  对于固定窗口大小，框架总结如下：\n# 固定窗口大小为k # 在s中 寻找窗口大小为k时的所包含最大元音字母个数 right = 0 while right\u0026lt;len(s): window.append(s[right]) right += 1 # 如果符合要求，说明窗口构造完成 if right\u0026gt;=k: # 这已经是一个窗口了，根据条件做一些事情 .","tags":null,"title":"滑动窗口"},{"categories":null,"contents":"Este archivo existe únicamente para responder a la URL /search con la plantilla de diseño search relacionada.\nNo se muestra ningún contenido aquí, todo el contenido se basa en la plantilla layouts/page/search.html\nEstablecer una prioridad muy baja en el mapa del sitio le dirá a los motores de búsqueda que éste no es un contenido importante.\nEsta implementación utiliza Fusejs, jquery y mark.js\nConfiguración inicial La búsqueda depende del tipo de contenido de salida adicional de JSON en config.toml\n``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nBúsqueda de archivos adicionales Para buscar campos adicionales definidos en el front matter, debes añadirlo en 2 lugares.\nEditar layouts/_default/index.JSON Esto expone los valores en /index.json: por ejemplo, para agregar categories ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEditar las opciones de fuse.js para buscar static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"Este archivo existe únicamente para responder a la URL /search con la plantilla de diseño search relacionada.\nNo se muestra ningún contenido aquí, todo el contenido se basa en la plantilla layouts/page/search.html\nEstablecer una prioridad muy baja en el mapa del sitio le dirá a los motores de búsqueda que éste no es un contenido importante.\nEsta implementación utiliza Fusejs, jquery y mark.js\nConfiguración inicial La búsqueda depende del tipo de contenido de salida adicional de JSON en config.","tags":null,"title":"Resultados de Búsqueda"},{"categories":null,"contents":"Este archivo existe únicamente para responder a la URL /search con la plantilla de diseño search relacionada.\nNo se muestra ningún contenido aquí, todo el contenido se basa en la plantilla layouts/page/search.html\nEstablecer una prioridad muy baja en el mapa del sitio le dirá a los motores de búsqueda que éste no es un contenido importante.\nEsta implementación utiliza Fusejs, jquery y mark.js\nConfiguración inicial La búsqueda depende del tipo de contenido de salida adicional de JSON en config.toml\n``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nBúsqueda de archivos adicionales Para buscar campos adicionales definidos en el front matter, debes añadirlo en 2 lugares.\nEditar layouts/_default/index.JSON Esto expone los valores en /index.json: por ejemplo, para agregar categories ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEditar las opciones de fuse.js para buscar static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"Este archivo existe únicamente para responder a la URL /search con la plantilla de diseño search relacionada.\nNo se muestra ningún contenido aquí, todo el contenido se basa en la plantilla layouts/page/search.html\nEstablecer una prioridad muy baja en el mapa del sitio le dirá a los motores de búsqueda que éste no es un contenido importante.\nEsta implementación utiliza Fusejs, jquery y mark.js\nConfiguración inicial La búsqueda depende del tipo de contenido de salida adicional de JSON en config.","tags":null,"title":"Resultados de Búsqueda"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"অনুসন্ধানের ফলাফল"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"অনুসন্ধানের ফলাফল"},{"categories":["Basic"],"contents":"一、GPT-1的结构 当时的两个问题：\n 没有一个合适的目标函数，不同的子任务（比如：翻译、分类、理解等）有自己的目标函数 怎么有效的把学到的 表征 传递到下游的子任务重。因为，NLP的子任务差异还是挺大的。  1、输入输出 2、编码 对于输入的一句文本串，机器是操作不了的，需要把这段文字串中的每个字转变成数字向量。那么如何将单词变成向量呢？\n  构建词表：将所有单词都搜集起来，通过训练一个分词模型，把一句文本split成：单词、固定式语句、组词、等等。比如：GPT的词表大小为 50257。 one-hot编码：比如：每个单词的one-hot编码，就是一个词表(50257)大小的一个向量，该词位置上的值为1，其余全是0. embedding：对于one-hot编码，大部分都是0填充，就是卑鄙的浪费。为了解决这个问题，模型学习了一个embedding函数：一个神经网络，把50257长度的1、0向量，输出n长度的数字向量。即：模型试图将词表映射到较小的空间。(这也比较合理：因为词表中本来就存在：近义词、同义词、等等)   3、位置信息编码(Position Encoding) 文本的位置信息很重要，就想图像中每个像素点的位置信息，不过输入一句话，跟顺序打乱，attention输出都是可以是一样的（如果顺序有变动，相应的权重变动一下就行）。所以，需要手动加入文本的位置信息。位置信息的计算：\n  比如：GPT允许一句输入最长2048个token。每个token经过one-hot编码、embedding后 维度为12288。 位置编码的输出是：2048*12288 维的信息。其中，2048方向可以看成时间t(或者离散的n); 12288方向可以看成不同的频率。 假设：从1~12288，频率从：$f, \u0026hellip;, f^{12288}$，就可以理解为 $T = 1/f^{12288}$ 进制下的数字表示法。每个位置就是可以是不一样。   4、注意力机制   文本的embedding + 位置编码，作为注意力机制的输入 $\\bf W_q, W_k, W_v$，三个可学习的矩阵，把输入的embedding向量，变换成向量：$\\bf q, k, v$。 attention计算：用搜索向量$\\bf q_i$，与所有key向量$\\bf{k_i}$，$i\\in(1,..N)$计算内积(表示相似度)。这个N个值分别作为$\\bf v_i$ $i \\in (1,\u0026hellip;,N)$ 的权重。最后计算出的向量就是一个head的attention输出 一个head的计算注意力后的向量维度为128，GPT采用96个head，拼接起来正好是12288维度。经过$\\bf W_z$ 转换后，作为attention模块的输出，维度与输入一致。   5、layer normalization 6、前馈神经网络 7、解码 96个注意力机制/前馈网络 后，输出是是 2048*12288的向量信息。不过词表是50257大小，所以需要把embedding的逆变换，把12288维度映射回50257大小。对下一个字的预测：输出一个50257维的向量，这个向量中的值表示词表中每个字的概率值，通过softmax之后，选出最大概率的字，或者选出top-k个最有可能得词（想象力的体现）。\n二、 ","date":"August 8, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0080_gpt/0020_gpt1_detail/","summary":"一、GPT-1的结构 当时的两个问题：\n 没有一个合适的目标函数，不同的子任务（比如：翻译、分类、理解等）有自己的目标函数 怎么有效的把学到的 表征 传递到下游的子任务重。因为，NLP的子任务差异还是挺大的。  1、输入输出 2、编码 对于输入的一句文本串，机器是操作不了的，需要把这段文字串中的每个字转变成数字向量。那么如何将单词变成向量呢？\n  构建词表：将所有单词都搜集起来，通过训练一个分词模型，把一句文本split成：单词、固定式语句、组词、等等。比如：GPT的词表大小为 50257。 one-hot编码：比如：每个单词的one-hot编码，就是一个词表(50257)大小的一个向量，该词位置上的值为1，其余全是0. embedding：对于one-hot编码，大部分都是0填充，就是卑鄙的浪费。为了解决这个问题，模型学习了一个embedding函数：一个神经网络，把50257长度的1、0向量，输出n长度的数字向量。即：模型试图将词表映射到较小的空间。(这也比较合理：因为词表中本来就存在：近义词、同义词、等等)   3、位置信息编码(Position Encoding) 文本的位置信息很重要，就想图像中每个像素点的位置信息，不过输入一句话，跟顺序打乱，attention输出都是可以是一样的（如果顺序有变动，相应的权重变动一下就行）。所以，需要手动加入文本的位置信息。位置信息的计算：\n  比如：GPT允许一句输入最长2048个token。每个token经过one-hot编码、embedding后 维度为12288。 位置编码的输出是：2048*12288 维的信息。其中，2048方向可以看成时间t(或者离散的n); 12288方向可以看成不同的频率。 假设：从1~12288，频率从：$f, \u0026hellip;, f^{12288}$，就可以理解为 $T = 1/f^{12288}$ 进制下的数字表示法。每个位置就是可以是不一样。   4、注意力机制   文本的embedding + 位置编码，作为注意力机制的输入 $\\bf W_q, W_k, W_v$，三个可学习的矩阵，把输入的embedding向量，变换成向量：$\\bf q, k, v$。 attention计算：用搜索向量$\\bf q_i$，与所有key向量$\\bf{k_i}$，$i\\in(1,..N)$计算内积(表示相似度)。这个N个值分别作为$\\bf v_i$ $i \\in (1,\u0026hellip;,N)$ 的权重。最后计算出的向量就是一个head的attention输出 一个head的计算注意力后的向量维度为128，GPT采用96个head，拼接起来正好是12288维度。经过$\\bf W_z$ 转换后，作为attention模块的输出，维度与输入一致。   5、layer normalization 6、前馈神经网络 7、解码 96个注意力机制/前馈网络 后，输出是是 2048*12288的向量信息。不过词表是50257大小，所以需要把embedding的逆变换，把12288维度映射回50257大小。对下一个字的预测：输出一个50257维的向量，这个向量中的值表示词表中每个字的概率值，通过softmax之后，选出最大概率的字，或者选出top-k个最有可能得词（想象力的体现）。","tags":["GPT-1"],"title":"GPT-1"},{"categories":["Basic"],"contents":"一、 T5\n二、 ","date":"August 8, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0120_t5/0010_t5_summary/","summary":"一、 T5\n二、 ","tags":["T5"],"title":"T5综述"},{"categories":["Basic"],"contents":"一、简介 It is coming soon.\n","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1005_can_summary/","summary":"一、简介 It is coming soon.","tags":["AIGC","CAN"],"title":"CAN"},{"categories":["Basic"],"contents":"一、简介 二、网络结构 ","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0025_chatglm/","summary":"一、简介 二、网络结构 ","tags":["aigc","ChatGLM"],"title":"ChatGLM"},{"categories":["Basic"],"contents":"一、简介 Anthropic公司推出的Claude。\n二、网络结构 ","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0030_claude/","summary":"一、简介 Anthropic公司推出的Claude。\n二、网络结构 ","tags":["aigc","Claude"],"title":"Claude"},{"categories":["Basic"],"contents":"一、简介 二、网络结构 ","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0035_cohere/","summary":"一、简介 二、网络结构 ","tags":["aigc","Cohere"],"title":"Cohere"},{"categories":["Basic"],"contents":"一、简介 It is coming soon.\n","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1010_dall_e/","summary":"一、简介 It is coming soon.","tags":["AIGC","DALL-E"],"title":"DALL-E"},{"categories":["Basic"],"contents":"一、简介 It is coming soon.\n","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1020_diffusion/","summary":"一、简介 It is coming soon.","tags":["AIGC","Diffusion"],"title":"Diffusion"},{"categories":["Basic"],"contents":"一、简介 二、网络结构 ","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0040_falcon/","summary":"一、简介 二、网络结构 ","tags":["aigc","Falcon"],"title":"Falcon"},{"categories":["Basic"],"contents":"一、简介 It is coming soon.\nhello\n","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1000_gan_summary/","summary":"一、简介 It is coming soon.\nhello","tags":["AIGC","GAN"],"title":"GAN"},{"categories":["Basic"],"contents":"一、简介 二、InstructGPT InstructGPT 通过人类的反馈，在GPT3上做微调。\n1、SFT模型 设计了一些prompt，人工写答案，搜集一批数据，用来fine-tune GPT3，得到一个SFT模型 (Supervised Fine-tune)，即：有监督的微调\n2、RW模型： 由于SFT的标注数据，成本比较大。这搞一个便宜点的。\n 设计一批prompt，每条prompt用GPT3采用很多条结果（生成模型的输出是概率性的，每次结果大概率是不一样的，generate有参数可以控制这些概率性） 人工标注：每个prompt的生成结果的排序 （打分标注，可比写答案的标注快多了） 训练一个奖励模型，这个奖励模型 就是对GPT3-6B的输出 进行打分，这个输出的分数 满足 人工标注的顺序。\n 作者没有采用GPT-175B模型，是因为在训练的过程中175B的不稳定，loss容易爆炸。\n 由于标注的是排序，RW模型的输出是score，所以有一个排序到score的映射。比如：一个prompt有K个答案。从k个答案中选2个，有 $C^2_k$种 结果对。每个结果对都是有人工标注的顺序的，在计算loss的时候保证这个顺序就行。每个prompt有 $C^2_k$ 个结果对，在算loss的时候，这 $C^2_k$ 个结果对一起计算。\n loss的话是一个标准的 Pairwise的 Ranking Loss $$loss(\\theta) = - \\frac{1}{C^2_k} E_{x,y_w,y_l \\in D} log(\\sigma[r_\\theta (x, y_w) - r_\\theta (x, y_l)])$$ 其中，$r_\\theta ()$ 表示GPT3-6B的输出score值，$\\sigma()$ 表示 Sigmoid函数。学习的目标是最大化这个loss。    3、强化学习SFT模型 在强化学习的框架下调整SFT模型：\n用PPO强化学习方法，fine-tune 之前的SFT模型，得出的模型就是InstructGPT，大小只有1.3B。\n作者尝试把预训练的梯度整合到PPO中，如下： $$ objective(\\phi) = E_{(x,y) \\in D_{\\pi_\\phi^{RL}}} [r_\\theta(x, y) - \\beta log(\\frac{\\pi^{RL}_\\phi(y|x)}{\\pi^{SFT}(y|x)})] + $$\n$$ \\gamma E_{x \\in D_{pretrain}} [log(\\pi^{RL}_\\phi(x))] $$\n其中，$r_{\\theta}()$ 就是第二步的RM模型；$\\pi_\\phi^{RL}$ 表示新的环境；$\\pi_\\phi^{SFT}$ 表示原来的环境；\n$log(\\frac{\\pi^{RL}_\\phi(y|x)}{\\pi^{SFT}(y|x)}) $ 表示新环境 与 旧环境差异，是一个正则项，作者希望与旧环境不能差异太大，所以添加了这个，类似KL散度。\n$ E_{x \\in D_{pretrain}} [log(\\pi^{RL}_\\phi(x))]$ 表示原GPT3预训练时的损失函数，目的是防止模型遗忘。\n在训练之前，是用第一步的SFT模型来初始化，$\\pi_\\phi^{SFT}$ 就是原来的SFT模型。\n从结果来看：还是人工打标的数据对模型的提升较大。可以看到经过监督训练的1.3B的模型，效果是好于175B的大模型。 三、相关 RLHF\nRLHF的相关技术，首次出自这篇文章： 《Fine-Tuning Language Models from Human Preferences》 该论文的作者Tom创建Anthropic公司，是该公司的CEO。\n后续，Anthropic公司发布了自己的大语言模型Claude，可以了解一下。\nKL散度：\n$$ D_{KL}(p||q) = E_p[log \\frac{p}{q}] = \\sum {p log \\frac{p}{q}} $$\n其中，$p$ 分布往往是多峰。$D_{KL}(p||q)$ 与 $D_{KL}(q||p)$ 是不一样的\n $D_{KL}(p||q)$ 表示前向散度，在监督学习中使用 $D_{KL}(q||p)$ 表示反向散度，在强化学习中使用  四、GPT-4 《GPT-4 Technical Report》\n","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0011_gpt/","summary":"一、简介 二、InstructGPT InstructGPT 通过人类的反馈，在GPT3上做微调。\n1、SFT模型 设计了一些prompt，人工写答案，搜集一批数据，用来fine-tune GPT3，得到一个SFT模型 (Supervised Fine-tune)，即：有监督的微调\n2、RW模型： 由于SFT的标注数据，成本比较大。这搞一个便宜点的。\n 设计一批prompt，每条prompt用GPT3采用很多条结果（生成模型的输出是概率性的，每次结果大概率是不一样的，generate有参数可以控制这些概率性） 人工标注：每个prompt的生成结果的排序 （打分标注，可比写答案的标注快多了） 训练一个奖励模型，这个奖励模型 就是对GPT3-6B的输出 进行打分，这个输出的分数 满足 人工标注的顺序。\n 作者没有采用GPT-175B模型，是因为在训练的过程中175B的不稳定，loss容易爆炸。\n 由于标注的是排序，RW模型的输出是score，所以有一个排序到score的映射。比如：一个prompt有K个答案。从k个答案中选2个，有 $C^2_k$种 结果对。每个结果对都是有人工标注的顺序的，在计算loss的时候保证这个顺序就行。每个prompt有 $C^2_k$ 个结果对，在算loss的时候，这 $C^2_k$ 个结果对一起计算。\n loss的话是一个标准的 Pairwise的 Ranking Loss $$loss(\\theta) = - \\frac{1}{C^2_k} E_{x,y_w,y_l \\in D} log(\\sigma[r_\\theta (x, y_w) - r_\\theta (x, y_l)])$$ 其中，$r_\\theta ()$ 表示GPT3-6B的输出score值，$\\sigma()$ 表示 Sigmoid函数。学习的目标是最大化这个loss。    3、强化学习SFT模型 在强化学习的框架下调整SFT模型：\n用PPO强化学习方法，fine-tune 之前的SFT模型，得出的模型就是InstructGPT，大小只有1.3B。\n作者尝试把预训练的梯度整合到PPO中，如下： $$ objective(\\phi) = E_{(x,y) \\in D_{\\pi_\\phi^{RL}}} [r_\\theta(x, y) - \\beta log(\\frac{\\pi^{RL}_\\phi(y|x)}{\\pi^{SFT}(y|x)})] + $$","tags":["aigc","GPT"],"title":"GPT"},{"categories":["Basic"],"contents":"一、简介 It is coming soon.\n","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1030_imagen/","summary":"一、简介 It is coming soon.","tags":["AIGC","Imagen"],"title":"Imagen"},{"categories":["Basic"],"contents":"一、简介 二、网络结构 1、LLaMa 2、LLaMa 2 数据方面\n LLaMa2训练了2000B的tokens，训练语料比LLaMa多了40%  2000B 个token的预训练集，提供了良好的性能和成本权衡；对最真实的来源进行上采样，以增加知识并抑制幻觉，保持真实 调查数据，以便用户更好地了解模型的潜在能力和局限性，保证安全。   上下文长度从2048提升到了4096 LLaMa2-chat 模型还接受了超过100w的人类标注的训练数据  开源数据选了 LLaMa2 使用监督微调 LLaMa2-chat 使用人类反馈强化学习(RLHF)进行迭代细化；包括拒绝采样、近端策略优化    网络方面\n RMSNorm 归一化 FFN中用swiGLU激活函数替换原来的Relu 旋转位置编码 RoPE 增加上下文长度 分组查询注意力 GQA  原始的 多头注意力：MHA 具有单个KV投影的原始多查询格式：MQA 具有8个KV投影的分组查询注意力变体：GQA    训练方面   预训练细节：\n 用AdamW优化器进行训练，其中： $β_1 =0.9，β_2 = 0.95，eps = 10−5$。 使用余弦调整学习率，预热2000steps，$lr$ 衰减到峰值的10% 使用0.1的权重衰减 、1.0的梯度裁剪    精调细节：\n 余弦学习率，$lr=2e-5$ 权重衰减0.1，batch_size=64，序列长度为4096 训练2个epoch 引入Ghost Attention 有助于控制多轮对话    ","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0015_llama/","summary":"一、简介 二、网络结构 1、LLaMa 2、LLaMa 2 数据方面\n LLaMa2训练了2000B的tokens，训练语料比LLaMa多了40%  2000B 个token的预训练集，提供了良好的性能和成本权衡；对最真实的来源进行上采样，以增加知识并抑制幻觉，保持真实 调查数据，以便用户更好地了解模型的潜在能力和局限性，保证安全。   上下文长度从2048提升到了4096 LLaMa2-chat 模型还接受了超过100w的人类标注的训练数据  开源数据选了 LLaMa2 使用监督微调 LLaMa2-chat 使用人类反馈强化学习(RLHF)进行迭代细化；包括拒绝采样、近端策略优化    网络方面\n RMSNorm 归一化 FFN中用swiGLU激活函数替换原来的Relu 旋转位置编码 RoPE 增加上下文长度 分组查询注意力 GQA  原始的 多头注意力：MHA 具有单个KV投影的原始多查询格式：MQA 具有8个KV投影的分组查询注意力变体：GQA    训练方面   预训练细节：\n 用AdamW优化器进行训练，其中： $β_1 =0.9，β_2 = 0.95，eps = 10−5$。 使用余弦调整学习率，预热2000steps，$lr$ 衰减到峰值的10% 使用0.1的权重衰减 、1.0的梯度裁剪    精调细节：\n 余弦学习率，$lr=2e-5$ 权重衰减0.1，batch_size=64，序列长度为4096 训练2个epoch 引入Ghost Attention 有助于控制多轮对话    ","tags":["aigc","llama"],"title":"LLaMa"},{"categories":["Basic"],"contents":"一、简介 It is coming soon.\n","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1025_midjourney/","summary":"一、简介 It is coming soon.","tags":["AIGC","Midjourney"],"title":"Midjourney"},{"categories":["Basic"],"contents":"一、 MLLM\n","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00400_vlp/0010_mllm/","summary":"一、 MLLM","tags":["MLLM","summary"],"title":"MLLM"},{"categories":["Basic"],"contents":"一、简介 1、PaLM 1 《PaLM: Scaling Language Modeling with Pathways》 这篇文章87页，并没有深度的讨论模型算法的结构，数据的清洗技巧，或者是训练的方式（估计感觉这块的创新性不是特别明显，也不是文章的主要目的）。 而是花了大量的篇幅去评估这个模型在multi-task的能力，比如翻译，代码修改，生成，问答等等。\n其中模型版本于训练集大小：\nGoogle PaLM 是一个 540B 参数密集型 Transformer 语言模型，在 780B 高质量、多样化文本的标记上进行训练。 它已经针对 3 种不同的尺寸进行了训练：8B、62B 和 540B，使用 6144 TPU v4 芯片使用 Pathways，这是一种新的 ML 系统，可跨多个 TPU（张量处理单元）Pod 进行高效训练。 当它被引入时，它在数百个 NLU 和 NLG 基准测试中产生了 SOTA 小样本学习结果。 这包括 Big-Bench 任务的性能大幅提升，以及多语言 NLG 和源代码生成功能的显着改进。 它还被证明可以使用思维链提示来解释笑话或逻辑推理，从而产生很好的解释。\nPaLM超越了许多之前的SOTA。作者归功于\n 更好的数据的清理， 更多的数据， 模型规模的进一步提升。  模型算法的改进比较少，从Model Architecture那一章看出，其实模型结构的变化并不明显，在激活层，ShareEmbedding，PosEmbedding等模块做了一些结构优选。核心的TransformerBlock的变种选择也更多是为了优化模型的训练效率。谷歌作为搜索技术的天花板，数据清洗的积累，以及对于数据的理解肯定是OpenAI这些公司无法比拟的。个人感觉这块是个比较明显的优势。\n与GPT-3相比的变化：\n  多查询注意力（Multi-query attention）：在每个注意力头中共享K/V（Key/Value）嵌入，但使用独立的Q（Query）嵌入。这样做可以在推理阶段显著提高模型的速度。 并行Transformer块：使用并行的Transformer块来提高训练时间，相较于传统的串行设计，可以减少约15%的训练时间。 SwiGLU激活函数：与GPT-3使用的GELU激活函数不同，这里采用了SwiGLU激活函数。 旋转位置编码RoPE嵌入：使用RoPE（Relative Positional Encodings）嵌入代替学习得到的嵌入方式，在长文本上具有更好的性能 。 输入-输出嵌入共享：输入和输出embedding矩阵是共享的。 无偏置向量：在mlp、normlayer等算法中，都不使用bias，对于大模型，可以提高训练稳定性。 SentencePiece与256k标记：使用SentencePiece进行分词处理，标记数量为256k。   2、PaLM 2 《PaLM 2 Technical Report》 这篇报告-总结：\n 实验验证了模型规模和数据规模是随算力同比增加的，大致为1:1时，性价比更好（消耗同样算力得到的模型表现更好）。 对于更大的模型（larger models），增加多语种数据不会降低其在英语上的表现。支持一百多种语言 数据量比v1大很多（v1的数据量是780B），据CNBC报道，v2数据量是3.6T（不保真）。 模型结构没有具体说，仅说了基于Transformer。参数规模上，这次PaLM2的模型家族中，最大的那个都会显著比v1版本（540B）的小很多（siginificantly smaller）。 对部分有害语料加入控制字符，来保证生成可控。 增加context长度不会损害模型在不需要这么长输入的任务上的表现。 刷点上，PaLM 2基本上全面优于v1版本，和GPT-4的比较，突出一个势均力敌，甚至隐隐有超过。 通过注入canaries实验（可以理解问人造一些数据），他们发现相对于正常数据，离群点（outlier point）仅需更小的重复次数就会被记忆。但是在真实语言数据中，这种现象又不是那么明显。 训练框架上，硬件自然是TPUv4，软件是Pathways，Paxml，JAX和GSPMD，看起来没过气网红tf的事了。  不同版本：\n二、网络结构 基于Transformer解码器\n","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0020_palm/","summary":"一、简介 1、PaLM 1 《PaLM: Scaling Language Modeling with Pathways》 这篇文章87页，并没有深度的讨论模型算法的结构，数据的清洗技巧，或者是训练的方式（估计感觉这块的创新性不是特别明显，也不是文章的主要目的）。 而是花了大量的篇幅去评估这个模型在multi-task的能力，比如翻译，代码修改，生成，问答等等。\n其中模型版本于训练集大小：\nGoogle PaLM 是一个 540B 参数密集型 Transformer 语言模型，在 780B 高质量、多样化文本的标记上进行训练。 它已经针对 3 种不同的尺寸进行了训练：8B、62B 和 540B，使用 6144 TPU v4 芯片使用 Pathways，这是一种新的 ML 系统，可跨多个 TPU（张量处理单元）Pod 进行高效训练。 当它被引入时，它在数百个 NLU 和 NLG 基准测试中产生了 SOTA 小样本学习结果。 这包括 Big-Bench 任务的性能大幅提升，以及多语言 NLG 和源代码生成功能的显着改进。 它还被证明可以使用思维链提示来解释笑话或逻辑推理，从而产生很好的解释。\nPaLM超越了许多之前的SOTA。作者归功于\n 更好的数据的清理， 更多的数据， 模型规模的进一步提升。  模型算法的改进比较少，从Model Architecture那一章看出，其实模型结构的变化并不明显，在激活层，ShareEmbedding，PosEmbedding等模块做了一些结构优选。核心的TransformerBlock的变种选择也更多是为了优化模型的训练效率。谷歌作为搜索技术的天花板，数据清洗的积累，以及对于数据的理解肯定是OpenAI这些公司无法比拟的。个人感觉这块是个比较明显的优势。\n与GPT-3相比的变化：\n  多查询注意力（Multi-query attention）：在每个注意力头中共享K/V（Key/Value）嵌入，但使用独立的Q（Query）嵌入。这样做可以在推理阶段显著提高模型的速度。 并行Transformer块：使用并行的Transformer块来提高训练时间，相较于传统的串行设计，可以减少约15%的训练时间。 SwiGLU激活函数：与GPT-3使用的GELU激活函数不同，这里采用了SwiGLU激活函数。 旋转位置编码RoPE嵌入：使用RoPE（Relative Positional Encodings）嵌入代替学习得到的嵌入方式，在长文本上具有更好的性能 。 输入-输出嵌入共享：输入和输出embedding矩阵是共享的。 无偏置向量：在mlp、normlayer等算法中，都不使用bias，对于大模型，可以提高训练稳定性。 SentencePiece与256k标记：使用SentencePiece进行分词处理，标记数量为256k。   2、PaLM 2 《PaLM 2 Technical Report》 这篇报告-总结：","tags":["aigc","PaLM"],"title":"PaLM"},{"categories":["Basic"],"contents":"一、简介 二、网络结构 ","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0045_vicuna/","summary":"一、简介 二、网络结构 ","tags":["aigc","Vicuna"],"title":"Vicuna"},{"categories":["Basic"],"contents":"一、简介 It is coming soon.\n","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0020_generate_image/1015_vqgan/","summary":"一、简介 It is coming soon.","tags":["AIGC","VQGAN"],"title":"VQGAN"},{"categories":["Basic"],"contents":"一、初始化 1、Xavier初始化 在全连接层的Xavier初始化：用 $N(0, 1/m)$ 的随机分布初始化。\n2、NTK参数化 除了直接用这种方式初始化外，还可以使用 参数化的方式：用 $N(0, 1)$ 的随机分布来初始化，但需要将输出结果除以 $\\sqrt{m}$，即： $$ y_j = b_j + \\frac{1}{\\sqrt{m}} \\sum_i{x_i w_{ij}} $$\n这个高斯过程被称为 \u0026ldquo;NTK参数化\u0026rdquo;，可以参考 《Neural Tangent Kernel: Convergence and Generalization in Neural Networks》，《On the infinite width limit of neural networks with a standard parameterization》。利用NTK参数化后，所有参数都可以用方差为1的分布初始化，这意味着每个参数的尺度大致是一个级别，这样的话我们就可以设置较大的学习率，加快收敛。\n","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0020_deeplearing_init/","summary":"一、初始化 1、Xavier初始化 在全连接层的Xavier初始化：用 $N(0, 1/m)$ 的随机分布初始化。\n2、NTK参数化 除了直接用这种方式初始化外，还可以使用 参数化的方式：用 $N(0, 1)$ 的随机分布来初始化，但需要将输出结果除以 $\\sqrt{m}$，即： $$ y_j = b_j + \\frac{1}{\\sqrt{m}} \\sum_i{x_i w_{ij}} $$\n这个高斯过程被称为 \u0026ldquo;NTK参数化\u0026rdquo;，可以参考 《Neural Tangent Kernel: Convergence and Generalization in Neural Networks》，《On the infinite width limit of neural networks with a standard parameterization》。利用NTK参数化后，所有参数都可以用方差为1的分布初始化，这意味着每个参数的尺度大致是一个级别，这样的话我们就可以设置较大的学习率，加快收敛。","tags":["机器学习","深度学习","初始化"],"title":"初始化"},{"categories":["Basic"],"contents":"一、简介 二、Deepspeed 三、Megatron-LM 《Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism》 Megatron 是一篇极具影响力的论文，介绍了高效的模型并行架构。Megatron引入了张量并行(tensor parallelism)，这是一种模型并行的变体，它将模型分割成多块，以实现层内模型并行，从而达到与单个GPU基准线76%效率相当的水平（尽管基准线只有峰值FLOPS的30%）。\nMegatron意识到如果，你有一个网络模型 $Y=f(XW)$，你沿着列拆分开了 $W=[W1, W2]$ ，然后 $Y=[f(XW1), f(XW2)]$，所以你不需要做任何操作来同步 $Y$，transformer中唯一需要同步（all-reduce）的点是：\n 正向传播中，在MLP块后拼接模型激活值之前添加dropout时需要同步。 反向传播中，在self-attention块的开始处需要进行同步。  通过在这两个关键点进行同步操作，可以保证Transformer模型在计算过程中的正确性和一致性。\n《Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model》 ","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0010_aigc_train_p/","summary":"一、简介 二、Deepspeed 三、Megatron-LM 《Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism》 Megatron 是一篇极具影响力的论文，介绍了高效的模型并行架构。Megatron引入了张量并行(tensor parallelism)，这是一种模型并行的变体，它将模型分割成多块，以实现层内模型并行，从而达到与单个GPU基准线76%效率相当的水平（尽管基准线只有峰值FLOPS的30%）。\nMegatron意识到如果，你有一个网络模型 $Y=f(XW)$，你沿着列拆分开了 $W=[W1, W2]$ ，然后 $Y=[f(XW1), f(XW2)]$，所以你不需要做任何操作来同步 $Y$，transformer中唯一需要同步（all-reduce）的点是：\n 正向传播中，在MLP块后拼接模型激活值之前添加dropout时需要同步。 反向传播中，在self-attention块的开始处需要进行同步。  通过在这两个关键点进行同步操作，可以保证Transformer模型在计算过程中的正确性和一致性。\n《Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model》 ","tags":["aigc","大模型","训练框架"],"title":"大模型训练框架"},{"categories":["Basic"],"contents":"一、简介 目前小型化的方案：\n 剪枝 Network Pruning 蒸馏 Knowledge Distillation 量化 Parameter Quantization Architecture Design Dynamic Computation  二、TensorRT ","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0015_aigc_train_mini/","summary":"一、简介 目前小型化的方案：\n 剪枝 Network Pruning 蒸馏 Knowledge Distillation 量化 Parameter Quantization Architecture Design Dynamic Computation  二、TensorRT ","tags":["aigc","大模型","小型化"],"title":"模型小型化"},{"categories":["Basic"],"contents":"一、简介 对于大语言模型应用的两种不同的使用方式：\n  “专才”：只精通指定任务。怎么让一个基础模型在指定任务上比较精通呢？有两种方式：\n 加外挂：比如：在bert后面添加几个fc层，完成指定任务 fine-tune： Adapter插件：固定原来模型，添加一个额外的模型插件。例如：Bitfit、AdapterBias、Houlsby、Prefix-tuning；ControlNet， LoRA，Text Inversion    “全才”：模型有各种背景知识，用户可以通过使用prompt指令，来要求模型按照指令输出。\n In-context Learning Instruction tuning Chain-of-Thought Prompting APE      二、大模型-使用策略 1、In-Context Learning 1. 解释1 《Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?》 In-context learning是一种学习范式，它允许语言模型通过以演示形式组织的若干个示例或者指令来学习任务。In-context learning（ICL）的核心在于从任务相关的类比样本中学习，ICL要求若干示例以特定形式进行演示，然后将当前输入x跟上述示例通过prompt拼接到一起作为语言模型的输入。本质上，它利用训练有素的语言模型根据演示的示例来估计候选答案的可能性。简单理解，就是通过若干个完整的示例，让语言模型更好地理解当前的任务，从而做出更加准确的预测。\n实验结论：\n ICL 中Ground Truth信息无关紧要。\n作者实验对比：没有示例、多个示例-且label是一一对应的、多个示例-且label是随机的。对比发现：  随机label 与 正确label 的效果相当，性能只下降了 $ 0 - 5\\%$。 没有示例，效果下降较多。    2. ICL的性能收益主要来自 独立规范的输入空间和标签空间，以及正确一致的演示格式。\n作者实验了这4个因素：输入空间、标签空间、演示格式。对比实验：把输入换成外部语料；把标签换成英语单词；缺少输入或者label。实验发现： 1. 把输入换成外部语料；把标签换成英语单词；缺少输入或者label。这些操作都会使得效果明显下降。 个人理解：比如做情感分析，示例：输入label。这种格式很重要，label是否正确不重要。\n大模型通过预训练，对文本时有理解能力的。ICL 的prompt中有多个 样例的格式，是让大语言模型知道当前是在做情感分析任务，而不是在做其他任务，按照情感分析的思路输出。并不是根据 prompt中的几个样例学习，而是唤醒机器要执行什么样的任务。\n2. 解释2 来自Google公司Jaon Wei的《LARGER LANGUAGE MODELS DO IN-CONTEXT LEARNING DIFFERENTLY》 Google的这篇文章解释：大语言模型在ICL中是有学习的，解释1的结论之所以成立，是因为解释1用的模型还不够大，在更大的模型中，在ICL中的学习会表现的更为明显。\n2、Instruction-tuning 1. FLAN 来自Google公司Jaon Wei的《FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS》 论文的结论：模型可以学习到人类设定好的指令，并根据已学到的知识，在测试时对于未见过的指令，结果有好的表现。\n3、Chain-of-Thought Prompting 来自Google公司Jaon Wei的《Chain-of-Thought Prompting Elicits Reasoning in Large Language Models》 发现：在推理的任务中，只是给一些示例，大模型的效果不好，如果能给到推理的思路过程，大模型的效果就会有明显提升。\nCoT的变形：\n来自Google公司的《Large Language Models are Zero-Shot Reasoners》，《LEAST-TO-MOST PROMPTING ENABLES COMPLEX REASONING IN LARGE LANGUAGE MODELS》 发现：由于这个推理思路是人工写的，这些数据量比较少，而且操作比较麻烦。作者的操作：\n 在生成答案之前，加了一个要求：Let\u0026rsquo;s think step by step. 生成多个答案，然后投票  4、APE 《LARGE LANGUAGE MODELS ARE HUMAN-LEVEL PROMPT ENGINEERS》 作者用大模型自己输出有用的prompt，通过筛选后，选出效果较好的。\n整体效果如下：\n","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0005_aigc_application/","summary":"一、简介 对于大语言模型应用的两种不同的使用方式：\n  “专才”：只精通指定任务。怎么让一个基础模型在指定任务上比较精通呢？有两种方式：\n 加外挂：比如：在bert后面添加几个fc层，完成指定任务 fine-tune： Adapter插件：固定原来模型，添加一个额外的模型插件。例如：Bitfit、AdapterBias、Houlsby、Prefix-tuning；ControlNet， LoRA，Text Inversion    “全才”：模型有各种背景知识，用户可以通过使用prompt指令，来要求模型按照指令输出。\n In-context Learning Instruction tuning Chain-of-Thought Prompting APE      二、大模型-使用策略 1、In-Context Learning 1. 解释1 《Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?》 In-context learning是一种学习范式，它允许语言模型通过以演示形式组织的若干个示例或者指令来学习任务。In-context learning（ICL）的核心在于从任务相关的类比样本中学习，ICL要求若干示例以特定形式进行演示，然后将当前输入x跟上述示例通过prompt拼接到一起作为语言模型的输入。本质上，它利用训练有素的语言模型根据演示的示例来估计候选答案的可能性。简单理解，就是通过若干个完整的示例，让语言模型更好地理解当前的任务，从而做出更加准确的预测。\n实验结论：\n ICL 中Ground Truth信息无关紧要。\n作者实验对比：没有示例、多个示例-且label是一一对应的、多个示例-且label是随机的。对比发现：  随机label 与 正确label 的效果相当，性能只下降了 $ 0 - 5\\%$。 没有示例，效果下降较多。    2. ICL的性能收益主要来自 独立规范的输入空间和标签空间，以及正确一致的演示格式。\n作者实验了这4个因素：输入空间、标签空间、演示格式。对比实验：把输入换成外部语料；把标签换成英语单词；缺少输入或者label。实验发现： 1. 把输入换成外部语料；把标签换成英语单词；缺少输入或者label。这些操作都会使得效果明显下降。 个人理解：比如做情感分析，示例：输入label。这种格式很重要，label是否正确不重要。","tags":["aigc","大模型","应用策略"],"title":"模型应用策略"},{"categories":["Basic"],"contents":"一、简介  问题1：在文本生成是，模型会一本正经的胡说八道，这种现象叫做模型的幻觉。 问题2：训练一个大模型，需要多少数据量呢？ 问题3：数据预处理，怎么过滤、去重 问题4：模型大小 与 数据大小 的关系？  二、模型问题 1、Calibration  问题1：在文本生成是，模型会一本正经的胡说八道，这种现象叫做模型的幻觉。\n《Language Models (Mostly) Know What They Know》 这篇论文发现：模型够大后，说谎才会心虚。  对于大模型，模型输出是正确的概率 VS 模型的自信度，这两个是相关的。当模型比较自信时，输出的结果是正确的概率就比较大。 对于小模型，模型输出是正确的概率 VS 模型的自信度，这两个是不相关的    其中，横轴：模型输出时的自信程度；纵轴：模型输出是正确的概率。黄色表示最大模型，自身表示最小模型。 三、数据问题  问题2：训练一个大模型，需要多少数据量呢？\n训练一个大模型，需要多少数据量呢？《When Do You Need Billions of Words of Pretraining Data?》     问题3：数据预处理，怎么过滤、去重?\n数据预处理：《Scaling Language Models: Methods, Analysis \u0026amp; Insights from Training Gopher》  过滤有害的内容，通过Google的审核接口 去掉一些 HTML 前端的一些tag 规则过滤，去掉低质量的文本。 去重 剔除测试数据    问题4：模型大小 与 数据大小 的关系？ 《Training Compute-Optimal Large Language Models》 这篇文章发现：\n 小模型+大数据 和 大模型+小数据，这两个极端都不好。中间某个点事比较合适。这个合适的点：大概是：模型参数：63B 匹配 数据量：1.4T    根据这个结论，作者训练了一版模型：Chinchilla。70B模型参数，1.4T数据量。跟以前的模型对比：Chinchilla在绝大多数的任务上，效果较好。    model name model size tokens     LaMDA(2022) 137B 168B   GPT-3(2020) 175B 300B   Jurassic(2021) 178B 300B   Gopher(2021) 280B 300B   MT-NLG(2022) 530B 270B    ","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0020_aigc_error/","summary":"一、简介  问题1：在文本生成是，模型会一本正经的胡说八道，这种现象叫做模型的幻觉。 问题2：训练一个大模型，需要多少数据量呢？ 问题3：数据预处理，怎么过滤、去重 问题4：模型大小 与 数据大小 的关系？  二、模型问题 1、Calibration  问题1：在文本生成是，模型会一本正经的胡说八道，这种现象叫做模型的幻觉。\n《Language Models (Mostly) Know What They Know》 这篇论文发现：模型够大后，说谎才会心虚。  对于大模型，模型输出是正确的概率 VS 模型的自信度，这两个是相关的。当模型比较自信时，输出的结果是正确的概率就比较大。 对于小模型，模型输出是正确的概率 VS 模型的自信度，这两个是不相关的    其中，横轴：模型输出时的自信程度；纵轴：模型输出是正确的概率。黄色表示最大模型，自身表示最小模型。 三、数据问题  问题2：训练一个大模型，需要多少数据量呢？\n训练一个大模型，需要多少数据量呢？《When Do You Need Billions of Words of Pretraining Data?》     问题3：数据预处理，怎么过滤、去重?\n数据预处理：《Scaling Language Models: Methods, Analysis \u0026amp; Insights from Training Gopher》  过滤有害的内容，通过Google的审核接口 去掉一些 HTML 前端的一些tag 规则过滤，去掉低质量的文本。 去重 剔除测试数据    问题4：模型大小 与 数据大小 的关系？ 《Training Compute-Optimal Large Language Models》 这篇文章发现：","tags":["aigc","生成式","幻觉"],"title":"生成式-问题"},{"categories":["Basic"],"contents":"一、简介 graph LR; A(动态规划) -- B(Monte Carlo) B -- C(TD) C -- D(Q学习) D -- E(SARSA) E -- F(DQN) F -- G(PPO) G -- H(AC/A2C/A3C) H -- I(DDPG) I -- J(SAC)  二、 ","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00033_reinforce/0001_reinforce_summary/","summary":"一、简介 graph LR; A(动态规划) -- B(Monte Carlo) B -- C(TD) C -- D(Q学习) D -- E(SARSA) E -- F(DQN) F -- G(PPO) G -- H(AC/A2C/A3C) H -- I(DDPG) I -- J(SAC)  二、 ","tags":["深度学习","强化学习","综述"],"title":"综述"},{"categories":["Basic"],"contents":"在大语言模型的训练中，如果增大数据量，相应的应该减少学习率，这个跟原来的经验相反。\n模型大小与模型效果：\n《Emergent Abilities of Large Language Models》 这篇文章指出：随着模型大小的增大，模型效果先不会有明显提升；增加到一定程度，模型有个突然顿悟时刻。\n一、文本生成 1、GPT 参考\n2、PaLM 《PaLM: Scaling Language Modeling with Pathways》 PaLM才是真正的“大”模型。它是迄今为止训练的最大的密集语言模型，参数为 540B，需要 6144 个 TPU 来训练（这是 3 个完整的 TPU pod，每个包含 2048 个 TPU）。这太贵了！可能只有谷歌拥有资源+基础设施来做到这一点。使用的Token高达7800亿。PaLM是使用Google新一代PathWay分布式训练框架训练出来。\n与GPT-3相比的变化：\n  多查询注意力（Multi-query attention）：在每个注意力头中共享K/V（Key/Value）嵌入，但使用独立的Q（Query）嵌入。这样做可以在推理阶段显著提高模型的速度。 并行Transformer块：使用并行的Transformer块来提高训练时间，相较于传统的串行设计，可以减少约15%的训练时间。 SwiGLU激活函数：与GPT-3使用的GELU激活函数不同，这里采用了SwiGLU激活函数。 旋转位置编码RoPE嵌入：使用RoPE（Relative Positional Encodings）嵌入代替学习得到的嵌入方式，在长文本上具有更好的性能 。 输入-输出嵌入共享：输入和输出embedding矩阵是共享的。 无偏置向量：在mlp、normlayer等算法中，都不使用bias，对于大模型，可以提高训练稳定性。 SentencePiece与256k标记：使用SentencePiece进行分词处理，标记数量为256k。   所以，有很多变化！同样，其中很多都是常见的，例如使用 GPT-3 的学习嵌入向量已经非常过时了，现在几乎没有人这样做。\n3、ChatGLM Layer Normalization的顺序和残差连接被重新排列， 用于输出标记预测的单个线性层； ReLU s替换为GELU s 二维位置编码\n4、BLOOM 使用 ALiBi 位置嵌入，它根据键和查询的距离直接衰减注意力分数。 与原始的 Transformer 和 Rotary 嵌入相比，它可以带来更流畅的训练和更好的下游性能。ALiBi不会在词嵌入中添加位置嵌入；相反，它会使用与其距离成比例的惩罚来偏向查询键的注意力评分。 Embedding Layer Norm 在第一个嵌入层之后立即使用，以避免训练不稳定。 使用了 25 万个标记的词汇表。 使用字节级 BPE。 这样，标记化永远不会产生未知标记 两个全连接层：\n5、LLaMa LLaMa结合了PaLM和Chinchilla两个模型的最佳特点，并做出了一些改进：\n  预归一化（Pre-normalize）：在每个Transformer子层之前对输入进行预归一化。 使用RMSNorm：使用RMSNorm代替LayerNorm，与Gopher模型中一样。 SwiGLU激活函数：使用了PaLM中的SwiGLU激活函数，但是维度从PaLM的值改为了新的值。 旋转位置嵌入（Rotary positional embeddings）：采用RoPE（相对位置编码）替代了PaLM中的绝对位置嵌入法。 使用AdamW：与Chinchilla模型一样，使用AdamW优化算法。   在计算方面的变化有：\n  使用高效的注意力机制（Rabe \u0026amp; Staats, FlashAttention）。 梯度检查点（Gradient checkpointing）。   作者唯一的抱怨是他希望他们能够将模型训练更长时间，因为学习曲线与收敛相差甚远！\n6、Claude Claude Chat API 7、Cohere 8、Falcon 9、Vicuna 10、Guanaco 11、MPT 12、Lazarus 13、WizardLM 二、图像生成 1、GAN 2014年\n2、CAN 2017年\n3、DALL-E 2021年2月\n根据文本描述绘画，绘画水平一般。\n4、CLIP+VQGAN 2021年4月\n根据文本描述绘画，绘画水平一般。\n5、Disco Diffusion 2022年2月\n根据文本描述绘画，具有原创性，图片精美，渲染时间长。\n6、Midjourney 2022年3月\n根据文本描述绘画，适合人像，细节突出\n7、DALL-E2 2022年4月，OpenAI发布DALL-E 2，命名来源于著名画家Dali和机器人总动员Wall-E，是DALL-E的升级版，其分辨率是之前版本的4倍。\nDALL-E 2 由三个模型组成：CLIP模型、先验模型、扩散模型。\n CLIP模型主要是用来对齐文本和图像特征：获取文本编码 先验模型主要是将文本表征映射为图片表征：将文本编码映射为图片编码 扩散模型是根据图片表征来完成完整的图像：用图片编码生成完整的图片。  根据文本描述绘画，限制较多，对复杂文字理解准确，渲染快\n8、Stable Diffusion 2022年8月，慕尼黑大学的Robin Rombach和Patrick Esser的团队提出的文本生成图像模型，交互简单，生成速度快。Stable Diffusion主要由三部分组成，分别是 VAE、U-Net、CLIP文本编码器：\n 首先使用CLIP模型将文本转换为表征形式 然后引导扩散模型U-Net在低维表征上进行扩散 最后将扩散后的低维表征送入VAE中的解码器，从而生成图像。  在GAN和CLIP的基础上，Stable Diffusion模型开源，直接推动了AIGC技术的突破性发展。\nStable Diffusion 扩散模型的原理是：先添加噪声后降噪。即：给现有的图像逐步添加噪声，直到图像被完全破坏，然后根据给定的高斯噪声，逆向逐步还原出原图。在模型训练完毕后，只需要输入一段随机的高斯噪声，就能生成一张图像。\n根据文本描述绘画，具有原创性，灵活度高，图片精美，具有真实感，渲染快。\n9、Imagen 2022年11月\n优先开源，效果好于DALL-E\n三、国内 1、太极 腾讯基于自身在自然语言处理和图像多模态等方面积累的经验，打造了通用场景模型——太极文生图大模型。太极文生图采用了Diffusion路线\n2、文心一格 百度提出的AIGC大模型——ERNIE-ViLG 文生图模型，包括：工业设计、游戏制作、服装设计、Logo设计、盆栽设计、动漫设计、珠宝设计、传统艺术等领域。ERNIE-ViLG模型能够深刻地理解中文语境，更了解中国化。\n3、太乙 IDEA研究院开源的第一个中文版Stable Diffusion模型——太乙 Stable Diffusion，该模型基于0.2亿筛选过的中文图文对进行训练，从而实现了具备中文内核的AIGC模型。\n4、CogView 智源研究院于2022年上半年，推出的CogView 2.0和 CogVideo\n5、MSRA 2021年11月微软亚洲研究院与北京大学联合发布了女娲模型，女娲模型用来从输入的文本、图像、视频中生成图像或者视频。\n6、MagicMix 字节跳动公司发布了MagicMix模型，模型可以将任意两个语义进行组合，生成全新的概念，再基于新概念进行图像生成。\n7、DPM-Solver 清华大学的朱军教授团队提出的DPM-Solver，是一种针对扩散模型特殊设计的高效求解器。\n","date":"August 5, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0005_summary/0001_aigc_summary/","summary":"在大语言模型的训练中，如果增大数据量，相应的应该减少学习率，这个跟原来的经验相反。\n模型大小与模型效果：\n《Emergent Abilities of Large Language Models》 这篇文章指出：随着模型大小的增大，模型效果先不会有明显提升；增加到一定程度，模型有个突然顿悟时刻。\n一、文本生成 1、GPT 参考\n2、PaLM 《PaLM: Scaling Language Modeling with Pathways》 PaLM才是真正的“大”模型。它是迄今为止训练的最大的密集语言模型，参数为 540B，需要 6144 个 TPU 来训练（这是 3 个完整的 TPU pod，每个包含 2048 个 TPU）。这太贵了！可能只有谷歌拥有资源+基础设施来做到这一点。使用的Token高达7800亿。PaLM是使用Google新一代PathWay分布式训练框架训练出来。\n与GPT-3相比的变化：\n  多查询注意力（Multi-query attention）：在每个注意力头中共享K/V（Key/Value）嵌入，但使用独立的Q（Query）嵌入。这样做可以在推理阶段显著提高模型的速度。 并行Transformer块：使用并行的Transformer块来提高训练时间，相较于传统的串行设计，可以减少约15%的训练时间。 SwiGLU激活函数：与GPT-3使用的GELU激活函数不同，这里采用了SwiGLU激活函数。 旋转位置编码RoPE嵌入：使用RoPE（Relative Positional Encodings）嵌入代替学习得到的嵌入方式，在长文本上具有更好的性能 。 输入-输出嵌入共享：输入和输出embedding矩阵是共享的。 无偏置向量：在mlp、normlayer等算法中，都不使用bias，对于大模型，可以提高训练稳定性。 SentencePiece与256k标记：使用SentencePiece进行分词处理，标记数量为256k。   所以，有很多变化！同样，其中很多都是常见的，例如使用 GPT-3 的学习嵌入向量已经非常过时了，现在几乎没有人这样做。\n3、ChatGLM Layer Normalization的顺序和残差连接被重新排列， 用于输出标记预测的单个线性层； ReLU s替换为GELU s 二维位置编码\n4、BLOOM 使用 ALiBi 位置嵌入，它根据键和查询的距离直接衰减注意力分数。 与原始的 Transformer 和 Rotary 嵌入相比，它可以带来更流畅的训练和更好的下游性能。ALiBi不会在词嵌入中添加位置嵌入；相反，它会使用与其距离成比例的惩罚来偏向查询键的注意力评分。 Embedding Layer Norm 在第一个嵌入层之后立即使用，以避免训练不稳定。 使用了 25 万个标记的词汇表。 使用字节级 BPE。 这样，标记化永远不会产生未知标记 两个全连接层：","tags":["aigc","summary"],"title":"综述"},{"categories":["Basic"],"contents":"一、基本概念 二、 ","date":"August 1, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0050_parameter_estimation/","summary":"一、基本概念 二、 ","tags":["概率论","假设检验"],"title":"假设检验"},{"categories":["Basic"],"contents":"一、基本概念 随机实验：$E$ 样本空间：记为 $S$。随机实验 $E$ 的所有可能结果组成的集合，称为随机实验 $E$ 的样本空间。\n样本点：样本空间的元素，即：随机实验 $E$ 的每个结果。\n随机事件：随机实验 $E$ 的样本空间$S$的子集，称为 $E$ 的随机事件。\n基本事件：由单个样本点组成的单点集，成为基本事件。\n必然事件：样本空间 $S$ 集合，成为必然事件。\n不可能事件：空集 $\\varnothing$。 概率：随机实验 $E$，样本空间为 $S$。对于 $E$ 的每一件事 $A$，概率 记为 $P(A)$ 条件概率：事件 $A$ 已经发生的条件下，事件 $B$ 发生的概率。\n划分：随机实验 $E$，样本空间为 $S$。$B_1, B_2, \u0026hellip;, B_n$ 为 $E$ 的一组事件，若\n $B_iB_j= \\varnothing , i \\ne j, i,j=1,2,\u0026hellip;,n$ $B_1 \\cup B_2 \\cup \u0026hellip; \\cup B_n = S$  则，称 $B_1, B_2, \u0026hellip;, B_n$ 为样本空间 $S$ 的一个划分。\n贝叶斯公式：随机实验 $E$，样本空间为 $S$。$B_1, B_2, \u0026hellip;, B_n$ 为 $E$ 的一个划分，$A$ 为 $E$的一个事件。且 $P(B_i) \u0026gt; 0, P(A) \u0026gt; 0$。则 $$P(B_i|A)=\\frac{P(A|B_i)P(B_i)}{ \\sum_{j=1}^n P(A|B_j)P(B_j) } , i=1,2,\u0026hellip;,n $$\n  二、 ","date":"August 1, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0010_basic-conception/","summary":"一、基本概念 随机实验：$E$ 样本空间：记为 $S$。随机实验 $E$ 的所有可能结果组成的集合，称为随机实验 $E$ 的样本空间。\n样本点：样本空间的元素，即：随机实验 $E$ 的每个结果。\n随机事件：随机实验 $E$ 的样本空间$S$的子集，称为 $E$ 的随机事件。\n基本事件：由单个样本点组成的单点集，成为基本事件。\n必然事件：样本空间 $S$ 集合，成为必然事件。\n不可能事件：空集 $\\varnothing$。 概率：随机实验 $E$，样本空间为 $S$。对于 $E$ 的每一件事 $A$，概率 记为 $P(A)$ 条件概率：事件 $A$ 已经发生的条件下，事件 $B$ 发生的概率。\n划分：随机实验 $E$，样本空间为 $S$。$B_1, B_2, \u0026hellip;, B_n$ 为 $E$ 的一组事件，若\n $B_iB_j= \\varnothing , i \\ne j, i,j=1,2,\u0026hellip;,n$ $B_1 \\cup B_2 \\cup \u0026hellip; \\cup B_n = S$  则，称 $B_1, B_2, \u0026hellip;, B_n$ 为样本空间 $S$ 的一个划分。","tags":["概率论","基本概念"],"title":"基本概念"},{"categories":["Basic"],"contents":"一、基本概念 ","date":"August 1, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00025_math_knowledge/0020_math_convex_optimization_theory/0010_basic_conception/","summary":"一、基本概念 ","tags":["凸优化","基本概念"],"title":"基本概念"},{"categories":["Basic"],"contents":"一、基本概念 二、 ","date":"August 1, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0030_law_of_large_numbers/","summary":"一、基本概念 二、 ","tags":["概率论","大数定律","中心极限定理"],"title":"大数定律"},{"categories":["Basic"],"contents":"一、基本概念 二、 ","date":"August 1, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0090_stationary_stochastic_process/","summary":"一、基本概念 二、 ","tags":["概率论","平稳随机过程"],"title":"平稳随机过程"},{"categories":["Basic"],"contents":"一、基本概念 二、 ","date":"August 1, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0060_analysis_variance_regression/","summary":"一、基本概念 二、 ","tags":["概率论","方差分析","回归分析"],"title":"方差分析及回归分析"},{"categories":["Basic"],"contents":"一、基本概念 二、 ","date":"August 1, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0040_sample_distribution/","summary":"一、基本概念 二、 ","tags":["概率论","采样","分布"],"title":"样本及抽样分布"},{"categories":["Basic"],"contents":"一、基本概念 二、 ","date":"August 1, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0020_distribution_of_variables/","summary":"一、基本概念 二、 ","tags":["概率论","随机变量","分布"],"title":"随机变量及其分布"},{"categories":["Basic"],"contents":"一、基本概念 二、 ","date":"August 1, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0070_stochastic_process/","summary":"一、基本概念 二、 ","tags":["概率论","随机过程"],"title":"随机过程"},{"categories":["Basic"],"contents":"一、基本概念 二、 ","date":"August 1, 2023","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00025_math_knowledge/0010_math_probability_theory/0080_markov_process/","summary":"一、基本概念 二、 ","tags":["概率论","马尔科夫链"],"title":"马尔科夫链"},{"categories":["Basic"],"contents":"一、简介 二、模型 1、gradient-based 1. GAP 《Learning Deep Features for Discriminative Localizatiion》\n# 代码非常简单， 提取到特征图和目标类别全连接的权重，直接加权求和，再经过relu操作去除负值，最后归一化获取CAM，具体如下: # 获取全连接层的权重 self._fc_weights = self.model._modules.get(fc_layer).weight.data # 获取目标类别的权重作为特征权重 weights=self._fc_weights[class_idx, :] # 这里self.hook_a为最后一层特征图的输出 batch_cams = (weights.unsqueeze(-1).unsqueeze(-1) * self.hook_a.squeeze(0)).sum(dim=0) # relu操作,去除负值 batch_cams = F.relu(batch_cams, inplace=True) # 归一化操作 batch_cams = self._normalize(batch_cams) 2. Grad-CAM 《Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization》\n2、gradient-free ","date":"September 9, 2022","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0200_cam/","summary":"一、简介 二、模型 1、gradient-based 1. GAP 《Learning Deep Features for Discriminative Localizatiion》\n# 代码非常简单， 提取到特征图和目标类别全连接的权重，直接加权求和，再经过relu操作去除负值，最后归一化获取CAM，具体如下: # 获取全连接层的权重 self._fc_weights = self.model._modules.get(fc_layer).weight.data # 获取目标类别的权重作为特征权重 weights=self._fc_weights[class_idx, :] # 这里self.hook_a为最后一层特征图的输出 batch_cams = (weights.unsqueeze(-1).unsqueeze(-1) * self.hook_a.squeeze(0)).sum(dim=0) # relu操作,去除负值 batch_cams = F.relu(batch_cams, inplace=True) # 归一化操作 batch_cams = self._normalize(batch_cams) 2. Grad-CAM 《Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization》\n2、gradient-free ","tags":["vlp","summary"],"title":"CAM"},{"categories":["Basic"],"contents":"一、简介 参考， 论文， Gitlab\n二、 ","date":"May 9, 2022","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00400_vlp/0005_clip/","summary":"一、简介 参考， 论文， Gitlab\n二、 ","tags":["多模态","CLIP"],"title":"CLIP"},{"categories":["Basic"],"contents":"从2019年中~2020年中，对比学习火了一段时间，到ViT出来后，大量的研究这才投身于ViT。\n一、简介 什么是对比学习？\n简单来说就是，只要模型把相似的数据跟其他不相似的数据区分开就可以。比如：$A_1, A_2, \u0026hellip;$ 是狗，$B_1, B_2, \u0026hellip;$ 是猫，只要模型能把这两批数据区分开就行。\n所以，训练集中不需要明确的标签，只要能区分出那些数据之间是相似的，那些是与它们不相似的。\n所以，训练集中不必人为标注，只需要设计一些规则生产出这种类型的训练集就行。\n看下Hinton老爷子的《Self-organizing neural network that discovers surfaces in random-dot stereograms》 和 LeCun的《Dimensionality reduction by learning an invariant mapping》 对比学习为啥在cv领域被认为是无监督呢？：\n 通过设计一些巧妙的代理任务，就是pretext task：人为的定义一些规则，这些规则可以用来定义那些图片是相似的，那些图片是不相似的。\n例如：instance discrimination：如果有N张图片的数据集，随机一张图片$x_i$，对这个图片随机裁剪+数据增广，从同一张图片中通过裁剪+增广产生的数据，虽然有差异但是语义信息是一样的，所以是正样本(它们之间是相似的)，负样本就是除了图$x_i$之外的所有样本。  1、代理任务 代理任务(pretext task)的目的: 生成一个自监督的信号，从而充当ground truth这个标签信息\n有监督学习：训练时比较输出 $\\hat{Y}$ 和 groud truth $Y$；\n自监督学习：因为缺少groud truth，所以需要代理任务自己创建类似groud truth的信号。\n2、对比学习的loss 1)、InfoNCE loss noise contrastive estimation loss：其实就是一个交叉熵 $$ L_q = -log\\frac{exp(q\\cdot k_+ / \\tau)}{\\sum_{i=0}^{K} exp(q\\cdot k_i / \\tau)} $$ 分母：一个正样本，K个负样本；$\\tau$：温度超参数，值越大分布就越平缓，表示对每种的关注度越相似；值越小分布就越陡峭，表示比较关注比较困难的case，不容易收敛。\n3、数据 数据处理流程：\n 图片$x_1$经过不同的变换分别生成了不同的图片$x_1^1, x_1^2$，一般$x_1^1$为锚点作为基准；$x_1^2$是$x_1^1$的正样本；剩余的$x_2, x_3, \u0026hellip;, x_N$是$x_1^1$的负样本。 有了正负样本数据后，就是把这些数据丢进编码器提取特征；锚点的特征：$f_{11}$，正样本：$f_{12}$，负样本：$f_2, f_3, \u0026hellip;, f_N$ 对比学习的目的：在特征空间里，让锚点的特征$f_{11}$与正样本的特征$f_{12}$尽量靠近；与负样本的特征$f_2, f_3, \u0026hellip;, f_N$尽量远离。  二、初代对比网络 1、InstDisc 《Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination》2018 缺点是：字典特征不一致。\n 提出了个体判别这个代理任务。 把ImageNet的每张图片，表示层128维特征，提前算好存起来(memory bank) 对memory bank的特征进行动态更新 在计算loss时使用动量，来弥补字典特征的不一致性  2、InvaSpread 《Unsupervised Embedding Learning via Invariant and Spreading Instance Feature》2019：Invariant：对于相似的图片，特征尽量不变；Spreading：对于不相似的图片，特征尽量分散。 缺点：字典太小。\n 个体判别代理任务 在mni-batch内选择正样本和负样本：比如batch size = 256，正样本就是mini-batch内的每个样本，负样本就是除去该正样本后的所有样本和其数据增强后的样本。 目标函数为 NCE Loss的一个变体  3、CPC 《Representation Learning with Contrastive Predictive Coding》2018\n 预测未来的代理任务(生成式)：未来输入，通过已训的网络后的输出特征作为正样本；其他任意输入通过已训的网络后的输出特征作为负样本。  4、CMC 《Contrastive Multiview Coding》2019\n 多视角的代理任务：同一事物的不同视角的表征，是正样本；其他事物的表征，是负样本 不同视角会有不同的编码器 证明了多模态融合的可能性  三、二代目对比网络 1、MoCo MoCo(2020) 作为一个无监督的表征学习工作，不仅在分类领域逼近有监督的模型，还在检测、分割、人体关键点检测都超越了有监督的预训练模型，MoCo的出现证明了在视觉领域无监督训练是有前途的。\n问题：为什么无监督学习在NLP领域表现较好，在视觉领域效果不好呢？\n作者认为：NLP领域，每个token是一个独立的语义信息，其分布是一个离散的信号空间，由于token的独立性，在字典集合中，其就是一个分类任务，可以有类似标签的形式帮助训练；视觉是一个高维连续空间，不像token有很强的语义信息而且浓缩的比较好，导致视觉不能创建一个类似NLP的字典，没有这个字典就不容易建模，所以在视觉领域无监督学习还不如有监督学习。\n问题：作者设计动机？\n以往的工作，会受限于：a. 字典的大小；b. 字典内的一致性。\n 训练一个encoder，从图像数据里抽样出特征，由这些特征组成一个动态字典；  这个字典一定要大：字典越大，就可以表征更多的视觉信息。 在训练的时候，字典内要有一致性：各个key的尽量是通过相同的编码器产出的，不然query可能选择与自己的编码器相同的key，而不是真的和它含有相同语义信息的那个key。   对比学习使得正样本间距离尽量小，负样本距离尽量大    作者怎么设计的：\n 怎么构建大字典？所有数据集组成一个字典肯定不行，计算一次要花费很长时间，而且内存也不够。作者使用了队列这种数据结构，队列的大小就是字典的大小；队列可以很大(字典很大)，每次训练的mini-batch很小；队列中的元素不是每次都需要更新，每次更新mini-batch大小的数据。新的数据入队，最久的数据出队。字典大小(作者默认：65536)是一个超参数，可以是几千上万，训练时间都差不多。 怎么保持一致性呢？作者设计了动量编码器：例如：query的编码器为 $\\theta_q$，动量编码器为：$\\theta_k \\gets m \\theta_{k} + (1-m)\\theta_q$。设计一个较大的动量参数m(e.g., m=0.999)，使得动量编码器更新的比较缓慢，不会因为 $\\theta_q$引起太多的改变，所以近似保持一致性。只有 $\\theta_q$ 参与训练，$\\theta_k$ 是不参与训练的，是由0.999的上一个 $\\theta_k$+0.001的当前 $\\theta_q$组合而成。     # f_q, f_k: encoder networks for query and key # queue: dictionary as a queue of K keys (CxK) # m: momentum # t: temperature f_k.params = f_q.params # initialize for x in loader: # load a minibatch x with N samples x_q = aug(x) # a randomly augmented version x_k = aug(x) # another randomly augmented version q = f_q.forward(x_q) # queries: NxC k = f_k.forward(x_k) # keys: NxC k = k.detach() # no gradient to keys # positive logits: Nx1 l_pos = bmm(q.view(N,1,C), k.view(N,C,1)) # negative logits: NxK l_neg = mm(q.view(N,C), queue.view(C,K)) # logits: Nx(1+K) logits = cat([l_pos, l_neg], dim=1) # contrastive loss, Eqn.(1) labels = zeros(N) # positives are the 0-th loss = CrossEntropyLoss(logits/t, labels) # SGD update: query network loss.backward() update(f_q.params) # momentum update: key network f_k.params = m*f_k.params+(1-m)*f_q.params # update dictionary enqueue(queue, k) # enqueue the current minibatch dequeue(queue) # dequeue the earliest minibatch 2、SimCLR 《A simple framework for contrastive learning of visual representations》2020很简单。缺点就是 字典比较小。\n 在mni-batch内选择正样本和负样本：比如batch size = 256，正样本就是mini-batch内的每个样本，负样本就是除去该正样本后的所有样本和其数据增强后的样本。 训练时添加了一个mlp全连接层，就是那个 $g(\\sdot)$，这个简单的操作，在ImageNet数据集上提升了10个点。 更丰富的数据增强  3、MoCo v2 MoCo V2 基于初版MoCo和SimCLR的全连接层，做了一些优化，发现添加mlp全连接层真香\n 加了更丰富的数据增强 提升3个点 加了MLP 提升了6个点 加了cosine 的学习率 训练更多个epoch 提升4个点  4、SimCLR v2 《Big Self-Supervised Models are Strong Semi-Supervised Learners》\n 采用更大的模型，152-ResNet 添加了2层mlp 使用动量编码器  5、SwAV(swap assignment views) 《Unsupervised Learning of Visual Features by Contrasting Cluster Assignments》2020：给定同样一张图片，生成不同的视角；希望可以用一个视角得到的特征去预测另外一个视角得到的特征。\n multi crop 技术：全局和局部的特征都需要关注  四、三代目对比网络 不使用负样本\n1、BYOL 《Bootstrap your own latent: A new approach to self-supervised Learning》2020\n 对输入图片x，锚点通过一系列的变换，最后是 $q_{\\theta}(z_{\\theta})$；正样本通过一些列的变换，最后是 $sg(z'_{\\xi})$。这两个是输入图片的近似表示 让 $sg(z'_{\\xi})$ 做target，计算这两个的MSE-loss 模型最后就训练了编码器 $f_{\\theta}$，正样本的编码器 $f_{\\xi}$ 只是 $f_{\\theta}$ 的平均，也就是动量编码器。 模型没有使用负样本，只是用自己预测自己，为啥没有出现模型坍塌呢？  参考 这个 Blog 博主，通过一系列的实验，得出他的结论：BYOL能够学到东西，主要是因为Batch normalization。通过BN的操作，用整个batch的样本计算 均值、方差，然后用在batch内的各个样本上；这个操作相当于存在信息泄露，一个样本在计算是也能看到整个batch的信息，相当于一个平均的信息作为负样本；即使没有刻意提供负样本，但通过BN的操作也有了负样本的作用。 BYOL的作者听到后就不同意了，他也做了一些列的实验 《BYOL works even without batch statistics》，发现BN确实很香；不过，他认为是BN只是使得模型能够稳定训练，真正起作用的是一个很好的初始化。    2、SimSiam 《Exploring Simple Siamese Representation Learning》2020\n作者怎么设计的：\n 不需要负样本 不需要大的batch size 不需要动量编码器  # f: backbone + projection mlp # h: prediction mlp for x in loader: # load a minibatch x with n samples x1, x2 = aug(x), aug(x) # random augmentation z1, z2 = f(x1), f(x2) # projections, n-by-d p1, p2 = h(z1), h(z2) # predictions, n-by-d L = D(p1, z2)/2 + D(p2, z1)/2 # loss L.backward() # back-propagate update(f, h) # SGD update def D(p, z): # negative cosine similarity z = z.detach() # stop gradient p = normalize(p, dim=1) # l2-normalize z = normalize(z, dim=1) # l2-normalize return -(p*z).sum(dim=1).mean()      结论：类似于EM(Expectation-Maximization)算法。\n五、四代目对比网络 Transformer在对比学习上的应用\n1、MoCo V3 《An Empirical Study of Training Self-Supervised Vision Transformers》2021 相当于 MoCo V2 和SimSiam的合体\n 不训练 patch projection层。 把backbone有ResNet换成ViT；在训练时作者发现，准确度会时不时的塌陷。这个原因是什么呢？  作者通过观察回传梯度，在第一层梯度波动较大，说明这一层梯度不正常。作者设想：梯度既然不正常还不如直接不用梯度更新第一层，作者在第一层初始化后就直接冻住，不再更新第一层，实验后发现问题解决了。 所以，在ViT的第一层patch projection 还是有问题的，后续会被继续研究     # f_q: encoder: backbone + proj mlp + pred mlp # f_k: momentum encoder: backbone + proj mlp # m: momentum coefficient # tau: temperature for x in loader: # load a minibatch x with N samples x1, x2 = aug(x), aug(x) # augmentation q1, q2 = f_q(x1), f_q(x2) # queries: [N, C] each k1, k2 = f_k(x1), f_k(x2) # keys: [N, C] each loss = ctr(q1, k2) + ctr(q2, k1) # symmetrized loss.backward() update(f_q) # optimizer update: f_q f_k = m*f_k + (1-m)*f_q # momentum update: f_k # contrastive loss def ctr(q, k): logits = mm(q, k.t()) # [N, N] pairs labels = range(N) # positives are in diagonal loss = CrossEntropyLoss(logits/tau, labels) return 2 * tau * loss    2、DINO 《Emerging Properties in Self-Supervised Vision Transformers》2021 跟BYOL、SimSiam类似，也是预测型。\n跟MoCo V3 非常类似\n总结一下：\n # gs, gt: student and teacher networks # C: center (K) # tps, tpt: student and teacher temperatures # l, m: network and center momentum rates gt.params = gs.params for x in loader: # load a minibatch x with n samples x1, x2 = augment(x), augment(x) # random views s1, s2 = gs(x1), gs(x2) # student output n-by-K t1, t2 = gt(x1), gt(x2) # teacher output n-by-K loss = H(t1, s2)/2 + H(t2, s1)/2 loss.backward() # back-propagate # student, teacher and center updates update(gs) # SGD gt.params = l*gt.params + (1-l)*gs.params C = m*C + (1-m)*cat([t1, t2]).mean(dim=0) def H(t, s): t = t.detach() # stop gradient s = softmax(s / tps, dim=1) t = softmax((t - C) / tpt, dim=1) # center + sharpen return - (t * log(s)).sum(dim=1).mean()    ","date":"May 9, 2022","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00100_cv/0020_contrastive_learning/contrastive_learning/","summary":"从2019年中~2020年中，对比学习火了一段时间，到ViT出来后，大量的研究这才投身于ViT。\n一、简介 什么是对比学习？\n简单来说就是，只要模型把相似的数据跟其他不相似的数据区分开就可以。比如：$A_1, A_2, \u0026hellip;$ 是狗，$B_1, B_2, \u0026hellip;$ 是猫，只要模型能把这两批数据区分开就行。\n所以，训练集中不需要明确的标签，只要能区分出那些数据之间是相似的，那些是与它们不相似的。\n所以，训练集中不必人为标注，只需要设计一些规则生产出这种类型的训练集就行。\n看下Hinton老爷子的《Self-organizing neural network that discovers surfaces in random-dot stereograms》 和 LeCun的《Dimensionality reduction by learning an invariant mapping》 对比学习为啥在cv领域被认为是无监督呢？：\n 通过设计一些巧妙的代理任务，就是pretext task：人为的定义一些规则，这些规则可以用来定义那些图片是相似的，那些图片是不相似的。\n例如：instance discrimination：如果有N张图片的数据集，随机一张图片$x_i$，对这个图片随机裁剪+数据增广，从同一张图片中通过裁剪+增广产生的数据，虽然有差异但是语义信息是一样的，所以是正样本(它们之间是相似的)，负样本就是除了图$x_i$之外的所有样本。  1、代理任务 代理任务(pretext task)的目的: 生成一个自监督的信号，从而充当ground truth这个标签信息\n有监督学习：训练时比较输出 $\\hat{Y}$ 和 groud truth $Y$；\n自监督学习：因为缺少groud truth，所以需要代理任务自己创建类似groud truth的信号。\n2、对比学习的loss 1)、InfoNCE loss noise contrastive estimation loss：其实就是一个交叉熵 $$ L_q = -log\\frac{exp(q\\cdot k_+ / \\tau)}{\\sum_{i=0}^{K} exp(q\\cdot k_i / \\tau)} $$ 分母：一个正样本，K个负样本；$\\tau$：温度超参数，值越大分布就越平缓，表示对每种的关注度越相似；值越小分布就越陡峭，表示比较关注比较困难的case，不容易收敛。","tags":["backbone","contrastive learning"],"title":"contrastive learning"},{"categories":["Basic"],"contents":"一、简介 1、Transformer用在CV领域 在NLP中，Transformer的输入是一个时间步长为T的序列，比如：basic版bert，T=512，每个token embeding为768维特征。如何把二维图片转化为一维呢？\n $\\bf \\color{red} \\times$ 如果把每个像素点看做是一个样本，铺平后是一维序列。但是，图片大小 224*224=50176，远远大于Transformer的最大序列长度。 $\\bf \\color{red} \\times$ 卷积和Transformer一起用：《Non-local Neural Networks》(2018)、《End-to-End Object Detection with Transformers》(2020) 为了减小序列的长度，不直接使用输入图片，而是使用feature map 转换为序列。比如：ResNet50在最后的阶段的输出尺寸为 14x14，拉平后序列长度只有196。 $\\bf \\color{red} \\times$ 抛弃卷积使用定制化的自注意力机制：《Stand-Alone Self-Attention in Vision Models》(2019) 采用的是 孤立自注意力。用一个局部的小窗口做自注意力； 《Stand-alone axial-attention for panoptic segmentation》(2020) 采用的是轴注意力。在高度的方向上做自注意力、在宽度方向做自注意力。由于这些自注意力机制比较定制化，还没有在硬件上大规模加速计算，所以网络做不大。 $\\color{green} \\checkmark$ 对图片做些预处理，直接使用Transformer：将图片切分成一个个patch，然后每个patch作为一个token输入到Transformer中。  $224 \\times 224$ 的图片，切分成一个个 $16 \\times 16$ 的patch，最终切分出196个patch；每个patch的大小是：$16 \\times 16 \\times 3=768$，刚好是basic版bert每个token的维度。 多头注意力机制，12个头，每个头的k、q、v对应的维度是64维    二、网络 1、ViT ViT(2021) 直接把Transformer应用到图像处理，尽量改动最少，所以只对图像做预处理，让其符合NLP的输入形式， 思路：\n 图片尺寸 224x224，将图片切分成一个个patch，patch的大小16x16，每个patch作为一个token，即：14x14=196个patch，每个patch长16x16x3=768 学习一个线性矩阵$E$，尺寸为768x768，对每个patch做线性变换。多头注意力的话，basic版本12个头，所以12个196x64拼接起来，还是196x768。 位置编码：可学习的位置向量，尺寸为196x768 cls的输出作为提取的图片特征，用于后续的分类操作  实验结论：\n 额外使用cls做分类，为啥不用GAP呢？作者对比了这两种方式，发现如果参数调整好的话效果是一样的。所以为了在使用Transformer时改动最少，所以继续使用cls作为分类。 位置编码信息，每个位置编码信息是一个768维度的向量。图像是二维的，所以是否需要在位置编码里体现二维特征呢？作者实验发现，即使是一维的768维度的向量，其中也会学习到二维特征，所以没有必要故意设计二维特征。 归纳偏置(inductive bias)，在cnn中位置信息是贯穿在所有卷积操作中的，卷积是线性操作，具有偏移不变性。在ViT中除了添加了位置编码信息，是没有其他的空间信息的。所以作者认为是这个原因导致ViT在小规模的数据集上效果不好，在中/大规模的数据集上效果较好。 Transformer有处理长文本的能力，所以ViT能够捕获图片的全局信息，而并不是像cnn只用到感受野区域。 作者还提出是否可以类比BERT，mask掉一些patch，然后自监督训练，修复出mask掉的区域。这就是后来何大神的MAE。 ViT可以说是打通了CV和NLP的鸿沟，对多模态具有重要的意义。  可视化显示模型能学习的信息： 可学习的线性矩阵$\\bf E$，具体学习的信息：跟CNN很像，可以学习到颜色、纹理、等底层信息。  位置信息：发现可以学习到位置信息，同时也可以学习到行、列的信息；这就是为啥没有必要设计2维的位置信息。   图中，每列都有16个点，就是16个头的输出；纵轴：平均注意力的距离；横轴：网络的层数。图展示了ViT能不能注意到全局信息: a. 在前几层有相近的、距离远的，这说明，在刚开始模型就能注意到较远的像素，而不像CNN前几层因为感受野较小只能注意到相近的像素点；b. 在后几层距离都比较远，说明网络学习到的特征越来越具有语义信息。  作者用网络的最后一层的输出，映射回输入图片上，发现模型是可以获取图像的高阶语义信息，是可以关注到用于分类的图像区域。   2、BEiT 《BEiT: BERT Pre-Training of Image Transformers》(2021)\n3、PVT 《Pyramid Vision Transformer》(2021)\n4、Swin Swin Transformer(2021) 微软亚研究院发表在ICCV上的一篇文章，获取2021 best paper。 github \n把Transformer从NLP应用到vision领域，有两个挑战：\n 图中物体尺寸不一，比如一个行人、一辆骑车，在图中尺寸不同；就算是两个行人，也有大有小。NLP的同一个词，在图片中尺寸可能差别很大。 图像分辨率太大，如果以像素点为基本单位的话，则序列就非常大。为了减少序列长度，一些方法：使用feature  作者为了解决这两个问题，提出了Swin：\n 通过移动窗口学习出 序列特征作为Transformer的输入； 移动窗口(shifted window)能够使得相邻的两个窗口有了交互，变相的达到全局建模的能力。 层级结构(hierarchical architecture)，非常灵活，不仅可以提供不同尺度的特征信息；而且计算复杂度跟图片大小成线性关系    ViT：每个token代表的尺寸都是一样的，每一层看到的token的尺寸都是一样的；虽说通过多注意力机制能够把握全局信息，但是在多尺寸特征上的把握是不够的。而在vision中多尺寸的的特征是很重要的。 ViT：多注意力机制是在全图上计算，所以它的计算复杂度跟图片尺寸成平方的关系； Swin：在小窗口内计算多注意力，因为在视觉里有这样的先验：相邻的区域大概率是相似物体。对于视觉直接做全局注意力是浪费的 Swin：在卷积操作中，由于pooling操作提供了不同尺寸的特征；类比pooling，作者提出了patch merging：把相邻的小patch合并成一个大patch   怎么计算多注意力？滑动窗口的设计？\n图中灰框：是 $4\\times4$ 的小块；rgb三通道拉直后就是 $1\\times48$ 图中红框：是一个包含 $7\\times7$ 个小块的窗口，即：$7\\times7\\times48$。多注意力机制就是在这些窗口中计算的。\n滑动窗口：\n 在窗口里做多头注意力计算，只能关注到这个窗口的信息；只这样操作，就违背了Transformer的初衷(把握上下文)，所以作者采用滑动窗口，下一层的窗口与上一层的多个窗口相交，这样多个窗口之间就有了联系。作者提出的patch merging 合并到Transformer最后几层时，就会看到大部分图片的信息了。 为了统一的标准化计算，采用循环移位；  向右下移动两个位置；上面多出的部分循环移动到下面、左边多出的部分循环移动到右边、左上角多出的部分循环移动到右下角 在窗口内，循环移动的这些块之间 在图片中是不相邻的，所以不应该计算它们之间的联系(比如：上面是天空下面是地面)。作者就采用masked MSA 在计算完注意力后，把移动的块复原到原来的位置；保证信息的一致性       网络架构\n  输入图片：$224\\times224\\times3$，通过patch partition操作，把 $4\\times4$ 的小patch拉直后，变成：$56\\times56\\times48$\n  通过4个Swin Transformer块，生成不同尺度的特征。\n 第一个Swin Tranformer块 没有使用patch merging，尺寸信息为 $56\\times56\\times96$，其中 $C=96$；剩余的模块，都经过patch merging：让H、W减半，在channel上扩大2倍。即：$56\\times56\\times96 \\rArr 28\\times28\\times192 \\rArr 14\\times14\\times384 \\rArr 7\\times7\\times768$ (768怎么这么熟悉^_^) 每个Swin Transformer块，都含有两个Transformer块，第一个采用W-MSA（窗口-多头自注意力），第二个相匹配的采用SW-MSA（滑动窗口-多头自注意力）。    不同变体：\n Swin-T：$C=96, layer numbers = {2, 2, 6, 2}$ ，计算复杂度与ResNet50差不多 Swin-S：$C=96, layer numbers = {2, 2, 18, 2}$ ，计算复杂度与ResNet101差不多 Swin-B：$C=128, layer numbers = {2, 2, 18, 2}$ Swin-L：$C=192, layer numbers = {2, 2, 18, 2}$    patch merging\n其中，patch merging 的操作如下图所示：\n 把相邻 $2*2$ 的patch块，拉伸到channel维度；使得H、W方向降维，C方向升维 拉伸后变成 $4C$，如果希望是 $2C$的话，后续接一个全连接层   masked MSA\n 只要不是自己区域的向量相乘，就需要被mask掉 掩码矩阵：需要mask的区域为：-100，不需要mask掉的区域为：0。 softmax(计算好的自注意力矩阵（就是那个权重） + 掩码矩阵)，-100经过softmax后近似为0了。     W-MSA\n 在每个窗口中做多头自注意力计算，各个窗口之间是没有联系的 计算复杂度大概：$4hwC^2+2hwM^2C$，相比于全图片做自注意力计算($4hwC^2+2(hw)^2C$)，计算效率提升不少   SW-MSA\n 如果只在窗口内各自计算注意力，那么就没有整个图片上下文；通过移动窗口来使得相邻的窗口之间有联系 窗口的大小固定为 $7\\times7$，由于patch merging类似于pooling操作，所以随着层数的加深，窗口看到的感受野越来越大    5、MAE MAE(2021) 主要思想：随机mask掉一些patch，然后重构这些patch里的所有像素。\n 设计非对称的encoder-decoder架构\nencoder：作用在非mask的patch；将这些观察到的信息，映射到一个潜表示（在语义空间上的表示）\ndecoder：是一个轻量级的解码器，从这个潜表示中重构原始信号； 模型架构就是ViT，不一样的是输入是非mask的patchs  高比例的mask是比较有效的，因为低比例的mask可以通过简单的插值就能重组，使得模型学不到什么东西；高比例的mask迫使模型学习更有效的表征； 由于参与计算的是非mask的patch，高比例的mask使得计算速度加快好几倍。   图片patch与文本token的区别：图像只是被记录下来的光，没有语义分解成视觉上的词。后续的工作可以是：mask掉 不能构建语义段的patchs，就是这些patchs没有主体，只是包含主体的一小部分。  由于patch不是一个word，不是独立的语义，可能跟其他patch构成一个独立的语义word，这就说明图片相对于文本冗余信息太多，即使mask掉好多信息也可以重构出图片 模型学习到图片的全局信息，可以通过一些局部信息重构图片     6、 微软研究院提出的Focal Transformer在分类/检测/分割任务上表现SOTA！在ADE20K 语义分割上高达55.4 mIoU\n中科大、微软亚洲研究院提出的CSWin在ImageNet上高达87.5%准确率，在ADE20K上高达55.2mIoU github 北大提出的CBNetV2 github 三、CV各领域 1、目标检测 1). DETR(Detection Transformer) 《End-to-End Object Detection with Transformers》(2020)，是Transformer在目标检测领域的开山之作。\n2、图像分割 ","date":"May 9, 2022","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00100_cv/0030_vision_transformer/vision_transformer/","summary":"一、简介 1、Transformer用在CV领域 在NLP中，Transformer的输入是一个时间步长为T的序列，比如：basic版bert，T=512，每个token embeding为768维特征。如何把二维图片转化为一维呢？\n $\\bf \\color{red} \\times$ 如果把每个像素点看做是一个样本，铺平后是一维序列。但是，图片大小 224*224=50176，远远大于Transformer的最大序列长度。 $\\bf \\color{red} \\times$ 卷积和Transformer一起用：《Non-local Neural Networks》(2018)、《End-to-End Object Detection with Transformers》(2020) 为了减小序列的长度，不直接使用输入图片，而是使用feature map 转换为序列。比如：ResNet50在最后的阶段的输出尺寸为 14x14，拉平后序列长度只有196。 $\\bf \\color{red} \\times$ 抛弃卷积使用定制化的自注意力机制：《Stand-Alone Self-Attention in Vision Models》(2019) 采用的是 孤立自注意力。用一个局部的小窗口做自注意力； 《Stand-alone axial-attention for panoptic segmentation》(2020) 采用的是轴注意力。在高度的方向上做自注意力、在宽度方向做自注意力。由于这些自注意力机制比较定制化，还没有在硬件上大规模加速计算，所以网络做不大。 $\\color{green} \\checkmark$ 对图片做些预处理，直接使用Transformer：将图片切分成一个个patch，然后每个patch作为一个token输入到Transformer中。  $224 \\times 224$ 的图片，切分成一个个 $16 \\times 16$ 的patch，最终切分出196个patch；每个patch的大小是：$16 \\times 16 \\times 3=768$，刚好是basic版bert每个token的维度。 多头注意力机制，12个头，每个头的k、q、v对应的维度是64维    二、网络 1、ViT ViT(2021) 直接把Transformer应用到图像处理，尽量改动最少，所以只对图像做预处理，让其符合NLP的输入形式， 思路：\n 图片尺寸 224x224，将图片切分成一个个patch，patch的大小16x16，每个patch作为一个token，即：14x14=196个patch，每个patch长16x16x3=768 学习一个线性矩阵$E$，尺寸为768x768，对每个patch做线性变换。多头注意力的话，basic版本12个头，所以12个196x64拼接起来，还是196x768。 位置编码：可学习的位置向量，尺寸为196x768 cls的输出作为提取的图片特征，用于后续的分类操作  实验结论：","tags":["backbone","vision transformer"],"title":"vision transformer"},{"categories":["Basic"],"contents":"一、简介 It is coming soon.\n二、网络 1、 2、 3、 4、 ","date":"May 9, 2022","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00500_video/vidio_summary/","summary":"一、简介 It is coming soon.\n二、网络 1、 2、 3、 4、 ","tags":["video","summary"],"title":"简介"},{"categories":["Basic"],"contents":"一、简介 多模态学习，英文全程MultiModal Machine Learning(MMML)，从1970年 起步，已经经历了多个发展阶段，在2010年后，全面进入深度学习的阶段。多模态机器学习，以机器学习实现处理和理解多源模态信息的能力。图像、视频、音频、语义之间的多模态学习比较热门。比如互联网大型视频平台，都会将多模态技术用于视频理解业务，可以加视频封面、视频抽帧、文本信息融合。当计算机能够看懂视频，就可以做很多事儿了，比如：视频分类、审核、推荐、搜索、特效。\n多模态学习有5个研究方向：\n 多模态表示学习（Multimodal Representation） 模态转化（Translation） 对齐（Alignment） 多模态融合（Multimodal Fusion） 协同学习（Co-learning）  实际应用，比如：\n 视频网站上进度条，会显示那个时间段是高光时刻 自动驾驶领域，雷达、视觉与多传感器信息融合 视频的分类、审核、推荐、搜索、特效等等  1、VLP 微软发表的一篇文章《An Empirical Study of Training End-to-End Vision-and-Language Transformers》进行了大量的实验，对不同VLP模型、各个模块不同配置的效果。\nVLP通常都会遵循同一个框架，包含5大模块：\n Vision Encoder：主要有3中类型  使用object detection模型，比如：Faster R-CNN，识别图像中的目标区域，并生成每个目标区域的特征表示，输入到后续模型中 利用CNN模型提取grid feature作为图像输入 ViT采用的将图像分解成patch，每个patch生成embeding输入到模型。 随着Vision Transformer的发展，ViT的方式逐渐成为主流方式。   Text Encoder：包括BERT、RoBERTa、ELECTRA、ALBERT、DeBERTa等经典预训练语言模型结构。 Multimodel Fusion：主要指如何融合图像、文本，主要有2中：  co-attention：图像、文本分别使用Transformer编码，在每个Transformer模块中加入图像、文本的cross attention merged attention model，图像、文本在开始就拼接在一起，输入到Transformer   模型结构：主要有2中：  Encoder-only：这种比较常见 Encoder-Decoder   预训练任务：主要有3中：  Masked Language Modeling（MLM）类似BERT，随机mask掉部分token，用剩余的预测出被mask掉的token Masked Image Modeling，对输入的部分图像patch进行mask，然后预测被mask的patchs Image-Text Matching（ITM），预测image和text的pair对是否匹配，对比学习的预训练方法可以属于这类。    二、网络 Open AI 在2021年1月份发布的DALL-E和CLIP，属于结合图像和文本的多模态模型，其中DALL-E是基于文本来生成模型的模型；CLIP是用文本作为监督信号来训练可迁移的视觉模型，这两个工作带动了一波新的研究高潮。\n1、CLIP 参考\nCLIP\n2、DALL-E DALL-E：通过文本生成图片\n3、KOSMOS-1 《Language Is Not All You Need: Aligning Perception with Language Models》 代码：github 中介绍了一个多模态大型语言模型(MLLM)——KOSMOS-1。它可以感知一般模态、遵循指令(即零样本学习)以及在上下文中学习(即少样本学习)。研究目标：使感知与LLM保持一致，如此一来模型能够看到(see)和说话(talk)。研究者按照 《Language models are general-purpose interfaces》 的方式从头开始训练KOSMOS-1。\n","date":"May 9, 2022","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00400_vlp/0001_vlp_summary/","summary":"一、简介 多模态学习，英文全程MultiModal Machine Learning(MMML)，从1970年 起步，已经经历了多个发展阶段，在2010年后，全面进入深度学习的阶段。多模态机器学习，以机器学习实现处理和理解多源模态信息的能力。图像、视频、音频、语义之间的多模态学习比较热门。比如互联网大型视频平台，都会将多模态技术用于视频理解业务，可以加视频封面、视频抽帧、文本信息融合。当计算机能够看懂视频，就可以做很多事儿了，比如：视频分类、审核、推荐、搜索、特效。\n多模态学习有5个研究方向：\n 多模态表示学习（Multimodal Representation） 模态转化（Translation） 对齐（Alignment） 多模态融合（Multimodal Fusion） 协同学习（Co-learning）  实际应用，比如：\n 视频网站上进度条，会显示那个时间段是高光时刻 自动驾驶领域，雷达、视觉与多传感器信息融合 视频的分类、审核、推荐、搜索、特效等等  1、VLP 微软发表的一篇文章《An Empirical Study of Training End-to-End Vision-and-Language Transformers》进行了大量的实验，对不同VLP模型、各个模块不同配置的效果。\nVLP通常都会遵循同一个框架，包含5大模块：\n Vision Encoder：主要有3中类型  使用object detection模型，比如：Faster R-CNN，识别图像中的目标区域，并生成每个目标区域的特征表示，输入到后续模型中 利用CNN模型提取grid feature作为图像输入 ViT采用的将图像分解成patch，每个patch生成embeding输入到模型。 随着Vision Transformer的发展，ViT的方式逐渐成为主流方式。   Text Encoder：包括BERT、RoBERTa、ELECTRA、ALBERT、DeBERTa等经典预训练语言模型结构。 Multimodel Fusion：主要指如何融合图像、文本，主要有2中：  co-attention：图像、文本分别使用Transformer编码，在每个Transformer模块中加入图像、文本的cross attention merged attention model，图像、文本在开始就拼接在一起，输入到Transformer   模型结构：主要有2中：  Encoder-only：这种比较常见 Encoder-Decoder   预训练任务：主要有3中：  Masked Language Modeling（MLM）类似BERT，随机mask掉部分token，用剩余的预测出被mask掉的token Masked Image Modeling，对输入的部分图像patch进行mask，然后预测被mask的patchs Image-Text Matching（ITM），预测image和text的pair对是否匹配，对比学习的预训练方法可以属于这类。    二、网络 Open AI 在2021年1月份发布的DALL-E和CLIP，属于结合图像和文本的多模态模型，其中DALL-E是基于文本来生成模型的模型；CLIP是用文本作为监督信号来训练可迁移的视觉模型，这两个工作带动了一波新的研究高潮。","tags":["vlp","summary"],"title":"简介"},{"categories":["Basic"],"contents":" Tensor\n 每个张量Tensor都有一个相应的torch.Storage，用来保存数据。\ntorch.Storage: 是一个单一数据类型的连续一维数组。每个Tensor都有一个对应的相同数据类型的存储：class torch.FloatStorage 类tensor：提供了一个存储 多维的、横向视图，并定义了数值运算。 torch.Tensor.abs()：会在原地计算，并返回改变后的tensor\ntorch.Tensor.abd()：在一个新的tensor中计算结果    变量\n Variable 在torch.autograd.Variable中，Variable的结构图：data：Variable的tensor数值 grad_fn：表示得到这个Variable的操作，\ngrad：表示Variable的反向传播梯度 示例1：x = Variable(torch.Tensor([1]), requires_grad=Ture) 其中：requires_grad=True ：这个参数表示是否对这个变量求梯度。\nx.backward()：自动求导。自动求导不需要再去明确地写明那个函数对那个函数求导，直接通过这行代码就可以对所有的需要梯度的变量进行求导。\nx.grad：存放的就是x的梯度值 示例2：y.backward(torch.FloatTensor([1,0.1,0.01]))，表示得到的梯度分别乘以1,0.1,0.01 Variable和Tensor本质上没有区别，不过Variable会被放入一个计算图中，然后进行前向传播、反向传播、自动求导。 tensor与Variable之间的转换： tensor —to—\u0026gt; Variable：b=Variable(a)   一、Tensor信息  torch.is_tensor(obj)\t判断是否为tensor torch.is_storage(obj)\t判断obj是一个pytorch storage对象 torch.set_default_tensor_type() torch.numel(Tensor)\t返回张量中元素的个数  二、创建Tensor  torch.Tensor([[1,2],[3,4]])。创建\u0026mdash;返回指定数值的张量 torch.randn(*sizes, out=None)。创建\u0026mdash;返回标准正态分布的随机数张量。标准正态分布，形状由sizes定义 torch.randperm(n, out=None)。创建\u0026mdash;返回0~n-1之间的随机整数1维张量。返回一个从0~n-1的随机整数排列 torch.rand(*sizes, out=None)。创建\u0026mdash;返回[0, 1)的均匀分布张量 torch.arange(start, end, step=1, out=None)。创建\u0026mdash;返回一个1维张量。[start, end) 以step为步长的一组序列值 torch.range(start, end, step=1, out=None)。创建\u0026mdash;返回一个1维张量。[start, end) 以step为步长的1维张量 torch.zeros(*sizes, out=None)。创建\u0026mdash;返回一个全为0的张量。生成一个tensor, 数值为0，形状由sizes定义 torch.eye(n, m=none，out=None)。创建\u0026mdash;返回一个2维单位矩阵张量。对角线为1，其他位置为0。m默认为n torch.from_numpy(ndarray)。创建\u0026mdash;返回张量。将numpy.ndarray转换为Tensor。Tensor与ndarray共享同一个内存空间 torch.linspace(start, end, steps=100, out=None)。创建\u0026mdash;返回一个1维张量。[start, end]间生成steps个样本 torch.logspace(start, end, steps=100, out=None)。创建\u0026mdash;返回一个1维张量。[10^start, 10^end]间生成steps个样本  三、随机Tensor  torch.manual_seed(seed)。随机\u0026mdash;种子。seed(int or long) torch.initial_seed() torch.get_rng_state() torch.set_rng_state(new_state) torch.default_generator() torch.bernoulli(input, out=None)。随机\u0026mdash;伯努利分布。\ninput: 输入张量包含用于抽取上述二元随机值得概率。所以，输入的值在[0,1]区间。output: 与输入维度一致 torch.multinomial(input,num_samples,replacement=False,out=None)。随机\u0026mdash;从多项分布中抽取num_samples个。\ninput: 包含概率值得张量。num_samples(int): 抽取的样本数。replacement(bool): 决定是否能重复抽取。 torch.normal(means,std)。随机\u0026mdash;正态分布抽取随机数。\nmeans(Tensor): 均值。std(Tensor): 标准差。输出: 与means, std 维度一致  四、切片Tensor  torch.cat(inputs, dimension=0)。切片\u0026mdash;连接。在给定维度上对输入的张量序列进行连接操作。\ninputs: sequence of Tensors 任意相同类型的Tensor序列。dimension: int 沿着此维度连接张量序列 torch.chunk(tensor, chunks, dim=0)。切片\u0026mdash;分块。tensor：待分块的输入张量。\nchunks(int)：分块的个数。dim(int)：沿着此维度进行分块 torch.gather(input,dim,index,out=None)。切片\u0026mdash;聚合。沿给定轴，将输入索引张量index指定位置的值进行聚合 torch.index_select(input,dim,index,out)。切片\u0026mdash;在dim轴方向，按照index下标取切片。\ninput:输入张量。dim: 索引的轴。index: 包含索引下标的一维张量 torch.nonzero(input,out=None)。\ninput: 输入张量。返回非零的坐标，输出格式：每行是一个非零的坐标。非零坐标 torch.split(tensor,split_size,dim=0)。切片\u0026mdash;分割。\nsplit_size(int): 单个分块的形状大小。dim(int): 沿此维度进行分割 torch.squeeze(input,dim=None,out=None)。切片\u0026mdash;把维度值为1的去掉。维度(A-1-B-1-C) \u0026mdash;\u0026gt; 维度(A-B-C) torch.unsqueeze(input, dim=None)。切片\u0026mdash;添加值为1的维度 torch.stack(squence, dim=0)。切片\u0026mdash;拼接。沿着一个新维度对输入张量序列进行连接 torch.t(input, out=None)。转置。input: 输入2维张量 torch.transpose(input,dim0,dim1,out=None)。转置 torch.unbind(tensor,dim=0)。移除指定维后，返回一个元组，包含了沿着指定维切片后的各个切片 torch.outer()。内积。outer(a, b) = aTb torch.polar() torch.view() 相当于torch.reshape, torch.resize。-1: 代表自动调整这个维度上的元素个数，保证总数不变。 torch.unsqueeze(input,dim,out=None)。切片\u0026mdash;添加维度1。对输入的指定位置插入维度1  五、序列化  torch.save(obj, f, pickle_module=\u0026lt;\u0026gt;, pickle_protocol=2)\t保存模型变量到磁盘\nobj: 保存对象。\nf: 类文件对象或者一个保存文件名的字符串。\npickle_module: 用于pickling元数据和对象的模块。\npickle_protocol: 指定pickle protocal可以覆盖默认参数\t保存一个对象到一个硬盘文件\n torch.load(f, map_location=None, pickle_module=\u0026lt;\u0026gt;) 从磁盘加载模型变量\nf: 类文件对象或者一个保存文件名的字符串。\nmap_location: 一个函数或字典规定如果remap存储位置。\npickle_module:\t从磁盘文件中读取一个通过torch.save保存的对对象  六、并行化  torch.get_num_threads() torch.set_num_threads(int)  ","date":"April 8, 2022","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0050_pytorch/0040_tensor/","summary":"Tensor\n 每个张量Tensor都有一个相应的torch.Storage，用来保存数据。\ntorch.Storage: 是一个单一数据类型的连续一维数组。每个Tensor都有一个对应的相同数据类型的存储：class torch.FloatStorage 类tensor：提供了一个存储 多维的、横向视图，并定义了数值运算。 torch.Tensor.abs()：会在原地计算，并返回改变后的tensor\ntorch.Tensor.abd()：在一个新的tensor中计算结果    变量\n Variable 在torch.autograd.Variable中，Variable的结构图：data：Variable的tensor数值 grad_fn：表示得到这个Variable的操作，\ngrad：表示Variable的反向传播梯度 示例1：x = Variable(torch.Tensor([1]), requires_grad=Ture) 其中：requires_grad=True ：这个参数表示是否对这个变量求梯度。\nx.backward()：自动求导。自动求导不需要再去明确地写明那个函数对那个函数求导，直接通过这行代码就可以对所有的需要梯度的变量进行求导。\nx.grad：存放的就是x的梯度值 示例2：y.backward(torch.FloatTensor([1,0.1,0.01]))，表示得到的梯度分别乘以1,0.1,0.01 Variable和Tensor本质上没有区别，不过Variable会被放入一个计算图中，然后进行前向传播、反向传播、自动求导。 tensor与Variable之间的转换： tensor —to—\u0026gt; Variable：b=Variable(a)   一、Tensor信息  torch.is_tensor(obj)\t判断是否为tensor torch.is_storage(obj)\t判断obj是一个pytorch storage对象 torch.set_default_tensor_type() torch.numel(Tensor)\t返回张量中元素的个数  二、创建Tensor  torch.Tensor([[1,2],[3,4]])。创建\u0026mdash;返回指定数值的张量 torch.randn(*sizes, out=None)。创建\u0026mdash;返回标准正态分布的随机数张量。标准正态分布，形状由sizes定义 torch.randperm(n, out=None)。创建\u0026mdash;返回0~n-1之间的随机整数1维张量。返回一个从0~n-1的随机整数排列 torch.rand(*sizes, out=None)。创建\u0026mdash;返回[0, 1)的均匀分布张量 torch.arange(start, end, step=1, out=None)。创建\u0026mdash;返回一个1维张量。[start, end) 以step为步长的一组序列值 torch.range(start, end, step=1, out=None)。创建\u0026mdash;返回一个1维张量。[start, end) 以step为步长的1维张量 torch.zeros(*sizes, out=None)。创建\u0026mdash;返回一个全为0的张量。生成一个tensor, 数值为0，形状由sizes定义 torch.","tags":["torch","Tensor"],"title":"Tensor和变量"},{"categories":["Basic"],"contents":"一、数据类型 1、torch的数据类型 torch.Tensor 是默认的torch.FloatTensor 的简称。\n 剥离出一个Tensor参与计算，不参与求导：Tensor后加 .detach()\n各个数据类型之间的转换：\n 方法一：在Tensor后加，.long(), .int(), .float(), .double() 方法二：可以用 .to()函数      数据类型 CPU tensor GPU tensor     32-bit float torch.FloatTensor torch.cuda.FloatTensor   64-big float torch.DoubleTensor torch.cuda.DoubleTensor   16-bit float N/A torch.cuda.HalfTensor   8-bit integer(unsigned) torch.ByteTensor torch.cuda.ByteTensor   8-bit integer(signed) torch.CharTensor torch.cuda.CharTensor   16-bit integer(signed) torch.ShortTensor torch.cuda.ShortTensor   32-bit integer(signed) torch.IntTensor torch.cuda.IntTensor   64-bit integer(signed) torch.LongTensor torch.cuda.LongTensor    2、Tensor与numpy   Tensor 转 Numpy: data.numpy() Numpy 转 Tensor：torch.from_numpy(data)   3、Tensor与python数据   Tensor 转 单个python数据：data.item() Tensor 转 list ：data.to_list()   4、Tensor数据位置   CPU张量 \u0026ndash;转\u0026ndash;\u0026gt; GPU张量：data.cuda() GPU张量 \u0026ndash;转\u0026ndash;\u0026gt; CPU张量：data.cpu()   二、魔术命令%    操作 解释     %timeit a.sum() 检测某条语句的执行时间   %hist 查看输入历史   %paste 执行粘贴板中的代码   %cat ***.py 查看某一个文件的内容   %run -i **.py 执行文件，-i 代表在当前命名空间中执行   %quickref 显示快速参考   %who 显示当前命名空间中的变量   %debug 进入调试模式   %magic 查看所有魔术命令   %env 查看系统环境变量   %xdel 删除变量并删除其在Ipython上的一切引用    三、 四、 ","date":"April 8, 2022","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0050_pytorch/0020_basic/","summary":"一、数据类型 1、torch的数据类型 torch.Tensor 是默认的torch.FloatTensor 的简称。\n 剥离出一个Tensor参与计算，不参与求导：Tensor后加 .detach()\n各个数据类型之间的转换：\n 方法一：在Tensor后加，.long(), .int(), .float(), .double() 方法二：可以用 .to()函数      数据类型 CPU tensor GPU tensor     32-bit float torch.FloatTensor torch.cuda.FloatTensor   64-big float torch.DoubleTensor torch.cuda.DoubleTensor   16-bit float N/A torch.cuda.HalfTensor   8-bit integer(unsigned) torch.ByteTensor torch.cuda.ByteTensor   8-bit integer(signed) torch.CharTensor torch.cuda.CharTensor   16-bit integer(signed) torch.ShortTensor torch.cuda.ShortTensor   32-bit integer(signed) torch.IntTensor torch.cuda.IntTensor   64-bit integer(signed) torch.","tags":["torch","基础操作"],"title":"基础操作"},{"categories":["Basic"],"contents":"一、数学计算  torch.abs(input)。 数学\u0026mdash;绝对值 torch.add(input, value)。数学\u0026mdash;对张量的每个元素加value值 torch.div(input, value)。数学\u0026mdash;逐元素除法，将input逐元素除以标量value torch.div(input, other)。数学\u0026mdash;逐元素除法。\n两个张量input和other逐元素相除.这两个维度可以不同，但元素数量一定要一致。输出: 与input维度一致 torch.mul(input, value)。数学\u0026mdash;逐元素乘法 torch.mul(input, other)。数学\u0026mdash;逐元素乘法 torch.fmod(inpur, divisor, out)。数学\u0026mdash;取余 torch.remainder(input, divisor, out)。数学\u0026mdash;取余 相当于 %。\ndivisor: 标量或者张量 逐元素 torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None)。数学\u0026mdash; 像素点相除后相加。\nout = tensor .+ value*(tensor1./tensor2) torch.addcmul(tensor, value=1, tensor1, tensor2, out=None)。数学\u0026mdash; 像素点相乘后相加。\nout = tensor .+ value*(tensor1 .* tensor2) torch.neg(input)。数学\u0026mdash;取负。out = -1 * input。 torch.reciprocal(input)。数学\u0026mdash;倒数。out = 1.0 / input。 torch.sign(input)。数学\u0026mdash;取正负符号 torch.sin(Tensor)。数学\u0026mdash;正弦 torch.cos(Tensor)。数学\u0026mdash;余弦 torch.tan(Tensor)。数学\u0026mdash;正切 torch.sinh(Tensor)。数学\u0026mdash;双曲正弦 torch.cosh(Tensor)。数学\u0026mdash;双曲余弦 torch.tanh(Tensor)。数学\u0026mdash;双曲正切 torch.asin(Tensor)。数学\u0026mdash;反正弦 torch.acos(input)。数学\u0026mdash;反余弦 torch.atan(Tensor)。数学\u0026mdash;反正切 torch.atan2(input1, input2, out=None)。数学\u0026mdash; torch.ceil(input)。数学\u0026mdash;向上取整 torch.floor(input)。数学\u0026mdash;向下取整 torch.round(input, out)。数学\u0026mdash;四舍五入 torch.clamp(input, min, max, out=None)。数学\u0026mdash;销掉最小最大。将input张量每个元素，夹在[min,max]之间 torch.exp(tensor)。数学\u0026mdash;指数 torch.pow(input, exponent, out)。数学\u0026mdash;逐元素求exponent次幂 torch.rsqrt(tensor)。数学\u0026mdash;平方根倒数。out = 1.0 / input^0.5 torch.frac(tensor)。数学\u0026mdash;返回逐元素的小数部分 torch.sqrt(tensor)。数学\u0026mdash;平方根。out = input^0.5 torch.lerp(start, end, weight, out)。数学\u0026mdash;线性插值。out = start + weight(end-start) torch.log(tensor, out=None)。数学\u0026mdash;自然对数。out = log(input) torch.loglp(tensor, out=None)。数学\u0026mdash;input+1的自然对数。out = log(input+1) torch.sigmoid(input)。数学\u0026mdash;sigmoid torch.cumsum(input, dim, out)。数学\u0026mdash;累加。沿指定维度的累加和 torch.cumprod(input, dim, out)。数学\u0026mdash;累积。沿指定维度累积 torch.dist(input, other, p=2, out)。数学\u0026mdash;两个Tensor之间的范数 torch.norm(input, p=2, dim, out=None)。数学\u0026mdash;单个Tensor的范数。返回输入张量的p的范数 torch.mean(input, dim, out=None)。数学\u0026mdash;均值 torch.std(input, dim, out=None)。数学\u0026mdash;标准差。返回张量在指定维度上的标准差 torch.var(input, dim, out=None)。数学\u0026mdash;方差 torch.sum(input, dim, out=None)。数学\u0026mdash;和 torch.median(input, dim=-1, values=None, indices=None)。数学\u0026mdash;中位数 torch.mode(input, dim=-1, values=None, indices=None)。数学\u0026mdash;众数 torch.prod(input, dim, out=None)。数学\u0026mdash;所有元素的积。输出张量在指定维度上所有元素的积 torch.cross(input, other, dim=-1, out=None)。数学\u0026mdash;叉积。输出两个张量的向量积,dim维上size必须为3  二、逻辑计算  torch.eq(input, other, out=None)。比较\u0026mdash; 等于 像素级 torch.equal(tensor1, tensor2)。比较\u0026mdash; Tensor，是否具有相同的形状和元素值 torch.ge(input, other, out=None)。比较\u0026mdash; 大于等于 像素级 torch.gt(input, other, out=None)。比较\u0026mdash; 大于 像素级 torch.le(input, other, out=None)。比较\u0026mdash; 小于等于 像素级 torch.lt(input, other, out=None)。比较\u0026mdash; 小于 像素级 torch.ne(input, other, out=None)。比较\u0026mdash; 不等于 像素级 torch.max(input, dim, max=None, max_indices=None)。比较\u0026mdash; 取最大值。在指定维度上取最大值 torch.min(input, dim, min=None, min_indices=None)。比较\u0026mdash; 取最小值。在指定维度上取最小值 torch.kthvalue(input, k, dim=None, out=None)。比较\u0026mdash; 取第k个最小值。取张量在指定维度上第k个最小值，默认最后一维 torch.sort(input, dim=None, descending=False, out=None)。比较\u0026mdash; 排序 torch.topk(input, k, dim=None, largest=True, sorted=True, out=None)。比较\u0026mdash; 取第k个最大值  三、复数域  torch.view_as_complex()。将实数 [a, b] 转为复数域 [a, bj]。复数域 torch.view_as_real()。将复数 [a, bj] 转为实数域 [a, b] torch.flatten(input, start_dim=0, end_dim=1)。默认将张量拉成一维的向量  四、矩阵操作  torch.matmul(input, other, out)。矩阵\u0026mdash; 矩阵相乘 torch.mm(input, other, out)。矩阵\u0026mdash; 矩阵相乘 toch.mv(mat, vec, out=None)。矩阵向量。矩阵\u0026mdash; 矩阵向量 torch.addbmm(beta=1, mat, alpha=1, batch1, batch2, out=None)。矩阵\u0026mdash; batch 相乘后相加。\nbatch1: $ b·n·m $ ; batch2: $b·m·p$。\nmat: $n·p$; out: $n·p$;。\nres = $beta·mat + alpha·sum(batch1_i·batch2_i),i\\in[0~b]$。\nout: batch个2维矩阵相乘后,再相加 torch.addmm(beta=1, mat, alpha=1, mat1, mat2, out=None)。矩阵\u0026mdash; 单个矩阵 相乘后相加。\n$ out=beta·mat + alpha·mat_1·mat_2 $ torch.addmv(beta=1, tensor, alpha=1, mat, vec, out=None)。矩阵\u0026mdash; 单个矩阵 矩阵*向量后相加。\nvec: 向量。mat: 矩阵。$out=beta·tensor + alpha·(mat·vec)$ torch.addr(beta=1, mat, alpha=1, vec1, vec2, out)。矩阵\u0026mdash; 向量*向量后相加。\n$out=beta·mat + alpha·(vec_1·vec_2)$ torch.baddbmm(beta=1, mat, alpha=1, batch1, batch2, out=None)。矩阵\u0026mdash; batch 单个矩阵相乘后单个相加。\n$out=beta·mat_i + alpha·(batch1_i·batch2_i)$ torch.bmm(batch1, batch2, out=None)。矩阵\u0026mdash; batch 单个矩阵相乘。\n$out=batch1_i·batch2_i$ torch.ger(vec1, vec2, out=None)。矩阵\u0026mdash; 向量相乘生成矩阵 torch.inverse(input, out=None)。矩阵\u0026mdash; 取逆 torch.dot(tensor1, tensor2)。矩阵\u0026mdash; 内积。计算两个张量的内积, 两个张量都是一维向量 torch.eig(a, eigenvectors=False, out=None)。矩阵\u0026mdash; 特征值+特征向量 torch.symeig(input, eigenvectors=False, upper=True, out=None)。矩阵\u0026mdash; 实对称矩阵的特征值+特征向量 torch.qr(input, out=None)。矩阵\u0026mdash; QR分解 torch.svd(input, some=True, out=None)。矩阵\u0026mdash; 奇异值分解 torch.gesv(B, A, out=None)。矩阵\u0026mdash; 线性方程组的解。\n$X, Lu = torch.gesv(B, A)$, 返回线性方程$A·x=B$的解 torch.btrifact(A, info=None)。矩阵\u0026mdash; 方程组求解 IntTensor。返回一个元组，包含LU分解和pivots torch.btrisolve(b, LU_data, LU_pivots)。矩阵\u0026mdash; 方程组求解r。返回线性方程组Ax=b的LU解 torch.diag(input, diagonal=0, out)。矩阵\u0026mdash; 对角线 torch.histc(input, bins=100, min=0, max=0, out=None)。矩阵\u0026mdash;直方图。bins(int):直方图分区个数 torch.trace(input)。矩阵\u0026mdash; 对角线和。返回输入2维矩阵对角线元素的和 torch.tril(input, k=0, out)。矩阵\u0026mdash; 下三角 torch.triu(input, k=0, out)。矩阵\u0026mdash; 上三角 torch.gels(B, A, out=None)。矩阵\u0026mdash; 最小二乘解。输出：元组，X: 最小二乘解 qr: QR分解的细节  ","date":"April 8, 2022","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0050_pytorch/0030_mathematical/","summary":"一、数学计算  torch.abs(input)。 数学\u0026mdash;绝对值 torch.add(input, value)。数学\u0026mdash;对张量的每个元素加value值 torch.div(input, value)。数学\u0026mdash;逐元素除法，将input逐元素除以标量value torch.div(input, other)。数学\u0026mdash;逐元素除法。\n两个张量input和other逐元素相除.这两个维度可以不同，但元素数量一定要一致。输出: 与input维度一致 torch.mul(input, value)。数学\u0026mdash;逐元素乘法 torch.mul(input, other)。数学\u0026mdash;逐元素乘法 torch.fmod(inpur, divisor, out)。数学\u0026mdash;取余 torch.remainder(input, divisor, out)。数学\u0026mdash;取余 相当于 %。\ndivisor: 标量或者张量 逐元素 torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None)。数学\u0026mdash; 像素点相除后相加。\nout = tensor .+ value*(tensor1./tensor2) torch.addcmul(tensor, value=1, tensor1, tensor2, out=None)。数学\u0026mdash; 像素点相乘后相加。\nout = tensor .+ value*(tensor1 .* tensor2) torch.neg(input)。数学\u0026mdash;取负。out = -1 * input。 torch.reciprocal(input)。数学\u0026mdash;倒数。out = 1.0 / input。 torch.sign(input)。数学\u0026mdash;取正负符号 torch.sin(Tensor)。数学\u0026mdash;正弦 torch.cos(Tensor)。数学\u0026mdash;余弦 torch.tan(Tensor)。数学\u0026mdash;正切 torch.sinh(Tensor)。数学\u0026mdash;双曲正弦 torch.cosh(Tensor)。数学\u0026mdash;双曲余弦 torch.tanh(Tensor)。数学\u0026mdash;双曲正切 torch.asin(Tensor)。数学\u0026mdash;反正弦 torch.acos(input)。数学\u0026mdash;反余弦 torch.","tags":["torch","数学计算"],"title":"数学计算"},{"categories":["Basic"],"contents":"一、数据预处理 import torch from torch.utils.data import Dataset, DataLoader, TensorDataset from torch.autograd import Variable import numpy as np class MyDataset(Dataset): \u0026#34;\u0026#34;\u0026#34; 下载数据、初始化数据，都可以在这里完成 \u0026#34;\u0026#34;\u0026#34; def __init__(self): xy = np.loadtxt(\u0026#39;../dataSet/diabetes.csv.gz\u0026#39;, delimiter=\u0026#39;,\u0026#39;, dtype=np.float32) # 使用numpy读取数据 self.x_data = torch.from_numpy(xy[:, 0:-1]) self.y_data = torch.from_numpy(xy[:, [-1]]) self.len = xy.shape[0] def __getitem__(self, index): return self.x_data[index], self.y_data[index] def __len__(self): return self.len # 创建Dataset对象 dataset = MyDataset() # 创建DataLoadder对象 dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2) # 循环DataLoader对象 num_epoches = 100 for epoch in range(num_epoches) for img, label in dataloader: # 将数据从dataloader中读取出来，一次读取的样本数为32个 # class torch.utils.data.DataLoader(dataset, # 加载数据的数据集 # batch_size=24, # 每批加载多少个样本 # shuffle=False, # Ture：每个epoch对数据打乱 # sampler=None, # 定义从数据集中提取样本的策略，返回一个样本 # batch_sampler=None, # # num_workers=12, # 0：表示数据将在主进程中加载,12表示开12个进程 # collate_fn=,  # pin_memory=False,  # drop_last=False,  # timeout=0,  # worker_init_fn=None)  DataLoader 中：\n 其中: __getitem__()的含义： 如果在类中定义了__getitem__()方法，那么类的实例对象P就可以实现P[key]取值。当实例对象P[key]运算时，就会调用类中的__getitem__()方法 其中：__iter__()的含义：\n每次迭代，都会执行__iter__()，返回的值放在data_r中。每次迭代返回的是一个batch的数据。      torchvision的包      torchvision.datasets    torchvision.models 包含了常用的网络结构，并提供了预训练模型   torchvision.transforms 提供了一般的图像转换操作类     torchvision.transforms：提供了一般的图像转换操作类：\n torchvision.transforms.ToTensor()\n功能：\n输入的Tensor数据类型必须是float32，才有这样的功能：把shape=(H,W,C)像素值范围为[0,255]的PIL.Image或者numpy.ndarrsy，转换为shape=(C,H,W)像素值范围为[0.0,1.0]的torch.FloatTensor torchvision.transforms.Normmalize(mean, std)\n功能：\nmean=(R,G,B), std=(R,G,B), 公式channel=(channel-mean)/std进行规范化 torchvision.transforms.RandomCrop(size, padding=0)\n功能：裁剪\n torchvision.transforms.RandomSizedCrop(size, interpolation=2)\n功能：裁剪\n torchvision.transforms.Compose()\n功能：把多个transform组合起来使用\n示例：\nfrom torchvision import transforms as transforms transform = transforms.Compose([ transforms.Resize(96), transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.1,0.1,0.1)) ])\ntrain_dataset = torchvision.datasets.MNIST(root='./data/', train=True, transform=transform, download=True)\n   二、模型 1、模型的定义  模型定义\n  模型定义时：需要进行更新的参数注册为Parameter，不需要进行更新的参数注册为buffer\n 网络中的参数保存成OrderedDict形式，这些参数包括2种：nn.Parameter 和 buffer torch.nn.register_parameter() 用于注册Parameter实例到当前Module; Module.parameters()函数会返回当前Module中注册的所有Parameter迭代器。\n创建：\n 将模型的成员变量(self.xxx)通过 nn.Parameter()创建，会自动注册到parameters中 通过nn.Parameter()创建普通的Parameter对象，不作为模型的成员变量，然后将Parameter对象通过register_parameter()进行注册   torch.nn.register_buffer() 用于注册Buffer实例到当前Module中; Module.buffers()函数会返回当前Module中注册的Buffer迭代器    模型保存的参数是 Model.state_dict() 返回的OrderedDict，包含当前Module中注册的所有Parameter和Buffer\n  模型进行设备移动时，模型中注册的参数(parameter和buffer)会同时进行移动\n     torch.nn.Parameter()。\n功能：将一个不可训练的类型Tensor转换为可训练的类型。并将这个parameter绑定到这个module里。    定义MyModel\n 必须继承nn.Module这个类，要让pytorch知道这个类是一个Module 在__init__(self)中，设置好需要的组件 在forward(self, x)中，用定义好的组建进行组装，像搭建积木一样把网络结果搭建出来。   # Model 模块 class Module(object): dump_patches = False _version = 1 def __init__(self): \u0026#34;\u0026#34;\u0026#34; Initializes internal Module state, shared by both nn.Module and ScriptModule. \u0026#34;\u0026#34;\u0026#34; torch._C._log_api_usage_once(\u0026#34;python.nn_module\u0026#34;) self.training = True self._parameters = OrderedDict() # 2. self._buffers = OrderedDict() # 1.  self._backward_hooks = OrderedDict() self._forward_hooks = OrderedDict() self._forward_pre_hooks = OrderedDict() self._state_dict_hooks = OrderedDict() self._load_state_dict_pre_hooks = OrderedDict() self._modules = OrderedDict() def forward(self, *input): raise NotImplementedError # 1. add a persistent buffer to the module.  # \u0026gt;\u0026gt;\u0026gt; self.register_buffer(\u0026#39;running_mean\u0026#39;, torch.zeros(num_features)) def register_buffer(self, name, tensor): \u0026#39;\u0026#39;\u0026#39;...\u0026#39;\u0026#39;\u0026#39; self._buffers[name] = tensor # 2. add a parameter to the module. def register_parameter(self, name, param): \u0026#39;\u0026#39;\u0026#39;...\u0026#39;\u0026#39;\u0026#39; self._parameters[name] = param # 3. add a child module to the current module. def add_module(self, name, module): \u0026#39;\u0026#39;\u0026#39;...\u0026#39;\u0026#39;\u0026#39; self._modules[name] = module # Typical use includes initializing the parameters of a model def apply(self, fn): return self # Moves all model parameters and buffers to the GPU def cuda(self, device=None): return self # Moves all model parameters and buffers to the CPU def cpu(self): return self # cast all parameters and buffers to :attr: `dst_type` def type(self, dst_type): return self # Moves and/or casts the parameters and buffers # args: device # dtype # tensor def to(self, *args, **kwargs): return self # Registers a backward hook on the module def register_backward_hook(self, hook): def register_forward_pre_hook(self, hook): def register_forward_hook(self, hook): # Returns a dictionary containing a whole state of the module. def state_dict(self, destination=None, prefix=\u0026#39;\u0026#39;, keep_vars=False): #  def load_state_dict(self, state_dict, strict=True): # Returns an iterator over module parameters. def parameters(self, recurse=True): for name, param in self.named_parameters(recurse=recurse): yield param # Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. def named_parameters(self, prefix=\u0026#39;\u0026#39;, recurse=True): yield elem # Returns an iterator over module buffers. def buffers(self, recurse=True): for name, buf in self.named_buffers(recurse=recurse): yield buf # Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. def named_buffers(self, prefix=\u0026#39;\u0026#39;, recurse=True): yield elem # Returns an iterator over all modules in the network. def modules(self): for name, module in self.named_modules(): yield module # Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. def named_modules(self, memo=None, prefix=\u0026#39;\u0026#39;): yield m # Sets the module in training mode. def train(self, mode=True): return self # Sets the module in evaluation mode. def eval(self): return self.train(False) # Change if autograd should record operations on parameters in this module. def requires_grad_(self, requires_grad=True): return self # Sets gradients of all model parameters to zero. def zero_grad(self): # def _get_name(self): return self.__class__.__name__ # 2、模型的保存域加载 这主要有两种方法序列化和恢复模型。\n  第一种（推荐）只保存和加载模型参数：\n保存：\ntorch.save(the_model.state_dict(), PATH) 读取：先读取Model的网络定义，在读取模型参数\nthe_model = TheModelClass(*args, **kwargs) the_model.load_state_dict(torch.load(PATH))   第二种保存和加载整个模型：\n保存：\ntorch.save(the_model, PATH)\n读取：因为保存了整个模型，可以直接加载\nthe_model = torch.load(PATH)\n  3、模型初始化 import torch import torch.nn as nn import torch.nn.functional as F net = BuildModle() for m in net.modules(): m.weight.data.normal_(0,math.sqrt(2./n)) m.weight.data.fill_(1) m.bias.data.zero_() 初始化的方法：torch.nn.init\n torch.nn.init.xavier_normal_(tensor,) torch.nn.init.xavier_uniform(tensor,) torch.nn.init.kaiming_normal_() torch.nn.init.kaiming_normal_(tensor, a=0, mode=\u0026lsquo;fan_in\u0026rsquo;, nonlinearity=\u0026lsquo;leaky_relu\u0026rsquo;) torch.nn.init.uniform_(tensor, a=0, b=1)。均匀分布 U(a,b) torch.nn.init.normal_(tensor, mean=0, std=1)。正态分布 torch.nn.init.constant_(tensor, val)。常数 torch.nn.init.eye_(tensor)。单位矩阵 torch.nn.init.orthogonal_(tensor, gain=1)。正交初始化 torch.nn.init.sparse_(tensor, sparsity, std=0.01)。从正态分布 N~(0, std)中进行稀疏化。\nsparsity：每个column稀疏的比例  三、学习率   自定义调整：\n示例：torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1) 参数：\noptimizer: 优化器 lr_lambda: 为 optimizer.param_groups中的每个组计算一个乘法因子 last_epoch: 是从last_start开始后已经记录了多少个epoch  有序调整-StepLR：\n示例：torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1) 参数：\noptimizer: 优化器 step_size(int): 学习率下降间隔数。将学习率调整为 lr*gamma gamma(float): 学习率调整倍数，默认为0.1 last_epoch: 是从last_start开始后已经记录了多少个epoch  有序调整-MultiStepLR：\n示例：torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1) 参数：\noptimizer: 优化器 milestones(list): lr 改变时的epoch数，比如[10,15,20,22,]。将学习率调整为 lr * gamma gamma(float): 学习率调整倍数，默认为0.1 last_epoch: 是从last_start开始后已经记录了多少个epoch  有序调整-ExponentialLR：\n示例：torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1) 参数：\noptimizer: 优化器 gamma(float): 学习率调整倍数，默认为0.1。 每个epoch都衰减 lr = lr * gamma last_epoch: 是从last_start开始后已经记录了多少个epoch    四、优化器    优化步骤 解释     class torch.optim.Optimizer(params, defaults) params: Variable或者dict的iterable。指定了什么参数应当被优化\ndefaults：包含了优化选项默认值的字典   load_stat_dict(state_dict) 加载optimizer状态\nstate_dict: optimizer的状态。应当是一个调用\nstate_dict()所返回的对象   state_dict() 以dict返回optimizer的状态。包含两项：\n1、state：一个保存了当前优化状态的dict\n2、param_groups：一个包含了全部参数组的dict   step(closure) 进行单次优化(参数更新)\nclosure(一个函数callable)：一个重新评价模型并返回loss的闭包。   zero_grad() 清空所有被优化过的Variable的梯度    # 实例 optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9) optimizer.zero_grad() loss_fn(model(input), target).backward() optimizer.step() 参数组(param_groups): 在finetune时，某层定制学习率、某层学习率置零操作中，都会涉及参数组的概念。\n optimizer对参数的管理是基于组的概念，可以为每一组参数配置特定的 lr, momentum, weight_decay等 参数组在optimizer中表现为一个list(self.param_groups), 其中每个元素是一个dict，表示一个参数及其相应的配置 dict中包括 params, weight_decay, lr, momentum 等字段。   常用优化器：\n torch.optim.SGD() 示例：optimizer = torch.optim.SGD(model.parameters(), lr=0.07) 参数: params: 待优化参数的iterable或者是定义了参数组的dict lr=1e-5: 学习率 momentum=0 ： 动量因子 dampening=0 ：动量的抑制因子 weight_decay=0：权重衰减 nesterov=False：使用nesterov动量    torch.optim.Rprop() 示例：optimizer = torch.optim.Rprop(params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50)) 参数: params (iterable): – 待优化参数的iterable或者是定义了参数组的dict lr (float, 可选): – 学习率（默认：1e-2） etas (Tuple[float, float], 可选): – 一对（etaminus，etaplis）, 它们分别是乘法的增加和减小的因子（默认：0.5，1.2） step_sizes (Tuple[float, float], 可选): – 允许的一对最小和最大的步长（默认：1e-6，50）     torch.optim.RMSprop() 示例：optimizer = torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False) 参数: params (iterable): – 待优化参数的iterable或者是定义了参数组的dict lr (float, 可选): – 学习率（默认：1e-2） momentum (float, 可选): – 动量因子（默认：0） alpha (float, 可选): – 平滑常数（默认：0.99） eps (float, 可选): – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8） centered (bool, 可选): – 如果为True，计算中心化的RMSProp，并且用它的方差预测值对梯度进行归一化 weight_decay (float, 可选): – 权重衰减（L2惩罚）（默认: 0）     torch.optim.ASGD() 示例：optimizer = torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0) 参数: params (iterable): – 待优化参数的iterable或者是定义了参数组的dict lr (float, 可选): – 学习率（默认：1e-2） lambd (float, 可选): – 衰减项（默认：1e-4） alpha (float, 可选): – eta更新的指数（默认：0.75） t0 (float, 可选): – 指明在哪一次开始平均化（默认：1e6） weight_decay (float, 可选): – 权重衰减（L2惩罚）（默认: 0）     torch.optim.Adamax() 示例：optimizer = torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0) 参数: params (iterable): – 待优化参数的iterable或者是定义了参数组的dict lr (float, 可选): – 学习率（默认：2e-3） betas (Tuple[float, float], 可选): – 用于计算梯度以及梯度平方的运行平均值的系数 eps (float, 可选): – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8） weight_decay (float, 可选): – 权重衰减（L2惩罚）（默认: 0）     torch.optim.Adam() 示例：optimizer = torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0) 参数: params (iterable): – 待优化参数的iterable或者是定义了参数组的dict lr (float, 可选): – 学习率（默认：1e-3） betas (Tuple[float, float], 可选): – 用于计算梯度以及梯度平方的运行平均值的系数（默认：0.9，0.999） eps (float, 可选): – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8） weight_decay (float, 可选): – 权重衰减（L2惩罚）（默认: 0）     torch.optim.Adagrad() 示例：optimizer = torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0) 参数: params (iterable): – 待优化参数的iterable或者是定义了参数组的dict lr (float, 可选): – 学习率（默认: 1e-2） lr_decay (float, 可选): – 学习率衰减（默认: 0） weight_decay (float, 可选): – 权重衰减（L2惩罚）（默认: 0）     torch.optim.Adadelta() 示例：optimizer = torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0) 参数: params (iterable): – 待优化参数的iterable或者是定义了参数组的dict rho (float, 可选): – 用于计算平方梯度的运行平均值的系数（默认：0.9） eps (float, 可选): – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-6） lr (float, 可选): – 在delta被应用到参数更新之前对它缩放的系数（默认：1.0） weight_decay (float, 可选): – 权重衰减（L2惩罚）（默认: 0）    class Optimizer(object): def __init__(self, params, defaults): torch._C._log_api_usage_once(\u0026#34;python.optimizer\u0026#34;) self.defaults = defaults self.state = defaultdict(dict) self.param_groups = [] # 元素: {\u0026#34;params\u0026#34;: [torch.nn.parameter.Parameter, ...]} param_groups = list(params) if not isinstance(param_groups[0], dict): param_groups = [{\u0026#39;params\u0026#39;: param_groups}] for param_group in param_groups: self.add_param_group(param_group) # Returns the state of the optimizer as a :class:`dict`. def state_dict(self): return { \u0026#39;state\u0026#39;: packed_state, \u0026#39;param_groups\u0026#39;: param_groups, } # Loads the optimizer state. def load_state_dict(self, state_dict): # Clears the gradients of all optimized :class:`torch.Tensor` def zero_grad(self): # Performs a single optimization step (parameter update) def step(self, closure): raise NotImplementedError # Add a param group to the :class:`Optimizer` s `param_groups`. def add_param_group(self, param_group): 五、loss 六、 ","date":"April 8, 2022","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0050_pytorch/0050_train_model/","summary":"一、数据预处理 import torch from torch.utils.data import Dataset, DataLoader, TensorDataset from torch.autograd import Variable import numpy as np class MyDataset(Dataset): \u0026#34;\u0026#34;\u0026#34; 下载数据、初始化数据，都可以在这里完成 \u0026#34;\u0026#34;\u0026#34; def __init__(self): xy = np.loadtxt(\u0026#39;../dataSet/diabetes.csv.gz\u0026#39;, delimiter=\u0026#39;,\u0026#39;, dtype=np.float32) # 使用numpy读取数据 self.x_data = torch.from_numpy(xy[:, 0:-1]) self.y_data = torch.from_numpy(xy[:, [-1]]) self.len = xy.shape[0] def __getitem__(self, index): return self.x_data[index], self.y_data[index] def __len__(self): return self.len # 创建Dataset对象 dataset = MyDataset() # 创建DataLoadder对象 dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2) # 循环DataLoader对象 num_epoches = 100 for epoch in range(num_epoches) for img, label in dataloader: # 将数据从dataloader中读取出来，一次读取的样本数为32个 # class torch.","tags":["torch","训练","模型"],"title":"模型训练"},{"categories":["Basic"],"contents":"一、argparse模块 import argparse # 创建ArgumentParser()解析对象 parser = argparse.ArgumentParser() # 使用add_argument()方法，添加参数 parser.add_argument(\u0026#39;--integer\u0026#39;, type=int, default=0, help=\u0026#39;displayas integer\u0026#39;) parser.add_argument(\u0026#39;--string\u0026#39;, type=str, default=\u0026#39;\u0026#39;, help=\u0026#39;displayas string\u0026#39;) args = parser.parse_args()  add_argument 的参数：\n name or flags - 选项字符串的名字或者列表，例如 foo 或者 -f, \u0026ndash;foo。 action - 命令行遇到参数时的动作，默认值是 store。  store_const，表示赋值为const； append，将遇到的值存储成列表，也就是如果参数重复则会保存多个值; append_const，将参数规范中定义的一个值保存到一个列表； count，存储遇到的次数；此外，也可以继承 argparse.Action 自定义参数解析；   nargs - 应该读取的命令行参数个数，可以是具体的数字，或者是?号，当不指定值时对于 Positional argument 使用 default，对于 Optional argument 使用 const；或者是 * 号，表示 0 \u0026gt; 或多个参数；或者是 + 号表示 1 或多个参数。 const - action 和 nargs 所需要的常量值。 default - 不指定参数时的默认值。 type - 命令行参数应该被转换成的类型。 choices - 参数可允许的值的一个容器。 required - 可选参数是否可以省略 (仅针对可选参数)。 help - 参数的帮助信息，当指定为 argparse.SUPPRESS时表示不显示该参数的帮助信息. metavar - 在 usage 说明中的参数名称，对于必选参数默认就是参数名称，对于可选参数默认是全大写的参数名称. dest - 解析后的参数名称，默认情况下，对于可选参数选取最长的名称，中划线转换为下划线.   ","date":"December 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0090_argparse/","summary":"一、argparse模块 import argparse # 创建ArgumentParser()解析对象 parser = argparse.ArgumentParser() # 使用add_argument()方法，添加参数 parser.add_argument(\u0026#39;--integer\u0026#39;, type=int, default=0, help=\u0026#39;displayas integer\u0026#39;) parser.add_argument(\u0026#39;--string\u0026#39;, type=str, default=\u0026#39;\u0026#39;, help=\u0026#39;displayas string\u0026#39;) args = parser.parse_args()  add_argument 的参数：\n name or flags - 选项字符串的名字或者列表，例如 foo 或者 -f, \u0026ndash;foo。 action - 命令行遇到参数时的动作，默认值是 store。  store_const，表示赋值为const； append，将遇到的值存储成列表，也就是如果参数重复则会保存多个值; append_const，将参数规范中定义的一个值保存到一个列表； count，存储遇到的次数；此外，也可以继承 argparse.Action 自定义参数解析；   nargs - 应该读取的命令行参数个数，可以是具体的数字，或者是?号，当不指定值时对于 Positional argument 使用 default，对于 Optional argument 使用 const；或者是 * 号，表示 0 \u0026gt; 或多个参数；或者是 + 号表示 1 或多个参数。 const - action 和 nargs 所需要的常量值。 default - 不指定参数时的默认值。 type - 命令行参数应该被转换成的类型。 choices - 参数可允许的值的一个容器。 required - 可选参数是否可以省略 (仅针对可选参数)。 help - 参数的帮助信息，当指定为 argparse.","tags":["python","request"],"title":"argparse"},{"categories":["Basic"],"contents":"一、简介 1、CUDA CUDA：英伟达开发的一个通用并行计算平台和编程模型，能让你调用GPU的指令集及其并行计算单元。基于cuda编程可以利用GPU的并行计算引擎来更高效地计算。\n特点：\n GPU有更多的计算核心，适合数据并行的计算密集型任务。 CPU有较少的运算核心，适合实现复杂的逻辑计算，用于控制密集型任务。 对比一下：  CPU \u0026ndash; 线程是重量级的，上下文切换开销较大。 负责处理逻辑复杂的串行程序 GPU \u0026ndash; 由于存在较多核心，线程是轻量级的。负责处理数据密集型的并行机选程序    2、CUDA编程模型 CUDA编程模型是一个异构模型，需要CPU和GPU协同工作，在CUDA中有两个重要的概念：host和device。\nhost: CPU + 其内存\ndevice: GPU + 其内存\n典型的CUDA程序的执行流程：\n 分配host内存，并进行数据初始化 分配device内存，并从host将数据copy到device上 调用CUDA的核函数在device上完成指定的运算 将device上的运算结果copy到host上 释放device和host上分配的内存。  3、cuDNN cuDNN: CUDA Deep Neural Network软件库，是一个用于深度神经网络的GPU加速原语库。\nTensorRT: 是一套用于高性能深度学习接口的SDK，其包含深度学习接口优化器、运行时优化器，能为深度学习接口提供低延迟和高通量的特性。\n二、CUDA安装 1、驱动安装 NVIDIA驱动\n关键点：CUDA和显卡驱动没有一一对应的关系，一般情况下安装最新的驱动。\n2、CUDA安装 CUDA下载\nCUDA: 只是一个工具包，在同一设备上可以安装多个不同的版本，比如：9.0，10.0，11.0。一般情况下安装最新的驱动，然后根据自己的需求选择不同CUDA工具包就行了。但在离线安装CUDA时会绑定CUDA和驱动程序，所以在使用多个CUDA的时候就不要选择离线安装CUDA了。\n安装步骤:\n 不用选择太高的cuda版本，太高反而兼容性不好，要兼顾Tensorflow等架构的版本 安装包下载后，一路默认安装就好。检查是否安装成功：nvcc -V cuda的安装包中包含NVIDIA驱动，安装时取消勾选安装驱动，只安装工具包就行  CUDA安装后，配置环境变量：\nCUDA10.1是之前安装的，CUDA11.1是之后安装的，所以默认CUDA10.1的环境变量在CUA11.1之前，CUDA_PATH环境变量被CUDA11.1覆盖\nCUDA版本切换：\n切换CUDA版本时，只需要切换环境变量中CUDA的顺序即可，比如让CUDA11.1生效，则CUDA11.1环境变量在CUDA10.1之前。\n3、cuDNN安装 cuDNN下载\ncuDNN：是一个SDK，是一个专门用于神经网路的加速包，它跟CUDA没有一一对应的关系。即：每个CUDA版本可能有好几个cuDNN版本，一般有一个最新版本的cuDNN版本与CUDA对应更好。\n安装步骤：\n 根据cuda版本选择对应的cudnn版本；不同系统，选择不同的型号。 下载的不是安装包，而是压缩文件，解压后将对应的文件拷贝到cuda安装路径对应的目录中。默认安装的路径：  复制 cudnn\\bin\\cudnn64_5.dll 到 C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\\ 复制 cudnn\\include\\cudnn.h 到 C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\include\\ 复制 cudnn\\lib\\x64\\cudnn.lib 到 C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\lib\\x64\\    4、CUDA版本切换  需要哪个版本时，就把环境变量中 CUDA_PATH、NVCUDASAMPLES_ROOT修改成相应的路径\n不用哪个版本时，就把环境变量中的path路径，修改为非实际路径 创建不同的虚拟环境，在虚拟环境中分别安装不同版本的TensorFlow，TensorFlow会根据自身版本的需求找到对应的cuda版本。在需要使用哪个版本时，激活哪个虚拟环境。  创建虚拟环境： conda create -n py37 python=3.7 进入该虚拟环境 \u0026ndash;\u0026gt; 进入该虚拟环境的路径：cd E:\\ProgramFiles\\anaconda3\\envs\\py37 mkdir .\\etc\\conda\\activate.d\nmkdir .\\etc\\conda\\deactivate.d 在activate.d中创建env_vars.bat，内容\n@set CUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin\n@set CUDA_INCLUDE=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\include\n@set CUDA_LIB64=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\extras\\CUPTI\\lib64\n@set CUDA_NVVP=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\libnvvp\n@set CUDA_lib=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\lib\\x64\n@set OLD_PATH=%PATH%\n@set PATH=%CUDA_PATH%;%CUDA_NVVP%;%CUDA_lib%;%PATH%;%CUDA_INCLUDE%;%CUDA_LIB64%; 在deactivate.d中创建同名文件env_vars.bat，内容\n@set PATH=%OLD_PATH%    5、TF/torch版本 \u0026amp; CUDA版本 \u0026amp; cuDNN版本 PyTorch 版本与CUDA的对应关系\n   torch版本  示例     v1.8.0 conda 安装 # CUDA 10.2\nconda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=10.2 -c pytorch\n# CUDA 11.1\nconda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge\n# CPU Only\nconda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cpuonly -c pytorch\n    pip 安装 # CUDA 11.0\npip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n# CUDA 10.2\npip install torch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0\n# CPU only\npip install torch==1.8.0+cpu torchvision==0.9.0+cpu torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n   v1.7.1 conda 安装 # CUDA 9.2\nconda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 cudatoolkit=9.2 -c pytorch\n# CUDA 10.1\nconda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 cudatoolkit=10.1 -c pytorch\n# CUDA 10.2\nconda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 cudatoolkit=10.2 -c pytorch\n# CUDA 11.0\nconda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 cudatoolkit=11.0 -c pytorch\n# CPU Only\nconda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 cpuonly -c pytorch\n    pip 安装 # CUDA 11.0\npip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n# CUDA 10.2\npip install torch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2\n# CUDA 10.1\npip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n# CUDA 9.2\npip install torch==1.7.1+cu92 torchvision==0.8.2+cu92 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n# CPU only\npip install torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n   v1.7.0 conda 安装 # CUDA 9.2\nconda install pytorch==1.7.0 torchvision==0.8.0 torchaudio==0.7.0 cudatoolkit=9.2 -c pytorch\n# CUDA 10.1\nconda install pytorch==1.7.0 torchvision==0.8.0 torchaudio==0.7.0 cudatoolkit=10.1 -c pytorch\n# CUDA 10.2\nconda install pytorch==1.7.0 torchvision==0.8.0 torchaudio==0.7.0 cudatoolkit=10.2 -c pytorch\n# CUDA 11.0\nconda install pytorch==1.7.0 torchvision==0.8.0 torchaudio==0.7.0 cudatoolkit=11.0 -c pytorch\n# CPU Only\nconda install pytorch==1.7.0 torchvision==0.8.0 torchaudio==0.7.0 cpuonly -c pytorch\n    pip 安装 # CUDA 11.0\npip install torch==1.7.0+cu110 torchvision==0.8.0+cu110 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n# CUDA 10.2\npip install torch==1.7.0 torchvision==0.8.0 torchaudio==0.7.0\n# CUDA 10.1\npip install torch==1.7.0+cu101 torchvision==0.8.0+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n# CUDA 9.2\npip install torch==1.7.0+cu92 torchvision==0.8.0+cu92 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n# CPU only\npip install torch==1.7.0+cpu torchvision==0.8.0+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n            TF版本 Python版本 编译器 构建工具 CUDA cnDNN     tensorflow-gpu-2.4.0 3.6~3.8 MSVC 2019 Bazel 3.1.0 8.0 11.0   tensorflow-gpu-2.3.0 3.5~3.8 MSVC 2019 Bazel 3.1.0 7.6 10.1   tensorflow-gpu-2.2.0 3.5~3.8 MSVC 2019 Bazel 2.0.0 7.6 10.1   tensorflow-gpu-2.1.0 3.5~3.7 MSVC 2019 Bazel 0.29.1 7.6 10.1   tensorflow-gpu-2.0.0 3.5~3.7 MSVC 2017 Bazel 0.26.1 7.4 10   tensorflow-gpu-1.15.0 3.5~3.7 MSVC 2017 Bazel 0.26.1 7.4 10   tensorflow-gpu-1.14.0 3.5~3.7 MSVC 2017 Bazel 0.26.1 7.4 10   tensorflow-gpu-1.13.0 3.5~3.7 MSVC 2015 Bazel 0.21.0 7.4 10   tensorflow-gpu-1.12.0 3.5~3.6 MSVC 2015 Bazel 0.15.0 7.2 9   tensorflow-gpu-1.11.0 3.5~3.6 MSVC 2015 Bazel 0.15.0 7 9   tensorflow-gpu-1.10.0 3.5~3.6 MSVC 2015 Cmake v3.6.3 7 9   tensorflow-gpu-1.9.0 3.5~3.6 MSVC 2015 Cmake v3.6.3 7 9   tensorflow-gpu-1.8.0 3.5~3.6 MSVC 2015 Cmake v3.6.3 7 9   tensorflow-gpu-1.7.0 3.5~3.6 MSVC 2015 Cmake v3.6.3 7 9   tensorflow-gpu-1.6.0 3.5~3.6 MSVC 2015 Cmake v3.6.3 7 9   tensorflow-gpu-1.5.0 3.5~3.6 MSVC 2015 Cmake v3.6.3 7 9   tensorflow-gpu-1.4.0 3.5~3.6 MSVC 2015 Cmake v3.6.3 6 8   tensorflow-gpu-1.3.0 3.5~3.6 MSVC 2015 Cmake v3.6.3 6 8   tensorflow-gpu-1.2.0 3.5~3.6 MSVC 2015 Cmake v3.6.3 5.1 8   tensorflow-gpu-1.1.0 3.5 MSVC 2015 Cmake v3.6.3 5.1 8   tensorflow-gpu-1.0.0 3.5 MSVC 2015 Cmake v3.6.3 5.1 8    三、查看版本 1、CUDA版本查看 查看已安装CUDA版本\n  直接在NVIDIA的控制面板里查看NVCUDA.DLL的版本\n注意：这个版本并不能绝对说明自己安装的CUDA工具包一定是这个版本 通过命令：nvcc -V 或者 nvcc \u0026ndash;version 直接通过文件查看\nLinux：进入安装目录，然后执行 cat version.txt win：CUDA的安装目录中，比如：C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.2 里面version.txt   2、cuDNN版本查看 cuDNN本质上就是一个C语言的H头文件。\ncudnn.h的头文件，直接打开查看，在最开始的部分有如下定义：\n# define CUDNN_MAJOR 7 # define CUDNN_MINOR 5 # define CUDNN_PATCHLEVEL 0  # define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 500 + CUDNN_PATCHLEVEL) 即：7500，也就是cudnn的版本为7.5.0\n  win：进入安装目录C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.2\\include 里 cudnn.h 打开查看 Linux：进入安装目录 cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2    ","date":"December 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0035_python/0010_build_env/0020_cuda_env/","summary":"一、简介 1、CUDA CUDA：英伟达开发的一个通用并行计算平台和编程模型，能让你调用GPU的指令集及其并行计算单元。基于cuda编程可以利用GPU的并行计算引擎来更高效地计算。\n特点：\n GPU有更多的计算核心，适合数据并行的计算密集型任务。 CPU有较少的运算核心，适合实现复杂的逻辑计算，用于控制密集型任务。 对比一下：  CPU \u0026ndash; 线程是重量级的，上下文切换开销较大。 负责处理逻辑复杂的串行程序 GPU \u0026ndash; 由于存在较多核心，线程是轻量级的。负责处理数据密集型的并行机选程序    2、CUDA编程模型 CUDA编程模型是一个异构模型，需要CPU和GPU协同工作，在CUDA中有两个重要的概念：host和device。\nhost: CPU + 其内存\ndevice: GPU + 其内存\n典型的CUDA程序的执行流程：\n 分配host内存，并进行数据初始化 分配device内存，并从host将数据copy到device上 调用CUDA的核函数在device上完成指定的运算 将device上的运算结果copy到host上 释放device和host上分配的内存。  3、cuDNN cuDNN: CUDA Deep Neural Network软件库，是一个用于深度神经网络的GPU加速原语库。\nTensorRT: 是一套用于高性能深度学习接口的SDK，其包含深度学习接口优化器、运行时优化器，能为深度学习接口提供低延迟和高通量的特性。\n二、CUDA安装 1、驱动安装 NVIDIA驱动\n关键点：CUDA和显卡驱动没有一一对应的关系，一般情况下安装最新的驱动。\n2、CUDA安装 CUDA下载\nCUDA: 只是一个工具包，在同一设备上可以安装多个不同的版本，比如：9.0，10.0，11.0。一般情况下安装最新的驱动，然后根据自己的需求选择不同CUDA工具包就行了。但在离线安装CUDA时会绑定CUDA和驱动程序，所以在使用多个CUDA的时候就不要选择离线安装CUDA了。\n安装步骤:\n 不用选择太高的cuda版本，太高反而兼容性不好，要兼顾Tensorflow等架构的版本 安装包下载后，一路默认安装就好。检查是否安装成功：nvcc -V cuda的安装包中包含NVIDIA驱动，安装时取消勾选安装驱动，只安装工具包就行  CUDA安装后，配置环境变量：\nCUDA10.1是之前安装的，CUDA11.1是之后安装的，所以默认CUDA10.1的环境变量在CUA11.1之前，CUDA_PATH环境变量被CUDA11.1覆盖\nCUDA版本切换：\n切换CUDA版本时，只需要切换环境变量中CUDA的顺序即可，比如让CUDA11.1生效，则CUDA11.1环境变量在CUDA10.1之前。\n3、cuDNN安装 cuDNN下载\ncuDNN：是一个SDK，是一个专门用于神经网路的加速包，它跟CUDA没有一一对应的关系。即：每个CUDA版本可能有好几个cuDNN版本，一般有一个最新版本的cuDNN版本与CUDA对应更好。\n安装步骤：\n 根据cuda版本选择对应的cudnn版本；不同系统，选择不同的型号。 下载的不是安装包，而是压缩文件，解压后将对应的文件拷贝到cuda安装路径对应的目录中。默认安装的路径：  复制 cudnn\\bin\\cudnn64_5.","tags":["python","cuda"],"title":"cuda"},{"categories":["Basic"],"contents":"一、ipdb 断点调试：\n         n(next) 下一条语句   s(step into) 进入函数调用的内部   b line_number(break) 给指定的行号位置加断点   c(continue) 给指定的文件（还没执行到的代码）中指定行号位置，打断点   r(return) 一直执行到下一个断点   j line_numver(jump) 可以跳过某段代码，直接执行指定行号所在的代码   cl(clear) 清楚断点，如果没有参数，则清除所有断点   restart 重新启动调试器   l first/second(list) 在ipdb调试环境中，默认只显示当前执行的代码行，以及上下各一行的代码，如果想要看到更多的上下文代码，可以使用该命令   w(where) 调试时可能会忘记自己目前做在的行号，可以使用w打印目前所在的行号位置，以及上下文信息   whatis variable_name 查看变量的类别，感觉有点鸡肋，用type也可以   a(argument) 当处于一个函数内部的时候，可以使用a打印传入函数的所有参数的值   p variable_name(print) 打印表达式的值   q 退出调试，并清楚所有信息    ","date":"December 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0040_ipdb/","summary":"一、ipdb 断点调试：\n         n(next) 下一条语句   s(step into) 进入函数调用的内部   b line_number(break) 给指定的行号位置加断点   c(continue) 给指定的文件（还没执行到的代码）中指定行号位置，打断点   r(return) 一直执行到下一个断点   j line_numver(jump) 可以跳过某段代码，直接执行指定行号所在的代码   cl(clear) 清楚断点，如果没有参数，则清除所有断点   restart 重新启动调试器   l first/second(list) 在ipdb调试环境中，默认只显示当前执行的代码行，以及上下各一行的代码，如果想要看到更多的上下文代码，可以使用该命令   w(where) 调试时可能会忘记自己目前做在的行号，可以使用w打印目前所在的行号位置，以及上下文信息   whatis variable_name 查看变量的类别，感觉有点鸡肋，用type也可以   a(argument) 当处于一个函数内部的时候，可以使用a打印传入函数的所有参数的值   p variable_name(print) 打印表达式的值   q 退出调试，并清楚所有信息    ","tags":["python","ipdb"],"title":"ipdb包"},{"categories":["Basic"],"contents":"一、logging模块 logging模块是Python内置的标准模块，主要用于输出运行日志，可以设置输出日志的等级、日志保存路径、日志文件回滚等；相比print，具备如下优点：\n 可以通过设置不同的日志等级，在release版本中只输出重要信息，而不必显示大量的调试信息； print将所有信息都输出到标准输出中，严重影响开发者从标准输出中查看其它数据；logging则可以由开发者决定将信息输出到什么地方，以及怎么输出；   logging模块与log4j的机制是一样的，只是具体的实现细节不同。模块提供logger，handler，filter，formatter。\n logger：提供日志接口，供应用代码使用。logger最长用的操作有两类：配置和发送日志消息。可以通过logging.getLogger(name)获取logger对象，如果不指定name则返回root对象，多次使用相同的name调用getLogger方法返回同一个logger对象。 handler：将日志记录（log record）发送到合适的目的地（destination），比如文件，socket等。一个logger对象可以通过addHandler方法添加到多个handler，每个handler又可以定义不同日志级别，以实现日志分级过滤显示。 filter：提供一种优雅的方式决定一个日志记录是否发送到handler。 formatter：指定日志记录输出的具体格式。formatter的构造方法需要两个参数：消息的格式字符串和日期字符串，这两个参数都是可选的。  与log4j类似，logger，handler和日志消息的调用可以有具体的日志级别（Level），只有在日志消息的级别大于logger和handler的级别。\n import logging # logger = logging.getLogger(__name__) logger.setLevel(level = logging.INFO) # 创建一个FileHandler handler = logging.FileHandler(\u0026#39;log.txt\u0026#39;) # 设置等级: DEBUG \u0026lt; INFO \u0026lt; WARNING \u0026lt; ERROR \u0026lt; CRITICAL，而日志的信息量是依次减少的 handler.setLevel(logging.INFO) # 设置输出消息的格式 formatter = logging.Formatter(\u0026#39;%(asctime)s- %(name)s- %(levelname)s- %(message)s\u0026#39;) handler.setFormatter(formatter) # 添加到logger中 logger.addHandler(handler) # 写入消息 logger.info(\u0026#34;Hello\u0026#34;) 二、消息格式    输出消息的格式 解释     %(levelno)s 打印日志级别的数值   %(levelname)s 打印日志级别的名称   %(pathname)s 打印当前执行程序的路径，其实就是sys.argv[0]   %(filename)s 打印当前执行程序名   %(funcName)s 打印日志的当前函数   %(lineno)d 打印日志的当前行号   %(asctime)s 打印日志的时间   %(thread)d 打印线程ID   %(threadName)s 打印线程名称   %(process)d 打印进程id   %(message)s 打印日志信息    ","date":"December 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0080_logging/","summary":"一、logging模块 logging模块是Python内置的标准模块，主要用于输出运行日志，可以设置输出日志的等级、日志保存路径、日志文件回滚等；相比print，具备如下优点：\n 可以通过设置不同的日志等级，在release版本中只输出重要信息，而不必显示大量的调试信息； print将所有信息都输出到标准输出中，严重影响开发者从标准输出中查看其它数据；logging则可以由开发者决定将信息输出到什么地方，以及怎么输出；   logging模块与log4j的机制是一样的，只是具体的实现细节不同。模块提供logger，handler，filter，formatter。\n logger：提供日志接口，供应用代码使用。logger最长用的操作有两类：配置和发送日志消息。可以通过logging.getLogger(name)获取logger对象，如果不指定name则返回root对象，多次使用相同的name调用getLogger方法返回同一个logger对象。 handler：将日志记录（log record）发送到合适的目的地（destination），比如文件，socket等。一个logger对象可以通过addHandler方法添加到多个handler，每个handler又可以定义不同日志级别，以实现日志分级过滤显示。 filter：提供一种优雅的方式决定一个日志记录是否发送到handler。 formatter：指定日志记录输出的具体格式。formatter的构造方法需要两个参数：消息的格式字符串和日期字符串，这两个参数都是可选的。  与log4j类似，logger，handler和日志消息的调用可以有具体的日志级别（Level），只有在日志消息的级别大于logger和handler的级别。\n import logging # logger = logging.getLogger(__name__) logger.setLevel(level = logging.INFO) # 创建一个FileHandler handler = logging.FileHandler(\u0026#39;log.txt\u0026#39;) # 设置等级: DEBUG \u0026lt; INFO \u0026lt; WARNING \u0026lt; ERROR \u0026lt; CRITICAL，而日志的信息量是依次减少的 handler.setLevel(logging.INFO) # 设置输出消息的格式 formatter = logging.Formatter(\u0026#39;%(asctime)s- %(name)s- %(levelname)s- %(message)s\u0026#39;) handler.setFormatter(formatter) # 添加到logger中 logger.addHandler(handler) # 写入消息 logger.info(\u0026#34;Hello\u0026#34;) 二、消息格式    输出消息的格式 解释     %(levelno)s 打印日志级别的数值   %(levelname)s 打印日志级别的名称   %(pathname)s 打印当前执行程序的路径，其实就是sys.","tags":["python","logging"],"title":"logging"},{"categories":["Basic"],"contents":"安装问题：在环境里安装OpenCV后，在pycharm上没有命令提示。这个可能是OpenCV版本的问题。\n解决方案：python3 -m pip install \u0026ndash;force-reinstall \u0026ndash;no-cache -U opencv-python==4.5.5.62\n一、连通域   cv2.connectedComponentsWithStats 示例：num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(image, connectivity=8, ltype=None)\n输入参数：\n* image: 二值图\n* connectivity：可选值为4或者8，表示使用4联通还是8联通\n* ltype：输出图像标记的类型，目前支持CV_32S、CV_16U\n输出参数：\n* num_labels: 所有连通域的数目\n* labels：图像上每个像素的标记\n* stats：每个标记的统计信息：是一个5列的矩阵[[x,y,width,height,面积]，]\n* centroids：连通域的中心点\n  cv2.connectedComponents 示例：num_objects, labels = cv2.connectedComponents(image)\n输入参数：\n* image: 二值图，8bit单通道图像\n输出参数：\n* num_labels: 所有连通域的数目\n  二、画图 ","date":"December 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0110_opencv/","summary":"安装问题：在环境里安装OpenCV后，在pycharm上没有命令提示。这个可能是OpenCV版本的问题。\n解决方案：python3 -m pip install \u0026ndash;force-reinstall \u0026ndash;no-cache -U opencv-python==4.5.5.62\n一、连通域   cv2.connectedComponentsWithStats 示例：num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(image, connectivity=8, ltype=None)\n输入参数：\n* image: 二值图\n* connectivity：可选值为4或者8，表示使用4联通还是8联通\n* ltype：输出图像标记的类型，目前支持CV_32S、CV_16U\n输出参数：\n* num_labels: 所有连通域的数目\n* labels：图像上每个像素的标记\n* stats：每个标记的统计信息：是一个5列的矩阵[[x,y,width,height,面积]，]\n* centroids：连通域的中心点\n  cv2.connectedComponents 示例：num_objects, labels = cv2.connectedComponents(image)\n输入参数：\n* image: 二值图，8bit单通道图像\n输出参数：\n* num_labels: 所有连通域的数目\n  二、画图 ","tags":["python","OpenCV"],"title":"OpenCV"},{"categories":["Basic"],"contents":"一、PIL模块 PIL: Python Imaging Library 已经是python平台上的图像处理的标准库了，PIL功能非常强大。由于PIL仅支持python2.7，加上年久失修，于是一群志愿者在PIL的基础上创建了兼容的版本，名字叫Pillow，支持最新Python3.x，又加入了许多新特征。\nfrom PIL import Image    操作 解释     Image.fromarray() 从一个numpy对象转换为一个PIL image对象   img = Image.open(\u0026lsquo;test.jpg\u0026rsquo;) 打开一个图像文件，返回值img是一个PIL图像对象。PIL是个足够智能的类库，可以根据文件扩展名来判断图像的格式。   img.save(\u0026lsquo;路径\u0026rsquo;) PIL会根据文件扩展名来判断图像的格式，如果图像文件不是该格式，会自动将其转换为该格式。   img.thumbnail((h,w)) 创建图像的缩略图, thumbnail()方法接受一个元组参数, 指定生成缩略图的尺寸. 然后将图像转换成指定尺寸的缩略图.   region = img.crop((左, 上, 右, 下)) 裁剪指定区域   region = region.transpose(Image.ROTATE_180)\nimg.paste(region, (左,上,右,下)) 旋转180，然后将该区域放回去   img.resize((h, w)) 调整图像尺寸, resize()方法的参数是一个元组, 用来指定新图像的尺寸   img.rotate(45) 逆时针旋转图像    ","date":"December 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0100_pil/","summary":"一、PIL模块 PIL: Python Imaging Library 已经是python平台上的图像处理的标准库了，PIL功能非常强大。由于PIL仅支持python2.7，加上年久失修，于是一群志愿者在PIL的基础上创建了兼容的版本，名字叫Pillow，支持最新Python3.x，又加入了许多新特征。\nfrom PIL import Image    操作 解释     Image.fromarray() 从一个numpy对象转换为一个PIL image对象   img = Image.open(\u0026lsquo;test.jpg\u0026rsquo;) 打开一个图像文件，返回值img是一个PIL图像对象。PIL是个足够智能的类库，可以根据文件扩展名来判断图像的格式。   img.save(\u0026lsquo;路径\u0026rsquo;) PIL会根据文件扩展名来判断图像的格式，如果图像文件不是该格式，会自动将其转换为该格式。   img.thumbnail((h,w)) 创建图像的缩略图, thumbnail()方法接受一个元组参数, 指定生成缩略图的尺寸. 然后将图像转换成指定尺寸的缩略图.   region = img.crop((左, 上, 右, 下)) 裁剪指定区域   region = region.transpose(Image.ROTATE_180)\nimg.paste(region, (左,上,右,下)) 旋转180，然后将该区域放回去   img.resize((h, w)) 调整图像尺寸, resize()方法的参数是一个元组, 用来指定新图像的尺寸   img.rotate(45) 逆时针旋转图像    ","tags":["python","PIL"],"title":"PIL"},{"categories":["Basic"],"contents":"一、anaconda环境 清华镜像源\n 可以通过从页面上下载，直接安装 可以是命令  wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2022.10-MacOSX-x86_64.sh sh Anaconda3-2022.10-MacOSX-x86_64.sh 配置环境变量：export PATH=~/anaconda3/bin:$PATH       操作 说明     conda config --show 查看配置   conda config --add channels 网址 添加源\nconda config \u0026ndash;add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\nconda config \u0026ndash;add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\nconda config \u0026ndash;set show_channel_urls yes   conda info -e 查看conda的虚拟环境   conda list 查看该环境下，已经安装的包-版本   conda search 包名 查看安装包，是否可通过conda安装   安装    conda install -n 环境名 包名\n-c 网址 在指定虚拟环境下安装，-c：全名是\u0026ndash;channel，指定来源，例如：\nconda install -c https://conda.anaconda.ort/pandas bottleneck   conda remove -n 环境名 包名 从指定虚拟环境下卸载   虚拟环境    conda create -n 环境名 python=3.8 创建一个虚拟环境，指定安装python的版本为3.8。-n全名是--name   conda remove -n 环境名 --all 删除一个虚拟环境   source activate 环境名 切换到一个虚拟环境   source deactivate 退出该环境   根据yml文件，创建环境、安装\nconda env create -f environment.yml 根据yml文件内容，创建环境、安装各种包\nyml文件内容，例如：\nname: py38\ndependencies:\n- python=3.8\n- pip:\n- mxnet==1.5.0\n- pandas==0.23.4\n- matplotlib==2.2.2\n   conda env export \u0026gt; environment.yml 导出虚拟环境，生成.yml文件。    二、pypi    国内pypi源 链接     清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/   中国科学技术大学 http://pypi.mirrors.ustc.edu.cn/simple/   阿里云 http://mirrors.aliyun.com/pypi/simple/   豆瓣 https://pypi.douban.com/simple/       pip 操作 说明     pip配置    pip config set global.index-url 网址 conda和pip默认国外站点下载，我们可以配置成国内镜像来加速下载   pip config unset global.index-url 取消pypi镜像配置   安装    pip install -i 网址 \u0026lt;包名\u0026gt; 安装，-i全名是--index，指定下载源   pip install \u0026lt;路径\u0026gt;/\u0026lt;包名\u0026gt; 安装本地的包   pip install \u0026lt;包名\u0026gt; --upgrade 升级包   pip uninstall \u0026lt;包名\u0026gt; 卸载   pip show -f \u0026lt;包名\u0026gt; 显示包所在目录   pip search \u0026lt;关键词\u0026gt; 搜索包    三、环境变量 新安装的工具包，执行文件的路径需要添加到环境变量中，系统才能访问到。可以使用export命令添加。\n   export 操作 说明     export [-fnp] [变量名]=[变量设置值] -f：代表变量名称中为函数名称\n-n：删除指定的变量，变量实际上未被删除，只是不会输出到后续指令的执行环境中\n-p：列出所有的shell赋予程序的环境变量    例如：mongodb 包   方法一： export PATH=/usr/local/mongodb/bin:$PATH\n生效方式：立即生效\n有效期：临时改变，只能在当前的终端窗口中生效，当前窗口关闭后就会回复原有的path配置\n用户局限：仅对当前用户   方法二： 修改.bashrc文件，或者.zshrc文件。这个要看用的是那个\n在文件中添加：export PATH=/usr/local/mongodb/bin:$PATH\n生效方式：执行 source .zshrc 或者 关闭当前终端，重新打开一个\n有效期：永久\n用户局限：仅对当前用户   方法三： 修改/etc/profile文件\n在文件中添加：export PATH=/usr/local/mongodb/bin:$PATH\n生效方式： 系统重启\n有效期：永久\n用户局限：所有用户    ","date":"December 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0035_python/0010_build_env/0010_pip_env/","summary":"一、anaconda环境 清华镜像源\n 可以通过从页面上下载，直接安装 可以是命令  wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2022.10-MacOSX-x86_64.sh sh Anaconda3-2022.10-MacOSX-x86_64.sh 配置环境变量：export PATH=~/anaconda3/bin:$PATH       操作 说明     conda config --show 查看配置   conda config --add channels 网址 添加源\nconda config \u0026ndash;add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\nconda config \u0026ndash;add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\nconda config \u0026ndash;set show_channel_urls yes   conda info -e 查看conda的虚拟环境   conda list 查看该环境下，已经安装的包-版本   conda search 包名 查看安装包，是否可通过conda安装   安装    conda install -n 环境名 包名","tags":["python","pip"],"title":"py-env"},{"categories":["Basic"],"contents":"一、request模块 requests模块：在python内置模块上进行了高度的封装，使得requests更方便。\nurl: uniform resource locator，统一资源定位符：互联网上标准资源的地址。\n格式：\n 模式/协议，比如：https、http 服务器名称(或者IP地址)，比如：api.github.com 路径和文件名，比如：events            requests.get(url)  get请求 \u0026mdash; 不带参数   requests.get(url, params={\u0026ldquo;参数1\u0026rdquo;:\u0026ldquo;值1\u0026rdquo;})  get请求 \u0026mdash; 带参数   requests.get(url, headers=header, cookie=cookie) header = {\u0026ldquo;content-type\u0026rdquo;: \u0026ldquo;application/json\u0026rdquo;,\u0026ldquo;user-agent\u0026rdquo;: \u0026ldquo;\u0026quot;} 定制headers   requests.get(url, proxies=proxies) proxies = {\u0026ldquo;http\u0026rdquo;: \u0026ldquo;ip1\u0026rdquo;, \u0026ldquo;https\u0026rdquo;: \u0026ldquo;ip2\u0026rdquo;} 代理        requests.post(url, data=json.dumps({\u0026quot;\u0026quot;:\u0026quot;\u0026quot;}))  post请求   requests.post(url, headers=header, cookie=cookie) cookie = {\u0026ldquo;cookie\u0026rdquo;:\u0026ldquo;cookie_info\u0026rdquo;} 定制cookie        Session() 会话对象，能够跨请求保持某些参数。 会话   s = requests.Session() header = {\u0026ldquo;user-agent\u0026rdquo;:\u0026quot;\u0026rdquo;,\u0026hellip;}\ns.header.update(header)\ns.auth = {\u0026ldquo;auth\u0026rdquo;, \u0026ldquo;password\u0026rdquo;}\nresponse = s.get(url) 或者 s.port(url)    from requests.auth import HTTPBasicAuth 另一种非常流行的http身份认证形式：摘要式身份认证 身份认证   response = requests.get(url, auth=HTTPBasicAuth(\u0026ldquo;user\u0026rdquo;,\u0026ldquo;password\u0026rdquo;)) requests.get(url, HTTPDigestAuth(\u0026ldquo;user\u0026rdquo;,\u0026ldquo;password\u0026rdquo;))     requests对象的get和post方法都会返回一个Response对象。这个对象里面存的是服务器返回的所有信息：\n 响应头、 响应状态码； 其中返回的网页部分会存在content和text两个对象中。  content：字节码 （有中文时，.content.decode(\u0026lsquo;utf-8\u0026rsquo;)） text ：字符串，beautifulsoup根据猜测的编码方式将content内容编码成字符串。    get 与post的不同：\n get方式，通过url提交数据；post方式，数据放置在header内 get方式，提交的数据最多只有1024Byte，post没有限制 get方式，从服务器取数据，url中会有少量的信息传送给服务器，用于说明要取什么样的数据； post方式是被设计用来向上放东西的，向服务器传送的是HTTP请求的内容，     response对象的操作       response.url     response.encoding     response.text 以encoding解析返回内容。根据响应头部的编码方式进行解码 字符串 内容   response.content 以字节(二进制)返回。会自动解析gzip和deflate压缩 二进制 内容   response.json() 模块内置的json解码器。以json形式返回，前提是返回的内容确实是json格式，否则会报错。 json格式 内容   response.headers 服务器响应头部，字典形式。    response.request.headers 返回发动到服务器的头信息    response.status_code 响应状态码    response.raise_for_status() 失败请求抛出的异常    response.cookies 返回响应中包含的cookie    response.history     response.elapsed 返回timedelta, 响应所用的时间 响应时间    ","date":"December 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0070_request/","summary":"一、request模块 requests模块：在python内置模块上进行了高度的封装，使得requests更方便。\nurl: uniform resource locator，统一资源定位符：互联网上标准资源的地址。\n格式：\n 模式/协议，比如：https、http 服务器名称(或者IP地址)，比如：api.github.com 路径和文件名，比如：events            requests.get(url)  get请求 \u0026mdash; 不带参数   requests.get(url, params={\u0026ldquo;参数1\u0026rdquo;:\u0026ldquo;值1\u0026rdquo;})  get请求 \u0026mdash; 带参数   requests.get(url, headers=header, cookie=cookie) header = {\u0026ldquo;content-type\u0026rdquo;: \u0026ldquo;application/json\u0026rdquo;,\u0026ldquo;user-agent\u0026rdquo;: \u0026ldquo;\u0026quot;} 定制headers   requests.get(url, proxies=proxies) proxies = {\u0026ldquo;http\u0026rdquo;: \u0026ldquo;ip1\u0026rdquo;, \u0026ldquo;https\u0026rdquo;: \u0026ldquo;ip2\u0026rdquo;} 代理        requests.post(url, data=json.dumps({\u0026quot;\u0026quot;:\u0026quot;\u0026quot;}))  post请求   requests.","tags":["python","request"],"title":"requests"},{"categories":["Basic"],"contents":"一、堆 import heapq    操作 解释 功能      例如：arr=[2, 9, 1, 4]    heapd.heapify(arr) 建堆，对列表arr建堆。\n也可以这样：\narr = [(5, \u0026lsquo;a\u0026rsquo;), (2, \u0026lsquo;b\u0026rsquo;), (8, \u0026lsquo;c\u0026rsquo;), (9, \u0026rsquo;d'), (6, \u0026lsquo;e\u0026rsquo;), (1, \u0026lsquo;f\u0026rsquo;)]\nheapq.heapify(arr) 然后arr就变成：\n[(1, \u0026lsquo;f\u0026rsquo;), (2, \u0026lsquo;b\u0026rsquo;), (5, \u0026lsquo;a\u0026rsquo;), (9, \u0026rsquo;d'), (6, \u0026lsquo;e\u0026rsquo;), (8, \u0026lsquo;c\u0026rsquo;)]\n 建堆   heapd.heappush(arr, 10) 添加元素，然后再向上调整堆。例如：在arr列表中添加5，然后在维持一个堆 添加   heapd.heappop(arr, 10) 提取堆顶，然后把堆尾放在堆顶，最后对堆顶做向下调整。把arr的堆顶元素提取出来。 pop   heapd.heappushpop(arr, 10) 用新元素与堆顶做比较，如果堆顶大于新元素，直接返回新元素。否则返回堆顶，并把新元素放在堆顶后向下调整    heapd.heapreplace(arr, 10) 返回堆顶，再把新元素放在堆顶，然后对堆顶做向下调整。 替换   heapd.merge([], [], []) 多路归并：把排好序的多个list， 归并成一个list\n例如：list(heapq.merge([1,3,4], [2,3,9], [5,8], reverse=False)) 多路归并   heapd.nlargest(n,iterable) 结果上等价于：soted(iterable, key=key, reverse=True)[:n]\n从一段数据上获取最大的n个数 前n个最大值   heapd.nsmallest(n,iterable) 结果上等价于：soted(iterable, key=key, reverse=False)[:n]\n从一段数据上获取最小的n个数 前n个最小值    二、 ","date":"December 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0060_heapd/","summary":"一、堆 import heapq    操作 解释 功能      例如：arr=[2, 9, 1, 4]    heapd.heapify(arr) 建堆，对列表arr建堆。\n也可以这样：\narr = [(5, \u0026lsquo;a\u0026rsquo;), (2, \u0026lsquo;b\u0026rsquo;), (8, \u0026lsquo;c\u0026rsquo;), (9, \u0026rsquo;d'), (6, \u0026lsquo;e\u0026rsquo;), (1, \u0026lsquo;f\u0026rsquo;)]\nheapq.heapify(arr) 然后arr就变成：\n[(1, \u0026lsquo;f\u0026rsquo;), (2, \u0026lsquo;b\u0026rsquo;), (5, \u0026lsquo;a\u0026rsquo;), (9, \u0026rsquo;d'), (6, \u0026lsquo;e\u0026rsquo;), (8, \u0026lsquo;c\u0026rsquo;)]\n 建堆   heapd.heappush(arr, 10) 添加元素，然后再向上调整堆。例如：在arr列表中添加5，然后在维持一个堆 添加   heapd.heappop(arr, 10) 提取堆顶，然后把堆尾放在堆顶，最后对堆顶做向下调整。把arr的堆顶元素提取出来。 pop   heapd.heappushpop(arr, 10) 用新元素与堆顶做比较，如果堆顶大于新元素，直接返回新元素。否则返回堆顶，并把新元素放在堆顶后向下调整    heapd.","tags":["python","堆"],"title":"堆-heapd"},{"categories":["Basic"],"contents":"一、字符    字符      中文-简体 \\u4e00-\\u9fa5   中文-繁体 \\u9fa6-\\u9fff   日文 \\u3040-\\u30fa   韩文 \\uac00-\\ud7ff    二、 三、 ","date":"December 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0050_re/","summary":"一、字符    字符      中文-简体 \\u4e00-\\u9fa5   中文-繁体 \\u9fa6-\\u9fff   日文 \\u3040-\\u30fa   韩文 \\uac00-\\ud7ff    二、 三、 ","tags":["python","正则"],"title":"正则"},{"categories":["Basic"],"contents":"import_module()函数 背景：一个函数运行，需要根据不同项目的配置，动态导入对应的配置文件。 例如：如下路径，向a模块中导入c.py中的对象 a\n├── a.py\n├── __init__.py\nb\n├── b.py\n├── c │　├── c.py　# 该文件中，有变量args=[]，class C\n│　├── __init__.py\n方案：\nimport importlib # 导入 params = importlib.import_module(\u0026#34;b.c.c\u0026#34;) # 对象中取出需要的对象 params.args # 取出变量 params.C # 取出类C ","date":"December 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0030_importlib/","summary":"import_module()函数 背景：一个函数运行，需要根据不同项目的配置，动态导入对应的配置文件。 例如：如下路径，向a模块中导入c.py中的对象 a\n├── a.py\n├── __init__.py\nb\n├── b.py\n├── c │　├── c.py　# 该文件中，有变量args=[]，class C\n│　├── __init__.py\n方案：\nimport importlib # 导入 params = importlib.import_module(\u0026#34;b.c.c\u0026#34;) # 对象中取出需要的对象 params.args # 取出变量 params.C # 取出类C ","tags":["python","importlib"],"title":"importlib包"},{"categories":["Basic"],"contents":"It\u0026rsquo;s coming soon. ","date":"September 9, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00100_cv/0060_image-matting/image-matting-animal/","summary":"It\u0026rsquo;s coming soon. ","tags":["matting","CV"],"title":"animal matting"},{"categories":["Basic"],"contents":"卷积神经网络的发展历程：\n一、Backbone 1. LeNet 论文\nLeNet：名字来源于第一作者Yann LeCun。是一个奠基性的网络，第一次将卷积神经网络推上舞台。\n 卷积层+最大池化：卷积层用来识别图像里的空间模式；最大池化用来降低卷积层对位置的敏感度。卷积层块由两个这样的基本单位重复堆叠构成。 LeNet可以在早起的小数据集上取得较好的效果，但是在更大的真实数据集上表现并不如人意。一方面：神经网络计算复杂，在GPU没有大量普及的20世纪90年代，训练一个多通道、多层、含有大量参数的卷积神经网络是很难完成的；另一方面：当年并没有深入研究参数初始化和非凸优化算法，导致复杂的神经网络的训练通常比较困难。 特征本身是由学习得来的，为了表征足够复杂的输入，特征本身应该分级表示。想要学习到复杂的多级特征，需要大量的带有标签的数据，这样才能表现得比其他经典方法要好。早期研究只基于小的公开数据集，自2009年ImageNet数据集创建以来，传统方法不再有优势。  输入：32*32  C1-卷积层：卷积层尺寸：6 * 28 * 28；卷积核尺寸：6 * 1 * 5 * 5；可训练参数：(5 * 5 + 1) * 6 S2-池化层：池化尺寸：2 * 2；步幅：2；方式：4个输入相加，然后乘以个可训练参数，加上个可训练参数，最后通过sigmoid；输出尺寸：6 * 14 * 14；可训练 参数：2 * 6 C3-卷积层：输出尺寸：16 * 10 * 10；卷积核尺寸: 16 * 6 * 5 * 5;\n组合方式：前6个map - 以S2中3个相邻的feature map\n再6个map - 以S2中4个相邻的feature map\n再3个map - 以S2中不相邻的4个feature map\n再1个map - 以S2中所有feature map   S4-池化层：输出尺寸：16 * 5 * 5；池化尺寸：2 * 2；步幅：2\n采样方式：4个输入相加，然后乘个可训参数，加上个可训参数，最后通过sigmoid C5-卷积层：输出尺寸：120*1；卷积核：120 * 16 * 5 * 5；可训参数：120 * (16 * 5 * 5 + 1) F6-全连接层：输出尺寸：84；对应一个 7 * 12的比特图；可训参数：84 * (120+1) Output层-全连接层：输出尺寸：10；分别代表数字0~9  2. AlexNet 论文\n2012年AlexNet横空出世，这个名字来源于一作的姓名(Alex Krizhevsky)，是Hinton实验室提出的，是卷积神经网络在大规模数据集上的开篇巨作。它赢得了2012年ImageNet图像识别挑战赛，首次证明了学习到的特征可以超越手工设计的特征，使视觉从业者从人工提取特征的特征工程中解脱出来，转向 从数据中自动提取需要的特征，做数据驱动。\n卷积层：卷积核11 * 11；步幅4；输出：96 * 54 *54\npool层：pool尺寸3 * 3；步幅2；输出：96 * 26 * 26\n卷积层：卷积核5 * 5；填充2；输出：256 * 26 * 26\npool层：pool尺寸3 * 3；步幅2；输出：265 * 12 * 12\n卷积层：卷积核3 * 3；填充1；输出：384 * 12 * 12\n卷积层：卷积核3 * 3；填充1；输出：384 * 12 * 12\n卷积层：卷积核3 * 3；填充1；输出：256 * 12 * 12\npool层：pool尺寸3 * 3；步幅2；输出：256 * 5 * 5\n全连接层：4096；DropOut(0.5)\n全连接层：4096；DropOut(0.5)\n输出层：1000\n创新点：\n 加深了网络深度 激活函数由sigmoid转换为Relu，作用：加快收敛速度；引入非线性，增强非线性的映射能力 使用DropOut，控制模型复杂度 数据增广：翻转、裁剪、颜色变化，进一步扩大数据集来缓解过拟合。 使用GPU计算 局部相应归一化(LRN)：N 表示通道数，在通道维度做局部归一化。比如在第i通道(x,y)像素点做归一化：前后n/2个通道上做局部归一化。  3. VGG 论文\nAlexNet：指明了深度卷积神经网络可以取得出色的结果，但并没有提供简单的规则，没有指导后来者如何设计新的网络。\nVGG(牛津Visual Geometry Group 实验室)：提供了可以通过重复使用简单的基础块来构建深度模型的思路。证明了网络深度对性能的影响。在 LSVRC2014比赛分类项目的第二名。\n创新点：\n 使用3*3的卷积核替换大尺寸卷积核。增加了网络深度，证明了网络深度对精度的影响 使用基础块 搭建网络的 思路  4. NiN 论文\nNiN(network in network)：提出了一个新思路。串联多个(卷积层+全连接层构成的小网络) 来构建一个深层网络。使用1*1的卷积层来替代全连接层，从而使得空间信息能够传递到后面层。 NiN基础块如下：\n模块堆叠：\n   NiN基础块 池化层 NiN基础块 池化层 NiN基础块 池化层 NiN基础块 全局平均池化层 Flatten    创新点：\n 使用1*1的卷积层 来替换全连接层，对准确率提升有效果。 串联多个小网络，搭建一个深层网络  5. GoogLeNet 论文 ImageNet分类Top5错误率：6.67% GoogLeNet 在ImageNet LSVRC2014比赛分类项目的第一名，名字上向LeNet致敬。GoogLeNet吸收了NiN网络串联的思想，并在此基础上做了很大改进。GoogLeNet采用了模块化的结构(Inception结构，名字与电影《盗梦空间》Inception同名)，方便添加和修改。\n模块堆叠：\n   Conv2D MaxPool2D Conv2D Conv2D MaxPool2D Inception Inception MaxPool2D Inception Inception Inception Inception Inception MaxPool2D Inception Inception 全局平均池化层    创新点：\n Inception模块里有4条并行的线路：1 * 1、3 * 3、5 * 5 的卷积层是用来抽取不同空间尺寸下的特征信息，其中中间两条线路会对输入先做1*1的卷积，是为了减少输入通道数，减低复杂度；第四条线路则使用3 * 3最大池化层，后接1 * 1卷积层来改变通道数。4条线路使用了合适的填充来保障输入与输出的尺寸一致。 采用4条线路并行，是想提取不同空间尺寸的特征信息，那种信息更有用，在模型训练时让数据自己选择。 GoogLeNet跟VGG一样，在主体卷积部分使用5个模块(block)，每个模块之间使用步幅为2的3 * 3最大池化层来减小输出宽高。  6. ResNet 论文 ImageNet分类Top5错误率：3.57% 随着网络逐渐加深，模型的误差不降反曾。ResNet针对这个问题，在2015年的ImageNet LSVRC-2015比赛中夺魁。以前的网络，网络层拟合的是映射f(x)，而ResNet的网络层拟合的是残差：f(x)-x。残差映射更易于捕捉恒等映射的细微波动。\n7. ResNeXt ResNeXt是ResNet和Inception的结合体，是2016年的ImageNet LSVRC-2016比赛的亚军。\n8. SENet SENet(2017)是ImageNet 2017（ImageNet收官赛）的冠军模型，ImageNet分类Top5错误率：2.25% 主要思想：\n Squeeze部分。即为压缩部分，原始feature map的维度为 H x W x C，Squeeze做的事情是把H x W x C压缩为1 x 1 x C，相当于把H x W压缩成一维了，实际中一般是用global average pooling实现的。H x W压缩成一维后，相当于这一维参数获得了之前H x W全局的视野，感受区域更广。 Excitation部分。得到Squeeze的1 x 1 x C的表示后，加入一个FC全连接层（Fully Connected），对每个通道的重要性进行预测，得到不同channel的重要性大小后再作用（激励）到之前的feature map的对应channel上，再进行后续操作  提升很大，并且代价很小，通过对通道进行加权，强调有效信息，抑制无效信息，注意力机制，并且是一个通用方法。\n9. DenseNet 论文(2017)\n受ResNet影响，DenseNet将输入和输出拼接在一起；ResNet是：输入+输出。DenseNet的主要构建模块：稠密快(dense block)和过渡层(transition layer)\n稠密块：主要定义输入和输出是如何连接的\n过渡层：用来调控通道数，h/w 尺寸。由于输入和输出拼接在一起，通道数增加，需要过渡层来调控。\n主要结论：\n 一些较早层提取出的特征仍然可能被较深层直接使用 过渡层 输出大量冗余特征 最后的分类层，虽然使用了之前的多层信息，但更偏向于使用最后几个feature map，说明在网络的最后几层，某些high-level的特征可能被产生  10. SKNet SKNet(2019) 是对SENet的改进。 SENet 在channel维度上做attention，而SKNet在SENet的基础上又引入了kernel维度上的attention，除此之外，还利用诸如分组卷积和多路卷积的trike来平衡计算量。\n11. CSPNet CSPNet(2019) 作者想提出一个计算量小效果还好的网络结构。具体来说作者希望：\n 增强CNN的学习能力 减少计算量 降低内存占用  作者把 CSPNet 应用到分类和检测任务中，发现性能都有所提升，特别是在检测任务中提升更为明显。这也是为什么后续的 YOLOv4 和 YOLOv5 的 backbone 都是基于 CSPNet 修改的\n12. EfficientNet EfficientNet(2019)\n12. VoVNet VoVNet(2019) 基于DenseNet，实现实时目标检测。 VoVNet2(2020)\n13. RepVGG RepVGG(2021)\n14. ViT ViT(2021)\n二、各领域的Backbone 1、 提升速度的Backbone 1. SqueezeNet SqueezeNet(2016) 的主要思想：\n 多用 1x1 的卷积核，而少用 3x3 的卷积核 在用 3x3 卷积的时候尽量减少 channel 的数量，从而减少参数量 延后用 pooling，因为 pooling 会减小 feature map size，延后用 pooling， 这样可以使 size 到后面才减小，而前面的层可以保持一个较大的 size，从而起到提高精度的作用。  2. MobileNet MobileNet (2017) 是通过优化卷积操作来达到轻量化的目的的，具体来说，文中通过 Deepwise Conv（其实是Deepwise Conv + Pointwise Conv）代替原始的卷积操作实现，从而达到减少计算的目的（通常所使用的是 3×3 的卷积核，计算量会下降到原来的九分之一到八分之一）\n3. ShuffleNet ShuffleNet(2017) 的核心思想是对卷积进行分组，从而减少计算量，但是由于分组相当于将卷积操作局限在某些固定的输入上，为了解决这个问题采用 shuffle 操作将输入打乱，从而解决这个问题。\n2、 目标检测的Backbone 1. 2. Darknet YOLO作者自己写的一个深度学习框架叫Darknet\n3.DetNet 旷视2018年提出的DetNet，是一个目标检测的backbone\n3、 姿态识别的Backbone 《Stacked Hourglass Networks for Human Pose Estimation》(2016)\n","date":"September 9, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00100_cv/0010_backbone/d1_3_backbone_net/","summary":"卷积神经网络的发展历程：\n一、Backbone 1. LeNet 论文\nLeNet：名字来源于第一作者Yann LeCun。是一个奠基性的网络，第一次将卷积神经网络推上舞台。\n 卷积层+最大池化：卷积层用来识别图像里的空间模式；最大池化用来降低卷积层对位置的敏感度。卷积层块由两个这样的基本单位重复堆叠构成。 LeNet可以在早起的小数据集上取得较好的效果，但是在更大的真实数据集上表现并不如人意。一方面：神经网络计算复杂，在GPU没有大量普及的20世纪90年代，训练一个多通道、多层、含有大量参数的卷积神经网络是很难完成的；另一方面：当年并没有深入研究参数初始化和非凸优化算法，导致复杂的神经网络的训练通常比较困难。 特征本身是由学习得来的，为了表征足够复杂的输入，特征本身应该分级表示。想要学习到复杂的多级特征，需要大量的带有标签的数据，这样才能表现得比其他经典方法要好。早期研究只基于小的公开数据集，自2009年ImageNet数据集创建以来，传统方法不再有优势。  输入：32*32  C1-卷积层：卷积层尺寸：6 * 28 * 28；卷积核尺寸：6 * 1 * 5 * 5；可训练参数：(5 * 5 + 1) * 6 S2-池化层：池化尺寸：2 * 2；步幅：2；方式：4个输入相加，然后乘以个可训练参数，加上个可训练参数，最后通过sigmoid；输出尺寸：6 * 14 * 14；可训练 参数：2 * 6 C3-卷积层：输出尺寸：16 * 10 * 10；卷积核尺寸: 16 * 6 * 5 * 5;\n组合方式：前6个map - 以S2中3个相邻的feature map\n再6个map - 以S2中4个相邻的feature map\n再3个map - 以S2中不相邻的4个feature map\n再1个map - 以S2中所有feature map   S4-池化层：输出尺寸：16 * 5 * 5；池化尺寸：2 * 2；步幅：2","tags":["backbone","卷积神经网络"],"title":"backbone net"},{"categories":["Basic"],"contents":"一、卷积 实际上，卷积操作需要对卷积核进行上下/左右翻转，然后用卷积核对输入进行滑动计算。由于网络学习目的是要\u0026quot;学习\u0026quot;出一个近最优解的权重，即：近最优解情况下卷积核的值，所以在卷积操作时，也就没必要在做翻转操作，反正卷积核的值是要被\u0026quot;调教\u0026quot;的，最终的卷积核的状态：可以看成是已经被上下/左右翻转过了。卷积操作也就变成互相关运算。\n卷积层解决的问题：\n 卷积层保留输入图片的形状，使图像的像素在高/宽两个方向上的相关性均可能被有效识别。 卷积层通过滑动窗口，将同一卷积核与不同位置的输入重复计算，参数共享，避免参数尺寸过大。  在卷积操作时，会有两个超参数：填充(padding)和步幅(stride)，根据输入尺寸和卷积核改变输出形状：\n假设：输入尺寸：nh * nw，卷积核尺寸：kh * kw\n 填充(padding)：在输入高和宽的两侧填充元素(通常是0)，一般来说：在高的两侧一共填充ph行；在宽的两侧一同填充pw列，一般填充的是偶数，即：nn.Conv2D(padding=(ph/2, pw/2)) 步幅(stride)：在滑动计算时，每次滑动的步长，假设：在高上步幅为sh，在宽上步幅为sw  则：输出尺寸： $$ \\tag{公式1} o_h = \\frac{n_h-k_h+p_h+s_h} {s_h }, o_w = \\frac{n_w-k_w+p_w+s_w} {s_w } $$\n1. 1*1卷积层 1*1卷积层：被看作是卷积操作的全连接层。这是为什么呢？\n1*1卷积的计算发生在通道维度上：输出的每个元素，来自输入中相同位置的元素在不同通道之间按权重叠加。假设我们将通道维度当作特征维度，将宽高维度上的元素看作数据样本，那么1*1卷积层的作用与全连接等价。 二、池化层 池化层(pooling)：缓解卷积层对位置的过渡敏感性。\n 浅层网络获取的是图像的细节信息，比如：纹理特征、边缘；高层网络获取的是图像的整体特征。池化层把感受野扩大，把图像的整体特征传递下去，网络越深感受野越大 池化层一般是最大池化或者平均池化，类比生物学的神经细胞：只有电解质信号超过一定阈值，才能激活下一个神经元，才能把信号传递下去。  三、批量归一化 batch normalization：在一个batch内，算出平均值a, 方差：b^2；然后对每个样本做归一化：c*(x-a)/b+d。其中c、d是需要训练的。\n$$ x_{i+1} = \\gamma \\frac{x_i - \\mu}{\\sigma} + \\beta $$ 由于数据的差异性，在卷积后可能会存在较大的波动。$\\frac{x_i - \\mu}{\\sigma}$ 的作用就是把数据统一拉回N(0,1)的标准正态分布；但是每个特征的分布不一定是标准正态分布，所以添加了可学习的参数：$\\gamma, \\beta$。 通过训练来调节实际的均值 $\\beta$ 和标准差 $\\gamma$ ，不过 $\\beta$ 和 $\\gamma$ 是在一定的范围内，不能波动太大。\n作用：\n 避免梯度的消失/爆炸，这是因为通过归一化(a的作用是偏移，b的作用是拉伸)，把原来可能波动比较大的数据，限制在一定的范围内，从而减弱了梯度 消失/爆炸 的问题。 批量归一化为啥会有效，有的说：通过 a,b的偏移和拉伸，相当于添加了一个随机噪声，因为a,b是在当前样本上算出来的，包含了随机性。由于c,d是可以学习的，所以这两个值是比较稳定的。 批量归一化，可以加速收敛，可以调大学习率。  ","date":"September 9, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00100_cv/0010_backbone/d1_1_backbone_cnn/","summary":"一、卷积 实际上，卷积操作需要对卷积核进行上下/左右翻转，然后用卷积核对输入进行滑动计算。由于网络学习目的是要\u0026quot;学习\u0026quot;出一个近最优解的权重，即：近最优解情况下卷积核的值，所以在卷积操作时，也就没必要在做翻转操作，反正卷积核的值是要被\u0026quot;调教\u0026quot;的，最终的卷积核的状态：可以看成是已经被上下/左右翻转过了。卷积操作也就变成互相关运算。\n卷积层解决的问题：\n 卷积层保留输入图片的形状，使图像的像素在高/宽两个方向上的相关性均可能被有效识别。 卷积层通过滑动窗口，将同一卷积核与不同位置的输入重复计算，参数共享，避免参数尺寸过大。  在卷积操作时，会有两个超参数：填充(padding)和步幅(stride)，根据输入尺寸和卷积核改变输出形状：\n假设：输入尺寸：nh * nw，卷积核尺寸：kh * kw\n 填充(padding)：在输入高和宽的两侧填充元素(通常是0)，一般来说：在高的两侧一共填充ph行；在宽的两侧一同填充pw列，一般填充的是偶数，即：nn.Conv2D(padding=(ph/2, pw/2)) 步幅(stride)：在滑动计算时，每次滑动的步长，假设：在高上步幅为sh，在宽上步幅为sw  则：输出尺寸： $$ \\tag{公式1} o_h = \\frac{n_h-k_h+p_h+s_h} {s_h }, o_w = \\frac{n_w-k_w+p_w+s_w} {s_w } $$\n1. 1*1卷积层 1*1卷积层：被看作是卷积操作的全连接层。这是为什么呢？\n1*1卷积的计算发生在通道维度上：输出的每个元素，来自输入中相同位置的元素在不同通道之间按权重叠加。假设我们将通道维度当作特征维度，将宽高维度上的元素看作数据样本，那么1*1卷积层的作用与全连接等价。 二、池化层 池化层(pooling)：缓解卷积层对位置的过渡敏感性。\n 浅层网络获取的是图像的细节信息，比如：纹理特征、边缘；高层网络获取的是图像的整体特征。池化层把感受野扩大，把图像的整体特征传递下去，网络越深感受野越大 池化层一般是最大池化或者平均池化，类比生物学的神经细胞：只有电解质信号超过一定阈值，才能激活下一个神经元，才能把信号传递下去。  三、批量归一化 batch normalization：在一个batch内，算出平均值a, 方差：b^2；然后对每个样本做归一化：c*(x-a)/b+d。其中c、d是需要训练的。\n$$ x_{i+1} = \\gamma \\frac{x_i - \\mu}{\\sigma} + \\beta $$ 由于数据的差异性，在卷积后可能会存在较大的波动。$\\frac{x_i - \\mu}{\\sigma}$ 的作用就是把数据统一拉回N(0,1)的标准正态分布；但是每个特征的分布不一定是标准正态分布，所以添加了可学习的参数：$\\gamma, \\beta$。 通过训练来调节实际的均值 $\\beta$ 和标准差 $\\gamma$ ，不过 $\\beta$ 和 $\\gamma$ 是在一定的范围内，不能波动太大。","tags":["卷积","cnn"],"title":"CNN"},{"categories":["Basic"],"contents":"在深度学习中，通过最小化损失函数使得训练误差最小化，由于损失函数一般都会比较复杂，很难直接求解析解，而是需要基于数值方法的优化算法找到近似解，即：数值解。在局域数值方法的优化算法中，损失函数就是目标函数(Objective Function)，\n1. 梯度下降法 梯度下降(gradient descent)的工作原理，以一维为例： 假设连续可导的函数 $f:\\Reals \\to \\Reals$ 的输入和输出都是标量，给定绝对值足够小的数 $\\epsilon$ ，根据泰勒展开式，近似： $$ f(x+\\epsilon) \\approx f(x) + \\epsilon f'(x) $$ 其中 $f'(x)$ 表示函数在x处的梯度。找到一个常数 $\\eta \u0026gt; 0$，使得 $\\lvert \\eta f'(x) \\rvert$ 足够小，那么可以将 $\\epsilon$ 提换为 $-\\eta f'(x)$，得到： $$ f(x-\\eta f'(x)) \\approx f(x) - \\eta f'(x)^{2} $$ 所以 $$ f(x-\\eta f'(x)) \\lesssim f(x) $$ 这就意味着，可以通过 $x \\gets x-\\eta f'(x)$ 来迭代x，函数 $f(x)$ 的值可能会降低。在梯度下降中，先取一个初始值 $x_0$ 和学习率 $\\eta\u0026gt;0$，然后不断通过上式迭代x，直到停止条件。学习率 $\\eta$ 是一个超参数，需要人工设定，如果学习率过小：会导致x更新缓慢从而需要更多的迭代次数；如果学习率过大，泰勒展开式不再成立，可能会出现振荡，无法保证会迭代出近似最优解。\n在每次迭代中，由于训练集较大，不可能把所有样本都加载到内存中，通常是随机均匀采样多个样本组成一个小批量，然后使用这个小批量来计算梯度，完成一次迭代，即：小批量随机梯度下降(batch gradient descent)。\n设：目标函数 $f(x): \\Reals^{d} \\to \\Reals$ 小批量数据集 $\\text{\\ss}$ 梯度计算： $$ g_t \\gets \\nabla f_{\\text{\\ss}_{t}}=\\frac {1} {\\lvert \\text{\\ss} \\rvert} \\displaystyle\\sum_{i \\in \\text{\\ss}_{t}} \\nabla f_i(x_{t-1}) $$\n$$ x_t \\gets x_{t-1} - \\eta_t g_t $$\n其中，$ \\lvert \\text{\\ss} \\rvert $ 表示批量大小，$\\eta_t$ 表示学习率，这两个都是超参数。\n2. 动量法 问题：自变量的梯度代表了目标函数在当前位置下降最快的方向，沿着该方向更新自变量，可能还是会有一些问题。例如：类似峡谷的函数，在有些方向上的梯度比较缓慢，在有些方向上梯度比较陡峭，在相同的学习率下，容易导致在梯度缓慢的方向收敛太慢；如果调大学习率，容易导致在梯度陡峭的方向上振荡。如下图，梯度在水平方向上为正，而在竖直方向上时上时下：\n动量法：动量法在迭代自变量时，不仅仅是利用当前的梯度值，而是利用过去一段时间的梯度值的平均。新的梯度更迭方向，不再是指下降最陡峭的方向，而是指向过去梯度的加权平均值的方向，越靠近当前时刻权重越重。\n$$ \\upsilon_t \\gets \\gamma \\upsilon_{t-1} + \\eta_t g_t $$\n$$ x_t \\gets x_{t-1} - \\upsilon_t $$\n其中，$0 \\leqslant \\gamma \u0026lt; 1$，当$\\gamma = 0$时，动量法等价于小批量随机梯度下降法。\n证明：\n我们先解释指数加权移动平均(exponentially weighted moving average)，然后在类比到动量法。\n$$ y_t = \\gamma y_{t-1} + (1-\\gamma)x_t $$ 其中，$0 \\leqslant \\gamma \u0026lt; 1$，在当前时间步$t$的变量$y_t$可以展开（类似信号系统中的激励与响应）：\n$$ \\begin{array}{cc} y_t \u0026amp; = (1-\\gamma)x_t + \\gamma y_{t-1} \\\\ \u0026amp; = (1-\\gamma)x_t + (1-\\gamma)\\gamma x_{t-1} + \\gamma^2 y_{t-2} \\\\ \u0026amp; = (1-\\gamma)x_t + (1-\\gamma)\\gamma x_{t-1} + \\dots + (1-\\gamma)\\gamma^{t-1} x_{1} + \\gamma^t y_{0} \\end{array} $$\n令$n=\\frac {1} {1-\\gamma}$，那么$(1-\\frac {1} {n})^n = \\gamma^{\\frac {1} {1-\\gamma}}$。\n有极限：$\\lim\\limits_{n \\to \\infty} (1-\\frac {1} {n})^n =\\lim\\limits_{\\gamma \\to 1} \\gamma^{\\frac {1} {1-\\gamma}}= exp(-1) \\approx 0.3679$\n对于$y_t$，可以看做是对最近$\\frac {1} {1-\\gamma}$个时间步的加权平均；忽略含有$\\gamma^{\\frac {1} {1-\\gamma}}$和比$\\gamma^{\\frac {1} {1-\\gamma}}$更高阶系数的项，即：当$\\gamma=0.95$时，可以看成对最近20时间步的$x_i$值的加权平均\n$$ y_t \\approx 0.05\\displaystyle\\sum_{i=0}^{19} 0.95^i x_{t-i} $$\n类比向量法\n$$ \\upsilon_t \\gets \\gamma \\upsilon_{t-1} + (1-\\gamma)\\frac {\\eta_t} {1-\\gamma} g_t $$\n$$ x_t \\gets x_{t-1} - \\upsilon_t $$\n 所以：向量$\\upsilon_t$实际上是对序列$\\frac {\\eta_{t-i}} {1-\\gamma} g_{t-i}$做指数加权移动平均；也就是说：动量法在每个时间步的自变量更新量近似于将最近的$\\frac {1} {1-\\gamma}$个时间步的更新量做指数加权移动平均。动量法中，自变量在各个方向上的移动幅度，不仅取决于当前梯度，还取决于历史各个梯度在各个方向上是否一致。如果在某个方向上时正时负，说明在该方向上有振荡，通过动量的向量相加，对于该情况会降低每次的更新量，使得梯度在该方向上不发散。\n 3. AdaGrad算法 问题：在统一学习率的情况下，梯度值较大的维度可能会振荡，梯度值较小的维度收敛可能会过慢。\nAdaGrad算法：根据自变量在每个维度的梯度值大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题。\n$$ s_t \\gets s_{t-1} + g_t \\odot g_t $$\n$$ x_t \\gets x_{t-1} - \\frac {\\eta} {\\sqrt{s_t + \\epsilon}} \\odot g_t $$\n其中，$\\odot$表示按元素相乘，$\\eta$表示学习率。目标函数自变量中每个元素的学习率通过按元素运算重新调整一下，每个元素都分别拥有自己的学习率。由于$s_t$一直在累加，所以每个元素的学习率在迭代过程中一直在降低，当学习率在迭代早期降得比较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。\n4. RMSProp算法 问题：AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。为了解决这一问题，RMSProp算法对AdaGrad算法做了一些修改。\nRMSProp算法：只是在AdaGrad算法中添加了 指数加权移动平均。\n$$ s_t \\gets \\gamma s_{t-1} + (1-\\gamma)g_t \\odot g_t $$\n$$ x_t \\gets x_{t-1} - \\frac {\\eta} {\\sqrt{s_t + \\epsilon}} \\odot g_t $$\n其中，$\\eta$是学习率，RMSProp算法的状态变量是对平方项$g_t \\odot g_t$的指数加权移动平均，所以可以看作最近$\\frac {1} {1-\\gamma}$个时间步的加权平均。如此一来，自变量每个元素的学习率在迭代过程中就不再一直降低。\n5. AdaDelta算法 AdaDelta算法：是另一个针对AdaGrad算法优化的算法，不过没有学习率这个超参数。\n$$ s_t \\gets \\gamma s_{t-1} + (1-\\gamma)g_t \\odot g_t $$\n$$ g_t' \\gets \\sqrt{\\frac {\\Delta x_{t-1} + \\epsilon} {s_t + \\epsilon}} \\odot g_t $$\n$$ \\Delta x_t \\gets \\gamma \\Delta x_{t-1} + (1-\\gamma)g_t' \\odot g_t' $$\nRMSProp算法，还维护一个额外的状态变量$\\Delta x_t$，用来记录自变量变化量$g_t'$按元素平方的指数加权移动平均。\n$$ x_t \\gets x_{t-1} - g_t' $$\n6. Adam算法 Adam算法：结合了动量变量$\\upsilon_t$ 和 RMSProp算法的梯度按元素平方和的指数加权移动平均。\n$$ \\upsilon_t \\gets \\beta_1 \\upsilon_{t-1} + (1-\\beta_1) g_t $$\n其中，$0 \\leqslant \\beta_1 \u0026lt; 1$（建议0.9），$\\upsilon_0$初始化为0，则：$\\upsilon_t = (1-\\beta_1)\\displaystyle\\sum_{i=1}^t \\beta_1^{t-i} g_i$\n将过去各时间步小批量随机梯度的权值相加：$(1-\\beta_1)\\displaystyle\\sum_{i=1}^t \\beta_1^{t-i}=1-\\beta_1^t$，当t较小时，过去各时间步梯度权值之和会较小，为了消除这样的影响，对任意时间步t，可以将向量$\\upsilon_t$再除以$1-\\beta_1^t$ ：\n$$ \\hat{\\upsilon}_t \\gets \\frac {\\upsilon_t} {1-\\beta_1^t} $$\n$$ s_t \\gets \\beta_2 s_{t-1} + (1-\\beta_2)g_t \\odot g_t $$\n$$ \\hat{s}_t \\gets \\frac {s_t} {1-\\beta_2^t} $$\n其中，$0 \\leqslant \\beta_2 \u0026lt; 1$（建议0.999），$s_0$初始化为0\nAdam算法使用修正后的变量$\\hat{\\upsilon}_t, \\hat{s}_t$，将模型参数中每个元素的学习率通过按元素运算重新调整。\n$$ g_t' \\gets \\frac {\\eta \\hat{\\upsilon}_t} {\\sqrt{\\hat{s}_t + \\epsilon}} $$ 其中，$\\eta$是学习率，$\\epsilon$是为了维持数值稳定性而添加的常数，例如$10^{-8}$。分子：是动量，可以在方向上消除发散；分母：在幅度上修改每个元素的学习率。\n$$ x_t \\gets x_{t-1} - g_t' $$\n","date":"September 9, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00100_cv/0010_backbone/d1_2_optimizer/","summary":"在深度学习中，通过最小化损失函数使得训练误差最小化，由于损失函数一般都会比较复杂，很难直接求解析解，而是需要基于数值方法的优化算法找到近似解，即：数值解。在局域数值方法的优化算法中，损失函数就是目标函数(Objective Function)，\n1. 梯度下降法 梯度下降(gradient descent)的工作原理，以一维为例： 假设连续可导的函数 $f:\\Reals \\to \\Reals$ 的输入和输出都是标量，给定绝对值足够小的数 $\\epsilon$ ，根据泰勒展开式，近似： $$ f(x+\\epsilon) \\approx f(x) + \\epsilon f'(x) $$ 其中 $f'(x)$ 表示函数在x处的梯度。找到一个常数 $\\eta \u0026gt; 0$，使得 $\\lvert \\eta f'(x) \\rvert$ 足够小，那么可以将 $\\epsilon$ 提换为 $-\\eta f'(x)$，得到： $$ f(x-\\eta f'(x)) \\approx f(x) - \\eta f'(x)^{2} $$ 所以 $$ f(x-\\eta f'(x)) \\lesssim f(x) $$ 这就意味着，可以通过 $x \\gets x-\\eta f'(x)$ 来迭代x，函数 $f(x)$ 的值可能会降低。在梯度下降中，先取一个初始值 $x_0$ 和学习率 $\\eta\u0026gt;0$，然后不断通过上式迭代x，直到停止条件。学习率 $\\eta$ 是一个超参数，需要人工设定，如果学习率过小：会导致x更新缓慢从而需要更多的迭代次数；如果学习率过大，泰勒展开式不再成立，可能会出现振荡，无法保证会迭代出近似最优解。\n在每次迭代中，由于训练集较大，不可能把所有样本都加载到内存中，通常是随机均匀采样多个样本组成一个小批量，然后使用这个小批量来计算梯度，完成一次迭代，即：小批量随机梯度下降(batch gradient descent)。\n设：目标函数 $f(x): \\Reals^{d} \\to \\Reals$ 小批量数据集 $\\text{\\ss}$ 梯度计算： $$ g_t \\gets \\nabla f_{\\text{\\ss}_{t}}=\\frac {1} {\\lvert \\text{\\ss} \\rvert} \\displaystyle\\sum_{i \\in \\text{\\ss}_{t}} \\nabla f_i(x_{t-1}) $$","tags":["optimizer"],"title":"optimizer"},{"categories":["Basic"],"contents":"一、简介 一图抵万言！本篇介绍神经网络的可视化工具和绘图软件。\n二、示意图 1、NN SVG 提供三种典型的神经网络绘图风格，个性化参数多；交互式绘图。 NN-SVG是由麻省理工学院弗兰克尔生物工程实验室开发的。可以绘制的图包括以节点形式展示的FCNN style，这个特别适合传统的全连接神经网络的绘制。\nGithub\nDemo\n2、PlotNeuralNet 底层基于latex的宏指令绘制，上层提供基于python的描述框架，绘制脚本简单。可以绘制复杂的网络结构。\nPlotNeuralNet 是由萨尔大学计算机科学专业的一个学生开发的，目前主要支持的是卷积神经网络，其中卷积层、池化层、bottleneck、skip-connection、up-conv、Softmax等常规的层在代码中都有定义，但缺少RNN相关的可视化层展示。\nGithub\n三、计算图 1、Netron Netron是一个神经网络可视化包，支持绝大多数神经网络操作。该功能包可以为不同节点显示不同的颜色，卷积层用蓝色显示，池化层和归一化层用绿色显示，数学操作用黑色显示。在使用方面，可以直接访问网页端，上传模型文件，就可以看到网络结构图，并可以进一步利用pip安装并引入到程序中通过浏览器查看模型的变化。\nGithub\nDemo\n","date":"September 9, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0100_draw_map_for_dl/","summary":"一、简介 一图抵万言！本篇介绍神经网络的可视化工具和绘图软件。\n二、示意图 1、NN SVG 提供三种典型的神经网络绘图风格，个性化参数多；交互式绘图。 NN-SVG是由麻省理工学院弗兰克尔生物工程实验室开发的。可以绘制的图包括以节点形式展示的FCNN style，这个特别适合传统的全连接神经网络的绘制。\nGithub\nDemo\n2、PlotNeuralNet 底层基于latex的宏指令绘制，上层提供基于python的描述框架，绘制脚本简单。可以绘制复杂的网络结构。\nPlotNeuralNet 是由萨尔大学计算机科学专业的一个学生开发的，目前主要支持的是卷积神经网络，其中卷积层、池化层、bottleneck、skip-connection、up-conv、Softmax等常规的层在代码中都有定义，但缺少RNN相关的可视化层展示。\nGithub\n三、计算图 1、Netron Netron是一个神经网络可视化包，支持绝大多数神经网络操作。该功能包可以为不同节点显示不同的颜色，卷积层用蓝色显示，池化层和归一化层用绿色显示，数学操作用黑色显示。在使用方面，可以直接访问网页端，上传模型文件，就可以看到网络结构图，并可以进一步利用pip安装并引入到程序中通过浏览器查看模型的变化。\nGithub\nDemo","tags":null,"title":"神经网络画图篇"},{"categories":["Basic"],"contents":"位置编码 1、绝对位置编码 最早出现于Transformer文章中，目的是为了弥补模型中位置信息的缺失。\n输入：$\\bold{X} \\in \\R^{n \\times d}$ 包含一个序列中n个词元的d维嵌入表示。\n位置编码：$\\bold{P} \\in \\R^{n \\times d}$, 矩阵第i行 偶数列、奇数列：用不同的频率、偏移来记录位置信息。 $$p_{i,2j} = sin(\\frac{i}{10000^{\\frac{2j}{d}}})$$ $$p_{i,2j+1} = cos(\\frac{i}{10000^{\\frac{2j}{d}}})$$\n在 $\\bold{X} + \\bold{P}$ 时，当$\\bold{X}$的幅度值比$\\bold{P}$小或者差不多时，可以增大$\\bold{X}$的幅度值，以保证$\\bold{X}$的主导性。 $$ \\bold{X} \\times M + \\bold{P} $$\n2、相对位置编码 Google于2018年提出的 《Self-Attention with Relative Position Representations》 。该方法出自Transformer的原班人马，通过在attention模块中加入可训练的参数，帮助模型来记住输入中的相对位置。\n3、ALiBi ALiBi\n4、旋转位置编码(RoPE) RoPE\n个人理解：对embedding向量做一个角度旋转。由于d维的向量旋转太复杂，只对2维的向量做旋转。所以d维的向量，有d/2个小向量。 旋转的基本角度：\n参考：苏剑林的blog def precompute_freqs_cis(dim: int, seq_len: int, theta: float = 10000.0): # 计算词向量元素两两分组之后，每组元素对应的旋转角度 freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)) # 生成 token 序列索引 t = [0, 1,..., seq_len-1] t = torch.arange(seq_len, device=freqs.device) # freqs.shape = [seq_len, dim // 2]  freqs = torch.outer(t, freqs).float() # torch.polar 的文档 # https://pytorch.org/docs/stable/generated/torch.polar.html # 计算结果是个复数向量 # 假设 freqs = [x, y] # 则 freqs_cis = [cos(x) + sin(x)i, cos(y) + sin(y)i] freqs_cis = torch.polar(torch.ones_like(freqs), freqs) return freqs_cis def apply_rotary_emb( xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # xq.shape = [batch_size, seq_len, dim] # xq_.shape = [batch_size, seq_len, dim // 2, 2] xq_ = xq.float().reshape(*xq.shape[:-1], -1, 2) xk_ = xk.float().reshape(*xk.shape[:-1], -1, 2) # 转为复数域 xq_ = torch.view_as_complex(xq_) xk_ = torch.view_as_complex(xk_) # 应用旋转操作，然后将结果转回实数域 # xq_out.shape = [batch_size, seq_len, dim] xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(2) xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(2) return xq_out.type_as(xq), xk_out.type_as(xk) class Attention(nn.Module): def __init__(self, args: ModelArgs): super().__init__() self.wq = Linear(...) self.wk = Linear(...) self.wv = Linear(...) self.freqs_cis = precompute_freqs_cis(dim, max_seq_len * 2) def forward(self, x: torch.Tensor): bsz, seqlen, _ = x.shape xq, xk, xv = self.wq(x), self.wk(x), self.wv(x) xq = xq.view(batch_size, seq_len, dim) xk = xk.view(batch_size, seq_len, dim) xv = xv.view(batch_size, seq_len, dim) # attention 操作之前，应用旋转位置编码 xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis) # scores.shape = (bs, seqlen, seqlen) scores = torch.matmul(xq, xk.transpose(1, 2)) / math.sqrt(dim) scores = F.softmax(scores.float(), dim=-1) output = torch.matmul(scores, xv) # (batch_size, seq_len, dim) # ...... ","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0030_transformer/0030_position/","summary":"位置编码 1、绝对位置编码 最早出现于Transformer文章中，目的是为了弥补模型中位置信息的缺失。\n输入：$\\bold{X} \\in \\R^{n \\times d}$ 包含一个序列中n个词元的d维嵌入表示。\n位置编码：$\\bold{P} \\in \\R^{n \\times d}$, 矩阵第i行 偶数列、奇数列：用不同的频率、偏移来记录位置信息。 $$p_{i,2j} = sin(\\frac{i}{10000^{\\frac{2j}{d}}})$$ $$p_{i,2j+1} = cos(\\frac{i}{10000^{\\frac{2j}{d}}})$$\n在 $\\bold{X} + \\bold{P}$ 时，当$\\bold{X}$的幅度值比$\\bold{P}$小或者差不多时，可以增大$\\bold{X}$的幅度值，以保证$\\bold{X}$的主导性。 $$ \\bold{X} \\times M + \\bold{P} $$\n2、相对位置编码 Google于2018年提出的 《Self-Attention with Relative Position Representations》 。该方法出自Transformer的原班人马，通过在attention模块中加入可训练的参数，帮助模型来记住输入中的相对位置。\n3、ALiBi ALiBi\n4、旋转位置编码(RoPE) RoPE\n个人理解：对embedding向量做一个角度旋转。由于d维的向量旋转太复杂，只对2维的向量做旋转。所以d维的向量，有d/2个小向量。 旋转的基本角度：\n参考：苏剑林的blog def precompute_freqs_cis(dim: int, seq_len: int, theta: float = 10000.0): # 计算词向量元素两两分组之后，每组元素对应的旋转角度 freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].","tags":["NLP","Transformer","位置编码"],"title":"位置编码"},{"categories":["Basic"],"contents":"一、Attention机制 如何有选择地引导注意力：\n非自主性提示： 基于环境中物体的突出性和易见性。比如 《辛德勒的名单》中的镜头：黑白镜头中的穿红衣服的小女孩。\n自主性提示： 选择受到 认知、意识的控制。\n在不受自我意识控制的情况下，与环境差别最大的事物，就越显眼、易见。\n在受到自我意识控制的情况下，意识偏向那个，就选择那个\n 查询(query)：自主性提示，类似于自我意识。\n键(key)：非自主提示，类似于事物的突出性、易见性。\n值(value)：感官输入，类似于具体的事物-值。\n   attention机制可以认为是一个这样的函数：\n$$ f(\\bold{q_j}) = \\sum_{i=1}^m \\alpha(\\bold{q}_j, \\bold{k}_i) \\bold{v}_i$$ 由$ \\bold{V}$ 的各个向量的加权平均，组成一个新的向量 $f(q_j)$。其中，权重的计算是通过 query向量和每个key向量 计算出来的，这个计算方式可以有多种，比如：加性注意力、缩放点积注意力\n$\\bold{Q} \\in \\R^{n \\times q}$: 查询矩阵，是由N个向量组成，每个向量有q个元素\nK-V: M个键值对集合。\n$\\bold{K} \\in \\R^{m \\times k}$: M个键向量组成的矩阵，每个键向量(k维)：就是每个字的标签信息\n$\\bold{V} \\in \\R^{m \\times v}$: M个值向量组成的矩阵，每个值向量(v维)：就是每个字的embeding\n1、加性注意力 $$\\alpha(\\bold{q}_j, \\bold{k}_i) = \\bold{w}_v^T tanh(\\bold{W}_q \\bold{q}_j + \\bold{W}_k \\bold{k}_i)$$ 其中，$\\bold{w}_v^T \\in \\R^h, \\bold{W}_q \\in \\R^{h \\times q}, \\bold{W}_k \\in \\R^{h \\times k}$ 是需要训练的。\n2、缩放点积注意力(SDPA) attention机制的SDPA(缩放点积注意力Scaled Dot-Product Attention)实现方式，计算效率更高，但是点积操作要求 $\\bold{q}$ 和 $\\bold{k}$ 具有相同的长度。\n假设：$\\bold{q}$ 和 $\\bold{k}$ 的所有元素都是独立的随机变量，并且满足标准正态分布 $N(0,1)$\n那么：两个向量的点积，服从正态分布 $N(0, d)$，其中 $d$ 就是$\\bold{q}$(或者$\\bold{k}$)的长度。\n所以：点积后，除以 $\\sqrt{d}$，即： $$\\alpha(\\bold{q}_j, \\bold{k}_i) = \\frac{\\bold{q}_j^T \\bold{k}_i}{\\sqrt{d}}$$ 基于n个查询、m个键值对，计算注意力，其中： $\\bold{Q} \\in \\R^{n \\times d}$、$\\bold{K} \\in \\R^{m \\times d}$、$\\bold{V} \\in \\R^{m \\times v}$ 的缩放点积注意力： $$softmax(\\frac{\\bold{Q} \\bold{K}^T}{\\sqrt{d}}) \\bold{V} \\in \\R^{n \\times v}$$\n具体操作：\n $\\bold{Q}$的每个向量 $\\bold{q}_i$ 做如下操作:  计算第i个向量 $q_i$ 与M个键向量的相似度(内积)，生成一个1*M的向量 对该向量做softmax操作(概率化) 用概率化后的值做M个值向量权重系数，做加权求和，生成一个加权后的embeding   $\\bold{Q}$的向量个数：表示需要多少个加权后的embeding，即：$\\tilde{V}$  3、多头注意力(MHA) 在实践中，当给定 query、key、value时，我们希望模型可以基于相同的注意力机制，学习到不同的行为，然后将不同的行为作为知识组合起来，以捕获序列内各种范围的依赖关系。因此，允许注意力机制组合使用 query、key、value的不同子空间表示，可能是有益的。所以，对给定的query、key、value，经过不同的线性变换获取其子空间表示，然后并行地送入注意力机制，最后把各个子空间的输出拼接起来，再通过一个可以学习的线性变换产生最终输出。\nMHA(多头注意力Multi-Head Attention) 实现方式：多路融合的SDPA，具体操作：\n 对Q、K、V矩阵做多次线性变换，例如：第i次变换的生成结果 $Q'_i, K'_i, V'_i$ 利用第i次线性变换后的 $Q'_i, K'_i, V'_i$，做SDPA操作，得到 $\\tilde{V_i}$ 对所有的 $\\tilde{V_i}$，在列方向上concat拼接起来  4、实际模式    QKV的关系      $Q \\neq K \\neq V$ QKV模式   $Q \\neq K=V$ QVV模式   $Q=K=V$ VVV模式，即：自注意力，自己即是查询向量，也是key向量；表示句子内部与自己相似的权重比较大    5、在seq2seq的应用 在seq2seq架构中，编码器生成各个时间步的上下文变量state，最后一时间步的state作为解码器的state。然而，有个问题：在解码器 解码某个词元时，并非所有输入词元都需要，或者说并非所有输入词元的贡献都一样，肯定是有的输入词元的贡献大一些。所以，在解码时能不能让贡献大的输入词元的state权重大一些呢？\nBahdanau等人提出了一个没有严格单向对齐限制的可微注意力模型。在预测词元时，如果不是跟所有输入词元都相关，模型使用仅跟当前预测相关的部分输入序列。\n$$c_{t'} = \\sum_{t=1}^T \\alpha(s_{t'-1}, h_t) h_t$$ 在解码时，需要 上一时间步的隐状态 $s_{t'-1}$ 和 上一时间步的真实值。添加attention的话，就要修改 $s_{t'-1}$，让其是 编码器各个隐状态的加权和，这就是attention的操作，即：\n 用解码器上一时间步的隐状态 $s_{t'-1}$ 作为查询 编码器各个隐状态 $h_t$ 其中 $t \\in [1, n]$ $\\alpha()$ 函数，采用加性注意力  参考 Transformer详解\n","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0030_transformer/0001_attention/","summary":"一、Attention机制 如何有选择地引导注意力：\n非自主性提示： 基于环境中物体的突出性和易见性。比如 《辛德勒的名单》中的镜头：黑白镜头中的穿红衣服的小女孩。\n自主性提示： 选择受到 认知、意识的控制。\n在不受自我意识控制的情况下，与环境差别最大的事物，就越显眼、易见。\n在受到自我意识控制的情况下，意识偏向那个，就选择那个\n 查询(query)：自主性提示，类似于自我意识。\n键(key)：非自主提示，类似于事物的突出性、易见性。\n值(value)：感官输入，类似于具体的事物-值。\n   attention机制可以认为是一个这样的函数：\n$$ f(\\bold{q_j}) = \\sum_{i=1}^m \\alpha(\\bold{q}_j, \\bold{k}_i) \\bold{v}_i$$ 由$ \\bold{V}$ 的各个向量的加权平均，组成一个新的向量 $f(q_j)$。其中，权重的计算是通过 query向量和每个key向量 计算出来的，这个计算方式可以有多种，比如：加性注意力、缩放点积注意力\n$\\bold{Q} \\in \\R^{n \\times q}$: 查询矩阵，是由N个向量组成，每个向量有q个元素\nK-V: M个键值对集合。\n$\\bold{K} \\in \\R^{m \\times k}$: M个键向量组成的矩阵，每个键向量(k维)：就是每个字的标签信息\n$\\bold{V} \\in \\R^{m \\times v}$: M个值向量组成的矩阵，每个值向量(v维)：就是每个字的embeding\n1、加性注意力 $$\\alpha(\\bold{q}_j, \\bold{k}_i) = \\bold{w}_v^T tanh(\\bold{W}_q \\bold{q}_j + \\bold{W}_k \\bold{k}_i)$$ 其中，$\\bold{w}_v^T \\in \\R^h, \\bold{W}_q \\in \\R^{h \\times q}, \\bold{W}_k \\in \\R^{h \\times k}$ 是需要训练的。","tags":["NLP","attention"],"title":"Attention"},{"categories":["Basic"],"contents":"一、背景 二、BART BART的全称是\n三、总结 ","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0150_bart/bart_summary/","summary":"一、背景 二、BART BART的全称是\n三、总结 ","tags":["BART"],"title":"BART综述"},{"categories":["Basic"],"contents":"一、transformers Hugging Face公司发布的transformers包，能够超级方便的引入训练模型：BERT、GPT2、\u0026hellip; transformers英文文档 transformers中文文档\n二、Tokenizer from transformers import BertTokenizerFast, BertTokenizer from transformers import GPT2TokenizerFast, GPT2LMHeadModel # 初始化tokenizer tokenizer = BertTokenizerFast(vocab_file=args.vocab_path, sep_token=\u0026#34;[SEP]\u0026#34;, pad_token=\u0026#34;[PAD]\u0026#34;, cls_token=\u0026#34;[CLS]\u0026#34;) # 对比 tokenizer.encode() 与 tokenizer.tokenize() sentence = \u0026#34;Hello, my son is cuting.\u0026#34; input_ids_1 = tokenizer.encode(sentence, add_special_tokens=False) # add_special_tokens=True 将句子转换成对应模型的输入形式，默认开启。就是首尾加上[cls]、[sep]。即：tensor([ 101, 7592, 1010, 2026, 2365, 2003, 3013, 2075, 1012, 102]) # add_special_tokens=False 首尾先不加[cls]、[sep] input_tokens = tokenizer.tokenize(sentence) # [\u0026#39;hello\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;son\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;cut\u0026#39;, \u0026#39;##ing\u0026#39;, \u0026#39;.\u0026#39;] input_ids_2 = tokenizer.convert_tokens_to_ids(input_tokens) # tensor([7592, 1010, 2026, 2365, 2003, 3013, 2075, 1012]) # 并没有开头和结尾的标记：[cls]、[sep] 其中tokenizer.encode()的参数\n add_special_tokens=True, 首尾是否添加[cls]、[sep] max_length=512, 设置最大长度，如果不设置的话，模型设置的最大长度为512，如果超过512会报错。所以启用这个参数，设置想要的最大长度，这样函数将只保留长度-2个token并转化成id。 pad_to_max_length=False, 是否按照最长长度补齐，默认关闭。此处可以通过tokenizer.padding_side=\u0026lsquo;left\u0026rsquo;设置补齐的位置在左边插入 truncation_strategy=\u0026lsquo;longest_first\u0026rsquo;, 截断机制，有四种方式 - \u0026lsquo;longest_first\u0026rsquo; 默认，读到不能再读，读满为止 - \u0026lsquo;only_first\u0026rsquo;, 只读入第一个序列 - \u0026lsquo;only_second\u0026rsquo;, 只读入第二个序列 - \u0026lsquo;do_not_truncate\u0026rsquo;, 不做截取，长了就报错 return_tensors=None, 返回的数据类型，默认是None, 可以选择TensorFlow版本(\u0026lsquo;tf\u0026rsquo;)和pytorch版本(\u0026lsquo;pt\u0026rsquo;)  三、总结 ","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00200_nlp/1000_code/bart_summary/","summary":"一、transformers Hugging Face公司发布的transformers包，能够超级方便的引入训练模型：BERT、GPT2、\u0026hellip; transformers英文文档 transformers中文文档\n二、Tokenizer from transformers import BertTokenizerFast, BertTokenizer from transformers import GPT2TokenizerFast, GPT2LMHeadModel # 初始化tokenizer tokenizer = BertTokenizerFast(vocab_file=args.vocab_path, sep_token=\u0026#34;[SEP]\u0026#34;, pad_token=\u0026#34;[PAD]\u0026#34;, cls_token=\u0026#34;[CLS]\u0026#34;) # 对比 tokenizer.encode() 与 tokenizer.tokenize() sentence = \u0026#34;Hello, my son is cuting.\u0026#34; input_ids_1 = tokenizer.encode(sentence, add_special_tokens=False) # add_special_tokens=True 将句子转换成对应模型的输入形式，默认开启。就是首尾加上[cls]、[sep]。即：tensor([ 101, 7592, 1010, 2026, 2365, 2003, 3013, 2075, 1012, 102]) # add_special_tokens=False 首尾先不加[cls]、[sep] input_tokens = tokenizer.tokenize(sentence) # [\u0026#39;hello\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;son\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;cut\u0026#39;, \u0026#39;##ing\u0026#39;, \u0026#39;.\u0026#39;] input_ids_2 = tokenizer.","tags":["Transformer"],"title":"code解析"},{"categories":["Basic"],"contents":"一、背景 二、ELECTRA ELECTRA的全称是Efficiently Learning an Encoder that Classifies Token Replacements Accurately。最主要的贡献是提出了新的预训练任务和框架，把生成式的Masked language model(MLM)预训练任务改成了判别式的Replaced token detection(RTD)任务，判断当前token是否被语言模型提换过。\n三、总结 ","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0200_electra/electra_summary/","summary":"一、背景 二、ELECTRA ELECTRA的全称是Efficiently Learning an Encoder that Classifies Token Replacements Accurately。最主要的贡献是提出了新的预训练任务和框架，把生成式的Masked language model(MLM)预训练任务改成了判别式的Replaced token detection(RTD)任务，判断当前token是否被语言模型提换过。\n三、总结 ","tags":["ELECTRA"],"title":"ELECTRA综述"},{"categories":["Basic"],"contents":"模型评估 评估指标：\n 困惑度：困惑度（perplexity）的基本思想是：给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好，公式如下 $PP(W)=P(w_1w_2\u0026hellip;w_N)^{\\frac{-1}{N}}$ 。由公式可知，句子概率越大，语言模型越好，迷惑度越小。困惑度p可以理解为，如果每个时间步都根据语言模型计算的概率分布随机挑词，那么平均情况下，挑多少个词才能挑到正确的那个 Prompt ranking accuracy：这个指标的定义和评价方法，来自《Hierarchical Neural Story Generation》。主要是关注引导语和生成的故事之间的相关性。具体做法是：在测试集中选择一对（p，g），p表示引导语，g表示生成的故事，在随机选取其他的引导语p1-p9，然后计算p和g的likelihood。条件一：（p，g）的相似性比（p1，g）的相似性大。 那么就取10000个测试集中的（p，g），满足条件一的部分占比，就称为Prompt ranking accuracy。 句子嵌入的相似度：计算引导语和生成的故事的句子嵌入（用GloVe取每个词的平均嵌入值）的余弦相似度。 评价连贯性：连贯性的评价方法，来自《Modeling local coherence: An entity-based approach》，主要思想是，在测试数据集中，对于一个故事s0，选择前面15个句子，打乱顺序，生成14个乱序的故事s1-s14。然后用语言模型计算s0-s14的可能性。对于s1-s14，如果可能性大于s0，就称为反例。 错误率定义为反例的占比。 评价单词的重复性和rareness  一、简介 基于文本预训练的GPT-1，GPT-2，GPT-3三代模型都是采用的以Transformer为核心结构的模型，不同的是模型的层数和词向量长度等超参，它们具体的内容如下：\n   模型 发布时间 层数 head hidden 参数量 预训练数据量     GPT-1 2018年6月 12 12 768 1.17亿 5GB   GPT-2 2019年2月 48 - 1600 15亿 40GB   GPT-3 2020年5月 96 96 12888 175B 45TB    二、GPT GPT(2018-06) 其创造性的提出以Transformer的解码器来训练生成式模型，后面Bert的作者估计是看到了这篇论文，据说两个月时间就发表了以Transformer编码器训练的Bert模型。总结下GPT-1模型：\n GPT-1 使用了一个仅有解码器的 Transformer 结构，每一个作为一个Layer，共有12层； 使用了一个 768 维的嵌入向量来表示输入序列中的每个词或标记，使用了 12 个并行的注意力头（attention heads）； 使用Adam优化器进行模型训练，在训练过程中，使用了学习率的 warmup 阶段和余弦退火调度机制，以平衡训练速度和模型性能； 模型权重被初始化为均值为 0、标准差为 0.02 的正态分布（N(0, 0.02)），使用字节对编码（Byte Pair Encoding，BPE）来对文本进行分词处理，分词后得到的词汇表大小为 40000； 激活函数是 GELU； 文本输入序列固定长度是512； 参数量 117M; 使用了学习得到的位置嵌入向量(position embedding)，而不是Attention is All You Need中使用的正弦位置嵌入向量；  三、GPT-2 GPT-2(2019-02)\nGPT-2的改进:\nGPT-2 是GPT语言模型开始变大的地方，这是 OpenAI 第一次训练超过 1B 个参数的模型。 通过提升模型的规模，来凸显GPT的优势。在 GPT-1 中，作者训练了单个模型，但在这里，作者训练了一系列模型。 与GPT-1相比，架构上有如下差异：\n 层归一化操作，有原来的post-norm换成了pre-norm，以加速训练和提高模型性能。此外，在最后一个自注意力块的输出上添加了额外的层归一化； 在权重初始化时，通过 $\\frac{1}{\\sqrt n}$ 进行缩放。这种缩放有助于减少梯度更新的方差，使训练过程更加稳定； 扩大了其词汇表的大小，词汇表大小约为 50,000（相比于约 40,000）； 增大文本输入序列长度 1024（相比于 512）这使得模型能够更好地理解和生成更长的文本； batch size大小为 512（相比于 64）较大的批次大小有助于提高训练效率和模型并行计算的能力。 最大的模型具有约 15 亿个参数。 数据集：GPT-2 构造了一个新数据集，WebText。全部来自于 Reddit 的外链，而且是那些获得至少三个赞的外链，经过清洗、去重后，得到8百万网页共计 40GB 文本数据。 WebText 数据集的特点在于全面而干净。  GPT-2的不同版本:\n   模型 Layers d_size ff_size Heads Parameters     GPT2-base 12 768 3072 12 1.17亿   GPT2-medium 24 1024 4096 16 3.45亿   GPT2-large 36 1280 5120 20 7.74亿   GPT2-xl 48 1600 6400 25 15.58亿    四、GPT-3 GPT-3(2020-05) GPT-3是大语言模型开始受到关注的开始。在论文中，作者训练了 10 个模型，参数范围从 1.25亿 个参数（“GPT-3 Small”）到 175B 个参数（“GPT-3”）。\n在GPT-3中，模型的架构与GPT-2完全相同。唯一的区别是它们在transformer的各层中使用了“交替的稠密和本地带状稀疏注意力模式”。简单来说，GPT-3在注意力机制上进行了优化，引入了稀疏注意力的概念。\n  传统的点积注意力在计算复杂度上较高，而稀疏注意力可以提供更高的扩展性，并且在处理长序列时具有更高的效率。这种改进使得GPT-3能够更好地处理长文本输入，并且在计算效率和模型表现方面有所提升。 GPT-3引入稀疏注意力的原因尚不清楚，也许是因为计算限制造成的，论文中并没详细的说明如何如何使用模型并行性训练模型，使得论文更难以复现。   五、chatGPT chatGPT(2022-12)\nInstructGPT ChatGPT的博客中讲到ChatGPT和InstructGPT的训练方式相同，不同点仅仅是它们采集数据上有所不同，但是并没有更多的资料来讲数据采集上有哪些细节上的不同。\n六、GPT-4 《GPT-4 Technical Report》\n BART(Bidirectional and Auto-Regressive Transformers，双向自回归转换器)\nprompt\nGoogle T5 (Text-to-Text Transfer Transformer)\n Masked language model(MLM) Replaced token detection(RTD)\n参考 GPT-chatbot\n","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0080_gpt/0010_gpt_summary/","summary":"模型评估 评估指标：\n 困惑度：困惑度（perplexity）的基本思想是：给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好，公式如下 $PP(W)=P(w_1w_2\u0026hellip;w_N)^{\\frac{-1}{N}}$ 。由公式可知，句子概率越大，语言模型越好，迷惑度越小。困惑度p可以理解为，如果每个时间步都根据语言模型计算的概率分布随机挑词，那么平均情况下，挑多少个词才能挑到正确的那个 Prompt ranking accuracy：这个指标的定义和评价方法，来自《Hierarchical Neural Story Generation》。主要是关注引导语和生成的故事之间的相关性。具体做法是：在测试集中选择一对（p，g），p表示引导语，g表示生成的故事，在随机选取其他的引导语p1-p9，然后计算p和g的likelihood。条件一：（p，g）的相似性比（p1，g）的相似性大。 那么就取10000个测试集中的（p，g），满足条件一的部分占比，就称为Prompt ranking accuracy。 句子嵌入的相似度：计算引导语和生成的故事的句子嵌入（用GloVe取每个词的平均嵌入值）的余弦相似度。 评价连贯性：连贯性的评价方法，来自《Modeling local coherence: An entity-based approach》，主要思想是，在测试数据集中，对于一个故事s0，选择前面15个句子，打乱顺序，生成14个乱序的故事s1-s14。然后用语言模型计算s0-s14的可能性。对于s1-s14，如果可能性大于s0，就称为反例。 错误率定义为反例的占比。 评价单词的重复性和rareness  一、简介 基于文本预训练的GPT-1，GPT-2，GPT-3三代模型都是采用的以Transformer为核心结构的模型，不同的是模型的层数和词向量长度等超参，它们具体的内容如下：\n   模型 发布时间 层数 head hidden 参数量 预训练数据量     GPT-1 2018年6月 12 12 768 1.17亿 5GB   GPT-2 2019年2月 48 - 1600 15亿 40GB   GPT-3 2020年5月 96 96 12888 175B 45TB    二、GPT GPT(2018-06) 其创造性的提出以Transformer的解码器来训练生成式模型，后面Bert的作者估计是看到了这篇论文，据说两个月时间就发表了以Transformer编码器训练的Bert模型。总结下GPT-1模型：","tags":["GPT"],"title":"GPT综述"},{"categories":["Basic"],"contents":"一、简介 RNNs中，需要的信息都放在隐藏层，当序列太长时，隐藏层累积了太多的信息，对前面太久的信息，就不容易获取到了。\n另外，有些信息不太重要，有些词比较重要，所以，设计了：\n更新门： $Z_t$ 有助于捕获序列中的长期依赖关系。当$Z_t = 0$时，并不是就没有$H_{t-1}$的信息了，而是$H_{t-1}$的信息通过正常的计算$H_t$的途径进来；而当$Z_t \u0026gt; 0$时，$H_{t-1}$的信息可以绕过正常的计算途径，直接添加到$H_t$中。\n重置门： $R_t$ 有助于捕获序列中的短期依赖关系。$\\tilde{H_t}$ 的计算跟RNNs计算相似，就是加了 $R_t$ 来限制 $H_{t-1}$，本来RNNs对太久的信息就不容易获取，所以 $R_t$ 的作用：是否忘掉历史没用的信息。\n$$R_t = sigmoid(X_tW_{xr}+H_{t-1}W_{hr}+b_r)$$ $$Z_t = sigmoid(X_tW_{xz}+H_{t-1}W_{hz}+b_z)$$ $$\\tilde{H_t} = tanh(X_tW_{xh} + (R_t \\odot H_{t-1})W_{hh} + b_h)$$ $$H_t = Z_t \\odot H_{t-1} + (1-Z_t)\\odot \\tilde{H_t}$$\n其中，$R_t$ ：表示在更新候选隐状态时，需要多少历史隐状态信息，$Z_t$ ：表示在算真正的隐状态时，需要多少新输入的$X_t$的信息，这两个的维度与隐状态是一致的。\n","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0020_rnn/gru/","summary":"一、简介 RNNs中，需要的信息都放在隐藏层，当序列太长时，隐藏层累积了太多的信息，对前面太久的信息，就不容易获取到了。\n另外，有些信息不太重要，有些词比较重要，所以，设计了：\n更新门： $Z_t$ 有助于捕获序列中的长期依赖关系。当$Z_t = 0$时，并不是就没有$H_{t-1}$的信息了，而是$H_{t-1}$的信息通过正常的计算$H_t$的途径进来；而当$Z_t \u0026gt; 0$时，$H_{t-1}$的信息可以绕过正常的计算途径，直接添加到$H_t$中。\n重置门： $R_t$ 有助于捕获序列中的短期依赖关系。$\\tilde{H_t}$ 的计算跟RNNs计算相似，就是加了 $R_t$ 来限制 $H_{t-1}$，本来RNNs对太久的信息就不容易获取，所以 $R_t$ 的作用：是否忘掉历史没用的信息。\n$$R_t = sigmoid(X_tW_{xr}+H_{t-1}W_{hr}+b_r)$$ $$Z_t = sigmoid(X_tW_{xz}+H_{t-1}W_{hz}+b_z)$$ $$\\tilde{H_t} = tanh(X_tW_{xh} + (R_t \\odot H_{t-1})W_{hh} + b_h)$$ $$H_t = Z_t \\odot H_{t-1} + (1-Z_t)\\odot \\tilde{H_t}$$\n其中，$R_t$ ：表示在更新候选隐状态时，需要多少历史隐状态信息，$Z_t$ ：表示在算真正的隐状态时，需要多少新输入的$X_t$的信息，这两个的维度与隐状态是一致的。","tags":["循环神经网络","GRU"],"title":"GRU网络"},{"categories":["Basic"],"contents":"一、简介 长短期记忆网络(LSTM)\n忘记门：$F_t = sigmoid(X_tW_{xf}+H_{t-1}W_{hf}+b_f)$ 输入门：$I_t = sigmoid(X_tW_{xi}+H_{t-1}W_{hi}+b_i)$ 输出门：$O_t = sigmoid(X_tW_{xo}+H_{t-1}W_{ho}+b_o)$ 候选记忆单元：$\\tilde{C_t} = tanh(X_tW_{xc} + (R_t \\odot H_{t-1})W_{hc} + b_c)$ 记忆单元：$C_t = F_t \\odot C_{t-1} + I_t\\odot \\tilde{C_t}$ 隐状态：$H_t = O_t \\odot tanh(C_t)$ 其中，$F_t, I_t, O_t, C_t, H_t, \\in \\R^{n \\times d}$\n","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0020_rnn/lstm/","summary":"一、简介 长短期记忆网络(LSTM)\n忘记门：$F_t = sigmoid(X_tW_{xf}+H_{t-1}W_{hf}+b_f)$ 输入门：$I_t = sigmoid(X_tW_{xi}+H_{t-1}W_{hi}+b_i)$ 输出门：$O_t = sigmoid(X_tW_{xo}+H_{t-1}W_{ho}+b_o)$ 候选记忆单元：$\\tilde{C_t} = tanh(X_tW_{xc} + (R_t \\odot H_{t-1})W_{hc} + b_c)$ 记忆单元：$C_t = F_t \\odot C_{t-1} + I_t\\odot \\tilde{C_t}$ 隐状态：$H_t = O_t \\odot tanh(C_t)$ 其中，$F_t, I_t, O_t, C_t, H_t, \\in \\R^{n \\times d}$","tags":["循环神经网络","LSTM"],"title":"LSTM网络"},{"categories":["Basic"],"contents":"一、文本预处理 1、词元-token 英文：在训练文本模型时，模型输入最小单元：可以是词元维度，也可以是字符维度(这样的话，模型还得学习怎么用字符组合成单词)\n中文：一般是字符维度；如果是词元维度，在模型之前需要进行分词，如果要使用词元维度，需要先分词，用空格间隔开。\n特殊词元：未知词元 \u0026lt;unk\u0026gt;，填充词元\u0026lt;pad\u0026gt;，序列开始词元 \u0026lt;bos\u0026gt;，序列结束词元 \u0026lt;eos\u0026gt;\n2、词表-vocabulary 把token映射到：一个从0开始的数字索引，也就是：\ntoken \u0026ndash;\u0026gt; idx：token_to_idx {0:then, 1:token, \u0026hellip;.}\nidx \u0026ndash;\u0026gt; token：idx_to_token: [the, token, \u0026hellip;.] 例如：\ntokens: 例如：一篇文章\n例如：[[一句话按照空格split后], [], [], ....]\nvocab：词表，代码里可以写成一个类，其元素有：\nself.idx_to_token ：['\u0026lt;unk\u0026gt;', \u0026lsquo;the\u0026rsquo;, \u0026hellip;] token的列表，按照token的个数降序排列\nself.token_to_idx ：{'\u0026lt;unk\u0026gt;': 0, \u0026lsquo;the\u0026rsquo;: 1, \u0026hellip;.} token\u0026ndash;\u0026gt;idx 的映射\ncorpus：语料库，先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计， 得到的统计结果称之为语料\n例如：[('\u0026lt;unk\u0026gt;', 1000), ('the', 900), ....]\n  二、深度循环神经网络 循环神经网络(Recurrent Netural Networks)：是具有隐状态的神经网络。\n类似于MLP多层感知机，RNNs只是添加了时间轴信息。比如，MLP的表示如下：\n$$ H = \\phi(XW_{xh} + b_h) $$ $$O = HW_{hq} + b_q $$\nRNNs的表示如下，需要$H_{t}$ 这个隐状态 记录历史： $$ H_{t} = \\phi(X_{t}W_{xh} + H_{t-1}W_{hh} + b_h)$$ $$ O_{t} = H_{t}W_{hq} + b_{q}$$\n假设语料库的大小为N，那么RNNs的每次预测，其实就是一个N分类。所以，评估一个语言模型的好坏，用的是交叉熵： $$\\frac{1}{n}\\sum_{t=1}^n-log P(x_t|x_{t-1},\\dots,x_1)$$ 由于历史原因，喜欢用困惑度perplexity来表示： $$exp(\\frac{1}{n}\\sum_{t=1}^n-log P(x_t|x_{t-1},\\dots,x_1))$$\nRNNs的应用： 文本生成、文本分类、问答/机器翻译、Tag生成；其输入/输出形式如下：\n三、双向循环神经网络 场景：填空题，”下文“传达了重要信息，这些重要信息关乎到选择那些词来填空。\n我__ 我__饿了 我__写作中 设计方案：概率图模型，设计一个隐马尔科夫模型：\n 在任意时间步t，存在某个隐变量 $h_t$，通过概率 $P(x_t|h_t)$ 控制观测到的 $x_t$。 任何 $h_t \\rarr h_{t+1}$ 转移，都是由一些状态转移概率 $P(h_{t+1}|h_t)$ 给出。  前向递归(forward recursion)：$\\pi_{t+1} = f(\\pi_t, x_t)$ 其中 $f$ 表示一些可被学习的函数。看起来就像循环神经网络中，隐变量的更新过程。这是前向计算 后向递归(backward recursion)：$\\rho_{t-1} = g(\\rho_t, x_t)$ 其中 $g$ 表示一些可被学习的函数。这是后向计算，知道未来数据何时可用，对隐马尔科夫模型是有益的。  双向循环神经网络(bidirectional RNNs)：添加了反向传递信息的隐藏层。\n 在训练阶段，能够利用过去、未来的数据来估计现在空缺的词；在测试阶段，只有过去的数据，因此精度将会很差 双向循环神经网络的计算速度非常慢  所以双向循环神经网络并不常用。\n","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0020_rnn/rnn_summary/","summary":"一、文本预处理 1、词元-token 英文：在训练文本模型时，模型输入最小单元：可以是词元维度，也可以是字符维度(这样的话，模型还得学习怎么用字符组合成单词)\n中文：一般是字符维度；如果是词元维度，在模型之前需要进行分词，如果要使用词元维度，需要先分词，用空格间隔开。\n特殊词元：未知词元 \u0026lt;unk\u0026gt;，填充词元\u0026lt;pad\u0026gt;，序列开始词元 \u0026lt;bos\u0026gt;，序列结束词元 \u0026lt;eos\u0026gt;\n2、词表-vocabulary 把token映射到：一个从0开始的数字索引，也就是：\ntoken \u0026ndash;\u0026gt; idx：token_to_idx {0:then, 1:token, \u0026hellip;.}\nidx \u0026ndash;\u0026gt; token：idx_to_token: [the, token, \u0026hellip;.] 例如：\ntokens: 例如：一篇文章\n例如：[[一句话按照空格split后], [], [], ....]\nvocab：词表，代码里可以写成一个类，其元素有：\nself.idx_to_token ：['\u0026lt;unk\u0026gt;', \u0026lsquo;the\u0026rsquo;, \u0026hellip;] token的列表，按照token的个数降序排列\nself.token_to_idx ：{'\u0026lt;unk\u0026gt;': 0, \u0026lsquo;the\u0026rsquo;: 1, \u0026hellip;.} token\u0026ndash;\u0026gt;idx 的映射\ncorpus：语料库，先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计， 得到的统计结果称之为语料\n例如：[('\u0026lt;unk\u0026gt;', 1000), ('the', 900), ....]\n  二、深度循环神经网络 循环神经网络(Recurrent Netural Networks)：是具有隐状态的神经网络。\n类似于MLP多层感知机，RNNs只是添加了时间轴信息。比如，MLP的表示如下：\n$$ H = \\phi(XW_{xh} + b_h) $$ $$O = HW_{hq} + b_q $$","tags":["循环神经网络"],"title":"RNN综述"},{"categories":["Basic"],"contents":"一、简介 谷歌大脑、谷歌研究院等团队于2017年联合发表文章《Attention Is All You Need》，提出了一种新的注意力 Seq2Deq 模型，以取代之前以RNN作为编/解码器实现的 Seq2Seq 模型。模型采用的也是编码器-解码器架构，但是在该模型中，编码器和解码器不再是 RNN结构，取而代之的是编码器栈（encoder stack）和解码器栈（decoder stack）（注：所谓的“栈”就是将同一结构重复多次，“stack”翻译为“堆叠”更为合适）。编码器栈和解码器栈中分别为连续N个具有相同结构的编码器和解码器。\n 编码器：由两部分组成（自注意力模块 + 前馈神经网络）\n自注意力模块：具体来说是“Multi-Head Attention”，即“多头注意力”模块\n全连接前馈网络 每个子网络都具有残差连接，其输出形式为 $LayerNorm(Sublayer(x)+x)$ ，其中 $Sublayer(x)$ 表示子网络对输入特征x进行的具体映射操作；$LayerNorm()$ 表示归一化操作。\n  解码器：由三部分组成（自注意力模块 + 编码-解码注意力模块 + 前馈神经网络）\n解码器中多了一个编码-解码注意力模块，用来利用当前已有的输出，来匹配输入特征（即：attention操作），然后拿计算出的新特征来计算当前时间步的输出。解码器中的自注意力模块与编码器不同是：这里只能看到当前时间步之前的输入，而不是全部的输入，所以需要有mask的操作。\n 论文中图：    二、Transformer 输入：序列的embeding表示 + 位置编码\n编码器：\n 多头注意力 + 残差连接(residual connection) \u0026ndash;\u0026gt; 层归一化(layer normalization) 基于位置的前馈网络(positionwise feed-forward network) + 残差连接(residual connection) \u0026ndash;\u0026gt; 层归一化(layer normalization)  class PositionWiseFFN(nn.Module): \u0026#34;\u0026#34;\u0026#34;基于位置的前馈网络\u0026#34;\u0026#34;\u0026#34; def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs): super(PositionWiseFFN, self).__init__(**kwargs) self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens) self.relu = nn.ReLU() self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs) def forward(self, X): return self.dense2(self.relu(self.dense1(X))) 解码器：\n 解码器自注意力：解码器中每个位置只能考虑该位置之前的所有位置，所以添加掩码 编码器-解码器注意力：query：前一个解码器层的输出；k和v：整个编码器的输出。目的是捕获与当前解码最相关的编码状态，而不是所有的编码状态都是同等重要。 基于位置的前馈网络(positionwise feed-forward network) + 残差连接(residual connection) \u0026ndash;\u0026gt; 层归一化(layer normalization)  1、位置编码 与CNN/RNN不同，自注意力是没有记录位置信息的。可以回顾自注意力的计算过程，在$q_j$ 跟 $\\bold{K}$、$\\bold{V}$ 计算后，生成一个把$\\bold{V}$各个向量加权后的向量，这里面是没有位置信息的，也就是说不管输入的 $\\bold{V}$ 的向量顺序如何变化，自注意力的输出是不会改变的。\n所以：需要位置编码将位置信息注入到输入里。\n输入：$\\bold{X} \\in \\R^{n \\times d}$ 包含一个序列中n个词元的d维嵌入表示。\n位置编码：$\\bold{P} \\in \\R^{n \\times d}$, 矩阵第i行 偶数列、奇数列：用不同的频率、偏移来记录位置信息。 $$p_{i,2j} = sin(\\frac{i}{10000^{\\frac{2j}{d}}})$$ $$p_{i,2j+1} = cos(\\frac{i}{10000^{\\frac{2j}{d}}})$$\n在 $\\bold{X} + \\bold{P}$ 时，当$\\bold{X}$的幅度值比$\\bold{P}$小或者差不多时，可以增大$\\bold{X}$的幅度值，以保证$\\bold{X}$的主导性。 $$ \\bold{X} \\times M + \\bold{P} $$\n2、层归一化 层归一化：《Layer Normalization》，在一个输入序列中，做归一化。\n 由于输入序列的长度是不确定的  批归一化：《Batch normalization》，在一个batch中，在通道维度 做归一化。\n 避免梯度消失/爆炸：这是因为通过归一化(偏移、拉伸)，把原来可能波动较大的数据，限制在一定的范围内 为啥有效：有的解释是：通过(偏移、拉伸)，相当于添加了一个随机噪声，因为 均值、方差是在当前小批量样本上算出来的，包含了随机性。 批归一化，限制波动的范围，所以可以调大学习率，可以加速收敛。  # 批归一化 mean = X.mean(dim=(0, 2, 3), keepdim=True) var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True) X_hat = (X - mean) / torch.sqrt(var + eps) 图像与文本\n 图像: (batch, c, h, w) ： (h, w: 图像的高和宽)、(c: 通道数) 文本: (batch, T, d) ：(T：序列长度)、(d: 每个词元embeding表示的维度)  类比一下：\n 每个图像中包含 $h \\times w$ 个像素点，这个数目 类似 文本的长度。 每个像素点在channel方向是一个向量，这个向量 类似 词元的embeding  也就是说：图像的每个像素点表示一个样本点，channel方向 表示该样本点的特征表示。这也就是为什么说 $1 \\times 1$ 的卷积核的作用相当于全连接层。\n  class AddNorm(nn.Module): \u0026#34;\u0026#34;\u0026#34;残差连接后进行层规范化\u0026#34;\u0026#34;\u0026#34; def __init__(self, normalized_shape, dropout, **kwargs): super(AddNorm, self).__init__(**kwargs) self.dropout = nn.Dropout(dropout) self.ln = nn.LayerNorm(normalized_shape) def forward(self, X, Y): return self.ln(self.dropout(Y) + X) 3、基于位置的前馈网络 输入：(batch, 序列长度, embeding维度)\n输出：(batch, 序列长度, 新特征维度)\n作用：类似于卷积中的 $1 \\times 1$ 卷积核，就是转换一下特征的维度，样本个数不变。\nclass PositionWiseFFN(nn.Module): \u0026#34;\u0026#34;\u0026#34;基于位置的前馈网络\u0026#34;\u0026#34;\u0026#34; def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs): super(PositionWiseFFN, self).__init__(**kwargs) self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens) self.relu = nn.ReLU() self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs) def forward(self, X): return self.dense2(self.relu(self.dense1(X))) 4、编码器中的attention 在编码器中使用VVV模式，该自注意力模块为 MHA 结构，其中V为上一个编码器对输入句子中的每个词的编码（这里的编码可以理解为 RNN 中的隐变量向量，即输入句子中每个词的内部表示。如果是第一个编码器这里的编码即每个词的嵌入向量）。编码器自注意力模块用来捕捉输入句子中词与词之间的关系。例如翻译句子“The dog is barking at the bird because it is angry”，这里的“it”到底说的是狗还是鸟？编码器自注意力模块就是为了在对“it”进行编码时，尽量使得 “dog”对其具有更高的影响力——即注意力权重。 下图为针对上述翻译问题的一个自注意力模块示意，其中 $x_1, \\dotsb, x_{11}$ 分别代表句子中每个词的编码（为简化起见不考虑结束符等辅助符号），$y_1, \\dotsb, y_{11}$ 分别为自注意力模块对每个词的输出， $e_{ij}$ 即为自注意力模型输出 时在输入 $x_i$ 上投射的注意力权重。下图以 $y_9$（即针对“it”一词）为例，示意了各个输入编码上的注意力权重分配。\nclass EncoderBlock(nn.Module): \u0026#34;\u0026#34;\u0026#34;transformer编码器块\u0026#34;\u0026#34;\u0026#34; def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias=False, **kwargs): super(EncoderBlock, self).__init__(**kwargs) self.attention = d2l.MultiHeadAttention( key_size, query_size, value_size, num_hiddens, num_heads, dropout, use_bias) self.addnorm1 = AddNorm(norm_shape, dropout) self.ffn = PositionWiseFFN( ffn_num_input, ffn_num_hiddens, num_hiddens) self.addnorm2 = AddNorm(norm_shape, dropout) def forward(self, X, valid_lens): Y = self.addnorm1(X, self.attention(X, X, X, valid_lens)) return self.addnorm2(Y, self.ffn(Y)) class TransformerEncoder(d2l.Encoder): \u0026#34;\u0026#34;\u0026#34;transformer编码器\u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, use_bias=False, **kwargs): super(TransformerEncoder, self).__init__(**kwargs) self.num_hiddens = num_hiddens self.embedding = nn.Embedding(vocab_size, num_hiddens) self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout) self.blks = nn.Sequential() for i in range(num_layers): self.blks.add_module(\u0026#34;block\u0026#34;+str(i), EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias)) def forward(self, X, valid_lens, *args): # 因为位置编码值在-1和1之间， # 因此嵌入值乘以嵌入维度的平方根进行缩放， # 然后再与位置编码相加。 X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens)) self.attention_weights = [None] * len(self.blks) for i, blk in enumerate(self.blks): X = blk(X, valid_lens) self.attention_weights[ i] = blk.attention.attention.attention_weights return X 5、解码器中的attention 在解码器中先使用VVV模式，该自注意力模块与编码器自注意力模块的结构非常类似，唯一不同的是添加了掩膜机制，这是由于在解码器中，自注意力模块只被允许处理当前项之前的那些项，这一点与编码器需要“看到”所有项是不同的。上述目标的实现方式非常简单，只要在softmax 注意力权重概率化之前，用掩膜将当前处理项之后的所有项隐去即可，即将注意力的计算改为如下具有掩膜的形式\n解码器自注意力模块之后是编码-解码注意力模块：该模块也被构造为MHA结构、QVV模式。其中Q来自于上一个解码器的输出，而V来自于最后一个编码器输出（即也是编码器栈的最终输出）。该注意力模块能够使得解码器中的每个项都能够有重点的“看一遍”输入序列中的每一个词，这一点与基于 RNN结构的 Seq2Seq 结构类似。\nclass DecoderBlock(nn.Module): \u0026#34;\u0026#34;\u0026#34;解码器中第i个块\u0026#34;\u0026#34;\u0026#34; def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i, **kwargs): super(DecoderBlock, self).__init__(**kwargs) self.i = i self.attention1 = d2l.MultiHeadAttention( key_size, query_size, value_size, num_hiddens, num_heads, dropout) self.addnorm1 = AddNorm(norm_shape, dropout) self.attention2 = d2l.MultiHeadAttention( key_size, query_size, value_size, num_hiddens, num_heads, dropout) self.addnorm2 = AddNorm(norm_shape, dropout) self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens) self.addnorm3 = AddNorm(norm_shape, dropout) def forward(self, X, state): enc_outputs, enc_valid_lens = state[0], state[1] # 训练阶段，输出序列的所有词元都在同一时间处理， # 因此state[2][self.i]初始化为None。 # 预测阶段，输出序列是通过词元一个接着一个解码的， # 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示 if state[2][self.i] is None: key_values = X else: key_values = torch.cat((state[2][self.i], X), axis=1) state[2][self.i] = key_values if self.training: batch_size, num_steps, _ = X.shape # dec_valid_lens的开头:(batch_size,num_steps), # 其中每一行是[1,2,...,num_steps] dec_valid_lens = torch.arange( 1, num_steps + 1, device=X.device).repeat(batch_size, 1) else: dec_valid_lens = None # 自注意力 X2 = self.attention1(X, key_values, key_values, dec_valid_lens) Y = self.addnorm1(X, X2) # 编码器－解码器注意力。 # enc_outputs的开头:(batch_size,num_steps,num_hiddens) Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens) Z = self.addnorm2(Y, Y2) return self.addnorm3(Z, self.ffn(Z)), state class TransformerDecoder(d2l.AttentionDecoder): def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, **kwargs): super(TransformerDecoder, self).__init__(**kwargs) self.num_hiddens = num_hiddens self.num_layers = num_layers self.embedding = nn.Embedding(vocab_size, num_hiddens) self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout) self.blks = nn.Sequential() for i in range(num_layers): self.blks.add_module(\u0026#34;block\u0026#34;+str(i), DecoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i)) self.dense = nn.Linear(num_hiddens, vocab_size) def init_state(self, enc_outputs, enc_valid_lens, *args): return [enc_outputs, enc_valid_lens, [None] * self.num_layers] def forward(self, X, state): X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens)) self._attention_weights = [[None] * len(self.blks) for _ in range (2)] for i, blk in enumerate(self.blks): X, state = blk(X, state) # 解码器自注意力权重 self._attention_weights[0][ i] = blk.attention1.attention.attention_weights # “编码器－解码器”自注意力权重 self._attention_weights[1][ i] = blk.attention2.attention.attention_weights return self.dense(X), state @property def attention_weights(self): return self._attention_weights ","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0030_transformer/0010_transformer_summary/","summary":"一、简介 谷歌大脑、谷歌研究院等团队于2017年联合发表文章《Attention Is All You Need》，提出了一种新的注意力 Seq2Deq 模型，以取代之前以RNN作为编/解码器实现的 Seq2Seq 模型。模型采用的也是编码器-解码器架构，但是在该模型中，编码器和解码器不再是 RNN结构，取而代之的是编码器栈（encoder stack）和解码器栈（decoder stack）（注：所谓的“栈”就是将同一结构重复多次，“stack”翻译为“堆叠”更为合适）。编码器栈和解码器栈中分别为连续N个具有相同结构的编码器和解码器。\n 编码器：由两部分组成（自注意力模块 + 前馈神经网络）\n自注意力模块：具体来说是“Multi-Head Attention”，即“多头注意力”模块\n全连接前馈网络 每个子网络都具有残差连接，其输出形式为 $LayerNorm(Sublayer(x)+x)$ ，其中 $Sublayer(x)$ 表示子网络对输入特征x进行的具体映射操作；$LayerNorm()$ 表示归一化操作。\n  解码器：由三部分组成（自注意力模块 + 编码-解码注意力模块 + 前馈神经网络）\n解码器中多了一个编码-解码注意力模块，用来利用当前已有的输出，来匹配输入特征（即：attention操作），然后拿计算出的新特征来计算当前时间步的输出。解码器中的自注意力模块与编码器不同是：这里只能看到当前时间步之前的输入，而不是全部的输入，所以需要有mask的操作。\n 论文中图：    二、Transformer 输入：序列的embeding表示 + 位置编码\n编码器：\n 多头注意力 + 残差连接(residual connection) \u0026ndash;\u0026gt; 层归一化(layer normalization) 基于位置的前馈网络(positionwise feed-forward network) + 残差连接(residual connection) \u0026ndash;\u0026gt; 层归一化(layer normalization)  class PositionWiseFFN(nn.Module): \u0026#34;\u0026#34;\u0026#34;基于位置的前馈网络\u0026#34;\u0026#34;\u0026#34; def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs): super(PositionWiseFFN, self).","tags":["NLP","Transformer"],"title":"Transformer"},{"categories":["Basic"],"contents":"一、word embedding 词向量：是用来表示词的向量或者表征，也可被认为是词的特征向量。把词映射为实数域向量的技术 \u0026ndash; 词嵌入(word embedding)\n最简单的方式：one-hot向量。\n 词库里假设有N个词，对所有词排序，用0~N-1作为每个词的索引。 每个词的one-hot向量：长度为N，在该次索引的位置为1，其他位置都是0  缺点：one-hot向量，不能表征两个词的相似度。比如我们常用余弦相似度，one-hot的向量都是相互垂直的。\n词嵌入是一种无监督学习。机器通过阅读大量的文章来学习的单词的意思，通过上下文信息来理解一个单词。怎么挖掘上下文信息：\n Count-based method。认为如果两个单词一起出现的频率很高，那么它们的word embedding feature应该很接近彼此，二者的内积就越接近这两个的单词在同一篇文章中出现的次数。GloVe 就是一种count-based的算法。 prediction-based method. ski-gram 和 CBOW 就是这种算法。  二、Count-based method 1、GloVe 《GloVe: Global Vectors for Word Representation》 上下文窗口内的词共现可以携带丰富的语义信息。例如，在一个大型语料库中，“固体”比“气体”更有可能与“冰”共现，但“气体”一词与“蒸汽”的共现频率可能比与“冰”的共现频率更高。此外，可以预先计算此类共现的全局语料库统计数据：这可以提高训练效率。\nGloVe模型基于平方损失 (Pennington et al., 2014)对跳元模型做了三个修改：\n 使用变量 $p_{ij} = x_{ij}$ 和 $q_{ij} = e^{(u^T_j v_i)}$ 而非概率分布，并取两者的对数。所以平方损失项是 $(log p_{ij} - log q_{ij})^2 = (u^T_j v_i - log x_{ij})^2$ 为每个词 $w_i$ 添加两个标量模型参数：中心词偏置 $b_i$ 和上下文词偏置 $c_i$。 用权重函数 $h(x_{ij})$ 替换每个损失项的权重，其中 $h(x)$ 在 $[0, 1]$ 的间隔内递增。  整合代码，训练GloVe是为了尽量降低以下损失函数： $$ \\sum_{i \\in V} \\sum_{j \\in V} h(x_{ij})(u^T_j v_i + b_i + c_j - log x_{ij})^2 $$\n三、prediction-based method word2vec工具，将每个词表示成一个定长的向量，并使得这些向量能较好地表达不同词之间的相似和类比关系。包含了两个模型，\n 跳字模型(skip-gram) 连续词袋模型(continuous bag of word, CBOW)  对于在语义上有意义的表示，它们的训练依赖于条件概率，条件概率可以被看作使用语料库中一些词来预测另一些单词。由于是不带标签的数据，因此跳元模型和连续词袋都是自监督模型。\n1、跳字模型 跳元模型假设一个词可以用来在文本序列中生成其周围的单词。以文本序列 “the”“man”“loves”“his”“son” 为例。假设中心词选择 “loves”，并将上下文窗口设置为2，如图所示，给定中心词 “loves”，跳元模型考虑生成上下文词“the”“man”“him”“son”的条件概率： $$ P(the, man, his, son | loves) $$ 假设上下文词是在给定中心词的情况下独立生成的（即条件独立性）。在这种情况下，上述条件概率可以重写为 $$ P(the | loves) · P(man | loves) · P(his | loves) · P(son | loves) $$\n在跳元模型中，每个词都有两个 $d$ 维向量表示，用于计算条件概率。更具体地说，对于词典中索引为 $i$ 的任何词，分别用 $v_i \\in \\R^d$ 和 $u_i \\in \\R^d$ 表示其用作中心词和上下文词时的两个向量。给定中心词 $w_c$ （词典中的索引 $c$），生成任何上下文词 $w_o$（词典中的索引 $o$）的条件概率可以通过对向量点积的softmax操作来建模： $$ P(w_o|w_c) = \\frac{e^{u^T_o v_c}}{\\sum_{i \\in V}e^{u^T_i v_c}} $$\n其中词表索引集 $V = {0, 2, \u0026hellip;, |V|-1}$。给定长度为 $T$ 的文本序列，其中时间步 $t$ 处的词表示为 $w^{(t)}$。假设上下文词是在给定任何中心词的情况下独立生成的。对于上下文窗口 $m$，跳元模型的似然函数是在给定任何中心词的情况下生成所有上下文词的概率： $$ \\prod^T_{t=1} \\prod_{-m \\le j \\le m, j \\ne 0} P(w^{(t+j)}|w^{(t)}) $$ 其中可以省略小于1或大于 $T$的任何时间步。\n2、连续词袋模型 连续词袋（CBOW）模型类似于跳元模型。与跳元模型的主要区别在于，连续词袋模型假设中心词是基于其在文本序列中的周围上下文词生成的。例如，在文本序列“the”“man”“loves”“his”“son”中，在“loves”为中心词且上下文窗口为2的情况下，连续词袋模型考虑基于上下文词“the”“man”“him”“son”（如图所示）生成中心词“loves”的条件概率，即：\n由于连续词袋模型中存在多个上下文词，因此在计算条件概率时对这些上下文词向量进行平均。具体地说，对于字典中索引 $i$ 的任意词，分别用 $v_i \\in \\R^d$ 和 $u_i \\in \\R^d$ 表示用作上下文词和中心词的两个向量（符号与跳元模型中相反）。给定上下文词 $w_{o1}, ..., w_{o2m}$（在词表中索引是 $o_1, ..., o_{2m}$ ）生成任意中心词 $w_c$ （在词表中索引是 $c$）的条件概率可以由以下公式建模: $$ P(w_c|w_{o1}, ..., w_{o2m}) = \\frac{e^{\\frac{1}{2m}u^T_c(v_{o1}+...+v_{o2m})}}{\\sum_{i \\in V}e^{\\frac{1}{2m}u_i^T(v_{o1}+...+v_{o2m})}} $$ 给定长度为 $T$的文本序列，其中时间步 $t$ 处的词表示为 $w^{(t)}$。对于上下文窗口 $m$ ，连续词袋模型的似然函数是在给定其上下文词的情况下生成所有中心词的概率： $$ \\prod^T_{t=1} P(w^{(t)}|w^{(t-m)},...,w^{(t-1)},w^{(t+1)},...,w^{(t+m)}) $$ 四、word2vec训练 不论是跳字模型还是连续词袋模型，由于条件概率使用了softmax运算，每一步的梯度计算都包含字典大小数目的项的累加。 对含几十万或者上百万词的较大词典来说，每次的梯度计算开销可能过大。为了降低计算复杂度，使用两种近似训练方法：\n 负采样(negative sampling) 层序(hierarchical softmax)  五、子词嵌入 fastText 问题：在上述模型中，将形状不同的单词用不同的向量来表示。例如：dog和dogs分别用不同的向量表示，模型中没有直接表示这两个向量的关系。鉴于此，fastText提出了子词嵌入(subword embedding)的方法，从而试图将构词信息引入Word2vec中的跳字模型。\n改进：\n 在fastText中，每个中心词：表示成子词的集合。 例如 用where来构建子词：  在单词的首尾添加特殊字符\u0026lt;\u0026gt;,以区分作为前后缀的子词 将单词当成一个由字符构成的序列来提取n元语法。例如n=3，得到所有长度为3的子词：\u0026lt;wh whe her ere re\u0026gt;    词典：所有词的子集的并集 模型中词w作为中心词的向量 $v_w$ 则表示成 $v_w = \\sum_{g \\in G_w} z_g$  ","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0010_word_embedding/word_embedding_summary/","summary":"一、word embedding 词向量：是用来表示词的向量或者表征，也可被认为是词的特征向量。把词映射为实数域向量的技术 \u0026ndash; 词嵌入(word embedding)\n最简单的方式：one-hot向量。\n 词库里假设有N个词，对所有词排序，用0~N-1作为每个词的索引。 每个词的one-hot向量：长度为N，在该次索引的位置为1，其他位置都是0  缺点：one-hot向量，不能表征两个词的相似度。比如我们常用余弦相似度，one-hot的向量都是相互垂直的。\n词嵌入是一种无监督学习。机器通过阅读大量的文章来学习的单词的意思，通过上下文信息来理解一个单词。怎么挖掘上下文信息：\n Count-based method。认为如果两个单词一起出现的频率很高，那么它们的word embedding feature应该很接近彼此，二者的内积就越接近这两个的单词在同一篇文章中出现的次数。GloVe 就是一种count-based的算法。 prediction-based method. ski-gram 和 CBOW 就是这种算法。  二、Count-based method 1、GloVe 《GloVe: Global Vectors for Word Representation》 上下文窗口内的词共现可以携带丰富的语义信息。例如，在一个大型语料库中，“固体”比“气体”更有可能与“冰”共现，但“气体”一词与“蒸汽”的共现频率可能比与“冰”的共现频率更高。此外，可以预先计算此类共现的全局语料库统计数据：这可以提高训练效率。\nGloVe模型基于平方损失 (Pennington et al., 2014)对跳元模型做了三个修改：\n 使用变量 $p_{ij} = x_{ij}$ 和 $q_{ij} = e^{(u^T_j v_i)}$ 而非概率分布，并取两者的对数。所以平方损失项是 $(log p_{ij} - log q_{ij})^2 = (u^T_j v_i - log x_{ij})^2$ 为每个词 $w_i$ 添加两个标量模型参数：中心词偏置 $b_i$ 和上下文词偏置 $c_i$。 用权重函数 $h(x_{ij})$ 替换每个损失项的权重，其中 $h(x)$ 在 $[0, 1]$ 的间隔内递增。  整合代码，训练GloVe是为了尽量降低以下损失函数： $$ \\sum_{i \\in V} \\sum_{j \\in V} h(x_{ij})(u^T_j v_i + b_i + c_j - log x_{ij})^2 $$","tags":["word embedding"],"title":"Word Embedding综述"},{"categories":["Basic"],"contents":"It\u0026rsquo;s coming soon. ","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00100_cv/0060_image-matting/image-matting-summary/","summary":"It\u0026rsquo;s coming soon. ","tags":["matting","CV"],"title":"抠图综述"},{"categories":null,"contents":"官方文档\ntorch目录下，树状图:\n├── quasirandom.py\n├── random.py random模块\n├── serialization.py\n├── storage.py\n├── tensor.py Tensor模块\n├── functional.py\n│\n├── cuda\n│　├── comm.py\n│　├── error.py\n│　├── memory.py\n│　├── nccl.py\n│　├── nvtx.py\n│　├── profiler.py\n│　├── random.py\n│　├── sparse.py\n│　└── streams.py\n│\n├── nn\n│　├── backends\n│　├── cpp.py\n│　├── functional.py\n│　├── grad.py\n│　├── init.py\n│　├── intrinsic\n│　│　├── modules\n│　│　│　└── fused.py\n│　│　├── qat\n│　│　│　└── modules\n│　│　│　├── conv_fused.py\n│　│　│　└── linear_relu.py\n│　│　└── quantized\n│　│　└── modules\n│　│　├── conv_relu.py\n│　│　└── linear_relu.py\n│　├── modules\n│　│　├── activation.py\n│　│　├── adaptive.py\n│　│　├── container.py\n│　│　├── conv.py\n│　│　├── distance.py\n│　│　├── dropout.py\n│　│　├── flatten.py\n│　│　├── fold.py\n│　│　├── instancenorm.py\n│　│　├── linear.py\n│　│　├── loss.py\n│　│　├── module.py\n│　│　├── normalization.py\n│　│　├── padding.py\n│　│　├── pooling.py\n│　│　├── rnn.py\n│　│　├── sparse.py\n│　│　├── transformer.py\n│　│　├── upsampling.py\n│　│　└── utils.py\n│　├── parallel\n│　│　├── data_parallel.py\n│　│　├── distributed.py\n│　│　├── parallel_apply.py\n│　│　├── replicate.py\n│　├── parameter.py\n│　├── qat\n│　│　└── modules\n│　│　├── conv.py\n│　│　└── linear.py\n│　├── quantized\n│　│　├── dynamic\n│　│　│　└── modules\n│　│　│　├── linear.py\n│　│　│　└── rnn.py\n│　│　├── functional.py\n│　│　└── modules\n│　│　├── activation.py\n│　│　├── conv.py\n│　│　├── functional_modules.py\n│　│　├── linear.py\n│　│　└── utils.py\n│　└── utils\n│　├── clip_grad.py\n│　├── convert_parameters.py\n│　├── fusion.py\n│　├── prune.py\n│　├── rnn.py\n│　└── spectral_norm.py\n│\n├── optim\n│　├── adadelta.py\n│　├── adagrad.py\n│　├── adam.py\n│　├── adamax.py\n│　├── adamw.py\n│　├── asgd.py\n│　├── lbfgs.py\n│　├── optimizer.py\n│　├── rmsprop.py\n│　├── rprop.py\n│　├── sgd.py\n│　└── sparse_adam.py\n│\n├── autograd\n│　├── anomaly_mode.py\n│　├── function.py\n│　├── grad_mode.py\n│　├── profiler.py\n│　└── variable.py\n│\n├── distributed\n│　├── autograd\n│　├── distributed_c10d.py\n│　├── optim\n│　│　└── optimizer.py\n│　├── rendezvous.py\n│　└── rpc\n│　├── api.py\n│　├── backend_registry.py\n│　├── constants.py\n│　└── internal.py\n│\n├── distributions\n│　├── bernoulli.py\n│　├── beta.py\n│　├── binomial.py\n│　├── categorical.py\n│　├── constraint_registry.py\n│　├── constraints.py\n│　├── distribution.py\n│　├── exp_family.py\n│　├── exponential.py\n│　├── gamma.py\n│　├── geometric.py\n│　├── gumbel.py\n│　├── independent.py\n│　├── kl.py\n│　├── laplace.py\n│　├── log_normal.py\n│　├── logistic_normal.py\n│　├── lowrank_multivariate_normal.py\n│　├── multinomial.py\n│　├── multivariate_normal.py\n│　├── negative_binomial.py\n│　├── normal.py\n│　├── pareto.py\n│　├── poisson.py\n│　├── relaxed_bernoulli.py\n│　├── relaxed_categorical.py\n│　├── studentT.py\n│　├── transformed_distribution.py\n│　├── transforms.py\n│　├── uniform.py\n│　├── utils.py\n│　└── weibull.py\n│\n├── jit\n│　├── annotations.py\n│　├── frontend.py\n│　├── quantized.py\n│　└── supported_ops.py\n│\n├── multiprocessing\n│　├── pool.py\n│　├── queue.py\n│　├── reductions.py\n│　└── spawn.py\n│\n├── quantization\n│　├── default_mappings.py\n│　├── fake_quantize.py\n│　├── fuse_modules.py\n│　├── observer.py\n│　├── qconfig.py\n│　├── quantize.py\n│　└── stubs.py\n│\n├── onnx\n│　├── operators.py\n│　├── symbolic_caffe2.py\n│　├── symbolic_opset10.py\n│　├── symbolic_opset11.py\n│　├── symbolic_opset7.py\n│　├── symbolic_opset8.py\n│　├── symbolic_opset9.py\n│　├── symbolic_registry.py\n│　└── utils.py\n│\n├── utils: 辅助模块\n│　├── backcompat\n│　├── bottleneck\n│　├── collect_env.py\n│　├── cpp_extension.py\n│　├── data\n│　│　│　├── collate.py\n│　│　│　├── pin_memory.py\n│　│　│　└── worker.py\n│　│　├── dataloader.py\n│　│　├── dataset.py\n│　│　├── distributed.py\n│　│　├── sampler.py\n│　├── dlpack.py\n│　├── file_baton.py\n│　│　└── constants.py\n│　├── mkldnn.py\n│　├── model_zoo.py\n│　├── tensorboard\n│　│　├── summary.py\n│　│　└── writer.py\n│\n└── version.py  PyTorch主要包括一下16个模块：\n  torch模块\n torch本身包含了PyTorch经常使用的激活函数：torch.sigmoid, torch.relu, torch.tanh 一些张量操作：torch.mm()(矩阵的乘法), torch.select()(张量元素的选择)等操作 生成张量：torch.zeros(), torch.randn()等操作。    torch.Tensor模块\ntorch.Tensor模块：定义了torch中的张量类型。张量：一定维度的矩阵。\n 张量类中包含着一些列的方法，返回新的张量或者更改当前的张量：根据PyTorch的命名规则，如果张量方法后缀带有下划线，该方法会修改张量本身的数据；反之则会返回新的张量。例如：Torch.add方法：返回新的张量；Torch.add_方法：修改当前张量的值。 torch.Storage负债torch.Tensor底层的数据存储，即：为一个张量分配连续的一维内存地址。    torch.sparse模块\ntorch.sparse模块：定义了稀疏张量，其中构造的稀疏张量采用的是COO格式(Coordinate)，用一个长整形定义非零元素的位置，用浮点数张量定义对应非零元素的值。稀疏张量之间可以做元素的算术运算和矩阵运算。\n  torch.cuda模块\ntorch.cuda模块：定义了与CUDA运算相关的一些列函数，包括：检测系统的CUDA是否可用、当前进程对应的GPU序号、清除GPU上的缓存、设置GPU的计算流、同步GPU上执行的所有核函数等。\n  torch.nn模块\ntorch.nn模块：是神经网络模块化的核心模块，该模块定义了一些神经网络的计算模块：nn.ConvNd(卷积层，其中N=1,2,3)、nn.Linear(全连接层)等。构建深度学习模型的时候，可以通过继承nn.Module类并重写forward方法来实现一个新的神经网络。\n torch.nn.functional模块：定义一些和神经网络相关的函数，包括卷积函数和池化函数等，这些函数也是深度学习网络构建的基础。需要指出的是：torch.nn中定义的模块一般会调用torch.nn.functional里的函数，比如：nn.ConvNd模块会调用torch.nn.functional.convNd函数。另外，torch.nn.functional里面还定义了一些不常用的激活函数：torch.nn.functional.relu6、torch.nn.functional.elu等。 torch.nn.init模块：定义了神经网络权重的初始化。    torch.optim模块\n 定义了一系列的优化器。比如：torch.optim.SGD(随机梯度下降法)、torch.optim.Adagrad、torch.optim.RMSprop、torch.optim.Adam等。 定义了一些学习率衰减的算法的子模块：torch.optim.lr_scheduler，这个子模块中包含了：torch.optim.lr_scheduler.StepLR(学习率阶梯下降算法)、torch.optim.lr_scheduler.CosineAnnealingLR(余弦退火算法)等学习率衰减算法。    torch.autograd模块\ntorch.autograd模块：是PyTorch的自动微分模块，定义了一系列的自动微分函数，包括torch.autograd.backward函数，主要用于：在求得损失函数之后进行反向梯度传播。torch.autograd.grad函数：用于一个标量张量对一个另一个张量求导(在代码中设置不参与求导的部分参数)。另外，这个模块还内置了数值梯度功能和检查自动微分引擎是否输出正确结果的功能。\n  torch.distribute模块\ntorch.distributed模块：是PyTorch的分布式计算模块，主要功能是提供PyTorch并行运行环境，其主要支持的后端有MPI、Gloo、NCCL三种。PyTorch分布式工作原理：启动多个并行的进程，每个进程(都拥有一个模型的备份，然后输入不同的训练数据)，计算损失函数，每个进程独立地做反向 传播，最后对所有进行权重张量的梯度做归约(Reduce)。用到后端的部分主要是：数据的广播(Broadcast)和数据的收集(Gather)\n Broadcast：把数据从一个节点(进程)传播到另一个节点(进程)，比如：归约后梯度张量的传播 Gather：把数据从其他节点(进程)转移到当前节点(进程)，比如：把梯度张量从其他节点转移到某个特定的节点，然后求梯度平均。\nPyTorch的分布式计算模块不但踢动了后端的一个包装，还提供了一些启动方式来启动多个进程，包括：通过网络(TCP)方式、通过环境变量方法、通过共享文件方式等。    torch.distributions模块\ntorch.distributions模块：提供了一系列类，使得PyTorch能够对不同的分布进行采样，并生成概率采样过程的计算图。在一些应用过程中，比如强化学习(Reinforcement Learning)，经常会使用一个深度学习模型来模拟在不同环境条件下采取的策略，其最后的输出是不同动作的概率。当深度学习模型输出概率之后，需要根据概率对策略进行采样来模拟当前的策略概率分布，最后用梯度下降法来让最优策略的概率最大(策略梯度算法PolicyGradient)。实际上，因为采样的输出结果是离散的，无法直接求导，所以不能使用反向传播的方法来优化网络。torch.distributions模块的存在就是为了解决这个问题。可以结合torch.distributions.Categorical进行采样，然后使用对数求导来规避这个问题。当然，除了服从多项式分布的torch.distributions.Categorical类，PyTorch还支持其他的分布(包括连续分布和离散分布)，比如torch.distributions.Normal类支持连续的正态分布的采样，可以用于连续的强化学习的策略。\n  torch.hub模块\ntorch.hub模块：提供了一系列预训练的模型，比如：torch.hub.list函数可以获取某个模型镜像站点的模型信息。通过torch.hub.load来加载预训练模型，载入后的模型可以保存到本地，并可以看到这些模型对应类支持的方法。\n  torch.jit模块\ntorch.jit模块：是PyTorch的即时编译器，这个模块存在的意义是把PyTorch的动态图换成可以优化和序列化的静态图，工作原理：通过输入预先定义好的张量，追踪整个动态图的构建过程，得到最终构建出来的动态图，然后转换为静态图(通过中间表示：IntermediateRepresentation，来描述最后得到的图)。通过JIT得到的静态图可以被保存，并且被PyTorch其他的前端支持。另外，JIT可以用来生成其他格式的神经网络描述文件，比如ONNX。torch.jit支持两种模式，即：脚本模式(ScriptModule)和追踪模式(Tracing)，这两个都能构建静态图，区别在于脚本模式支持控制流，追踪模式不支持，不过前者支持的神经网络模块比后者少。\n  torch.multiprocessing模块\ntorch.multiprocessing模块：定义PyTorch中的多进程API。通过这个模块可以启动不同的进程，每个进程运行不同的深度学习模型，并且能够在进程间共享张量，共享的张量可以在CPU上，也可在GPU上。多进程API还提供了与Python原生的多进程API相同的一系列函数，包括锁(Lock)和队列(Queue)等。\n  torch.random模块\ntorch.random模块：提供了一系列的方法来保存和设置随机数生成器的状态。因为神经网络的训练是一个随机过程，包括数据的输入、权重的初始化都具有一定的随机性。设置一个统一的随机种子可以有效地帮助我们测试不同结构神经网络的表现，有助于调试神经网络的结构。\n get_rng_state函数获取当前随机数生成器状态 set_rng_state函数：设置当前随机数生成器状态 manual_seed函数：设置随机种子 initial_seed函数：得到程序初始的随机种子。    torch.onnx模块\ntorch.onnx模块：定义了PyTorch导出和载入ONNX格式的深度学习模型描述文件。ONNX格式的存在：为了方便不同深度学习框架之间交换模型。引入这个模块可以方便PyTorch导出模型给其他深度学习架构使用，或者让PyTorch可以载入其他深度学习框架构建的深度学习模型。\n  PyTorch的辅助模块\n torch.utils.bottleneck模块：可以用来检测深度学习模型中模块的运行时间，从而可以找到导致性能瓶颈的那些模块，通过优化这些模块的运行时间，优化整个深度学习模型的性能。 torch.utils.checkpoint模块：可以用来节约深度学习使用的内存。因为梯度反向传播，在构建计算图的时候需要保存中间数据，而这些数据大大增加了深度学习的内存消耗。为了减少内存消耗，让迷你批次的大小得到提高，从而提升深度学习模型的性能和优化时的稳定性，可以通过这个模块记录中间数据的计算过程，然后丢掉这些中间数据，等需要用到的时候再从新计算这些数据，这个模块设计的核心思想是以计算时间换存储空间。 torch.utils.cpp_extension模块：定义了PyTorch的C++扩展。 torch.utils.data模块：引入了数据集和数据载入器的概念，前者代表包含了所有数据的数据集，通过索引能够得到某条特定的数据，后者通过对数据集的包装，可以对数据集进行随机排列和采样，得到一些列打乱顺序的批次。 torch.utils.dlpacl模块：定义了PyTorch张量和DLPack张量存储格式之间的转换，用于不同框架之间张量数据的交换。 torch.utils.tensorboard模块：是PyTorch对TensorBoard数据可视化工具的支持。TensorBoard原来是TF自带的数据可视化工具，能够显示深度学习模型在训练过程中损失函数、张量权重的直方图，以及模型训练过程中输出的文本、图像、视频等。TensorBoard的功能非常强大，而且是基于可交互的动态网页设计的，使用者可以通过预先提供的一系列功能来输出特定的训练过程细节。PyTorch支持TensorBoard可视化后，在训练过程中，可以方便地观察中间输出的张量，可以方便地调试深度学习模型。      ","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0050_pytorch/0010_torch_summary/","summary":"官方文档\ntorch目录下，树状图:\n├── quasirandom.py\n├── random.py random模块\n├── serialization.py\n├── storage.py\n├── tensor.py Tensor模块\n├── functional.py\n│\n├── cuda\n│　├── comm.py\n│　├── error.py\n│　├── memory.py\n│　├── nccl.py\n│　├── nvtx.py\n│　├── profiler.py\n│　├── random.py\n│　├── sparse.py\n│　└── streams.py\n│\n├── nn\n│　├── backends\n│　├── cpp.py\n│　├── functional.py\n│　├── grad.py\n│　├── init.py\n│　├── intrinsic\n│　│　├── modules","tags":null,"title":"简介"},{"categories":["Basic"],"contents":"一、简介 It\u0026rsquo;s coming soon.\n二、网络 1、R-CNN 《Rich feature hierarchies for accurate object detection and semantic segmentation》(2013)\n2、SPPNet 《Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》(2014)\n3、Fast R-CNN 《Fast R-CNN》(2015)\n4、Faster R-CNN 《Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks》(2016)\n5、FPN 《Feature Pyramid Networks for Object Detection》(2017)\n4、YOLO 《You Only Look Once: Unified, Real-Time Object Detection》(2016)\n5、YOLO V2 《YOLO9000: Better, Faster, Stronger》(2016)\n6、YOLO V3 《YOLOv3: An Incremental Improvement》(2018)\n7、Mask R-CNN 《Mask R-CNN》(2017) 是何恺明团队提出的一个基于Faster R-CNN模型的一种新型的分割模型，此论文斩获ICCV 2017的最佳论文。\n8、RetinaNet RetinaNet 原始论文为发表于 2017 ICCV 《Focal Loss for Dense Object Detection》(2017) one-stage 网络首次超越 two-stage 网络，拿下了 best student paper。\n9、Cascade 《Cascade R-CNN》(2017)\n10、ATSS 《Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection》(2019)\n11、RepPoints V2 《RepPoints V2: Verification Meets Regression for Object Detection》(2020)\n12、YOLO V4 《YOLOv4: Optimal Speed and Accuracy of Object Detection》(2020)\n","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00100_cv/0050_detect_object/object_detection_summary/","summary":"一、简介 It\u0026rsquo;s coming soon.\n二、网络 1、R-CNN 《Rich feature hierarchies for accurate object detection and semantic segmentation》(2013)\n2、SPPNet 《Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》(2014)\n3、Fast R-CNN 《Fast R-CNN》(2015)\n4、Faster R-CNN 《Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks》(2016)\n5、FPN 《Feature Pyramid Networks for Object Detection》(2017)\n4、YOLO 《You Only Look Once: Unified, Real-Time Object Detection》(2016)\n5、YOLO V2 《YOLO9000: Better, Faster, Stronger》(2016)\n6、YOLO V3 《YOLOv3: An Incremental Improvement》(2018)","tags":["目标检测","CV"],"title":"简介"},{"categories":["Basic"],"contents":"一、简介 It\u0026rsquo;s coming soon.\n二、网络-基于编码器-解码器 1、FCN 《Fully Convolutional Networks for Semantic Segmentation》(2015) 要说语义分割整体实现精度大的跨越还是在FCN（全卷积神经网络）提出之后。它完全改变了之前需要一个窗口来将语义分割任务转变为图片分类任务的观念，FCN完全丢弃了图片分类任务中全连接层，从头到尾都只使用到了卷积层。从FCN后，基于编码器解码器结构的经典网络结构如同雨后春笋般冒了出来\n2、U-Net 《U-Net: Convolutional Networks for Biomedical Image Segmentation》(2015) Unet网络是在医学影像分割中最常用的模型。它的典型特点是，它是U型对称结构，左侧是卷积层，右侧是上采样层（典型的编码器解码器结构）。\n另一个特点是，Unet网络的每个卷积层得到的特征图都会concatenate到对应的上采样层，从而实现对每层特征图都有效使用到后续计算中。也就是文中所说的skip-connection。这样，同其他的一些网络结构比如FCN比较，Unet避免了直接在高级feature map中进行监督和loss计算，而是结合了低级feature map中的特征，从而可以使得最终所得到的feature map中既包含了high-level 的feature，也包含很多的low-level的feature，实现了不同scale下feature的融合，提高模型的结果精确度。\n3、SegNet 《SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation》(2015) 是一个由剑桥大学团队开发的图像分割的开源项目，该项目可以对图像中的物体所在区域进行分割，例如车，马路，行人等，并且精确到像素级别\n4、Deeplab V1 《Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs》(2015) 2015 年的ICLR上提出DeepLab V1是结合了深度卷积神经网络（DCNNs）和概率图模型（DenseCRFs）的方法。它将DenseCRFs作为网络的后处理方法。采用DenseCRFs作为后处理的方法，简单来说，就是对一个像素进行分类的时候，不仅考虑DCNN的输出，而且考虑该像素点周围像素点的值，这样语义分割结果边界清楚。\n5、Deeplab V2 《DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs》(2017) 在实验中发现 DCNNs 做语义分割时精准度不够的问题，根本原因是重复的池化和下采样降低了分辨率。但是另一方面，重复的池化和下采样扩大了感受野，而感受野的扩大对语义分割任务来说也是至关重要的。针对这一问题，DeepLab v2采用的空洞卷积算法扩展感受野，与此同时不会降低特征图的分辨率。此外，deeplab v2基于空洞卷积，设计了ASPP模块。它组合了不同dilation rate的空洞卷积所产生的特征图。这样，不同空洞卷积产生的不同感受野的特征图被组合在了一起，从而获取了更加丰富的上下文信息。\n6、PSPnet 《Pyramid Scene Parsing Network》(2017)\n7、Deeplab V3 《Rethinking Atrous Convolution for Semantic Image Segmentation》(2017) deeplab v3的创新点一是改进了ASPP模块。其实也就是与原来的ASPP相比，新的ASPP模块能够聚集到全局的上下文信息，而之前的只能聚集局部的上下文。\n8、Deeplab V3+ 《Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation》(2018)\ndeeplab v3+的创新点：\n 是设计基于v3的decode module，使得结果变得更加精细； 是用modify xception作为backbone。  三、网络-基于注意力 1、DANet 《Dual Attention Network for Scene Segmentation》(2019)\n2、CCNet 《CCNet: Criss-Cross Attention for Semantic Segmentation》(2019)\n3、DANet 《Context Prior for Scene Segmentation》(2020)\n","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00100_cv/0055_semantic_segmentation/object_detection_summary/","summary":"一、简介 It\u0026rsquo;s coming soon.\n二、网络-基于编码器-解码器 1、FCN 《Fully Convolutional Networks for Semantic Segmentation》(2015) 要说语义分割整体实现精度大的跨越还是在FCN（全卷积神经网络）提出之后。它完全改变了之前需要一个窗口来将语义分割任务转变为图片分类任务的观念，FCN完全丢弃了图片分类任务中全连接层，从头到尾都只使用到了卷积层。从FCN后，基于编码器解码器结构的经典网络结构如同雨后春笋般冒了出来\n2、U-Net 《U-Net: Convolutional Networks for Biomedical Image Segmentation》(2015) Unet网络是在医学影像分割中最常用的模型。它的典型特点是，它是U型对称结构，左侧是卷积层，右侧是上采样层（典型的编码器解码器结构）。\n另一个特点是，Unet网络的每个卷积层得到的特征图都会concatenate到对应的上采样层，从而实现对每层特征图都有效使用到后续计算中。也就是文中所说的skip-connection。这样，同其他的一些网络结构比如FCN比较，Unet避免了直接在高级feature map中进行监督和loss计算，而是结合了低级feature map中的特征，从而可以使得最终所得到的feature map中既包含了high-level 的feature，也包含很多的low-level的feature，实现了不同scale下feature的融合，提高模型的结果精确度。\n3、SegNet 《SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation》(2015) 是一个由剑桥大学团队开发的图像分割的开源项目，该项目可以对图像中的物体所在区域进行分割，例如车，马路，行人等，并且精确到像素级别\n4、Deeplab V1 《Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs》(2015) 2015 年的ICLR上提出DeepLab V1是结合了深度卷积神经网络（DCNNs）和概率图模型（DenseCRFs）的方法。它将DenseCRFs作为网络的后处理方法。采用DenseCRFs作为后处理的方法，简单来说，就是对一个像素进行分类的时候，不仅考虑DCNN的输出，而且考虑该像素点周围像素点的值，这样语义分割结果边界清楚。\n5、Deeplab V2 《DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs》(2017) 在实验中发现 DCNNs 做语义分割时精准度不够的问题，根本原因是重复的池化和下采样降低了分辨率。但是另一方面，重复的池化和下采样扩大了感受野，而感受野的扩大对语义分割任务来说也是至关重要的。针对这一问题，DeepLab v2采用的空洞卷积算法扩展感受野，与此同时不会降低特征图的分辨率。此外，deeplab v2基于空洞卷积，设计了ASPP模块。它组合了不同dilation rate的空洞卷积所产生的特征图。这样，不同空洞卷积产生的不同感受野的特征图被组合在了一起，从而获取了更加丰富的上下文信息。","tags":["语义分割","CV"],"title":"简介"},{"categories":["Basic"],"contents":"一、编码器-解码器 架构 机器翻译：是把一个序列转换为另一个序列。为处理这种类型的输入和输出，设计这样的架构：\n 编码器：接受一个长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。\n 解码器：将固定形状的编码状态映射到长度可变的序列。\n  二、seq2seq Ilya Sutskever 等人设计的seq2seq：将编码器最后一时间步的state，作为解码器第一时间步的state使用。\nKyunghyun Cho 等人设计的seq2seq，将编码器最后一时间步的state，作为解码器每一个时间步的输入序列的一部分。\n","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0020_rnn/encode_decode/","summary":"一、编码器-解码器 架构 机器翻译：是把一个序列转换为另一个序列。为处理这种类型的输入和输出，设计这样的架构：\n 编码器：接受一个长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。\n 解码器：将固定形状的编码状态映射到长度可变的序列。\n  二、seq2seq Ilya Sutskever 等人设计的seq2seq：将编码器最后一时间步的state，作为解码器第一时间步的state使用。\nKyunghyun Cho 等人设计的seq2seq，将编码器最后一时间步的state，作为解码器每一个时间步的输入序列的一部分。","tags":["循环神经网络","编码器-解码器 架构"],"title":"编解码架构"},{"categories":["Basic"],"contents":"一、简介 1、为什么需要预训练 《Visualizing and Understanding the Effectiveness of BERT》  这篇文章指出:\n 首先，预训练能在下游任务中达到一个良好的初始点，与从头开始训练相比，预训练能带来更宽的最优点，更容易优化。尽管 BERT 对下游任务的参数设置过高，但微调程序对过拟合具有很强的鲁棒性。 其次，可视化结果表明，由于最佳值平坦且宽广，以及训练损失面和泛化误差面之间的一致性，微调 BERT 趋向于更好地泛化。 第三，在微调过程中，BERT 的低层更具不变性，这表明靠近输入的层学习到了更多可迁移的语言表征。   2、下游任务怎么Fine-tune 我们希望有一个预训练的模型，输入一串单词，输出一串嵌入向量，并且希望这些向量是可以考虑上下文的。那么要怎么做呢？\n最早是由CoVe提出用翻译的方法，来得到可以考虑上下文的向量。那如何通过翻译的方法来得到这个预训练模型呢，就是把该模型当成翻译的编码器，输入一个A语言的序列，然后有一个解码器，结合编码器的注意力，得到B语言的输出。\n虽然可以做到这件事，但是翻译任务需要大量的语言对数据，收集这么多语言对数据是比较困难的，所以我们期望可以用很容易得到的无标签文本得到一个这样的预训练模型。\n过去这样的方法被叫做无监督学习，不过现在通常叫做自监督学习。在自监督学习中，模型学会用部分输入去预测另外一部分输入。换句话说，就是输入的一部分用于预测输入中的其他部分。这种预测下一个单词的方法就是我们训练语言模型的方式。那么要用什么样的网络结构来训练这个模型呢？\n最早用的就是LSTM，比较知名的使用LSTM进行预训练的模型，就是​ ​ELMo​​。随着自注意的流行，很多人把LSTM换成Transformer。\n问题：\n 为什么预训练下一个单词的方法，能让我们得到代表单词意思的嵌入向量呢？\n   语言学家John Rupert Firth说过，你想要知道某个单词的意思，只要知道它和哪些单词一起出现。预测下一个单词其实做的是类似的事情。\n  假设我们有一些特定任务的标签数据，那如何微调模型呢？\n    一种做法是预选练的模型训练好后就固定了，变成一个特征Extrator。输入一个单词序列，通过这个预训练模型抽取一大堆特征，把这些特征丢到特征任务模型中，然后进行微调； 另外一种做法是把预训练的模型和特定任务的模型接在一起，在微调的时候，同时微调预训练模型和特定任务的模型。    如果微调整个模型，会遇到什么问题呢? 现在有三个不同的任务，每个任务中都有一个预训练好的模型，然后都微调整个模型。\n这三个预训练好的模型，在不同的任务微调里面，它们会变得不一样。每一个任务都需要存一个新的模型，包含微调的预训练模型和特定任务模型。这样的模型往往非常巨大，其中的参数非常多，导致需要占用特别多的空间。\n  怎么解决这个问题呢 有人提出 Adaptor 的概念，在预训练的模型中加入一些叫Apt(Adaptor)的层，在微调的时候，只微调Apt层。这篇文章中，将Adapter插在Feed-forward层之后，在预训练的时候是没有Adapter的，只有在微调的时候才插进去。并且在微调的时候，只调整Adapter层的参数。\n二、bert家族 1、修改Mask范围 那在BERT里面，要盖住哪些单词呢，原始的BERT里面是随机的。也许随机的不够好，尤其对于中文来说，如果盖住中文中的某个字，还是很容易从它附近的字猜出，比如“奥x会”，只要看到“奥”和“会”就可以猜到中间是”运”了。所以\n 有人提出 Whole Word Masking ​​盖住整个单词(中文里的词语)的方法，这样得到的模型可以学到更长的依赖关系。 可能只是盖住几个单词还不够好，ERNIE​(Baidu) ​​就提出了盖住短语级别(多个单词组成一个短语)和实体级别(需要识别出实体，然后盖住)。 还有一种Masking的方法，SpanBert​​​，思想很简单，一次盖住一排单词(token)。不用考虑什么短语啊、单词啊、实体啊。在SpanBert里面还提出了一种训练方法，叫SBO(Span Boundary Objective)，一般我们盖住了一些单词后，我们要把盖住的部分预测出现。而SBO通过被盖住范围的左右两边的向量，然后给定一个数值，比如3，代表要还原被盖住的第3个单词。然后SBO就知道，现在要还原3个位置。 还有一种方法，XLNet，从输入的文本序列中，随机一部分，去预测mask的结果，就是让各种各样不同的信息去预测一个单词，模型可以学到比较多的依赖关系。  2、生成式任务 一般讲到BERT，大家都会说BERT不适于用来做生成任务，因为BERT训练的时候，会看到MASK左右两边的单词，而在生成任务中，只能看到左边已经生成出来的单词，然后BERT就表现不好了。\n但是这种讨论只局限在autoregressive模型，即由左到右生成单词的模型，这符合我们的写字方式。\n但是non-autoregressive模型，不需要一定要由左到右生成序列，也许这种情况下BERT就比较适用了。\n有人提出把输入序列做一点破坏，然后希望输出序列能复制出输入序列，通过这种方法来对seq2seq模型进行预训练。\n那如何破坏输入序列呢，MASS和BART都探讨如何对输入进行破坏。\n  MASS的想法和原来的BERT很像，通过把序列中一些单词随机盖住。MASS的目的只要还原出盖住的部分就可以了。\n  BART提出了各种各样的破坏方法，比如：\n 删掉某些单词(Delete)； 打乱输入多个句子的顺序(permutation)； (❌: 效果不好) 交换序列中单词的位置(rotation)； (❌: 效果不好) 随机插入MASK(比如：原来AB单词之间没有其他单词，故意插入一个MASK去误导模型)或一个MASK盖多个单词(误导模型这里只有一个单词)(Text Infilling)。 (✅: 效果最好)    UniLM ，它是一个神奇的模型，能同时充当编码器、解码器和seq2seq模型的角色。它是一个有很多个自注意的(这里是Transformer)模型，没有分编码器和解码器。然后让这个模型同时训练三种任务，训练的时候，整个输入分成两部分，第一个部分，像编码器一样，能看整个部分的单词；而第二个部分，只能看到输出的单词(解码器)。\n 第一种：和BERT一样，把某些单词MASK起来； 第二种：类似GPT的训练，即把它当成语言模型来用； 第三种：就像BART和MASS一样，当成seq2seq来用。    3、YES/NO ELECTRA 上面的预训练的方法，都需要预测一些部分。或者是预测下一个单词，或者是预测被盖住的部分。其实预测的模型需要的训练量是很大的，ELECTRA不做预测，只回答是或者否。\n比如: 上面原来的句子是“the chef cooked the meal”，现在把“cooked”换成了“ate”。ELECTRA需要判断输入的单词中，哪些被替换了。\n这样的好处是：预测Y/N简单；并且每个输出都被用到，可以计算损失。不像训练BERT时，只要mask的部分才计算loss。\nELECTRA的效果还比较不错，从上图可以看到，在同样的运算量下，它的表现比其他模型要好，并且能更快地达到较好的效果。\n如何输入一个句子，得到这个句子的向量呢？\n我们之前说过，如何了解一个单词的意思，要看这个单词与哪些单词相邻。如我们是不是可以看某个句子的相邻句子，来猜测这个句子的意思呢？Skip Thought就是基于这个想法，通过一个seq2seq模型，输入某个句子，来预测它的下一个句子。如果有两个不同的句子，它们的下一个句子很像，那么这两个句子就会有类似的嵌入向量。不过Skip Thought难以训练，有一个升级版——Quick Thought。这个模型的思想是，有两个句子，分别通过编码器得到句向量，如果这两个句子是相邻的，那么就让这两个句向量越接近越好。\nRoBERTa ALBERT 如何判断两个句子是否相似？\n 在原始bert的输入中，有判断下一句的逻辑。[CLS]句子1[SEP]句子2[SEP]，在输出层有二分类判断。这种方法叫：NSP（Next Sentence Prediction） 还有一种方法叫SOP(Sentence order prediction)，输入两个相邻的句子，模型要输出YES；如果把两个句子反向，那么BERT要输出NO。ALBERT采用了这种思想。  4、知识图谱 在预训练的时候，加入外部知识，比如知识图谱。ERNIE(Tsinghua) ","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0100_bert/0020_bert_family/","summary":"一、简介 1、为什么需要预训练 《Visualizing and Understanding the Effectiveness of BERT》  这篇文章指出:\n 首先，预训练能在下游任务中达到一个良好的初始点，与从头开始训练相比，预训练能带来更宽的最优点，更容易优化。尽管 BERT 对下游任务的参数设置过高，但微调程序对过拟合具有很强的鲁棒性。 其次，可视化结果表明，由于最佳值平坦且宽广，以及训练损失面和泛化误差面之间的一致性，微调 BERT 趋向于更好地泛化。 第三，在微调过程中，BERT 的低层更具不变性，这表明靠近输入的层学习到了更多可迁移的语言表征。   2、下游任务怎么Fine-tune 我们希望有一个预训练的模型，输入一串单词，输出一串嵌入向量，并且希望这些向量是可以考虑上下文的。那么要怎么做呢？\n最早是由CoVe提出用翻译的方法，来得到可以考虑上下文的向量。那如何通过翻译的方法来得到这个预训练模型呢，就是把该模型当成翻译的编码器，输入一个A语言的序列，然后有一个解码器，结合编码器的注意力，得到B语言的输出。\n虽然可以做到这件事，但是翻译任务需要大量的语言对数据，收集这么多语言对数据是比较困难的，所以我们期望可以用很容易得到的无标签文本得到一个这样的预训练模型。\n过去这样的方法被叫做无监督学习，不过现在通常叫做自监督学习。在自监督学习中，模型学会用部分输入去预测另外一部分输入。换句话说，就是输入的一部分用于预测输入中的其他部分。这种预测下一个单词的方法就是我们训练语言模型的方式。那么要用什么样的网络结构来训练这个模型呢？\n最早用的就是LSTM，比较知名的使用LSTM进行预训练的模型，就是​ ​ELMo​​。随着自注意的流行，很多人把LSTM换成Transformer。\n问题：\n 为什么预训练下一个单词的方法，能让我们得到代表单词意思的嵌入向量呢？\n   语言学家John Rupert Firth说过，你想要知道某个单词的意思，只要知道它和哪些单词一起出现。预测下一个单词其实做的是类似的事情。\n  假设我们有一些特定任务的标签数据，那如何微调模型呢？\n    一种做法是预选练的模型训练好后就固定了，变成一个特征Extrator。输入一个单词序列，通过这个预训练模型抽取一大堆特征，把这些特征丢到特征任务模型中，然后进行微调； 另外一种做法是把预训练的模型和特定任务的模型接在一起，在微调的时候，同时微调预训练模型和特定任务的模型。    如果微调整个模型，会遇到什么问题呢? 现在有三个不同的任务，每个任务中都有一个预训练好的模型，然后都微调整个模型。\n这三个预训练好的模型，在不同的任务微调里面，它们会变得不一样。每一个任务都需要存一个新的模型，包含微调的预训练模型和特定任务模型。这样的模型往往非常巨大，其中的参数非常多，导致需要占用特别多的空间。\n  怎么解决这个问题呢 有人提出 Adaptor 的概念，在预训练的模型中加入一些叫Apt(Adaptor)的层，在微调的时候，只微调Apt层。这篇文章中，将Adapter插在Feed-forward层之后，在预训练的时候是没有Adapter的，只有在微调的时候才插进去。并且在微调的时候，只调整Adapter层的参数。\n二、bert家族 1、修改Mask范围 那在BERT里面，要盖住哪些单词呢，原始的BERT里面是随机的。也许随机的不够好，尤其对于中文来说，如果盖住中文中的某个字，还是很容易从它附近的字猜出，比如“奥x会”，只要看到“奥”和“会”就可以猜到中间是”运”了。所以\n 有人提出 Whole Word Masking ​​盖住整个单词(中文里的词语)的方法，这样得到的模型可以学到更长的依赖关系。 可能只是盖住几个单词还不够好，ERNIE​(Baidu) ​​就提出了盖住短语级别(多个单词组成一个短语)和实体级别(需要识别出实体，然后盖住)。 还有一种Masking的方法，SpanBert​​​，思想很简单，一次盖住一排单词(token)。不用考虑什么短语啊、单词啊、实体啊。在SpanBert里面还提出了一种训练方法，叫SBO(Span Boundary Objective)，一般我们盖住了一些单词后，我们要把盖住的部分预测出现。而SBO通过被盖住范围的左右两边的向量，然后给定一个数值，比如3，代表要还原被盖住的第3个单词。然后SBO就知道，现在要还原3个位置。 还有一种方法，XLNet，从输入的文本序列中，随机一部分，去预测mask的结果，就是让各种各样不同的信息去预测一个单词，模型可以学到比较多的依赖关系。  2、生成式任务 一般讲到BERT，大家都会说BERT不适于用来做生成任务，因为BERT训练的时候，会看到MASK左右两边的单词，而在生成任务中，只能看到左边已经生成出来的单词，然后BERT就表现不好了。","tags":["BERT","Family"],"title":"Bert家族"},{"categories":["Basic"],"contents":"一、背景 在使用预训练模型，处理下游任务时，有两类策略：基于特征(feature-based)、基于微调(fine-tuning)\n 基于特征：比如：ELMo，在使用时，对每个下游任务，创建一个跟这个任务相关的神经网络；预训练作为额外的特征跟输入一起输入到模型，预训练的额外特征可能会对要训练的模型有指导作用。 基于微调：比如：GPT，预训练模型在下游使用时，不需要改动太多，类似于视觉模型的fine-tuning，预训练完成特征提取，预训练模型后面添加个简单的网络用于实现具体任务。  1、上下文敏感 在自然语言中，有丰富的多义现象，一个词到底是什么意思，需要参考上下文才能判断。流行的上下文敏感表示：\n TagLM(language-model-augmented sequence tagger 语言模型增强的序列标记器) CoVe(Context Vectors 上下文向量) ELMo(Embeddings from Language Models 来自语言模型的嵌入)  ELMo 将来自预训练LSTM的所有中间层表示组合为输出表示 ELMo的表示，将作为添加特征添加到下游任务的有监督模型中    2、从特定任务到通用任务 ELMo显著改进了自然语言任务，但每个解决方案仍然依赖于一个特定的任务架构。怎么设计一个模型，让各个自然语言任务通用呢？\nGPT(Generative Pre Training 生成式预训练)：在Transformer的基础上，为上下文敏感设计了通用的模型。\n 预训练一个用于表示文本序列的语言模型 当将GPT应用于下游任务时，语言模型的后面接一个线性输出层，以预测任务的标签。GPT的下游任务的监督学习过程，只对预训练Transformer解码器中的所有参数做微调。 GPT只能从左到右  二、BERT BERT的全称是Bidirectional Encoder Representation from Transformers, 即双向Transformer的Encoder。Bert结合了ELMo和GPT的有点，其主要贡献：\n 双向的重要性 基于微调的掩码语言模型(Masked Language Modeling)：BERT随机遮掩词元，并使用来自双向上下文的词元以自监督的方式预测该遮掩词元。  1、构造输入 token embedding: 格式：\u0026lt;CLS\u0026gt;第一个文本序列\u0026lt;SEP\u0026gt;第二个文本序列\u0026lt;SEP\u0026gt;\nsegment embedding: 用来区分句子\nposition embedding: 在bert中 位置嵌入 是可学习的\ndef get_tokens_and_segments(tokens_a, tokens_b=None): \u0026#34;\u0026#34;\u0026#34;获取输入序列的词元及其片段索引\u0026#34;\u0026#34;\u0026#34; tokens = [\u0026#39;\u0026lt;cls\u0026gt;\u0026#39;] + tokens_a + [\u0026#39;\u0026lt;sep\u0026gt;\u0026#39;] # 0和1分别标记片段A和B segments = [0] * (len(tokens_a) + 2) if tokens_b is not None: tokens += tokens_b + [\u0026#39;\u0026lt;sep\u0026gt;\u0026#39;] segments += [1] * (len(tokens_b) + 1) return tokens, segments 2、MLM 词元维度\n在预训练任务中，随机选择15%的词元作为预测的遮掩词元。\n 80%的概率 替换为特殊词元 \u0026lt;mask\u0026gt; （填词） 10%的概率 替换为 随机词元 （纠错） 10%的概率 不做任何处理 （作弊）  class MaskLM(nn.Module): \u0026#34;\u0026#34;\u0026#34;BERT的掩蔽语言模型任务\u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs): super(MaskLM, self).__init__(**kwargs) self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens), nn.ReLU(), nn.LayerNorm(num_hiddens), nn.Linear(num_hiddens, vocab_size)) def forward(self, X, pred_positions): num_pred_positions = pred_positions.shape[1] pred_positions = pred_positions.reshape(-1) batch_size = X.shape[0] batch_idx = torch.arange(0, batch_size) # 假设batch_size=2，num_pred_positions=3 # 那么batch_idx是np.array（[0,0,0,1,1,1]） # batch_idx: batch * 序列大小 batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions) masked_X = X[batch_idx, pred_positions] # masked_x的形状：（batch, 每个序列中被mask词的个数, 词元特征维度） masked_X = masked_X.reshape((batch_size, num_pred_positions, -1)) mlm_Y_hat = self.mlp(masked_X) # 输出mlm_Y_hat形状：（batch, 每个序列中被mask词的个数, vocab_size） return mlm_Y_hat 3、预测下一句 句子维度\n尽管MLM能够使用上下文来表示词元，但它不能显式地建模文本对之间的逻辑关系，为了帮助理解两个文本序列之间的关系，BERT在预训练中考虑了一个二元分类：预测下一句。\n 在为预训练构建句子对儿时，50%的概率 句子对儿是连续句子；50%的概率 句子对儿不是连续句子。  class NextSentencePred(nn.Module): \u0026#34;\u0026#34;\u0026#34;BERT的下一句预测任务\u0026#34;\u0026#34;\u0026#34; def __init__(self, num_inputs, **kwargs): super(NextSentencePred, self).__init__(**kwargs) self.output = nn.Linear(num_inputs, 2) def forward(self, X): # X的形状：(batchsize,num_hiddens) return self.output(X) 4、bert模型 位置编码，是可学习的。nn.Parameter()\nclass BERTEncoder(nn.Module): \u0026#34;\u0026#34;\u0026#34;BERT编码器\u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len=1000, key_size=768, query_size=768, value_size=768, **kwargs): super(BERTEncoder, self).__init__(**kwargs) self.token_embedding = nn.Embedding(vocab_size, num_hiddens) self.segment_embedding = nn.Embedding(2, num_hiddens) self.blks = nn.Sequential() for i in range(num_layers): self.blks.add_module(f\u0026#34;{i}\u0026#34;, d2l.EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, True)) # 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数 self.pos_embedding = nn.Parameter(torch.randn(1, max_len, num_hiddens)) def forward(self, tokens, segments, valid_lens): # 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens） X = self.token_embedding(tokens) + self.segment_embedding(segments) X = X + self.pos_embedding.data[:, :X.shape[1], :] for blk in self.blks: X = blk(X, valid_lens) return X class BERTModel(nn.Module): \u0026#34;\u0026#34;\u0026#34;BERT模型\u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len=1000, key_size=768, query_size=768, value_size=768, hid_in_features=768, mlm_in_features=768, nsp_in_features=768): super(BERTModel, self).__init__() self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len=max_len, key_size=key_size, query_size=query_size, value_size=value_size) self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens), nn.Tanh()) self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features) self.nsp = NextSentencePred(nsp_in_features) def forward(self, tokens, segments, valid_lens=None, pred_positions=None): encoded_X = self.encoder(tokens, segments, valid_lens) # encoded_X 的形状：（批量大小，最大序列长度，num_hiddens） if pred_positions is not None: mlm_Y_hat = self.mlm(encoded_X, pred_positions) else: mlm_Y_hat = None # 用于下一句预测的多层感知机分类器的隐藏层，0是“\u0026lt;cls\u0026gt;”标记的索引 nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :])) return encoded_X, mlm_Y_hat, nsp_Y_hat 三、各式各样的Bert 《All The Ways You Can Compress BERT》\n总结 BERT虽然对上下文有很强的编码能力，但是缺乏细粒度语义的表示。比如：\n The sky is blue today. The sea is blue today. sky 和sea 明明是天和海的区别，却因为上下文一样而得到极为相似的编码。细粒度表示能力的缺失会对真实任务造成很大的影响。  参考 ","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0100_bert/0001_bert_summary/","summary":"一、背景 在使用预训练模型，处理下游任务时，有两类策略：基于特征(feature-based)、基于微调(fine-tuning)\n 基于特征：比如：ELMo，在使用时，对每个下游任务，创建一个跟这个任务相关的神经网络；预训练作为额外的特征跟输入一起输入到模型，预训练的额外特征可能会对要训练的模型有指导作用。 基于微调：比如：GPT，预训练模型在下游使用时，不需要改动太多，类似于视觉模型的fine-tuning，预训练完成特征提取，预训练模型后面添加个简单的网络用于实现具体任务。  1、上下文敏感 在自然语言中，有丰富的多义现象，一个词到底是什么意思，需要参考上下文才能判断。流行的上下文敏感表示：\n TagLM(language-model-augmented sequence tagger 语言模型增强的序列标记器) CoVe(Context Vectors 上下文向量) ELMo(Embeddings from Language Models 来自语言模型的嵌入)  ELMo 将来自预训练LSTM的所有中间层表示组合为输出表示 ELMo的表示，将作为添加特征添加到下游任务的有监督模型中    2、从特定任务到通用任务 ELMo显著改进了自然语言任务，但每个解决方案仍然依赖于一个特定的任务架构。怎么设计一个模型，让各个自然语言任务通用呢？\nGPT(Generative Pre Training 生成式预训练)：在Transformer的基础上，为上下文敏感设计了通用的模型。\n 预训练一个用于表示文本序列的语言模型 当将GPT应用于下游任务时，语言模型的后面接一个线性输出层，以预测任务的标签。GPT的下游任务的监督学习过程，只对预训练Transformer解码器中的所有参数做微调。 GPT只能从左到右  二、BERT BERT的全称是Bidirectional Encoder Representation from Transformers, 即双向Transformer的Encoder。Bert结合了ELMo和GPT的有点，其主要贡献：\n 双向的重要性 基于微调的掩码语言模型(Masked Language Modeling)：BERT随机遮掩词元，并使用来自双向上下文的词元以自监督的方式预测该遮掩词元。  1、构造输入 token embedding: 格式：\u0026lt;CLS\u0026gt;第一个文本序列\u0026lt;SEP\u0026gt;第二个文本序列\u0026lt;SEP\u0026gt;\nsegment embedding: 用来区分句子\nposition embedding: 在bert中 位置嵌入 是可学习的\ndef get_tokens_and_segments(tokens_a, tokens_b=None): \u0026#34;\u0026#34;\u0026#34;获取输入序列的词元及其片段索引\u0026#34;\u0026#34;\u0026#34; tokens = [\u0026#39;\u0026lt;cls\u0026gt;\u0026#39;] + tokens_a + [\u0026#39;\u0026lt;sep\u0026gt;\u0026#39;] # 0和1分别标记片段A和B segments = [0] * (len(tokens_a) + 2) if tokens_b is not None: tokens += tokens_b + [\u0026#39;\u0026lt;sep\u0026gt;\u0026#39;] segments += [1] * (len(tokens_b) + 1) return tokens, segments 2、MLM 词元维度","tags":["BERT"],"title":"Bert综述"},{"categories":["Basic"],"contents":"一、查阅文档 怎么查阅相关文档？ 官网\n1. 查阅模块里的所有函数和类 from mxnet import nd print(dir(nd.random))  __开头和结尾的函数 (python的特别对象) 可以忽略 _开头的函数 (一般为内部函数) 可以忽略 其余成员，可以根据名字 大致猜出是什么意思。  2. 查阅特定函数和类的使用 想了解某个函数或者类的具体用法，可以使用help函数。以NDArray中的ones_like函数为例。\nhelp(nd.ones_like) 注意：\n jupyter记事本里，使用?来将文档显示在另外一个窗口中。例如：nd.ones_like? 与 help(nd.ones_like)效果一样。nd.ones_like??会额外显示该函数实现的代码。  二、内存开销   原始操作 首先来个例子：Y = Y + X \u0026ndash;\u0026gt; 每个操作会新开内存来存储运算结果。 上例中，X，Y 变量首先存储在内存中，相加的计算结果会另外开辟内存来存储；然后变量Y在指向新的内存。 内存使用情况：\n内存id_x \u0026lt;\u0026ndash; X 内存id_y \u0026lt;\u0026ndash; Y 内存id_x+y \u0026lt;\u0026ndash; Y\n  Y[:] = X + Y 或者 Y += X 通过[:]把X+Y的结果写进Y对应的内存中。上述操作中，需要另外开辟内存来存储计算结果。 内存使用情况： 内存id_x \u0026lt;\u0026ndash; X 内存id_y \u0026lt;\u0026ndash; Y 内存id_x+y \u0026ndash;\u0026gt; 把内存id_x+y中数值复制到内存id_y中\n  使用运算符全名函数中的out参数 可以避免临时内存开销，使用运算符全名函数：nd.elemwise_add(X, Y, out=Y)。内存使用情况： 内存id_x \u0026lt;\u0026ndash; X 内存id_y \u0026lt;\u0026ndash; Y 内存id_y \u0026lt;\u0026ndash; 直接存放 X+Y 的计算结果\n  三、自动求梯度 MXNet提供的autograd模块，可以自动求梯度(gradient) from mxnet import autograd, nd # 1. 创建变量 x，并赋初值 x = nd.arrange(4).reshape((4, 1)) # 2. 为了求变量x的梯度，先调用attach_grad函数来申请存储梯度所需要的内存  x.attach_grad() # 3. 为了减少计算和内存开销，默认条件下MXNet是不会记录：求梯度的计算， # 需要调用record函数来要求MXNet记录与求梯度有关的计算。 print(autograd.is_training()) # False with autograd.record(): print(autograd.is_training()) # True y = 2*nd.dot(x.T, x) # 4. 调用backward函数自动求梯度。y必须是一个标量， # 如果y不是标量：MXNet会先对y中元素求和，然后对该和值求有关x的梯度 y.backward() 注意：\n 在调用record函数后，MXNet会记录并计算梯度； 默认情况下，autograd会改变运行模式：从预测模式转为训练模式。可以通过调用is_training函数来查看。  ","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0060_mxnet/0010_ndarray/0010_ndarray_summary/","summary":"一、查阅文档 怎么查阅相关文档？ 官网\n1. 查阅模块里的所有函数和类 from mxnet import nd print(dir(nd.random))  __开头和结尾的函数 (python的特别对象) 可以忽略 _开头的函数 (一般为内部函数) 可以忽略 其余成员，可以根据名字 大致猜出是什么意思。  2. 查阅特定函数和类的使用 想了解某个函数或者类的具体用法，可以使用help函数。以NDArray中的ones_like函数为例。\nhelp(nd.ones_like) 注意：\n jupyter记事本里，使用?来将文档显示在另外一个窗口中。例如：nd.ones_like? 与 help(nd.ones_like)效果一样。nd.ones_like??会额外显示该函数实现的代码。  二、内存开销   原始操作 首先来个例子：Y = Y + X \u0026ndash;\u0026gt; 每个操作会新开内存来存储运算结果。 上例中，X，Y 变量首先存储在内存中，相加的计算结果会另外开辟内存来存储；然后变量Y在指向新的内存。 内存使用情况：\n内存id_x \u0026lt;\u0026ndash; X 内存id_y \u0026lt;\u0026ndash; Y 内存id_x+y \u0026lt;\u0026ndash; Y\n  Y[:] = X + Y 或者 Y += X 通过[:]把X+Y的结果写进Y对应的内存中。上述操作中，需要另外开辟内存来存储计算结果。 内存使用情况： 内存id_x \u0026lt;\u0026ndash; X 内存id_y \u0026lt;\u0026ndash; Y 内存id_x+y \u0026ndash;\u0026gt; 把内存id_x+y中数值复制到内存id_y中","tags":["mxnet","NdArray"],"title":"NdArray使用"},{"categories":["Basic"],"contents":"一、os          os.path.basename()    os.path.dirname()    os.path.join()    os.path.exists()    os.path.isfile()    os.path.isdir()    os.path.abspath(__file__) 获取当前执行文件的绝对路径   os.listdir() 遍历该目录下的文件，返回文件名列表   os.walk() 遍历该目录，返回的是一个三元组(root, dirs, files)\nroot: 指的是当前正在遍历的文件夹的地址\ndirs: 是一个list，内容是该文件夹中所有的目录的名字，不包括子目录\nfiles：内容是该文件夹中所有的文件，不包括子目录   os.makedirs() 创建文件夹   os.remove() 删除文件夹   os.environ() 获取环境变量，比如：os.environ(\u0026lsquo;变量名\u0026rsquo;, \u0026lsquo;默认值\u0026rsquo;)    二、sys          sys.path 搜索路径   sys.platform 获取当前系统平台   sys.argv 实现从程序外部向程序传递参数   sys.exit([arg]) 程序中间的退出，arg=0为正常退出，例如：sys.exit(0)   sys.getdefaultencoding() 获取当前系统编码，一般为ascii   sys.setdefaultencoding() 设置系统默认编码，比如：sys.setdefaultencoding(\u0026lsquo;utf8\u0026rsquo;)   sys.getfilesystemencoding() 获取文件系统使用编码方式：\nwindoes: \u0026lsquo;mbcs\u0026rsquo;\nmac: \u0026lsquo;utf-8\u0026rsquo;   sys.stdin()    sys.stdout()    sys.stderr()     三、内置函数 1、字符判断    字符串检测方法      isalnum() 检测字符串是否由字母和数字组成   isalpha() 检测字符串是否只由字母组成。   isascii() 检测字符串是否都是ASCII编码的字符   isdigit() 检测字符串是否只由数字组成   islower() 检测字符串是否由小写字母组成   isupper() 检测字符串是否由大写字母组成   isdecimal() 检查字符串是否只包含十进制字符   isidentifier() 判断字符串是否是有效的Python标识符   isnumeric() 检测字符串是否只由数字组成   isspace() 检测字符串是否只由空白字符组成   istitle() 检测字符串中所有的单词拼写首字母是否为大写，且其他字母为小写    ","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0025_internal_module/","summary":"一、os          os.path.basename()    os.path.dirname()    os.path.join()    os.path.exists()    os.path.isfile()    os.path.isdir()    os.path.abspath(__file__) 获取当前执行文件的绝对路径   os.listdir() 遍历该目录下的文件，返回文件名列表   os.walk() 遍历该目录，返回的是一个三元组(root, dirs, files)\nroot: 指的是当前正在遍历的文件夹的地址\ndirs: 是一个list，内容是该文件夹中所有的目录的名字，不包括子目录\nfiles：内容是该文件夹中所有的文件，不包括子目录   os.makedirs() 创建文件夹   os.remove() 删除文件夹   os.environ() 获取环境变量，比如：os.environ(\u0026lsquo;变量名\u0026rsquo;, \u0026lsquo;默认值\u0026rsquo;)    二、sys          sys.","tags":["python","内置模块"],"title":"内置模块"},{"categories":["Basic"],"contents":"一、数据类型与操作    操作 说明      del A[i] 删除列表A中下标为i的元素，其后的每个元素都前移一个位置 列表-删除   A.pop() 弹出列表尾部元素，相当于出栈 列表-删除   A.pop(i) 弹出列表中任何位置出的元素 列表-删除   A.remove('a') 有时候不知道索引号，只知道要删除的值；remove只删除第一个指定的值 列表-删除   A.sort(reverse=True) 对列表A从大到小排序，列表A被永久改变 列表-排序   B=sorted(A) 排序后，A没有被改变 列表-排序   A.reverse() A列表被永久的翻转了一下 列表-翻转   ord() 获取字符的ASCII码，比如：两个字符相减：ord(\u0026lsquo;a\u0026rsquo;) - ord(\u0026lsquo;b\u0026rsquo;)          二、*和**的作用   * 在函数定义/调用时的应用\n 在函数定义时：*让python创建一个名为topping的空元组，并将收到的所有值封装在这个元组中。  def make_pizza(size, *topping): # 定义 ...  在调用时：*操作符自动把参数列表拆开  toppings = [\u0026#39;nushroom\u0026#39;, \u0026#39;green peppers\u0026#39;, \u0026#39;extra cheese\u0026#39;] make_pizza(size, *toppings) # 调用   ** 在函数定义/调用时的应用\n 在函数定义时：** 让python创建一个名为user_info的空字典，并将收到的所有键值对都封装到这个字典中。  def build_profile(first, last, **user_info): # 定义 ...  在调用时：** 操作符自动把参数字典拆开  user_infos = {} build_profile(first, last, **user_infos) # 调用   三、%的作用  %字符：标记转换说明符的开始 转换标志：    表示左对齐；     表示在转换值之前要加上正负号；   \u0026ldquo;\u0026quot;(空白字符)表示正数之前保留空格； 0表示转换值如果位数不够用0填充。   (.)前的数：最小字段宽度：转换后的字符串至少应该具有该值指定的宽度。 (.)后的数：精度值： \u0026lsquo;{:.2f}'.format()，format的精度  pi = 3.141592653 print(\u0026#39;{:.2f}\u0026#39;.format(pi)) 3.14 # 字段宽：10，精度：3 print(\u0026#39;%10.3f\u0026#39; % pi) 3.142 # 用*从后面的元组中读取字段宽度和精度 print(\u0026#39;%.*f\u0026#39; % (3, pi)) 3.142 # 用0填充空白 print(\u0026#39;%010.3f\u0026#39; % pi) 000003.142 # 左对齐 print(\u0026#39;%-10.3f\u0026#39; % pi) 3.142 # 显示正负号 print(\u0026#39;%+f\u0026#39; % pi) +3.141593 四、类的继承 约定：\n python中首字母大写的名称为类名 类中的函数成为方法 类中的变量成为属性  __init__：是一个特殊的方法，每当根据Dog类创建新实例时，python会自动运行它。其形参self是必不可少的，且必须在前面。 当python实例化对象时，会调用这个__init__()方法来创建Dog实例，自动传入实参self，self是一个指向实例本身的引用，每个与类相关联的方法调用都自动传递实参self。\n类的继承：\nsuper()，解决了子类调用父类方法的一些问题，父类多次被调用时，只执行一次。\nclass Car(): def __init__(self, make, model, year): self.make = make self.model = model self.year = year self.odometer_reading = 0 def read_odoeter(self): print(\u0026#34;\u0026#34;) class ElectricCar(Car): def __init__(self, make, model, year): super().__init__(make, model, year) self.battery_size = 70 def describe_battery(self): print(\u0026#34;\u0026#34;) ","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0020_basic_operator/","summary":"一、数据类型与操作    操作 说明      del A[i] 删除列表A中下标为i的元素，其后的每个元素都前移一个位置 列表-删除   A.pop() 弹出列表尾部元素，相当于出栈 列表-删除   A.pop(i) 弹出列表中任何位置出的元素 列表-删除   A.remove('a') 有时候不知道索引号，只知道要删除的值；remove只删除第一个指定的值 列表-删除   A.sort(reverse=True) 对列表A从大到小排序，列表A被永久改变 列表-排序   B=sorted(A) 排序后，A没有被改变 列表-排序   A.reverse() A列表被永久的翻转了一下 列表-翻转   ord() 获取字符的ASCII码，比如：两个字符相减：ord(\u0026lsquo;a\u0026rsquo;) - ord(\u0026lsquo;b\u0026rsquo;)          二、*和**的作用   * 在函数定义/调用时的应用\n 在函数定义时：*让python创建一个名为topping的空元组，并将收到的所有值封装在这个元组中。  def make_pizza(size, *topping): # 定义 .","tags":["python","基础操作"],"title":"基础操作"},{"categories":["Basic"],"contents":"一、字符编码  ASCII：计算机是美国人发明的，所以最早只考虑了简单的26个字母和一些控制字符，所以只用7-bit组合出128个组合，编号0~127，存储的时候凑成了一个byte。这个组合没有考虑其他国家，比如汉字就不只128个，于是中国为汉字编码发明了GB2312编码，其他国家也有自己的各种编码，互不兼容。\n为了统一，提出了unicode编码，包含了各个国家的文字，对每个字符都用2个byte来表示，英文的话就在前面加0。\nunicode对于英文就会有些浪费，为了解决这个问题，为了节约硬盘空间/ 网络带宽，又发明了utf-8编码，1个字符可能会被编码成1~6个字节，英文还是1个字节，汉字变成了3个字节，只有在生僻字才会在4个字节。\n    字符 ASCII unicode utf-8     A 01000001 00000000 01000001 01000001   中  01001110 00101101 11100100 10111000 10101101   字符应用层的形式  字符在内存的形式 字符在硬盘/网络中的形式    二、解析/转换 图片在网络中获取下来是二进制的格式(bytes)；或者通过 open('***.jpg', \u0026lsquo;rb\u0026rsquo;) 读取的图片也是二进制的格式\n  bytes格式 \u0026lt;-\u0026gt; str\n bytes: 是(二进制)数字序列，是utf-8的编码形式。该格式的变量是不可修改的。  str \u0026ndash;\u0026gt; bytes : 使用str.encode()方法 bytes \u0026ndash;\u0026gt; str : 使用bytes.decode()方法   bytearray(): 该格式的变量是可以修改的  a = \u0026#39;人生苦短\u0026#39; # 此时b的格式是bytes，是不能修改的，即不能操作：b[:6] = \u0026#39;生命\u0026#39;.encode()  b = a.encode() # \\xe4\\xba\\xba\\xe7\\x94\\x9f\\xe8\\x8b\\xa6\\xe7\\x9f\\xad c = bytearray(b) # 转变为bytearray格式，就可以修改了 c[:6] = bytearray(\u0026#39;生命\u0026#39;.encode())   bytes格式 \u0026lt;-\u0026gt; numpy  bytes \u0026ndash;\u0026gt; numpy :  img_np = np.asarray(bytearray(content), dtype=\u0026#39;uint8\u0026#39;) # 或者 img_np = np.frombuffer(content, dtype=\u0026#39;uint8\u0026#39;)  numpy \u0026ndash;\u0026gt; bytes : img_content = img_np.tobytes()    bytes格式 \u0026lt;-\u0026gt; PIL\n bytes \u0026ndash;\u0026gt; PIL :  content = b\u0026#39;\\x...\u0026#39; # 二进制序列 utf-8编码格式 img_pil = PIL.Image.open(BytesIO(content))  PIL \u0026ndash;\u0026gt; bytes :  from PIL import Image from io import BytesIO # BytesIO: 在内存中读写bytes.  # 例如：f = BytesIO() f.write(\u0026#39;中文\u0026#39;.encode()) f.getvalue() img_pil = Image.open(\u0026#39;***.png\u0026#39;) f = BytesIO() img_pil.save(f, format=\u0026#39;PNG\u0026#39;) # PNG参数：四通道；JPEG参数：三通道 img_bytes = f.getvalue() # 转二进制    bytes格式 \u0026lt;-\u0026gt; opencv\n bytes -\u0026gt; cv2  img_np = np.asarray(bytearray(content), dtype=\u0026#39;uint8\u0026#39;) img_cv = cv2.imdecode(img_np, cv2.IMREAD_UNCHANGED)  cv2 -\u0026gt; bytes  success, encode_img = cv2.imencode(\u0026#39;.jpg\u0026#39;, img_cv) img_bytes = encode_img.tostring()   PIL \u0026lt;-\u0026gt; np\n PIL -\u0026gt; np  img_np = np.array(img_pil)  np -\u0026gt; PIL  img_pil = Image.fromarray(img_np)   PIL \u0026lt;-\u0026gt; cv2 PIL的图片是RGB模式，cv2的图片是BGR格式\n PIL -\u0026gt; cv2  img_cv = cv2.cvtColor(numpy.asarray(img_pil), cv2.COLOR_RGB2BGR)  cv2 -\u0026gt; PIL  img_pil = Image.fromarray(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))   ","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0010_encode_mode/","summary":"一、字符编码  ASCII：计算机是美国人发明的，所以最早只考虑了简单的26个字母和一些控制字符，所以只用7-bit组合出128个组合，编号0~127，存储的时候凑成了一个byte。这个组合没有考虑其他国家，比如汉字就不只128个，于是中国为汉字编码发明了GB2312编码，其他国家也有自己的各种编码，互不兼容。\n为了统一，提出了unicode编码，包含了各个国家的文字，对每个字符都用2个byte来表示，英文的话就在前面加0。\nunicode对于英文就会有些浪费，为了解决这个问题，为了节约硬盘空间/ 网络带宽，又发明了utf-8编码，1个字符可能会被编码成1~6个字节，英文还是1个字节，汉字变成了3个字节，只有在生僻字才会在4个字节。\n    字符 ASCII unicode utf-8     A 01000001 00000000 01000001 01000001   中  01001110 00101101 11100100 10111000 10101101   字符应用层的形式  字符在内存的形式 字符在硬盘/网络中的形式    二、解析/转换 图片在网络中获取下来是二进制的格式(bytes)；或者通过 open('***.jpg', \u0026lsquo;rb\u0026rsquo;) 读取的图片也是二进制的格式\n  bytes格式 \u0026lt;-\u0026gt; str\n bytes: 是(二进制)数字序列，是utf-8的编码形式。该格式的变量是不可修改的。  str \u0026ndash;\u0026gt; bytes : 使用str.encode()方法 bytes \u0026ndash;\u0026gt; str : 使用bytes.decode()方法   bytearray(): 该格式的变量是可以修改的  a = \u0026#39;人生苦短\u0026#39; # 此时b的格式是bytes，是不能修改的，即不能操作：b[:6] = \u0026#39;生命\u0026#39;.","tags":["python","字符编码"],"title":"字符编码"},{"categories":["Basic"],"contents":"一、线程与进程     进程 线程      进程：是一个应用程序在处理机上的一次执行过程，是具有一定独立功能的程序在某数据集上的一次运行，是一个动态的概念。进程是系统进行资源分配和调度的独立单位。 线程：是进程中的一个实体，是CPU调度和分派的基本单位，线程自己基本上不拥有系统资源，它与同属于一个进程内的其他线程共享进程的全部资源。   地址空间 进程有自己独立的地址空间 进程中至少有一个线程，它们共享进程的地址空间   资源 进程是资源分配和拥有的单位 进程内的多个线程共享进程的资源   调度  线程是进程内的一个执行单元，也是进程内的可调度实体，也是处理器调度的基本单位    二、多线程 1、threading模块 python主要是通过thread和threading这两个模块来实现多线程，thread模块是比较底层的模块，threading模块是对thread做了一些封装，使用更方便。但是由于GIL的存在，无法使用threading充分利用CPU资源，如果想充分发挥多核CPU的计算能力，需要使用multiprocessing模块\npython 3.x 已经摒弃了python 2.x中采用函数式thread模块来产生线程的方式。而是通过threading模块创建新的线程：\n  通过threading.Thread(Target=可执行方法)\nimport threading pro_list = [] mult_image_label_list = [] for index, img_list in enumerate(mult_image_label_list): # 创建线程 t1 = threading.Thread(target=函数名, args=(index, img_list)) pro_list.append(t1) for thread in pro_list: # 将线程设置为保护线程，否则会被无限挂起。 thread.setDaemon(True) thread.start() # 该位置---子线程与父线程同时执行，父线程执行完后，同时结束子线程的执行。 # 如果不添加join()语句，父线程结束后，子线程就会结束。 # 如果需要在子线程执行完后，父线程才结束，需要添加join()，让父进程一直处于阻塞状态，直到所有子线程执行完毕。 for thread in pro_list: # 在子线程结束前，父线程一直处于阻塞状态。让子线程执行完，才执行父线程，添加join()。 thread.join()   继承threading.Thread定义子类，并重写run()方法和init()\n实例化后调用start()方法启动新线程，即：它调用了线程的run()方法。\nimport threading import time class myThread(threading.Thread): def __init__(self, threadID, name, counter): threading.Thread.__init__(self) self.threadID = threadID self.name = name self.counter = counter def run(self): print(\u0026#34;Starting \u0026#34; + self.name) print_time(self.name, self.counter, 5) print(\u0026#34;Exiting \u0026#34; + self.name) def print_time(threadName, delay, counter): while counter: time.sleep(delay) print(\u0026#34;%sprocess at: %s\u0026#34; % (threadName, time.ctime(time.time()))) counter -= 1 # 创建新线程 thread1 = myThread(1, \u0026#34;Thread-1\u0026#34;, 1) thread2 = myThread(2, \u0026#34;Thread-2\u0026#34;, 2) # 开启线程 thread1.start() thread2.start() # 等待线程结束 thread1.join() thread2.join() print(\u0026#34;Exiting Main Thread\u0026#34;) 上例中thread1和thread2执行顺序是乱序的，如果要使其有序，需要进行线程同步。\n如果多个线程共同对某个数据操作，可能会出现不可预料的结果，为了保证数据的正确性，需要对多个线程进行同步。threading.Lock()有acquire方法(进行加锁)和release方法(进行解锁)，对于需要每次只允许一个线程操作的数据，可以将其操作放在acquire和release方法之间。线程同步的方式：锁机制、同步队列\n 锁机制  class myThread(threading.Thread): def __init__(self, threadID, name, counter, lock): threading.Thread.__init__(self) self.threadID = threadID self.name = name self.counter = counter self.lock = lock def run(self): print(\u0026#34;Starting \u0026#34; + self.name) # 加锁 self.lock.acquire() print_time(self.name, self.counter, 5) # 解锁 self.lock.release() print(\u0026#34;Exiting \u0026#34; + self.name) def print_time(threadName, delay, counter): while counter: time.sleep(delay) print(\u0026#34;%sprocess at: %s\u0026#34; % (threadName, time.ctime(time.time()))) counter -= 1 lock = threading.Lock() # 创建新线程 thread1 = myThread(1, \u0026#34;Thread-1\u0026#34;, 1, lock) thread2 = myThread(2, \u0026#34;Thread-2\u0026#34;, 2, lock) # 开启线程 thread1.start() thread2.start() # 等待线程结束 thread1.join() thread2.join() print(\u0026#34;Exiting Main Thread\u0026#34;) 线程同步队列queue\npython 2.x 提供的Queue，python3.x中提供的是queue。其中queue模块找那个提供了同步的、线程安全队列类，包括：FIFO(先入先出队列)、LIFO(后入先出队列)、PriorityQueue(优先级别队列)。这些队列都实现了锁原语，能够在多线程中直接使用。\n可以使用队列来实现线程间的同步。    queue常用方法      queue.qsize() 返回队列的大小   queue.empty() 如果队列为空，返回True，否则返回False   queue.full() 如果队列满了，返回True，否则返回False   queue.get() 获取队列   queue.get_nowait() 相当于Queue.get(False)   queue.put() 写入队列   queue.put_nowait(item) 相当于Queue.put(item, False)   queue.task_done() 在完成一项工作之后，向任务已经完成的队列发送一个信号   queue.join() 实际上意味着等到队列为空，再执行别的操作      #!/usr/bin/python3 import queue import threading import time exitFlag = 0 class myThread (threading.Thread): def __init__(self, threadID, name, q): threading.Thread.__init__(self) self.threadID = threadID self.name = name self.q = q def run(self): print (\u0026#34;开启线程：\u0026#34; + self.name) process_data(self.name, self.q) print (\u0026#34;退出线程：\u0026#34; + self.name) def process_data(threadName, q): while not exitFlag: queueLock.acquire() if not workQueue.empty(): data = q.get() queueLock.release() print (\u0026#34;%sprocessing %s\u0026#34; % (threadName, data)) else: queueLock.release() time.sleep(1) threadList = [\u0026#34;Thread-1\u0026#34;, \u0026#34;Thread-2\u0026#34;, \u0026#34;Thread-3\u0026#34;] nameList = [\u0026#34;One\u0026#34;, \u0026#34;Two\u0026#34;, \u0026#34;Three\u0026#34;, \u0026#34;Four\u0026#34;, \u0026#34;Five\u0026#34;] queueLock = threading.Lock() workQueue = queue.Queue(10) threads = [] threadID = 1 # 创建新线程 for tName in threadList: thread = myThread(threadID, tName, workQueue) thread.start() threads.append(thread) threadID += 1 # 填充队列 queueLock.acquire() for word in nameList: workQueue.put(word) queueLock.release() # 等待队列清空 while not workQueue.empty(): pass # 通知线程是时候退出 exitFlag = 1 # 等待所有线程完成 for t in threads: t.join() print (\u0026#34;退出主线程\u0026#34;)   2、ThreadPoolExecutor线程池  传统多线程问题：一个线程的运行时间可以分为3部分：线程的启动时间、线程体的运行时间、线程的销毁时间。如果线程不能被重用，这就意味着每次创建都需要经过启动、运行、销毁这3个过程。这必然会增加系统响应的时间，降低效率。另外一种高效的解决方法——线程池。\n线程池：把任务放进队列中，然后开N个线程，每个线程都取队列中取一个任务，执行完了之后告诉系统我执行完了，然后接着从队列中取下一个任务，直至队列中所有任务取空，退出线程。由于线程预先被穿件并放入线程池中，同时处理完当前任务之后并不销毁而是被安排处理下一个任务，因此能够避免多次穿件线程，从而节省线程创建和销毁的开销，能带来更好的性能和系统稳定性。\n线程池设置：服务器CPU核数有限，能够同时并发的线程数有限，并不是开得越多越好。线程切换是有开销的，如果线程切换过于频繁，反而会使性能降低。假设N核服务器，通过执行业务的单线程分析出本地计算时间x，等待时间为y，则工作线程数设置为 N*(x+y)/x，能让CPU的利用率最大化。\n 从python3.2开始，标准库提供了concurrent.futures模块，它提供了ThreadPoolExecutor和ProcessPoolExecutor两个类，实现了对threading和multiprocessing的进一步抽象。\nfrom concurrent.futures import ThreadPoolExecutor, as_completed import time # 参数times用来模拟网络请求的时间 def get_html(times): time.sleep(times) print(\u0026#34;get page {}s finished\u0026#34;.format(times)) return times executor = ThreadPoolExecutor(max_workers=2) urls = [3, 2, 4] # 并不是真的url # 方法：as_completed all_task = [executor.submit(get_html, url) for url in urls] for future in as_completed(all_task): data = future.result() print(\u0026#34;in main: get page {}s success\u0026#34;.format(data)) # 执行结果 # get page 2s finished # in main: get page 2s success # get page 3s finished # in main: get page 3s success # get page 4s finished # in main: get page 4s success # 方法：map for data in executor.map(get_html, urls): print(\u0026#34;in main: get page {}s success\u0026#34;.format(data)) # 执行结果 # get page 2s finished # get page 3s finished # in main: get page 3s success # in main: get page 2s success # get page 4s finished # in main: get page 4s success ThreadPoolExecutor构造实例的时候，传入max_workers参数：来设置线程池中最多能同时运行的线程数目。\n   常用方法      submit() 用来提交线程池需要执行的任务到线程池中，并返回该任务的句柄，注意：submit不是阻塞的，而是立即返回。\n任务句柄能够使用done()方法来判断该任务是否结束。   cancel() 可以取消提交的任务。如果任务已经在线程池中运行了，就取消不了了。   result() 获取任务的返回值，这个方法内部是阻塞的。   as_completed() 判断线程池中那些任务结束了。as_completed方法是一个生成器，在没有任务完成的时候，会阻塞；在有任务完成时，会yield该任务，然后继续阻塞   map()    wait()     三、多进程 1、multiprocessing模块 multiprocessing模块是python中的多进程管理包，与thread.Thread类似，可以利用multiprocessing.Process对象来创建一个进程。该Process对象与Thread对象的用法相同，也有start()，run()，join()方法。\n  在unix平台上，在某个进程结束之后，该进程需要被其父进程调用wait，否则进程成为僵尸进程(zombie)，所以，有必要对每个Process对象调用join方法(等同于wait)。 multiprocessing模块提供了threading包没有的IPC(比如Pipe和Queue)，效率更高，应该优先考虑Pipe和Queue，避免使用Lock/Event/Semaphore/Condition等同步方式。 多进程应该避免共享资源。多线程本来就共享资源，可以方便的使用全局变量。各进程有自己的独立空间，共享资源会降低程序的效率。对于多进程，可以通过Manager方法来共享资源。   多进程： import multiprocessing q_input = multiprocessing.Queue(100) q_output = multiprocessing.Queue(100) all_task = [] for i in range(10): all_task.append(multiprocessing.Process(target=函数名, args=(形参))) for p in all_task: p.daemon = True p.start() for p in all_task: p.join() 进程池(Process Pool) 进程池可以创建多个进程，这些进程就像随时待命的士兵，准备执行任务，一个进程池中可以容纳多个待命的进程。如下：Pool创建了一个容许5个进程的进程池，每个进程都执行f函数，利用map方法将f()函数作用到表的每个元素上。\nimport multiprocessing def f(x): return x**2 pool = multiprocessing.Pool(processes=5) #-------map-------# result = pool.map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9]) print(result) #------apply_async--------# all_task = [] for i in range(1, 10): # 进程池中维持processes=5个进程 all_task.append(pool.apply_async(f, (i,))) pool.close() pool.join() # 结果 result = [] for res in all_task: result.append(res.get())    方法 说明     apply_async(func, args=()) 从进程池中取出一个进程执行func函数，args为该函数的参数，它将返回一个AsyncResult对象，可以用该对象的get()方法来获取结果。非阻塞   close() 进程池不能再创建新的进程   join() wait进程池中的全部进程，必须对Pool先调用close()方法才能join    共享内存 可以使用Value或Array将数据存储在共享内存映射中\nfrom multiprocessing import Process, Value, Array def f(n, a): n.value = 3.1415927 for i in range(len(a)): a[i] = -a[i] if __name__ == \u0026#39;__main__\u0026#39;: # d: 表示双精度浮点数据 # i: 表示有符号整数 num = Value(\u0026#39;d\u0026#39;, 0.0) arr = Array(\u0026#39;i\u0026#39;, range(10)) p = Process(target=f, args=(num, arr)) p.start() p.join() print(num.value) print(arr[:]) # 结果 # 3.1415927 # [0, -1, -2, -3, -4, -5, -6, -7, -8, -9] 服务进程 Manager() 返回的管理器对象控制一个服务进程：用来保存Python对象并允许其他进程使用代理操作他们。利用Manager()可以通过共享进程的方法共享数据。\n管理器支持的数据类型有：list、dict、Namespace、Lock、RLock、Semaphore、BoundedSemaphore、Condition、Event、Barrier、Queue、Value 和 Array\nfrom multiprocessing import Process,Manager def func1(shareList,shareValue,shareDict,lock): with lock: shareValue.value+=1 shareDict[1]=\u0026#39;1\u0026#39; shareDict[2]=\u0026#39;2\u0026#39; for i in xrange(len(shareList)): shareList[i]+=1 if __name__ == \u0026#39;__main__\u0026#39;: manager=Manager() list1=manager.list([1,2,3,4,5]) dict1=manager.dict() array1=manager.Array(\u0026#39;i\u0026#39;,range(10)) value1=manager.Value(\u0026#39;i\u0026#39;,1) lock=manager.Lock() proc=[Process(target=func1,args=(list1,value1,dict1,lock)) for i in xrange(20)] for p in proc: p.start() for p in proc: p.join() print list1 print dict1 print array1 print value1 # 结果 # [21, 22, 23, 24, 25] # {1: \u0026#39;1\u0026#39;, 2: \u0026#39;2\u0026#39;} # array(\u0026#39;i\u0026#39;, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) # Value(\u0026#39;i\u0026#39;, 21) 2、ProcessPoolExecutor模块 ProcessPoolExecutor在使用上和ThreadPoolExecutor大致一样，在futures中的方法也是相同的，但对于map()方法，ProcessPoolExecutor会对一个参数chunksize，将迭代对象切成块，将其作为分开的任务提交给pool，对于很大的iterables，设置较大的chunksize可以提高性能。\nfrom concurrent.futures import ProcessPoolExecutor, as_completed import time # 参数times用来模拟网络请求的时间 def get_html(times): time.sleep(times) print(\u0026#34;get page {}s finished\u0026#34;.format(times)) return times executor = ProcessPoolExecutor(max_workers=2) urls = [3, 2, 4] # 并不是真的url # 方法：as_completed all_task = [executor.submit(get_html, url) for url in urls] for future in as_completed(all_task): data = future.result() print(\u0026#34;in main: get page {}s success\u0026#34;.format(data)) # 执行结果 # get page 2s finished # in main: get page 2s success # get page 3s finished # in main: get page 3s success # get page 4s finished # in main: get page 4s success # 方法：map for data in executor.map(get_html, urls): print(\u0026#34;in main: get page {}s success\u0026#34;.format(data)) ","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0035_python/0050_sdk_lib/0020_multiprocessing/","summary":"一、线程与进程     进程 线程      进程：是一个应用程序在处理机上的一次执行过程，是具有一定独立功能的程序在某数据集上的一次运行，是一个动态的概念。进程是系统进行资源分配和调度的独立单位。 线程：是进程中的一个实体，是CPU调度和分派的基本单位，线程自己基本上不拥有系统资源，它与同属于一个进程内的其他线程共享进程的全部资源。   地址空间 进程有自己独立的地址空间 进程中至少有一个线程，它们共享进程的地址空间   资源 进程是资源分配和拥有的单位 进程内的多个线程共享进程的资源   调度  线程是进程内的一个执行单元，也是进程内的可调度实体，也是处理器调度的基本单位    二、多线程 1、threading模块 python主要是通过thread和threading这两个模块来实现多线程，thread模块是比较底层的模块，threading模块是对thread做了一些封装，使用更方便。但是由于GIL的存在，无法使用threading充分利用CPU资源，如果想充分发挥多核CPU的计算能力，需要使用multiprocessing模块\npython 3.x 已经摒弃了python 2.x中采用函数式thread模块来产生线程的方式。而是通过threading模块创建新的线程：\n  通过threading.Thread(Target=可执行方法)\nimport threading pro_list = [] mult_image_label_list = [] for index, img_list in enumerate(mult_image_label_list): # 创建线程 t1 = threading.Thread(target=函数名, args=(index, img_list)) pro_list.append(t1) for thread in pro_list: # 将线程设置为保护线程，否则会被无限挂起。 thread.setDaemon(True) thread.","tags":["python","并行"],"title":"并行操作"},{"categories":["Basic"],"contents":"一、异常名称    异常名称 描述     BaseException 所有异常的基类   SystemExit 解释器请求退出   KeyboardInterrupt 用户中断执行(通常是输入^C)   Exception 常规错误的基类   StopIteration 迭代器没有更多的值   GeneratorExit 生成器(generator)发生异常来通知退出   SystemExit Python 解释器请求退出   StandardError 所有的内建标准异常的基类   ArithmeticError 所有数值计算错误的基类   FloatingPointError 浮点计算错误   OverflowError 数值运算超出最大限制   ZeroDivisionError 除(或取模)零 (所有数据类型)   AssertionError 断言语句失败   AttributeError 对象没有这个属性   EOFError 没有内建输入,到达EOF 标记   EnvironmentError 操作系统错误的基类   IOError 输入/输出操作失败   OSError 操作系统错误   WindowsError 系统调用失败   ImportError 导入模块/对象失败   KeyboardInterrupt 用户中断执行(通常是输入^C)   LookupError 无效数据查询的基类   IndexError 序列中没有没有此索引(index)   KeyError 映射中没有这个键   MemoryError 内存溢出错误(对于Python 解释器不是致命的)   NameError 未声明/初始化对象 (没有属性)   UnboundLocalError 访问未初始化的本地变量   ReferenceError 弱引用(Weak reference)试图访问已经垃圾回收了的对象   RuntimeError 一般的运行时错误   NotImplementedError 尚未实现的方法   SyntaxError Python 语法错误   IndentationError 缩进错误   TabError Tab 和空格混用   SystemError 一般的解释器系统错误   TypeError 对类型无效的操作   ValueError 传入无效的参数   UnicodeError Unicode 相关的错误   UnicodeDecodeError Unicode 解码时的错误   UnicodeEncodeError Unicode 编码时错误   UnicodeTranslateError Unicode 转换时错误   Warning 警告的基类   DeprecationWarning 关于被弃用的特征的警告   FutureWarning 关于构造将来语义会有改变的警告   OverflowWarning 旧的关于自动提升为长整型(long)的警告   PendingDeprecationWarning 关于特性将会被废弃的警告   RuntimeWarning 可疑的运行时行为(runtime behavior)的警告   SyntaxWarning 可疑的语法的警告   UserWarning 用户代码生成的警告    ","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0040_error/","summary":"一、异常名称    异常名称 描述     BaseException 所有异常的基类   SystemExit 解释器请求退出   KeyboardInterrupt 用户中断执行(通常是输入^C)   Exception 常规错误的基类   StopIteration 迭代器没有更多的值   GeneratorExit 生成器(generator)发生异常来通知退出   SystemExit Python 解释器请求退出   StandardError 所有的内建标准异常的基类   ArithmeticError 所有数值计算错误的基类   FloatingPointError 浮点计算错误   OverflowError 数值运算超出最大限制   ZeroDivisionError 除(或取模)零 (所有数据类型)   AssertionError 断言语句失败   AttributeError 对象没有这个属性   EOFError 没有内建输入,到达EOF 标记   EnvironmentError 操作系统错误的基类   IOError 输入/输出操作失败   OSError 操作系统错误   WindowsError 系统调用失败   ImportError 导入模块/对象失败   KeyboardInterrupt 用户中断执行(通常是输入^C)   LookupError 无效数据查询的基类   IndexError 序列中没有没有此索引(index)   KeyError 映射中没有这个键   MemoryError 内存溢出错误(对于Python 解释器不是致命的)   NameError 未声明/初始化对象 (没有属性)   UnboundLocalError 访问未初始化的本地变量   ReferenceError 弱引用(Weak reference)试图访问已经垃圾回收了的对象   RuntimeError 一般的运行时错误   NotImplementedError 尚未实现的方法   SyntaxError Python 语法错误   IndentationError 缩进错误   TabError Tab 和空格混用   SystemError 一般的解释器系统错误   TypeError 对类型无效的操作   ValueError 传入无效的参数   UnicodeError Unicode 相关的错误   UnicodeDecodeError Unicode 解码时的错误   UnicodeEncodeError Unicode 编码时错误   UnicodeTranslateError Unicode 转换时错误   Warning 警告的基类   DeprecationWarning 关于被弃用的特征的警告   FutureWarning 关于构造将来语义会有改变的警告   OverflowWarning 旧的关于自动提升为长整型(long)的警告   PendingDeprecationWarning 关于特性将会被废弃的警告   RuntimeWarning 可疑的运行时行为(runtime behavior)的警告   SyntaxWarning 可疑的语法的警告   UserWarning 用户代码生成的警告    ","tags":["python","异常"],"title":"异常"},{"categories":["Basic"],"contents":"一、pandas import pandas as pd import pickle # 使用pandas对pickle进行操作 df = pd.DataFrame(np.arange(20).reshape(4, 5))    操作 解释     pickle.load()    pickle.dump()    pd.DataFrame()    df.to_pickle('**.pkl') to_pickle() 属性可以生成pickle文件，对数据进行永久存储   df.read_pickle('**.pkl') 从存储的pkl文件中，读取pickle数据   df.head(5) 查看前几行的数据，默认是前5行   df.tail(5) 查看后几行的数据，默认是前5行   df.values 查看DataFrame里的数据，返回是一个数组   df.iloc[k] 查看某一行的数据，   df.shape 查看行列数   df[\u0026lsquo;a\u0026rsquo;:\u0026lsquo;b\u0026rsquo;] 切片，是表示的行切片   df.loc[:, \u0026lsquo;A\u0026rsquo;:\u0026lsquo;B\u0026rsquo;] 索引，表示的是列索引   df.T 转置   df.describe() 对数据列进行描述性统计。如果列是非数值型的，不进行统计\n包括：count, mean, std, min, 25%, 50%, 75%, max   df.sum() 默认对每列求和； df.sum(1) 对每行求和   df.apply() 数乘运算。例如：df.apply(lambda x: x*2)   df**2 乘方   df[\u0026lsquo;add\u0026rsquo;] = [] 与字典一样，新增一列   df.insert(i, \u0026lsquo;add\u0026rsquo;, []) 在某列处新增   df.join(df_other, how=\u0026lsquo;inner\u0026rsquo;/\u0026lsquo;outer\u0026rsquo;) inner: 交集\n合并\nouter: 并集   pd.concat([df_1, df_2, df_3]) 合并多个   df.drop_duplicates(subset=None, keep=\u0026lsquo;first\u0026rsquo;,inplace=False) subset: 指定是那些列去重\nkeep: 去重后留下第几行\ninplace: 是否作用于原来的df    二、orc文件 1、读 import pyorc with open(\u0026#34;\u0026#34;, \u0026#34;rb\u0026#34;) as fr: reader = pyorc.Reader(fr) \n只读取选中的字段值：可以通过 column_indices 或者 column_names 参数 可实现\n reader = pyorc.Reader(fr, column_names=(\u0026quot;_col0, \u0026ldquo;_col5\u0026rdquo;)) reader = pyorc.Reader(fr, column_indices=(1,5), struct_repr=StructRepr.DICT)  起点是0 struct_repr：返回数据的格式，可以 tuple \u0026ndash;\u0026gt; dictionary       操作 解释 作用     str(reader.schema) struct\u0026lt;_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string\u0026gt; 获取表的字段/类型   reader.read(N) 读取N行结果，返回一个[(), \u0026hellip;, ()] 获取N行结果   next(reader)  获取下一行   reader.seek(k) 读取数据时，跳过k行 跳过k行   reader.num_of_stripes orc文件被分为很多条，每条都是相互独立的 读取条数   reader.read_stripe(k)  读取第k条    2、写 import pyorc with open(\u0026#34;\u0026#34;, \u0026#34;wb\u0026#34;) as fw: writer = pyorc.Writer(fw, \u0026#34;struct\u0026lt;col0:int, col1:string\u0026gt;\u0026#34;) writer.write((0, \u0026#34;test_0\u0026#34;))    操作 解释     tuple格式写入 writer = pyorc.Writer(fw, \u0026quot;struct\u0026lt;col0:int, col1:string\u0026gt;\u0026quot;)\nwriter.write((0, \u0026ldquo;test_0\u0026rdquo;))   dict格式写入 writer = pyorc.Writer(fw, \u0026quot;struct\u0026lt;col0:int,col1:string\u0026gt;\u0026quot;, struct_repr=StructRepr.DICT)\nwriter.write({\u0026ldquo;col0\u0026rdquo;: 0, \u0026ldquo;col1\u0026rdquo;: \u0026ldquo;test_0\u0026rdquo;})    三、xlsx文件 import xlrd # 读文件 xls_file = xlrd.open_workbook(\u0026#39;\u0026#39;) # 打开工作簿, num: 为第num个工作簿 xls_sheet = xls_file.sheets()[num-1] # 读取行/列数据（整行/整列） row_value = xls_sheet.row_values(num-1) col_value = xls_sheet.col_values(num-1) # 读取某行某列元素 value = xls_sheet.cell(row_num, col_num).value ","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0050_file/","summary":"一、pandas import pandas as pd import pickle # 使用pandas对pickle进行操作 df = pd.DataFrame(np.arange(20).reshape(4, 5))    操作 解释     pickle.load()    pickle.dump()    pd.DataFrame()    df.to_pickle('**.pkl') to_pickle() 属性可以生成pickle文件，对数据进行永久存储   df.read_pickle('**.pkl') 从存储的pkl文件中，读取pickle数据   df.head(5) 查看前几行的数据，默认是前5行   df.tail(5) 查看后几行的数据，默认是前5行   df.values 查看DataFrame里的数据，返回是一个数组   df.iloc[k] 查看某一行的数据，   df.shape 查看行列数   df[\u0026lsquo;a\u0026rsquo;:\u0026lsquo;b\u0026rsquo;] 切片，是表示的行切片   df.loc[:, \u0026lsquo;A\u0026rsquo;:\u0026lsquo;B\u0026rsquo;] 索引，表示的是列索引   df.","tags":["python","文件"],"title":"文件读取"},{"categories":["Basic"],"contents":"一、tf.layers tf.layers模块在TensorFlow2.0中已经被完全移除了，用tf.keras.layers定义层是新的标准。\n二、tf.losses tf.losses模块包含了经常使用的、能够实现独热编码的损失函数。\n三、tf.train 1. Optimizer TensorFlow提供的优化器\n   优化器 功能     tf.train.Optimizer    tf.train.GradientDescentOptimizer    tf.train.AdadeltaOptimizer    tf.train.AdagtadOptimizer    tf.train.AdagradDAOptimizer    tf.train.MomentumOptimizer    tf.train.AdamOptimizer    tf.train.FtrlOptimizer    tf.train.ProximalGradientDescentOptimizer    tf.train.ProximalAdagradOptimizer    tf.train.RMSProOptimizer     Optimizer类与其子类的继承关系：\n def minimize(self, loss, # 损失值， tensor # 全局训练步数，随着模型迭代优化自增， variable global_step=None, # 待训练模型参数的列表， list var_list=None, # 计算梯度和更新参数模型时的并行化程度，可选值GATE_OP,GATE_NONE,GATE_GRAPH # GATE_NONE 无同步，最大化并行执行效率，将梯度计算和模型参数更新完全并行化。 # GATE_OP，操作级同步，对于每个操作，分别确保所有梯度在使用前都计算完成。 # GATE_GRAPH，图级同步，最小化并行执行效率，确保所有模型参数的梯度计算完成。 gate_gradients=GATE_OP, # 聚集梯度值的方法， Enum aggregation_methed=None, # 是否将梯度计算放置到对应操作所在同一个设备，默认否，Boolean colocate_gradients_with_ops=False, # 优化器在数据流图中的名称，string nmae=None, # 损失值的梯度  grad_loss=None)       属性 功能介绍     _name 表示优化器的名称   _use_locking 表示是否在并发更新模型参数时加锁   minimize 最小化损失函数，该方法会依次调用compute_gradients和apply_gradients   compute_gradients 计算模型所有参数的梯度值,返回\u0026lt;梯度，响应参数\u0026gt;的键值对列表   apply_gradients 将梯度值更新到对应的模型参数，优化器的apply_gradients成员方法内部会调用tf.assign，tf.assign_add,tf.assign_sub方法完成模型参数的更新。    自定义优化器\n 分为三步骤：\n 计算梯度：调用compute_gradients方法，依据指定的策略求得梯度值。 处理梯度：用户按照自己的需求处理梯度值，例如：进行梯度裁剪和梯度加权 应用梯度：调用apply_gradients方法，将处理后的梯度值应用到模型参数，实现模型更新。   def define_optimizer(learning_rate,loss): # 定义优化器 optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=0.5) # 计算梯度 grads_and_vars=optimizer.compute_gradients(loss=loss) # 处理梯度 for i,(g,v) in enumerate(grads_and_vars): if g is not None: grads_and_vars[i]=(tf.clip_by_norm(g,5),v) # 应用梯度 return optimizer.apply_gradients(grads_and_vars) 2. Saver 保存模型参数很重要，训练中断后，可以根据保存的参数继续迭代。Saver 是TensorFlow Python API提供的、能够保存当前模型变量的对象，Saver对象：只能保存变量，不能保存图结构，所以更常用于训练迭代过程，防止中断重启。 SavedModel对象：可以同时保存图结构和变量，所以Saved Model对象与(将训练好的模型用在生产中)的行为联系更紧密。\ntf.train.Saver()\n四、tf.summary 可以记录数据流图、直方图、标量值、分布、日志图和其他多种数据类型。\n   操作 解释 功能     tf.summary.scalar() 例如：tf.summary.scalar(\u0026lsquo;loss\u0026rsquo;, loss) 记录标量值   tf.summary.Filewirter() 可以关联不同的路径，这样可以可视化不同阶段的数据情况     ","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0040_tf/w0010_compat/0020_tf_compat_train/","summary":"一、tf.layers tf.layers模块在TensorFlow2.0中已经被完全移除了，用tf.keras.layers定义层是新的标准。\n二、tf.losses tf.losses模块包含了经常使用的、能够实现独热编码的损失函数。\n三、tf.train 1. Optimizer TensorFlow提供的优化器\n   优化器 功能     tf.train.Optimizer    tf.train.GradientDescentOptimizer    tf.train.AdadeltaOptimizer    tf.train.AdagtadOptimizer    tf.train.AdagradDAOptimizer    tf.train.MomentumOptimizer    tf.train.AdamOptimizer    tf.train.FtrlOptimizer    tf.train.ProximalGradientDescentOptimizer    tf.train.ProximalAdagradOptimizer    tf.train.RMSProOptimizer     Optimizer类与其子类的继承关系：\n def minimize(self, loss, # 损失值， tensor # 全局训练步数，随着模型迭代优化自增， variable global_step=None, # 待训练模型参数的列表， list var_list=None, # 计算梯度和更新参数模型时的并行化程度，可选值GATE_OP,GATE_NONE,GATE_GRAPH # GATE_NONE 无同步，最大化并行执行效率，将梯度计算和模型参数更新完全并行化。 # GATE_OP，操作级同步，对于每个操作，分别确保所有梯度在使用前都计算完成。 # GATE_GRAPH，图级同步，最小化并行执行效率，确保所有模型参数的梯度计算完成。 gate_gradients=GATE_OP, # 聚集梯度值的方法， Enum aggregation_methed=None, # 是否将梯度计算放置到对应操作所在同一个设备，默认否，Boolean colocate_gradients_with_ops=False, # 优化器在数据流图中的名称，string nmae=None, # 损失值的梯度  grad_loss=None)       属性 功能介绍     _name 表示优化器的名称   _use_locking 表示是否在并发更新模型参数时加锁   minimize 最小化损失函数，该方法会依次调用compute_gradients和apply_gradients   compute_gradients 计算模型所有参数的梯度值,返回\u0026lt;梯度，响应参数\u0026gt;的键值对列表   apply_gradients 将梯度值更新到对应的模型参数，优化器的apply_gradients成员方法内部会调用tf.","tags":["TF","训练"],"title":"模型训练"},{"categories":["Basic"],"contents":"一、环境变量 1、临时环境变量    操作 说明     os.environ['WORKON_HOME']=\u0026quot;变量\u0026quot; 设置环境变量   os.environ['CUDA_VISIBLE_DEVICES']=\u0026quot;1\u0026quot; 设置显卡设备   os.environ.get('WORKON_HOME') 获取环境变量-方法1   os.getenv('path') 获取环境变量-方法2-推荐   del os.environ['WORKON_HOME'] 删除环境变量       os.environ['HOMEPATH'] 当前用户主目录   os.environ['TEMP'] 临时目录路径   os.environ['PATHEXT'] 可以执行文件   os.environ['SYSTEMROOT'] 系统主目录   os.environ['LOGONSERVER'] 机器名   os.environ['PROMPT'] 设置提示符    2、永久环境变量    操作 说明 功能     path = r\u0026quot;路径\u0026quot;\ncommand = r\u0026quot;setx WORK1 %s /m\u0026quot;%path\nos.system() /m 表示系统变量，不加/m表示用户变量     3、内部变量    操作 说明     __doc__ 获取文件的注释   __file__ 获取当前文件的路径   __name__ 获取导入文件的路径加文件名称。当前文件，其值为__main__\nimport 一个模块或者一个函数时，在该模块文件中__name__的值是模块名\n执行一个模块时，__name__ 的值是__main__   __package__ 获取导入文件的路径。当前文件，其值为 None   __cached__    __builtins__ 内置函数在这里    实例：获取该执行文件的绝对路径：os.path.dirname(os.path.abspath(__file__))\n二、yield 带有yield函数在python中被称为generator。以菲波那切数列为例，介绍一下，yield的功能：\n 输出菲波那切数列list\n缺点：返回list，运行时占用的内存随着参数max的增大而增大，如果要控制内存，最好不要用list来存储中间结果，而是通过iterable对象来迭代。  def fab(max): n, a, b = 0, 0, 1 L = [] while n \u0026lt; max: L.append(b) a, b = b, a+b n = n + 1 return L iterable 的方法实现：通过next()函数不断返回数列的下一个数，内存占用始终未常数。\n缺点：不够简洁  class Fab(object): def __init__(self, max): self.max = max self.n, self.a, self.b = 0, 0, 1 def __iter__(self): return self def next(self): if self.n \u0026lt; self.max: r = self.b self.a, self.b = self.b, self.a+self.b self.n = self.n+1 return r raise StopIteration() # 调用 for n in Fab(5): print(n) 使用yield：yield把一个函数变成一个generator，调用fab()函数时不会执行该函数，而是返回一个iterable对象。在for循环执行时，每次循环都会执行fab函数内部的代码。  def fab(max): n, a, b = 0, 0, 1 while n \u0026lt; max: yield b a, b = b, a+b n = n+1 # 调用 for n in fab(5): print(n) 三、闭包 # 定义 def line_conf(a, b): def line(x): return a*x+b return line # 定义两条直线 line_a = line_conf(2,1) # y=2x+1 line_b = line_conf(3,2) # y=3x+2 print(line_conf().__closure__) # 闭包函数的属性   闭包函数的必要条件\n 闭包函数(例如：line_conf())，必须返回一个函数对象 闭包函数返回的函数对象(例如：line())，必须引用外部变量(一般不能是全局变量)，而返回的那个函数对象(例如：line())内部不一定要return    作用域分析\n 函数的作用域是由def关键词界定的，函数内的代码访问变量的方式是：从其所在层级由内向外寻找 函数属性：闭包函数将函数的唯一实例保存在它内部的__closure__属性中，在再次创建函数实例时，闭包检查函数实例已存在自己的属性中，不会再让它创建新的实例，而是将现有的实例返回。    四、装饰器  实例  # 定义 def a_new_decorator(a_func): def wrapTheFunction(): print(\u0026#39;I am doing some boring work before executing a_func()\u0026#39;) a_func() print(\u0026#39;I am doing some boring work after executing a_func()\u0026#39;) return wrapTheFunction def a_function_requiring_decoration(): print(\u0026#39;I am the function which needs some decoration to remove my foul smell.\u0026#39;) # 调用 a_function_requiring_decoration() # 结果：I am the function which needs some decoration to remove my foul smell. a_function_requiring_decoration = a_new_decorator(a_function_requiring_decoration) a_function_requiring_decoration() # 结果： # I am doing some boring work before executing a_func() # I am the function which needs some decoration to remove my foul smell. # I am doing some boring work after executing a_func() 用@简化代码\n缺点：print(a_function_requiring_decoration.name) 返回的是装饰器: wrapTheFunction  # 定义 def a_new_decorator(a_func): def wrapTheFunction(): print(\u0026#39;I am doing some boring work before executing a_func()\u0026#39;) a_func() print(\u0026#39;I am doing some boring work after executing a_func()\u0026#39;) return wrapTheFunction @a_new_decorator def a_function_requiring_decoration(): print(\u0026#39;I am the function which needs some decoration to remove my foul smell.\u0026#39;) # 调用 a_function_requiring_decoration() # 结果： # I am doing some boring work before executing a_func() # I am the function which needs some decoration to remove my foul smell. # I am doing some boring work after executing a_func() @蓝本 可以利用@wraps接受一个函数进行装饰，并加入复制函数名称、注释文档、参数列表等功能。  # 定义 from functools import wraps def a_new_decorator(a_func): @wraps(a_func) def wrapTheFunction(): print(\u0026#39;I am doing some boring work before executing a_func()\u0026#39;) a_func() print(\u0026#39;I am doing some boring work after executing a_func()\u0026#39;) return wrapTheFunction @a_new_decorator def a_function_requiring_decoration(): print(\u0026#39;I am the function which needs some decoration to remove my foul smell.\u0026#39;) # 调用 a_function_requiring_decoration() # 结果： # I am doing some boring work before executing a_func() # I am the function which needs some decoration to remove my foul smell. # I am doing some boring work after executing a_func() print(a_function_requiring_decoration.__name__) # 结果：a_function_requiring_decoration 五、内置函数   eval('字符串')：把字符串作为语句执行 作用：解析并执行字符串，并将返回结果输出。eval()函数将去掉字符串的两个引号，将其解释为一个变量。\n 1）单引号，双引号，eval()函数都将其解释为int类型；eval(\u0026lsquo;100\u0026rsquo;)，输出的是int类型。 2）三引号则解释为str类型。eval('\u0026ldquo;hello\u0026rdquo;')，输出的是字符串    input() : 键盘输入 作用：接收键盘的输入，返回的是字符串类型。 input和eval函数结合使用：\n 1）从键盘输入，接收一个字符串类型： a = input(\u0026lsquo;请输入一个字符串：') 2）从键盘输入，接收一个整型： a = eval(input(\u0026lsquo;请输入一个数字：'))    lambda 匿名函数\n格式：lambda[arg1[,arg2,\u0026hellip;,argN]] : 表达式 例如：test = lambda x, y: x+y\n  sorted 排序 b=sorted(a.items(), key=lambda item:item[0], reverse = True)\n a.items() 表示可迭代的tuple列表 key=lambda item:item[0]：按照key值排序; lambda x:x[0] reverse = True：降序排序    六、内置模块-os    操作 解释     os.path.basename()    os.path.dirname()    os.path.join()    os.path.exists() 判断该路径是否存在   os.path.isfile() 判断是不是文件   os.path.isdir() 判断是不是目录   os.path.abspath() 获取绝对路径\n例如：os.path.abspath(file)获取当前文件的绝对路径   os.listdir() 遍历该目录下的文件，返回文件名列表   os.walk() 遍历目录，返回一个三元组(root,dirs,files)\nroot: 指的是当前正在遍历的文件夹本身的目录\ndirs: 是一个list，内容是该文件夹中所有的目录的名字(不包含子目录)\nfiles: 是一个list，内容是该文件夹中所有的文件(不包括子目录)   os.makedirs 创建一个目录   os.remove 删除一个目录   os.environ 环境变量\n例如：获取环境变量：os.environ.get(\u0026lsquo;环境变量名\u0026rsquo;, \u0026lsquo;默认值\u0026rsquo;)    ","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0035_python/0020_internal_lib/0030_advance_operator/","summary":"一、环境变量 1、临时环境变量    操作 说明     os.environ['WORKON_HOME']=\u0026quot;变量\u0026quot; 设置环境变量   os.environ['CUDA_VISIBLE_DEVICES']=\u0026quot;1\u0026quot; 设置显卡设备   os.environ.get('WORKON_HOME') 获取环境变量-方法1   os.getenv('path') 获取环境变量-方法2-推荐   del os.environ['WORKON_HOME'] 删除环境变量       os.environ['HOMEPATH'] 当前用户主目录   os.environ['TEMP'] 临时目录路径   os.environ['PATHEXT'] 可以执行文件   os.environ['SYSTEMROOT'] 系统主目录   os.environ['LOGONSERVER'] 机器名   os.environ['PROMPT'] 设置提示符    2、永久环境变量    操作 说明 功能     path = r\u0026quot;路径\u0026quot;","tags":["python","进阶操作"],"title":"进阶操作"},{"categories":["Basic"],"contents":"在TensorFlow 2中使用兼容性模块，必须使用tf.compat.v1替换tf，并且在导入TensorFlow软件包后添加一行tf.compat.v1.disable_eager_execution()函数来关闭eager执行模式。\nimport tensorflow as tf tf.compat.v1.disable_eager_execution() 简介 数据流是一种编程模型，被广泛地应用于并行计算中。TF使用数据流图来表示计算中各个运算之间的关系，在数据流图中，节点：表示计算单元(即：操作tf.Operation)；边：表示被计算单元消费/生产的数据(即：tf.Tensor)。 数据流图，可以被导出成一个可移植的、编程语言不相关的表示(ProtoBuf)，这种表示可以被其他语言使用，来创建一个图并在会话中使用它。\ndef graph_demo(): a = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=tf.float32) b = tf.constant([[10, 0, 0], [0, 0.5, 0], [0, 0, 2]]) c = tf.constant([[1, -1, 3]], dtype=tf.float32) y = tf.add(tf.matmul(a, b), c, name=\u0026#39;result\u0026#39;) writer = tf.summary.FileWriter(os.path.join(root_dir, \u0026#39;log/matmul\u0026#39;), tf.get_default_graph()) writer.close() return y # 在终端启动TensorBoard对图进行可视化 tensorboard --logdir log/matmul  上例中创建一个数据流图，然后用TensorBoard对这个图进行可视化。\n tf.summary.FileWriter 创建了一个tf.summary.SummaryWriter来保存一个图像化表示，这个writer对象创建时，初始化参数包括：a.该图像化表示的存储路径；b.一个tf.Graph对象，可以使用tf.get_default_graph函数返回默认图 tf.get_default_graph 函数，返回默认图。   在执行时，调用TF API创建数据流图，这个阶段并没有进行计算。\n1、图-tf.Graph TF是一个C++库，我们只是用python来用简单的方式来构造数据流图，python简化了数据流图的描述阶段，因为它无须特意显示定义一个图，而是会默认一个tf.Graph。\n图的定义:\n 隐式定义：在我们用tf.*搭建一个图时，TensorFlow总是会定义一个默认的tf.Graph，可以通过tf.get_default_graph访问。隐式定义限制了TF的表示能力，因为它被限制只能使用一个图。 显式定义：可以显式地定义一个计算图，因此每个应用可以有多个图。这种方式的表现能力更强，但并不常用，因为需要多个图的应用不常见。 TF通过tf.Graph()创建一个tf.Graph对象，并通过as_default方法创建一个上下文管理器，每个上下文中定义的运算都被放进相应的图中。实际上，tf.Graph()对象定义了一个它所包含的tf.Operation对象的命名空间。 import tensorflow as tf def graph_define(): g1 = tf.Graph() g2 = tf.Graph() with g1.as_default(): a = tf.constant([[1, 2, 3], [3, 4, 6], [7, 8, 9]], dtype=tf.float32) b = tf.constant([[9, 0, 0], [0, 1, 0], [0, 0, 0.5]]) c = tf.constant([[1, -1, 3]], dtype=tf.float32) y = tf.add(tf.matmul(a, b), c, name=\u0026#39;result\u0026#39;) with g2.as_default(): with tf.name_scope(\u0026#39;scope_2\u0026#39;): x = tf.constant(1, name=\u0026#39;x\u0026#39;) print(x) # Tensor(\u0026#34;scope_2/x:0\u0026#34;, shape=(), dtype=int32) with tf.name_scope(\u0026#39;scope_3\u0026#39;): x = tf.constant(10, name=\u0026#39;x\u0026#39;) print(x) # Tensor(\u0026#34;scope_3/x:0\u0026#34;, shape=(), dtype=int32) y = tf.constant(12) z = x*y writer = tf.summary.FileWriter(os.path.join(root_dir, \u0026#39;log/two_graphs/g1\u0026#39;), g1) writer = tf.summary.FileWriter(os.path.join(root_dir, \u0026#39;log/two_graphs/g2\u0026#39;), g2) writer.close()   图的集合：\n每个tf.Graph，用集合机制 来存储与图结构相关的元数据，一个集合由一个键值唯一标识，其内容是一个对象/运算的列表。使用者通常不需要关注集合是否存在，因为它们是TF为了正确定义一个图所使用的。\n图中节点名：\n 后缀：图中每个节点的名字都是唯一的，如果有重复，为了避免重复，TF会添加:id形式的后缀。 在定义时如果没有指定节点的name，TF就会用Operation（操作）的名字来命名，输出的tf.Tensor和其相关的tf.Operation名字相同，只是可能会加上后缀。 前缀：可以通过tf.name_scope函数定义一个上下文，为该上下文中所有的运算添加命名范围前缀。  图中的计算：\n 作为一个C++库，TF数据类型是严格的静态类型，这意味着在图定义阶段必须知道每个运算/张量的类型，且参与运算的数据类型必须相同。 可以使用运算符重载，来简化一些常用的数学运算。运算符重载使得图定义更便捷，并且与tf.*的API调用完全等价，只是有一点：不能给运算指定名字。 y = tf.add(tf.matmul(A, x), b, name=\u0026#39;result\u0026#39;) # 等价 y = A @ x + b    运算符 操作名 运算符 操作名 运算符 操作名 运算符 操作名     __neg__ unary - __abs__ abs() __invert__ unary ~ __add__ binary +   __sub__ binary - __mul__ binary 元素* __floordiv__ binary // __truediv__ binary /   __mod__ binary % __pow__ binary ** __and__ binary \u0026amp; __or__ binary |   __xor__ binary ^ __le__ binary \u0026lt; __lt__ binary \u0026lt;= __gt__ binary \u0026gt;   __ge__ binary \u0026gt;= __matmul__ binary @            2、图放置-tf.device tf.device创建一个和设备相符的上下文管理器，这个函数运行使用者将同一个上下文下的所有运算放置在相同的设备上。tf.device指定的设备不仅仅是物理设备，它能指定远程服务器、远程设备、远程工作者、不同种类的物理设备(GPU、CPU、TPU)。\n 格式：/job:\u0026lt;JOB_NAME\u0026gt;/task:\u0026lt;TASK_INDEX\u0026gt;/device:\u0026lt;DEVICE_TYPE\u0026gt;:\u0026lt;DEVICE_INDEX\u0026gt;\n \u0026lt;JOB_NAME\u0026gt;：是一个由字母和数字构成的字符串，首个字符不能是数字 \u0026lt;TASK_INDEX\u0026gt;：是一个非负整数，代表在名为\u0026lt;JOB_NAME\u0026gt;的工作中的任务编号 \u0026lt;DEVICE_TYPE\u0026gt;：是一个已经注册过的设备类型(CPU或者GPU) \u0026lt;DEVICE_INDEX\u0026gt;：是一个非负整数，代表设备的索引号   def device_demo(): with tf.device(\u0026#39;/CPU:0\u0026#39;): a = tf.constant([[1, 2, 3], [3, 4, 6], [7, 8, 9]], dtype=tf.float32) b = tf.constant([[9, 0, 0], [0, 1, 0], [0, 0, 0.5]]) c = tf.constant([[1, -1, 3]], dtype=tf.float32) with tf.device(\u0026#39;/GPU:0\u0026#39;): mul = tf.matmul(a, b, name=\u0026#39;mul_result\u0026#39;) y = tf.add(mul, c, name=\u0026#39;add_result\u0026#39;) writer = tf.summary.FileWriter(os.path.join(root_dir, \u0026#39;log/device\u0026#39;), tf.get_default_graph()) writer.close() 3、图执行-tf.Session 静态图，图的定义与执行完全分离，在eager执行模式中不是这样。tf.Session：是一个TF提供的类，用来表示Python程序与C++运算库之间的联系，是唯一能直接与硬件通信、将运算放置到指定的设备上、使用本地和分布式TF运行库的类。它的主要目的：根据定义的图，具体地实现各个计算。 tf.Session对象是高度优化过的，一旦被正确构建，它会将tf.Graph缓存起来以加速其执行，tf.Session作为物理资源的拥有者，必须以一个文件描述符的方式来做下面的工作：\n 通过创建tf.Session来获取资源（等价于open操作系统调用） 使用这些资源（等价于在文件描述符上使用 读/写 操作） 使用tf.Session.close释放资源（通常会使用一个上下文管理器，不需要手动销毁释放资源）  1). tf.Session的三个参数 Session(target='', graph=None, config=None)\n  target：配置执行引擎 常见的场景：\n  使用当前的本地的硬件来执行图\nwith tf.Session() as sess: # 使用session去执行 某些操作 sess.run(...)   一些更复杂的场景：使用一个远程TensorFlow服务器，可以通过使用服务器的url(grpc://)来指定tf.Session的target参数\n# TensorFlow服务器的 ip和port ip = \u0026#39;192.168.1.90\u0026#39; port = \u0026#39;9877\u0026#39; with tf.Session(target=f\u0026#39;grpc://{ip}:{port}\u0026#39;) as sess: sess.run(...)     graph: 指定需要使用的图。tf.Session会使用默认的图对象，在需要运算多个图时，可以指定需要使用的图。tf.Session对象每次只能处理一个图。\n  config: 硬件/网络配置，这个配置通过tf.ConfigProto对象来指定，用来控制Session的行为。tf.ConfigProto比较复杂，选项也比较多，最常用的选项有下面两个：\n allow_soft_placement：当为True时：启动软设备安排，即：不是所有运算都会按照图定义的那样，被安排在指定的设备上。这是为了防止这种情况：比如GPU不存在，或者原来存在现在出了些问题，TensorFlow没有检测到该设备，就可以把指定给这个设备的运算，安排到其他正确的设备上。 gpu_options.allow_growth：当为True时：会改变GPU显存分配器的工作方式。分配器默认的工作方式：tf.Session被创建时就会分配所有可用的GPU显存。当allow_growth=True时，分配器会以逐步递增的方式分配显存。这是为了适应这种情况：在研究环境下，GPU资源是共享的，当一个tf.Session执行时，不能占用所有资源，其他人也还在使用。 per_process_gpu_memory_fraction：手动限定显存的使用量 log_device_placement：当为True时，会获取Operations和Tensor被指派到的设备号，在终端会打印出各个操作是在那些设备上运行的。  config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True) config.gpu_options.allow_growth=True config.gpu_options.per_process_gpu_memory_fraction = 0.4 #占用40%显存 with tf.Session(config=config) as sess: # 使用session去执行 某些操作 sess.run(...)   2). sess.run() sess.run(y)的工作方式如下：\n y是一个运算的输出节点，回溯y的输入 递归的回溯所有节点，直到无法找到父节点 评估输入 跟踪依赖图：分析各个节点的关系 执行计算  feed_dict：可以把外部的数据，注入计算图中，相当于重写计算图里的某个值。跟tf.placeholder配合使用，完成外部的数据流入计算图。 tf.placeholder：重写运算符。其创建的目的就是：当外面的值没有注入图中时，就会抛出一个错误。\ndef session_demo(): a = tf.constant([[1, 2, 3], [3, 4, 6], [7, 8, 9]], dtype=tf.float32) b = tf.constant([[9, 0, 0], [0, 1, 0], [0, 0, 0.5]]) c = tf.constant([[1, -1, 3]], dtype=tf.float32) y = tf.add(tf.matmul(a, b), c, name=\u0026#39;result\u0026#39;) with tf.Session() as sess: a_value, b_value, c_value = sess.run([a, b, c]) y_value = sess.run(y) # 重写 y_new = sess.run(y, feed_dict={c: np.zeros((1, 3))}) print(f\u0026#39;a: {a_value}\\nb: {b_value}\\nc: {c_value}\\ny: {y_value}\u0026#39;) print(f\u0026#39;y_new: {y_new}\u0026#39;) 4、图中的变量 一个变量是一个tf.Variable对象，用于维护图的状态，作为图中其他节点的输入。tf.Tensor和tf.Variable对象可以用相同的方式使用，不过tf.Variable拥有更多的属性：\n 一个变量必须要被初始化 一个变量默认被加到全局变量和可训练变量集合中  1. 变量声明 声明变量的两种方式：需要(type, shape)\n  tf.Variable：是一个类，创建一个变量，同时它需要指定一个初始值。\n变量的赋值，可以使用assign函数：比如：w.assign(w+0.1)等价于w.assign_add(0.1)。其实，变量的初始化操作，就是把初始值assign给每个变量。\n tf.Variable是一个类\n__init__( initial_value=None,\ntrainable=True, # 是否可训练\ncollections=None,\nvalidate_shape=True,\ncaching_device=None,\nname=None,\nvariable_def=None,\ndtype=None,\nexpected_shape=None,\nimport_scope=None,\nconstraint=None)\n size_in = 100 size_out = 100 # w的初始值是有tf.truncated_normal运算产生，服从正太分布N(0, 0.1) w = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.1), name=\u0026#39;w\u0026#39;) # b的初始值是有tf.constant运算产生的常量来初始化 b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\u0026#39;b\u0026#39;) with tf.Session() as sess: # 变量的初始化 sess.run(w.initializer)   tf.get_variable：更复杂，但拥有更强的表现能力。例如：如果我们需要变量共享，就不能使用tf.Variable定义，只能使用tf.get_variable。tf.get_variable和tf.variable_scope一起使用，通过它的reuse参数，实现tf.get_variable的变量共享能力。其中，tf.get_variable不受tf.name_scope的影响。 TensorFlow提供的tf.layers模块，包含了几乎所有常用的层，这些层内部都是用tf.get_variable来定义的，因此，这些层可以和tf.variable_scope一起使用来共享它们的变量。\n tf.get_variable是一个函数：\n(\nname,\nshape=None,\ndtype=None,\ninitializer=None,\nregularizer=None,\ntrainable=True,\ncollections=None,\ncaching_device=None,\npartitioner=None,\nvalidate_shape=None,\nuse_resource=None,\ncustom_getter=None,\nconstraint=None\n)\n with tf.variable_scope(\u0026#39;scope\u0026#39;): a = tf.get_variable(\u0026#39;v\u0026#39;, [1]) with tf.variable_scope(\u0026#39;scope\u0026#39;, reuse=True): b = tf.get_variable(\u0026#39;v\u0026#39;, [1]) print(a.name, b.name) # scope/v:0 scope/v:0   2. 变量初始化 TensorFlow变量随机初始化，例如：w = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.1), name='w') 常见的随机函数：\n   操作 功能     tf.random_normal() 正态分布，参数:(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)   tf.truncated_normal 正态分布，但如果随机出来的值偏离平均值超过了2个标准差，那么这个数将会被重新随机, 参数如上   tf.random_uniform 平均分布，参数：([m, n], 最小值, 最大值, 取值类型)   tf.random.gamma Gamma分布，参数：([m, n], 形状参数 $\\alpha$，尺度参数 $\\beta$, 取数类型)   常数函数    tf.zeros() 参数：(shape, dtype=tf.float32, name=None)， shape的格式: [m, n]   tf.ones() 参数：(shape, dtype=tf.float32, name=None), shape的格式: [m, n]   tf.fill() 参数：(shape, value, name=None), shape的格式: [m, n]   tf.constant() 参数：(value, dtype=None, shape=None, name=\u0026lsquo;Const\u0026rsquo;, verify_shape=False)例如：tf.constant([1, 2, 3, 4, 5, 6, 7]) =\u0026gt; [1 2 3 4 5 6 7]tensor = tf.constant(-1.0, shape=[2, 3]) =\u0026gt; [[-1. -1. -1.],[-1. -1. -1.]]   tf.range() 参数：tf.range(start, limit, delta)   tf.linspace() 参数：(start, stop, num) 功能： (stop - start)/(num - 1)      传入初始值\n 在session中执行时，变量必须要初始化：\na. 全部变量初始化： tf.global_variables_initializers():其实内部实现：=tf.variabels_initializer(tf.global_variables())\nb. 部分变量初始化：tf.variables_initializer([变量])\nc. 检查变量是否初始化成功：tf.is_variable_initialized：检查变量是否初始化；tf.report_uninitialized_variables：获取未初始化的变量集合；tf.assert_variables_initialized：断言变量已经初始化。\n with tf.Session() as sess: sess.run(tf.global_variable_initializer()) # 初始化所有变量   从checkpoint文件中恢复变量的值\n当我们创建Saver实例时，它的构造方法会向当前的数据流图中添加一对操作：SaveOp和RestoreOp\n  SaveOp负责向checkpoint文件中写入变量\nsaver = tf.train.Saver() saver.save(sess, \u0026#39;/tmp/summary/test.ckpt\u0026#39;)   RestoreOp负责从checkpoint文件中恢复变量\nsaver = tf.train.Saver() saver.restore(sess, \u0026#39;/tmp/summary/test.ckpt\u0026#39;)     3. 变量的访问  通过在tf.global_variable()变量表中，根据变量名进行匹配查找  x = tf.Variable(1,name=\u0026#39;x\u0026#39;) y = tf.get_variable(name=\u0026#39;y\u0026#39;,shape=[1,2]) for var in tf.global_variables(): #返回全部变量列表 if var.name == \u0026#39;x:0\u0026#39;: print(var) 利用tf.get_tensor_by_name，在图中根据name查找  import tensorflow as tf x = tf.Variable(1,name=\u0026#39;x\u0026#39;) y = tf.get_variable(name=\u0026#39;y\u0026#39;,shape=[1,2]) graph = tf.get_default_graph() x1 = graph.get_tensor_by_name(\u0026#34;x:0\u0026#34;) y1 = graph.get_tensor_by_name(\u0026#34;y:0\u0026#34;) ","date":"September 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0040_tf/w0010_compat/0010_tf_compat_summary/","summary":"在TensorFlow 2中使用兼容性模块，必须使用tf.compat.v1替换tf，并且在导入TensorFlow软件包后添加一行tf.compat.v1.disable_eager_execution()函数来关闭eager执行模式。\nimport tensorflow as tf tf.compat.v1.disable_eager_execution() 简介 数据流是一种编程模型，被广泛地应用于并行计算中。TF使用数据流图来表示计算中各个运算之间的关系，在数据流图中，节点：表示计算单元(即：操作tf.Operation)；边：表示被计算单元消费/生产的数据(即：tf.Tensor)。 数据流图，可以被导出成一个可移植的、编程语言不相关的表示(ProtoBuf)，这种表示可以被其他语言使用，来创建一个图并在会话中使用它。\ndef graph_demo(): a = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=tf.float32) b = tf.constant([[10, 0, 0], [0, 0.5, 0], [0, 0, 2]]) c = tf.constant([[1, -1, 3]], dtype=tf.float32) y = tf.add(tf.matmul(a, b), c, name=\u0026#39;result\u0026#39;) writer = tf.summary.FileWriter(os.path.join(root_dir, \u0026#39;log/matmul\u0026#39;), tf.get_default_graph()) writer.close() return y # 在终端启动TensorBoard对图进行可视化 tensorboard --logdir log/matmul  上例中创建一个数据流图，然后用TensorBoard对这个图进行可视化。\n tf.summary.FileWriter 创建了一个tf.summary.SummaryWriter来保存一个图像化表示，这个writer对象创建时，初始化参数包括：a.该图像化表示的存储路径；b.一个tf.Graph对象，可以使用tf.get_default_graph函数返回默认图 tf.get_default_graph 函数，返回默认图。   在执行时，调用TF API创建数据流图，这个阶段并没有进行计算。","tags":["TF","静态图"],"title":"静态图"},{"categories":["Basic"],"contents":"一、Normlization介绍 一般而言，样本特征由于来源及度量单位不同，其尺度往往差异很大。如果尺度差异很大，神经网络就比较难训练。为了提高训练效率，对输入特征做归一化，把不同的尺度压缩到一定范围内，尺度统一后，大部分位置的梯度方向近似于最优解搜索方向。这样，在用梯度下降法进行求解时，每一步梯度的方向都基本上指向最小值，训练效率会大大提高。\n 归一化：泛指把数据特征转换为相同尺度的方法，比如：\n 把数据特征映射到 [0, 1] 或者 [-1, 1] 区间 映射为服从 N(0, 1) 的标准正态分布   1、逐层归一化 逐层归一化可以有效提高训练效率的原因：\n  更好的尺度不变形\n深度神经网路中，一个神经层的输入是之前神经层的输出。给定一个神经层 $l$，它之前的神经层 $1, 2, \u0026hellip;, l-1$，的参数变化会导致其输入的分布发生很大的变化。当使用随机梯度下降法训练网络时，每次参数更新都会导致该神经层的输入分布发生变化，层数越高，其输入分布会改变得越明显。\n为了缓解这个问题，可以对每个神经层的输入进行归一化，使其分布保持稳定。不管底层的参数如何变化，高层的输入相对稳定。另外，尺度不变性，可以使我们更加高效地进行参数初始化以及超参数选择。\n  更平滑的优化地形\n逐层归一化，一方面可以是大部分神经层的输入处于不饱和区域，从而让梯度变大，避免梯度消失问题；另一方面可以使得神经网络的优化地形（Optimization Landscape）更加平滑，并使梯度变得更加稳定，从而允许使用更高的学习率，并加快收敛速度。\n  1. 批量归一化 批量归一化（Batch Normalization）对神经网络中的任意中间层进行归一化。\n$$ a^{(l)} = f(z^{(l)}) = f(Wa^{(l-1)}+b) $$ $f(·)$ 是激活函数，$W$ 和 $b$ 是可学习的参数。\n为了提高优化效率，就要使净输入 $z^l$ 的分布一致，比如：都归一化为标准正态分布。虽然归一化操作可以应用在输入 $a^{(l-1)}$ 上，但归一化 $z^l$ 更加有利于优化。因此，在实践中，归一化操作一般应用在仿射变换之后，激活函数之前。\n2. 层归一化 层归一化（Layer Normalization）是和批量归一化非常类似的方法，与批量归一化不同的是，层归一化是对一个中间层的所有神经元进行归一化。\n二、Norm的位置 在目前大模型中 Normalization 的位置：\npre Norm 的状态： $x_{t+1} = x_t + F_t(Norm(x_t))$\npost Norm 的状态： $x_{t+1} = Norm(x_t + F_t(x_t))$\n1、为什么Pre效果弱于post 来自苏神的解释: pre-Norm的深度有水分，到一定深度后增加网络深度的效果等同于增加宽度，而不是深度。 Pre-Norm $$ \\begin{aligned} x_{t+1} \u0026amp;= x_t + F_t(Norm(x_t)) \\\n\u0026amp;= x_{t-1} + F_{t-1}(Norm(x_{t-1})) + F_t(Norm(x_t)) \\\n\u0026amp;= \u0026hellip; \\\n\u0026amp;=x_0 + F_0(Norm(x_0)) + \u0026hellip; + F_{t-1}(Norm(x_{t-1})) + F_t(Norm(x_t)) \\end{aligned} $$\n当 $t$ 比较大时，$F_{t-1}(Norm(x_{t-1}))$ 与 $F_t(Norm(x_t))$ 很接近，等效于一个更宽的 $t$ 层模型，所以，在Pre-Norm中多层叠加的结果更多的是增加宽度而不是深度，深度上有水分。在模型训练中，深度通常比宽度更重要。\nPost-Norm\n回顾一下残差链接：$x_{t+1} = x_t + F_t(x_t)$\n由于残差分支的存在，$x_{t+1}$的方差是 $x_t$ 与 $F_t(x_t)$ 之和 $\\sigma^2_1 + \\sigma^2_2$，残差会进一步放大方差 。所以需要缩小方差，其中Normalization就可以实现。在Norm过程中方差的变化：\n$$ \\begin{aligned} x_l \u0026amp;= \\frac{x_{l-1}}{2^{1/2}} + \\frac{F_{l-1}(x_{l-1})}{2^{1/2}} \\\n\u0026amp;=\u0026hellip; \\\n\u0026amp;= \\frac{x_0}{2^{l/2}} + \\frac{F_0(x_0)}{2^{l/2}} + \\frac{F_1(x_1)}{2^{(l-1)/2}} + \u0026hellip; + \\frac{F_{l-1}(x_{l-1})}{2^{1/2}} \\end{aligned} $$\n在每条残差通道上都有权重缩小，距离越远的削弱的越严重，原始残差效果越来越弱，因此还是不容易训练。所以，post-Norm 通常要warmup+较小的学习率才能收敛。相关分析见：《On Layer Normalization in the Transformer Architecture》 苏神认为：在初始阶段保持一个恒等式，即：引入一个初始化0的参数 $\\alpha_t$。从0开始以固定的、很小的步长慢慢递增，直到增加到 $\\alpha_t = 1$就固定下来。苏神在实验结果中，这种更新模式获得了最优的结果。 $$ x_{t+1} = x_t + \\alpha_t F_t(x_t) $$\n2、RMSNorm为啥有效 Layer Normalizaton $$ y = \\frac{x-E(x)}{\\sqrt{Var(x) + \\varepsilon}} $$\nRMSNorm $$ \\bar{a_i} = \\frac{a_i}{\\sqrt{\\frac{1}{n} \\sum^n_{i=1}a^2_i}} g_i $$\n《Root Mean Square Layer Normalization》 直接去掉了 $E(x)$ 的计算，相当于只是把分布的方差变成了1，中心不一定在0。《Do Transformer Modifications Transfer Across Implementations and Applications?》 这篇文章做了比较充分的对比实验，显示RMSNorm的优越性。\n同样在这篇论文中 《Analyzing and Improving the Image Quality of StyleGAN》 提出了一个问题：\n在StyleGAN2里，发现所用的 Instance Normalization会导致部分生成图片出现 “水珠”，他们 最终去掉了 Instance Normalization并换用了一个叫 Weight demodulation 的东西。但他们发现如果保留 Instance Normalization 但去掉 中心 $E(x)$ 操作，也能改善这种现象。这也能佐证 中心 $E(x)$ 操作可能会带来负面效果。\n一种直观的猜测：中心 $E(x)$ 类似于全连接层的 bias，存储的是关于预训练任务的一种先验分布信息，而把这种先验分布信息直接存储在模型中，反而可能会导致模型的迁移能力下降。随意 $T5$ 不仅去掉了Layer Normalization的 中心 $E(x)$，也把每一层的bias项去掉了。\n","date":"August 5, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0015_deeplearning_norm/","summary":"一、Normlization介绍 一般而言，样本特征由于来源及度量单位不同，其尺度往往差异很大。如果尺度差异很大，神经网络就比较难训练。为了提高训练效率，对输入特征做归一化，把不同的尺度压缩到一定范围内，尺度统一后，大部分位置的梯度方向近似于最优解搜索方向。这样，在用梯度下降法进行求解时，每一步梯度的方向都基本上指向最小值，训练效率会大大提高。\n 归一化：泛指把数据特征转换为相同尺度的方法，比如：\n 把数据特征映射到 [0, 1] 或者 [-1, 1] 区间 映射为服从 N(0, 1) 的标准正态分布   1、逐层归一化 逐层归一化可以有效提高训练效率的原因：\n  更好的尺度不变形\n深度神经网路中，一个神经层的输入是之前神经层的输出。给定一个神经层 $l$，它之前的神经层 $1, 2, \u0026hellip;, l-1$，的参数变化会导致其输入的分布发生很大的变化。当使用随机梯度下降法训练网络时，每次参数更新都会导致该神经层的输入分布发生变化，层数越高，其输入分布会改变得越明显。\n为了缓解这个问题，可以对每个神经层的输入进行归一化，使其分布保持稳定。不管底层的参数如何变化，高层的输入相对稳定。另外，尺度不变性，可以使我们更加高效地进行参数初始化以及超参数选择。\n  更平滑的优化地形\n逐层归一化，一方面可以是大部分神经层的输入处于不饱和区域，从而让梯度变大，避免梯度消失问题；另一方面可以使得神经网络的优化地形（Optimization Landscape）更加平滑，并使梯度变得更加稳定，从而允许使用更高的学习率，并加快收敛速度。\n  1. 批量归一化 批量归一化（Batch Normalization）对神经网络中的任意中间层进行归一化。\n$$ a^{(l)} = f(z^{(l)}) = f(Wa^{(l-1)}+b) $$ $f(·)$ 是激活函数，$W$ 和 $b$ 是可学习的参数。\n为了提高优化效率，就要使净输入 $z^l$ 的分布一致，比如：都归一化为标准正态分布。虽然归一化操作可以应用在输入 $a^{(l-1)}$ 上，但归一化 $z^l$ 更加有利于优化。因此，在实践中，归一化操作一般应用在仿射变换之后，激活函数之前。\n2. 层归一化 层归一化（Layer Normalization）是和批量归一化非常类似的方法，与批量归一化不同的是，层归一化是对一个中间层的所有神经元进行归一化。\n二、Norm的位置 在目前大模型中 Normalization 的位置：\npre Norm 的状态： $x_{t+1} = x_t + F_t(Norm(x_t))$","tags":["机器学习","深度学习","归一化"],"title":"归一化"},{"categories":["Basic"],"contents":"一、激活函数 1、Sigmoid函数 logistic函数\n$$ \\varphi(v) = \\frac{1}{1+e^{-av}} $$\ntanh函数\n$$ \\varphi(v) = tanh(v) = \\frac{1-e^{-v}}{1+e^{-v}} $$\n分段线性函数\n$$ \\varphi(v) = \\begin{cases} 1 \u0026amp;\\text{if } v \\geqslant \\theta \\\nkv \u0026amp;\\text{if } - \\theta \u0026lt; v \u0026lt; \\theta \\\n0 \u0026amp;\\text{if } v \\leqslant 0 \\end{cases} $$\n概率型函数\n$$ P(1) = \\frac{1}{1+e^{-\\frac{x}{T}}} $$\n2、ReLU函数 relu函数有助于梯度收敛，收敛速度快了6倍。但仍然有缺陷：\n在x\u0026lt;0是，梯度为0，一旦变成负将无法影响训练，这种现象叫做死区。如果学习率较大，会发现40%的死区。如果有一个合适的学习率，死区会大大减少。 $$ ReLU(x) = max(0, x) = \\begin{cases} x \u0026amp;\\text{if } x \\geqslant 0 \\\n0 \u0026amp;\\text{if } x \u0026lt; 0 \\end{cases} $$\n带滞漏的ReLU\n$$ LeakyReLU(x) = \\begin{cases} x \u0026amp;\\text{if } x \\geqslant 0 \\\n\\gamma x \u0026amp;\\text{if } x \u0026lt; 0 \\end{cases} $$\n缓解了死区，不过 $\\gamma$ 是个超参，人为设定的不准，调参影响较大。\n$\\gamma = 0.01$ ，当神经元处于非激活状态时，也能有一个非零的梯度可以更新参数，避免永远不能被激活。\n带参数的ReLU\n$$ PReLU(x) = \\begin{cases} x \u0026amp;\\text{if } x \\geqslant 0 \\\n\\gamma_i x \u0026amp;\\text{if } x \u0026lt; 0 \\end{cases} $$\n引入一个可学习的参数 $\\gamma_i$ ，不同神经元可以有不同的参数。\nELU函数：Exponential Linear Unit 指数线性单元\n在小于0的部分使用指数，具备relu的优点，同时ELU也解决了relu函数自身死区的问题。不过ELU函数指数操作稍稍增大了工作量\n$$ ELU(x) = \\begin{cases} x \u0026amp;\\text{if } x \\geqslant 0 \\\n\\gamma(e^x-1) \u0026amp;\\text{if } x \u0026lt; 0 \\end{cases} $$\nSoftplus函数\n$$ Softplus(x) = log(1+e^x) $$ Softplus函数可以看做ReLU函数的平滑版本，其导数刚好是logistic函数。Softplus函数虽然也具有单侧抑制、宽兴奋边界的特性，但没有稀疏激活性。\nswish\n该函数是google大脑提出的一个新的激活函数，从图像上来看，与relu差不多，唯一区别较大的是接近0的负半轴区域。 $$ swish(x) = \\frac{x}{1+e^{-x}} $$\n二、损失函数 损失函数（Loss Function），是用来评价模型的预测值和真实值不一样的程度。常见的损失函数有：\n 均方差损失函数 $$ MSE = \\frac{1}{2N}\\sum^{N}_{i=1}(t_i-y_i)^2 $$ 平均绝对误差损失函数 $$ MAE = \\frac{1}{N}\\sum^{N}_{i=1}|t_i-y_i| $$ 交叉熵损失函数（Cross Entropy Loss Function）\n二分类任务： $$ E = - \\frac{1}{N}\\sum^N_{i=1}(t_i \\log y_i + (1-t_i) \\log (1-y_i)) $$ $N$：表示样本数量；$y_i$：表示样本 $i$ 预测为正类的概率；$t_i$：表示样本 $i$ 的目标值。\n在多分类任务时，可以看成二分类的扩展： $$ E = - \\frac{1}{N}\\sum_i^N\\sum^{M}_{c=1}(t_i \\log y_i) $$ $N$：表示样本数量；$M$：表示类别的数量；$y_i$：表示样本 $i$ 属于类别 $c$ 的概率；$t_i$：表示样本 $i$ 的目标值(只有目标值为1，其余都是0)。\n**熵**：表征的是期望的稳定性，其值越小越稳定；熵越大，表示该事件发生的可能性越小，风险度会越大。\n**交叉熵**：主要应用于度量两个概率分布之间的差异性。  当两个分布完全相同时，交叉熵的取值最小 交叉熵的值是非负的，并且预测值于目标值越接近，交叉熵的值就越小 对于损失函数而言，如果模型输出预测值于目标值相同，那么损失函数就越小。    三、学习规则 神经网络的学习规则：希望模型的损失函数最小，即：风险最小化准则。\n对于训练的评价准确：有一个较小的期望风险。但是由于不知道真实的数据分布和映射函数，所以实际上无法计算模型的期望风险：$R(\\bold \\theta)$。给定一个训练集 $\\bold D$，可以计算的是 经验风险（Empirical Risk） ，即：在训练集上的平均损失。根据大数定理 当训练集大小趋于无穷大时，经验风险就趋于期望风险。然而，在通常情况下，我们无法获取无限的训练样本，并且训练样本往往是真实数据的一个很小的子集（或许包含了一定的噪声数据）。因此，最优的学习规则：就是能够找到一组参数 $\\hat{\\bold \\theta}$，使得经验风险最小，这就是经验风险最小化准则(Empirical Risk Minimization)。\n1、极大似然估计 估计类条件概率（似然）的一种常用策略是：先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。\n似然函数（Likelihood Function）：是统计模型中参数的函数。当给定 联合样本值 $\\bold x$ 时，关于参数 $\\bold \\theta$ 的似然函数 $L(\\bold \\theta | \\bold x)$，在数值上等于给定参数 $\\bold \\theta$ 后变量 $\\bold x$ 的概率。 $$ L(\\bold \\theta | \\bold x) = P(\\bold X = \\bold x|\\bold \\theta) $$ 实际上，概率模型的训练过程就是参数估计过程。对于参数估计，统计学界的两个学派分别提供了不同的解决方案：\n 频率主义学派（Frequentist）：认为参数虽然未知，但客观存在固定值。因此，可以通过优化似然函数等准则来确定参数值。 贝叶斯学派（Batesian）：认为参数是为观测到的随机变量，其本身也可有分布，因此，可先假定参数服从一个先验分布，然后基于观测到的数据计算参数的后验分布。  2、欠/过拟合 1. 误差 训练误差(training error): 训练模型在训练数据集(training set)上表现出的误差。 泛化误差(generalization error)：模型在任意一个测试数据集(test set)上表现出的误差的期望。\n训练集(training set)：用来产出模型参数。\n验证集(validation set)：由于无法从训练误差评估泛化误差，因此从训练集中预留一部分数据作为验证集，主要用来选择模型。 测试集(test set)：在模型参数选定后，实际使用。\n2. 欠/过拟合 欠拟合(underfitting)：模型的表现能力不足。\n 训练样本足够，模型参数不足  过拟合(overfitting)：模型的表现能力过剩。\n 训练样本不足，模型参数足够：样本不足导致特征较少，相当于模型足够表征数据的特征，产生过拟合现象。  3. 优化过拟合 增大训练集可能会减轻过拟合，但是获取训练数据往往代价很高。可以在模型方面优化一下，减轻过拟合现象。\n  权重衰减(weight decay)： 对模型参数计算 $L_2$ 范数正则化。即：在原Loss中添加对模型参数的惩罚。使得模型学到的权重参数较接近于0。权重衰减通过惩罚绝对值较大的模型参数，为需要学习的模型增加了限制。这可能对过拟合有效。\n  丢弃法(dropout)：针对隐藏层中的各个神经元，以概率p随机丢弃，有可能该成神经元被全部清零。这样，下一层的计算无法过渡依赖该层的任意一个神经元，从而在训练中可以用来对付过拟合。在测试中，就不需要丢弃了。\n例如：对隐藏层使用丢弃法，丢弃概率: p，那么hi 有p的概率被清零；不丢弃概率: 1-p，为了保证隐藏层的期望值不变E(p')=E(p)，需要对不丢弃的神经元做拉伸，即：$$h'_i = \\frac{\\xi_i} {1-p} h_i$$ 其中：随机变量ξi 为0和1的概率分别为p和1-p   权重衰减（Weight Decay）：在每次参数更新时，引入一个衰减系数： $$ \\theta_t \\gets (1-\\beta)\\theta_{t-1}-\\eta g_t $$ 其中 $\\beta$ 为权重衰减系数，一般取值比较小，比如：0.0005。在标准的随机梯度下降中，权重衰减正则化和 $L_2$ 正则化的效果相同；但是在较为复杂的优化方法(比如：Adam)中，权重衰减正则化和 $L_2$ 正则化并不等价。\n  数据增强（Data Augmentation）：可以减轻网络的过拟合现象。通过对训练数据进行交换可以得到泛化能力更强的网络。\n  四、优化方法 在深度学习中，通过最小化损失函数使得训练误差最小化，由于损失函数一般都会比较复杂，很难直接求解析解，而是需要基于数值方法的优化算法找到近似解，即：数值解。在局域数值方法的优化算法中，损失函数就是目标函数(Objective Function)，\n1. 梯度下降法 梯度下降(gradient descent)的工作原理，以一维为例： 假设连续可导的函数 $f:\\Reals \\to \\Reals$ 的输入和输出都是标量，给定绝对值足够小的数 $\\epsilon$ ，根据泰勒展开式，近似： $$ f(x+\\epsilon) \\approx f(x) + \\epsilon f'(x) $$ 其中 $f'(x)$ 表示函数在x处的梯度。找到一个常数 $\\eta \u0026gt; 0$，使得 $\\lvert \\eta f'(x) \\rvert$ 足够小，那么可以将 $\\epsilon$ 提换为 $-\\eta f'(x)$，得到： $$ f(x-\\eta f'(x)) \\approx f(x) - \\eta f'(x)^{2} $$ 所以 $$ f(x-\\eta f'(x)) \\lesssim f(x) $$ 这就意味着，可以通过 $x \\gets x-\\eta f'(x)$ 来迭代x，函数 $f(x)$ 的值可能会降低。在梯度下降中，先取一个初始值 $x_0$ 和学习率 $\\eta\u0026gt;0$，然后不断通过上式迭代x，直到停止条件。学习率 $\\eta$ 是一个超参数，需要人工设定，如果学习率过小：会导致x更新缓慢从而需要更多的迭代次数；如果学习率过大，泰勒展开式不再成立，可能会出现振荡，无法保证会迭代出近似最优解。\n在每次迭代中，由于训练集较大，不可能把所有样本都加载到内存中，通常是随机均匀采样多个样本组成一个小批量，然后使用这个小批量来计算梯度，完成一次迭代，即：小批量随机梯度下降(batch gradient descent)。\n设：目标函数 $f(x): \\Reals^{d} \\to \\Reals$ 小批量数据集 $\\text{\\ss}$ 梯度计算： $$ g_t \\gets \\nabla f_{\\text{\\ss}_{t}} $$\n$$ = \\frac {1} {\\lvert \\text{\\ss} \\rvert} \\sum_{i \\in \\text{\\ss}_t} \\nabla f_i(x_t) $$\n$$ x_t \\gets x_{t-1} - \\eta_t g_t $$\n其中，$ \\lvert \\text{\\ss} \\rvert $ 表示批量大小，$\\eta_t$ 表示学习率，这两个都是超参数。\n2. 动量法 问题：自变量的梯度代表了目标函数在当前位置下降最快的方向，沿着该方向更新自变量，可能还是会有一些问题。例如：类似峡谷的函数，在有些方向上的梯度比较缓慢，在有些方向上梯度比较陡峭，在相同的学习率下，容易导致在梯度缓慢的方向收敛太慢；如果调大学习率，容易导致在梯度陡峭的方向上振荡。如下图，梯度在水平方向上为正，而在竖直方向上时上时下：\n动量法：动量法在迭代自变量时，不仅仅是利用当前的梯度值，而是利用过去一段时间的梯度值的平均。新的梯度更迭方向，不再是指下降最陡峭的方向，而是指向过去梯度的加权平均值的方向，越靠近当前时刻权重越重。\n$$ \\upsilon_t \\gets \\gamma \\upsilon_{t-1} + \\eta_t g_t $$\n$$ x_t \\gets x_{t-1} - \\upsilon_t $$\n其中，$0 \\leqslant \\gamma \u0026lt; 1$，当$\\gamma = 0$时，动量法等价于小批量随机梯度下降法。\n证明：\n我们先解释指数加权移动平均(exponentially weighted moving average)，然后在类比到动量法。\n$$ y_t = \\gamma y_{t-1} + (1-\\gamma)x_t $$ 其中，$0 \\leqslant \\gamma \u0026lt; 1$，在当前时间步$t$的变量$y_t$可以展开（类似信号系统中的激励与响应）：\n$$ \\begin{array}{cc} y_t \u0026amp; = (1-\\gamma)x_t + \\gamma y_{t-1} \\\\ \u0026amp; = (1-\\gamma)x_t + (1-\\gamma)\\gamma x_{t-1} + \\gamma^2 y_{t-2} \\\\ \u0026amp; = (1-\\gamma)x_t + (1-\\gamma)\\gamma x_{t-1} + \\dots + (1-\\gamma)\\gamma^{t-1} x_{1} + \\gamma^t y_{0} \\end{array} $$\n令$n=\\frac {1} {1-\\gamma}$，那么$(1-\\frac {1} {n})^n = \\gamma^{\\frac {1} {1-\\gamma}}$。\n有极限：$\\lim\\limits_{n \\to \\infty} (1-\\frac {1} {n})^n =\\lim\\limits_{\\gamma \\to 1} \\gamma^{\\frac {1} {1-\\gamma}}= exp(-1) \\approx 0.3679$\n对于$y_t$，可以看做是对最近$\\frac {1} {1-\\gamma}$个时间步的加权平均；忽略含有$\\gamma^{\\frac {1} {1-\\gamma}}$和比$\\gamma^{\\frac {1} {1-\\gamma}}$更高阶系数的项，即：当$\\gamma=0.95$时，可以看成对最近20时间步的$x_i$值的加权平均\n$$ y_t \\approx 0.05\\displaystyle\\sum_{i=0}^{19} 0.95^i x_{t-i} $$\n类比向量法\n$$ \\upsilon_t \\gets \\gamma \\upsilon_{t-1} + (1-\\gamma)\\frac {\\eta_t} {1-\\gamma} g_t $$\n$$ x_t \\gets x_{t-1} - \\upsilon_t $$\n 所以：向量$\\upsilon_t$实际上是对序列$\\frac {\\eta_{t-i}} {1-\\gamma} g_{t-i}$做指数加权移动平均；也就是说：动量法在每个时间步的自变量更新量近似于将最近的$\\frac {1} {1-\\gamma}$个时间步的更新量做指数加权移动平均。动量法中，自变量在各个方向上的移动幅度，不仅取决于当前梯度，还取决于历史各个梯度在各个方向上是否一致。如果在某个方向上时正时负，说明在该方向上有振荡，通过动量的向量相加，对于该情况会降低每次的更新量，使得梯度在该方向上不发散。\n 3. AdaGrad算法 问题：在统一学习率的情况下，梯度值较大的维度可能会振荡，梯度值较小的维度收敛可能会过慢。\nAdaGrad算法：根据自变量在每个维度的梯度值大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题。\n$$ s_t \\gets s_{t-1} + g_t \\odot g_t $$\n$$ x_t \\gets x_{t-1} - \\frac {\\eta} {\\sqrt{s_t + \\epsilon}} \\odot g_t $$\n其中，$\\odot$表示按元素相乘，$\\eta$表示学习率。目标函数自变量中每个元素的学习率通过按元素运算重新调整一下，每个元素都分别拥有自己的学习率。由于$s_t$一直在累加，所以每个元素的学习率在迭代过程中一直在降低，当学习率在迭代早期降得比较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。\n4. RMSProp算法 问题：AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。为了解决这一问题，RMSProp算法对AdaGrad算法做了一些修改。\nRMSProp算法：只是在AdaGrad算法中添加了 指数加权移动平均。\n$$ s_t \\gets \\gamma s_{t-1} + (1-\\gamma)g_t \\odot g_t $$\n$$ x_t \\gets x_{t-1} - \\frac {\\eta} {\\sqrt{s_t + \\epsilon}} \\odot g_t $$\n其中，$\\eta$是学习率，RMSProp算法的状态变量是对平方项$g_t \\odot g_t$的指数加权移动平均，所以可以看作最近$\\frac {1} {1-\\gamma}$个时间步的加权平均。如此一来，自变量每个元素的学习率在迭代过程中就不再一直降低。\n5. AdaDelta算法 AdaDelta算法：是另一个针对AdaGrad算法优化的算法，不过没有学习率这个超参数。\n$$ s_t \\gets \\gamma s_{t-1} + (1-\\gamma)g_t \\odot g_t $$\n$$ g_t' \\gets \\sqrt{\\frac {\\Delta x_{t-1} + \\epsilon} {s_t + \\epsilon}} \\odot g_t $$\n$$ \\Delta x_t \\gets \\gamma \\Delta x_{t-1} + (1-\\gamma)g_t' \\odot g_t' $$\nRMSProp算法，还维护一个额外的状态变量$\\Delta x_t$，用来记录自变量变化量$g_t'$按元素平方的指数加权移动平均。\n$$ x_t \\gets x_{t-1} - g_t' $$\n6. Adam算法 Adam算法：结合了动量变量$\\upsilon_t$ 和 RMSProp算法的梯度按元素平方和的指数加权移动平均。\n$$ \\upsilon_t \\gets \\beta_1 \\upsilon_{t-1} + (1-\\beta_1) g_t $$\n其中，$0 \\leqslant \\beta_1 \u0026lt; 1$（建议0.9），$\\upsilon_0$初始化为0，则：$\\upsilon_t = (1-\\beta_1)\\displaystyle\\sum_{i=1}^t \\beta_1^{t-i} g_i$\n将过去各时间步小批量随机梯度的权值相加：$(1-\\beta_1)\\displaystyle\\sum_{i=1}^t \\beta_1^{t-i}=1-\\beta_1^t$，当t较小时，过去各时间步梯度权值之和会较小，为了消除这样的影响，对任意时间步t，可以将向量$\\upsilon_t$再除以$1-\\beta_1^t$ ：\n$$ \\hat{\\upsilon}_t \\gets \\frac {\\upsilon_t} {1-\\beta_1^t} $$\n$$ s_t \\gets \\beta_2 s_{t-1} + (1-\\beta_2)g_t \\odot g_t $$\n$$ \\hat{s}_t \\gets \\frac {s_t} {1-\\beta_2^t} $$\n其中，$0 \\leqslant \\beta_2 \u0026lt; 1$（建议0.999），$s_0$初始化为0\nAdam算法使用修正后的变量$\\hat{\\upsilon}_t, \\hat{s}_t$，将模型参数中每个元素的学习率通过按元素运算重新调整。\n$$ g_t' \\gets \\frac {\\eta \\hat{\\upsilon}_t} {\\sqrt{\\hat{s}_t + \\epsilon}} $$ 其中，$\\eta$是学习率，$\\epsilon$是为了维持数值稳定性而添加的常数，例如$10^{-8}$。分子：是动量，可以在方向上消除发散；分母：在幅度上修改每个元素的学习率。\n$$ x_t \\gets x_{t-1} - g_t' $$\n五、模型评估 在深度学习中，用来衡量模型的好坏标准有很多，比如：混淆矩阵、准确率、精确率、召回率、F值等\n1、混淆矩阵     预测类别     真实类别         P N     T TP TN   F FP FN    2、准确率、精确率、召回率   准确率（Accuracy）：正确预测的各个类别的数量 / 总数 $$ A_c = \\frac {TP + FN} {TP + TN + FP + FN} $$\n  精确率（Precision）：单个类别的正确预测数量 / 所有预测为这类的数量 $$ P_c = \\frac {TP} {TP + FP} $$\n  召回率（Recall）：单个类别的正确预测数量 / 所有这类真实值的数量 $$ R_c = \\frac {TP} {TP + TN} $$\n  F值（F Measure）：是一个综合指标，为精确率和召回率的调和平均 $$ F = \\frac {(1+\\beta^2)P_c R_c} {\\beta^2 P_c + R_c} $$ $\\beta$：用于平衡精确率和召回率的重要性，一般取值为 1。$\\beta = 1$时，$F$ 值称为 $F_1$ 值，是精确率和召回率的调和平均，表示：精确率和召回率同等重要。$\\beta \u0026lt; 1$ 表示：精确率更重要一些; $\\beta \u0026gt; 1$ 表示：召回率更重要一些。\n  3、ROC/AUC/PR曲线   ROC曲线（Receiver Operating Characteristic）称为接受者操作特征曲线。\n 以 “真正例”（TPR）为y轴；以“假正例”（FPR）为x轴 (0, 1): 表示所有的样本都正确分类 (1, 0): 表示避开了所有正确的答案 (0, 0): 表示分类器把每个样本都预测为负例 (1, 1): 表示分类器把所有样本都预测为正例 ROC曲线越靠近左上角，模型的准确性就越高，一般ROC曲线是光滑的，那么基本上可以判断模型没有太大的过拟合。    AUC曲线（Area Under Curve）的值为ROC曲线下面的面积，AUC的值越大，表示模型的性能越好。若分类器的性能极好，则AUC=1。但是在现实中，没有如此完美的模型，一般 $AUC \\in (0.5, 1)$\n AUC=1：完美预测 $0.5 \u0026lt; AUC \u0026lt; 1$：优于随机猜测 $AUC = 0.5$：与随机猜测一样 $AUC \u0026lt; 0.5$：比随机猜测还差    PR曲线（Precision Recall）：表示精确率和召回率的曲线\n 以精确率为y轴；以召回率为x轴。    ","date":"August 5, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0010_deeplearning_problem/","summary":"一、激活函数 1、Sigmoid函数 logistic函数\n$$ \\varphi(v) = \\frac{1}{1+e^{-av}} $$\ntanh函数\n$$ \\varphi(v) = tanh(v) = \\frac{1-e^{-v}}{1+e^{-v}} $$\n分段线性函数\n$$ \\varphi(v) = \\begin{cases} 1 \u0026amp;\\text{if } v \\geqslant \\theta \\\nkv \u0026amp;\\text{if } - \\theta \u0026lt; v \u0026lt; \\theta \\\n0 \u0026amp;\\text{if } v \\leqslant 0 \\end{cases} $$\n概率型函数\n$$ P(1) = \\frac{1}{1+e^{-\\frac{x}{T}}} $$\n2、ReLU函数 relu函数有助于梯度收敛，收敛速度快了6倍。但仍然有缺陷：\n在x\u0026lt;0是，梯度为0，一旦变成负将无法影响训练，这种现象叫做死区。如果学习率较大，会发现40%的死区。如果有一个合适的学习率，死区会大大减少。 $$ ReLU(x) = max(0, x) = \\begin{cases} x \u0026amp;\\text{if } x \\geqslant 0 \\\n0 \u0026amp;\\text{if } x \u0026lt; 0 \\end{cases} $$","tags":["机器学习","深度学习","网络结构"],"title":"深度学习-结构"},{"categories":["Basic"],"contents":"论文入口\n一、机器学习 目前，人工智能研究领域主要体现在一下几个方面：\n  智能感知：通过模拟人的感知能力（视觉、听觉、嗅觉）对外部信息进行感知和识别，并能够对信息进行加工和处理，从而做出反应。\n  智能学习：学习是人工智能的主要标志和获取知识的重要手段，研究机器通过模拟人的学习能力，如何从小样本、大数据中学习，主要有：\n 监督学习：（Supervised Learning）表示机器学习的数据是带有标记的，这些标记可以包括：数据类别、数据属性、特征点位置等。这些标记作为预期效果，不断修正机器的预测结果。常见的监督学习有分类、回归、结构化学习。 半监督学习：（Semi-Supervised Learning）利用少量标注数据和大量无标注数据进行学习的方式。常用的半监督学习算法有：自训练、协同训练 非监督学习：（Unsupervised Learning）表示机器学习的数据是没有标记的。常见的无监督学习有：聚类、降维 强化学习：（Reinforcement Learning）通过智能体和环境的交互，不断学习并调整策略的机器学习算法。这种算法带有一种激励机制，如果智能体根据环境做出一个正确的动作，则施予一定的“正激励”；如果是错误的动作，则给与一定的“负激励”。通过不断地累加激励，以获取激励最大化的回报。做火热的应用就是 AlphaGo Zero    认知推理：模拟人的认知能力，主要研究知识表示、推理、规划、决策等，主要有自然语言处理、脑科学。\n    二、表征学习 表征：为了提高机器学习系统的准确率，需要将输入信息转化为有效的特征，或者更一般性地称为 表征（Representation） 表征学习：如果有一种算法可以自动地学习有效的特征，并提高最终机器学习模型的性能，那么这种学习就可以叫做 表征学习。 表征学习的关键是解决语义鸿沟（Semantic Gap）问题。即：输入数据的底层特征与高层语义信息之间的不一致性和差异性。\n机器学习中经常用两种方式表示特征：局部表示(Local Representation)、分布式表示(Distributed Representation)。\n比如：颜色的表示。\n 局部表示：也称为离散表示或者符号表示，比如：one-hot向量的形式。假设所有颜色 构成一个词表 $\\bold V$，此时，可以用一个 $|\\bold V|$ 维的one-hot向量来表示一中颜色。但是，one-hot向量的维数很高，且不能扩展，如果有一种新的颜色，就需要增加一维来表示。不同颜色之间的相似度都为0，无法直到“红色”和“中国红”的相似度要高于“红色”和“黑色”的相似度。 分布式表示：另一种表示颜色的方法是用RGB值来表示颜色，不同颜色对应RGB三维空间中的一个点。分布式表示通常可以表示低维的稠密向量。  嵌入：神经网络将高维的局部表示空间 $\\R^{|\\bold V|}$，映射到一个非常低维的分布式表示空间 $\\R^{D}$。在这个低维空间中，每个特征不再是坐标轴上的点，而是分散在整个低维空间中，在机器学习中，这个过程也成为嵌入（Embedding）。比如：自然语言中词的分布式表示也经常叫做词嵌入。\n要学习到一种好的高层次语义表示（一般为分布式表示），通常只有从底层特征开始，经过多步非线性转换才能得到。深层结构的优点是可以提高特征的重用性，从而指数级增强表示能力。因此，表示学习的关键是构建具有一定深度的多层次特征表示。\n  三、深度学习 深度学习是机器学习的一个重要的、新的研究领域，源于对神经网络的进一步研究，通常采用包含多个隐藏层的神经网络结构，目的是建立、模拟人脑学习过程。\n在描述深度学习之前，先回顾下机器学习和深度学习的关系。\n  机器学习：研究如何使用计算机系统利用经验改善性能。在机器学习的众多研究方向中，表征学习关注如何自动找出表示数据的合适方式，以便更好地将输入变换为正确的输出。\n  深度学习：是具有多级表示的表征学习方法。在每一级，深度学习通过简单的函数将该级的表示变换为更高级的表示。因此，深度学习模型也可以看作是由许多简单函数复合而成的函数。当这些复合函数足够多时，就可以表达非常复杂的变换。\n作为表征学习的一种，深度学习将自动找出每一级表示数据的合适方式。逐级表示越来越抽象的概念或模式。高层特征是由底层特征通过推演归纳得到。\n深度学习可通过学习一种深层非线性网络结构来表征输入数据，实现复杂函数逼近，具有很强的从少数样本集中学习数据集本质特征的能力。深度学习的主要思想：通过自学习的方法，学习到训练数据的结构，并在该结构上进行有监督训练微调。 以图像为例，它的输入是一堆原始像素值，模型中逐级表示为： graph LR; A(特定位置和角度的边缘) -- B(由边缘组合得出的花纹) B -- C(由多种花纹进一步汇合得到的特定部位) C -- D(由特定部位组合得到的整个目标)    1、神经元 神经元模型：\n 每个神经元都是一个多输入、单输出的信息处理单元 神经元输入分兴奋性输入和抑制性输入两种类型 神经元具有空间整合特性和阈值特性 神经元输入与输出间有固定的时滞，主要取决于突触延迟 忽略时间整合作用和不应期 神经元本身是非时变的，即：其突触时延和突触强度均为常数  graph LR; A1(x1) -- |输入| B1(Wk1) A2(x2) -- |输入| B2(Wk2) A3(x3) -- |输入| B3(Wk3) B1 -- |权值| C(求和节点) B2 -- |权值| C(求和节点) B3 -- |权值| C(求和节点) C(求和节点) -- |vk| D(激活函数) D -- |yk| E(输出)  四、学习方式 1、多阶段 在一些复杂任务重，传统机器学习方法需要将一个任务的输入和输出人为地切割成很多子模块（或者多个阶段），每个子模块分开学习。比如：要完成一个自然语言理解任务，一般需要： graph LR; A(分词) -- B(词性标注) B -- C(句法分析) C -- D(语义分析) D -- E(语义推理)  这种学习方式有两个问题：\n 每个模块都需要单独优化，并且其优化目标和任务总体目标并不能保证一致。 错误传播，即：前一步的错误会对后续的模型造成很大的影响。  2、端到端 训练过程中不进行分模块或分阶段训练，而是直接优化任务的总体目标。中间过程不需要人为干预，无需其他额外信息。因此，端到端学习，需要解决贡献度分配问题。目前大部分采用神经网路模型的深度学习都是端到端学习。\n五、学术会议    简称 介绍     ICLR 国际表征学习大会（International Conference on Learning Representations）: 主要聚焦深度学习   NeurIPS 神经信息处理系统大会（Annual Conference on Neural Information Processing Systems）：交叉学科会议，但偏重于机器学习，主要包括神经信息处理、统计方法、学习理论及应用   ICML 国际机器学习会议（International Conference on Machine Learning）：机器学习顶级会议。深度学习作为近年来的热点，也占据了ICML   IJCAI 国际人工智能联合会议（International Joint Conference on Artificial Intelligence）：人工智能领域顶尖的综合性会议，历史悠久，从1969年开始举办。   AAAI 国际人工智能协会（AAAI Conference on Artificial Intelligence）：人工智能领域的顶级会议，每年二月份左右召开，一般在北美。    人工智能的子领域 - 专业学术会议   CVPR IEEE国际计算机视觉与模式识别会议（IEEE Conference on Computer Vision and Pattern Recognition）   ICCV 计算机视觉国际大会（International Conference on Computer Vision）   ACL 国际计算语音学协会（Annual Meeting of the Association for Computational Linguistics）   EMNLP 自然语言处理实证方法会议（Conference on Empirical Methods in Natural Language Processing）    ","date":"August 5, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0005_deeplearning_summary/","summary":"论文入口\n一、机器学习 目前，人工智能研究领域主要体现在一下几个方面：\n  智能感知：通过模拟人的感知能力（视觉、听觉、嗅觉）对外部信息进行感知和识别，并能够对信息进行加工和处理，从而做出反应。\n  智能学习：学习是人工智能的主要标志和获取知识的重要手段，研究机器通过模拟人的学习能力，如何从小样本、大数据中学习，主要有：\n 监督学习：（Supervised Learning）表示机器学习的数据是带有标记的，这些标记可以包括：数据类别、数据属性、特征点位置等。这些标记作为预期效果，不断修正机器的预测结果。常见的监督学习有分类、回归、结构化学习。 半监督学习：（Semi-Supervised Learning）利用少量标注数据和大量无标注数据进行学习的方式。常用的半监督学习算法有：自训练、协同训练 非监督学习：（Unsupervised Learning）表示机器学习的数据是没有标记的。常见的无监督学习有：聚类、降维 强化学习：（Reinforcement Learning）通过智能体和环境的交互，不断学习并调整策略的机器学习算法。这种算法带有一种激励机制，如果智能体根据环境做出一个正确的动作，则施予一定的“正激励”；如果是错误的动作，则给与一定的“负激励”。通过不断地累加激励，以获取激励最大化的回报。做火热的应用就是 AlphaGo Zero    认知推理：模拟人的认知能力，主要研究知识表示、推理、规划、决策等，主要有自然语言处理、脑科学。\n    二、表征学习 表征：为了提高机器学习系统的准确率，需要将输入信息转化为有效的特征，或者更一般性地称为 表征（Representation） 表征学习：如果有一种算法可以自动地学习有效的特征，并提高最终机器学习模型的性能，那么这种学习就可以叫做 表征学习。 表征学习的关键是解决语义鸿沟（Semantic Gap）问题。即：输入数据的底层特征与高层语义信息之间的不一致性和差异性。\n机器学习中经常用两种方式表示特征：局部表示(Local Representation)、分布式表示(Distributed Representation)。\n比如：颜色的表示。\n 局部表示：也称为离散表示或者符号表示，比如：one-hot向量的形式。假设所有颜色 构成一个词表 $\\bold V$，此时，可以用一个 $|\\bold V|$ 维的one-hot向量来表示一中颜色。但是，one-hot向量的维数很高，且不能扩展，如果有一种新的颜色，就需要增加一维来表示。不同颜色之间的相似度都为0，无法直到“红色”和“中国红”的相似度要高于“红色”和“黑色”的相似度。 分布式表示：另一种表示颜色的方法是用RGB值来表示颜色，不同颜色对应RGB三维空间中的一个点。分布式表示通常可以表示低维的稠密向量。  嵌入：神经网络将高维的局部表示空间 $\\R^{|\\bold V|}$，映射到一个非常低维的分布式表示空间 $\\R^{D}$。在这个低维空间中，每个特征不再是坐标轴上的点，而是分散在整个低维空间中，在机器学习中，这个过程也成为嵌入（Embedding）。比如：自然语言中词的分布式表示也经常叫做词嵌入。\n要学习到一种好的高层次语义表示（一般为分布式表示），通常只有从底层特征开始，经过多步非线性转换才能得到。深层结构的优点是可以提高特征的重用性，从而指数级增强表示能力。因此，表示学习的关键是构建具有一定深度的多层次特征表示。\n  三、深度学习 深度学习是机器学习的一个重要的、新的研究领域，源于对神经网络的进一步研究，通常采用包含多个隐藏层的神经网络结构，目的是建立、模拟人脑学习过程。\n在描述深度学习之前，先回顾下机器学习和深度学习的关系。\n  机器学习：研究如何使用计算机系统利用经验改善性能。在机器学习的众多研究方向中，表征学习关注如何自动找出表示数据的合适方式，以便更好地将输入变换为正确的输出。\n  深度学习：是具有多级表示的表征学习方法。在每一级，深度学习通过简单的函数将该级的表示变换为更高级的表示。因此，深度学习模型也可以看作是由许多简单函数复合而成的函数。当这些复合函数足够多时，就可以表达非常复杂的变换。\n作为表征学习的一种，深度学习将自动找出每一级表示数据的合适方式。逐级表示越来越抽象的概念或模式。高层特征是由底层特征通过推演归纳得到。\n深度学习可通过学习一种深层非线性网络结构来表征输入数据，实现复杂函数逼近，具有很强的从少数样本集中学习数据集本质特征的能力。深度学习的主要思想：通过自学习的方法，学习到训练数据的结构，并在该结构上进行有监督训练微调。 以图像为例，它的输入是一堆原始像素值，模型中逐级表示为： graph LR; A(特定位置和角度的边缘) -- B(由边缘组合得出的花纹) B -- C(由多种花纹进一步汇合得到的特定部位) C -- D(由特定部位组合得到的整个目标)    1、神经元 神经元模型：","tags":["机器学习","深度学习","简介"],"title":"深度学习开篇"},{"categories":["Basic"],"contents":"官方文档\n线上工具\n一、基础篇 1. 输入公式   行内公式： 格式：$数学公式$ 例如：$x^2=1$ :  $x^2=1$\n  行间公式：\n$$\n数学公式\n$$\n例如: $$f(x)=\\int_{-\\infty}^\\infty\\widehat f\\xi\\ e^{2\\pi i\\xi x}\\ d\\xi$$ $$f(x)=\\int_{-\\infty}^\\infty\\widehat f\\xi\\ e^{2\\pi i\\xi x}\\ d\\xi$$\n  二、进阶篇 1. 声调/变音符号 \\dot{a}, \\ddot{a}, \\acute{a}, \\grave{a}\n$\\dot{a}, \\ddot{a}, \\acute{a}, \\grave{a}$\n\\check{a}, \\breve{a}, \\tilde{a}, \\bar{a}\n$\\check{a}, \\breve{a}, \\tilde{a}, \\bar{a}$\n\\hat{a}, \\widehat{a}, \\vec{a}, \\tilde{a}, \\widetilde{a}\n$\\hat{a}, \\widehat{a}, \\vec{a}, \\tilde{a}, \\widetilde{a}$\na', a'' \n$a', a''$\n2. 标准函数   指数/上下标\n 指数：\\exp_a b = a^b, \\exp b = e^b, 10^m\n$\\exp_a b = a^b, \\exp b = e^b, 10^m$ 前置上下标：{}_1^2\\!X_3^4\n${}_1^2!X_3^4$ 导数：x', \\dot{x}, \\ddot{y}\n$x', \\dot{x}, \\ddot{y}$    对数\n\\ln c, \\lg d = \\log e, \\log_{10} f\n$\\ln c, \\lg d = \\log e, \\log_{10} f$\n  三角函数\n \\sin a, \\cos b, \\tan c, \\cot d, \\sec e, \\csc f\n$\\sin{a}, \\cos b, \\tan c, \\cot d, \\sec e, \\csc f$ \\arcsin a, \\arccos b, \\arctan c\n$\\arcsin a, \\arccos b, \\arctan c$    绝对值\n \\left\\vert s \\right\\vert\n$\\left\\vert s \\right\\vert$ \\lVert z \\rVert\n$\\lVert z \\rVert$    最大值/最小值\n\\min(x,y), \\max(x,y)\n$\\min(x,y), \\max(x,y)$\n  3. 界限/极限 \\min x, \\max y, \\inf s, \\sup t, \n$\\min x, \\max y, \\inf s, \\sup t$\n\\lim_{x \\to \\infty} \\frac{1}{n(n+1)}\n$\\lim_{x \\to \\infty} \\frac{1}{n(n+1)}$\n4. 微分/导数 dt, \\mathrm{d}t, \\partial t, \\nabla\\psi\n$dt, \\mathrm{d}t, \\partial t, \\nabla\\psi$\ndy/dx, \\mathrm{d}y/\\mathrm{d}x\n$dy/dx, \\mathrm{d}y/\\mathrm{d}x$\n\\frac{dy}{dx}, \\frac{\\mathrm{d}y}{\\mathrm{d}x}, \\frac{\\partial^2}{\\partial x_1\\partial x_2}y\n$\\frac{dy}{dx}, \\frac{\\mathrm{d}y}{\\mathrm{d}x}, \\frac{\\partial^2}{\\partial x_1\\partial x_2}y$\n\\prime, \\backprime, f^\\prime, f', f'', f^{(3)}, \\dot y, \\ddot y\n$\\prime, \\backprime, f^\\prime, f', f'', f^{(3)}, \\dot y, \\ddot y$\n5. 根号/分数 \\surd, \\sqrt{2}, \\sqrt[n]{}, \\sqrt[3]{\\frac{x^3+y^3}{2}}\n$\\surd, \\sqrt{2}, \\sqrt[n]{}, \\sqrt[3]{\\frac{x^3+y^3}{2}}$\n6. 运算符            $\\sum$ \\sum $\\prod$ \\prod $\\bigotimes$ \\bigotimes $\\bigvee$ \\bigvee   $\\int$ \\int $\\coprod$ \\coprod $\\bigoplus$ \\bigoplus $\\bigwedge$ \\bigwedge   $\\iint$ \\iint $\\intop$ \\intop $\\bigodot$ \\bigodot $\\bigcap$ \\bigcap   $\\iiint$ \\iiint $\\smallint$ \\smallint $\\biguplus$ \\biguplus $\\bigcup$ \\bigcup   $\\oint$ \\oint $\\oiint$ \\oiint $\\oiiint$ \\oiiint $\\bigsqcup$ \\bigsqcup         $+$ + $\\cdot$ \\cdot $\\gtrdot$ \\gtrdot $x \\pmod a$ x \\pmod a   $-$ - $\\cdotp$ \\cdotp $\\intercal$ \\intercal $x \\pod a$ x \\pod a   $/$ / $\\centerdot$ \\centerdot $\\land$ \\land $\\rhd$ \\rhd   $*$ * $\\circ$ \\circ $\\leftthreetimes$ \\leftthreetimes $\\rightthreetimes$ \\rightthreetimes   $\\amalg$ \\amalg $\\circledast$ \\circledast $\\ldotp$ \\ldotp $\\rtimes$ \\rtimes   $\\And$ \\And $\\circledcirc$ \\circledcirc $\\lor$ \\lor $\\setminus$ \\setminus   $\\ast$ \\ast $\\circleddash$ \\circleddash $\\lessdot$ \\lessdot $\\smallsetminus$ \\smallsetminus   $\\barwedge$ \\barwedge $\\Cup$ \\Cup $\\lhd$ \\lhd $\\sqcap$ \\sqcap   $\\bigcirc$ \\bigcirc $\\cup$ \\cup $\\ltimes$ \\ltimes $\\sqcup$ \\sqcup   $\\bmod$ \\bmod $\\curlyvee$ \\curlyvee $x \\mod a$ x\\mod a $\\times$ \\times   $\\boxdot$ \\boxdot $\\curlywedge$ \\curlywedge $\\mp$ \\mp $\\unlhd$ \\unlhd   $\\boxminus$ \\boxminus $\\div$ \\div $\\odot$ \\odot $\\unrhd$ \\unrhd   $\\boxplus$ \\boxplus $\\divideontimes$ \\divideontimes $\\ominus$ \\ominus $\\uplus$ \\uplus   $\\boxtimes$ \\boxtimes $\\dotplus$ \\dotplus $\\oplus$ \\oplus $\\vee$ \\vee   $\\bullet$ \\bullet $\\doublebarwedge$ \\doublebarwedge $\\otimes$ \\otimes $\\veebar$ \\veebar   $\\Cap$ \\Cap $\\doublecap$ \\doublecap $\\oslash$ \\oslash $\\wedge$ \\wedge   $\\cap$ \\cap $\\doublecup$ \\doublecup $\\pm$ \\pm or \\plusmn $\\wr$ \\wr    直接输入: $∫ ∬ ∭ ∮ ∏ ∐ ∑ ⋀ ⋁ ⋂ ⋃ ⨀ ⨁ ⨂ ⨄ ⨆$ ∯ ∰\n$+ - / * ⋅ ± × ÷ ∓ ∔ ∧ ∨ ∩ ∪ ≀ ⊎ ⊓ ⊔ ⊕ ⊖ ⊗ ⊘ ⊙ ⊚ ⊛ ⊝ ◯$\n7. 关系符号 =, \\ne, \\neq, \\equiv, \\not\\equiv\n$=, \\ne, \\neq, \\equiv, \\not\\equiv$\n\\doteq, \\doteqdot, \\overset{\\underset{\\mathrm{def}}{}}{=}\n$\\doteq, \\doteqdot, \\overset{\\underset{\\mathrm{def}}{}}{=}$\n\\sim, \\nsim, \\backsim, \\thicksim, \\simeq, \\backsimeq, \\eqsim, \\cong, \\ncong\n$\\sim, \\nsim, \\backsim, \\thicksim, \\simeq, \\backsimeq, \\eqsim, \\cong, \\ncong$\n\\approx, \\thickapprox, \\approxeq, \\asymp, \\propto, \\varpropto\n$\\approx, \\thickapprox, \\approxeq, \\asymp, \\propto, \\varpropto$\n\u0026lt;, \\nless, \\ll, \\not\\ll, \\lll, \\not\\lll, \\lessdot\n$\u0026lt;, \\nless, \\ll, \\not\\ll, \\lll, \\not\\lll, \\lessdot$\n\u0026gt;, \\ngtr, \\gg, \\not\\gg, \\ggg, \\not\\ggg, \\gtrdot\n$\u0026gt;, \\ngtr, \\gg, \\not\\gg, \\ggg, \\not\\ggg, \\gtrdot$\n\\le, \\leq, \\lneq, \\leqq, \\nleq, \\nleqq, \\lneqq, \\lvertneqq\n$\\le, \\leq, \\lneq, \\leqq, \\nleq, \\nleqq, \\lneqq, \\lvertneqq$\n\\ge, \\geq, \\gneq, \\geqq, \\ngeq, \\ngeqq, \\gneqq, \\gvertneqq\n$\\ge, \\geq, \\gneq, \\geqq, \\ngeq, \\ngeqq, \\gneqq, \\gvertneqq$\n\\leqslant, \\nleqslant, \\eqslantless\n$\\leqslant, \\nleqslant, \\eqslantless$\n\\geqslant, \\ngeqslant, \\eqslantgtr\n$\\geqslant, \\ngeqslant, \\eqslantgtr$\n\\lesssim, \\lnsim, \\lessapprox, \\lnapprox\n$\\lesssim, \\lnsim, \\lessapprox, \\lnapprox$\n\\gtrsim, \\gnsim, \\gtrapprox, \\gnapprox\n$\\gtrsim, \\gnsim, \\gtrapprox, \\gnapprox$\n8. 集合 \\empty \\emptyset, \\varnothing\n$\\empty, \\emptyset, \\varnothing$\n\\in, \\notin \\not\\in, \\ni, \\not\\ni\n$\\in, \\notin \\not\\in, \\ni, \\not\\ni$\n\\cap, \\Cap, \\sqcap, \\bigcap\n$\\cap, \\Cap, \\sqcap, \\bigcap$\n\\cup, \\Cup, \\sqcup, \\bigcup, \\bigsqcup, \\uplus, \\biguplus\n$\\cup, \\Cup, \\sqcup, \\bigcup, \\bigsqcup, \\uplus, \\biguplus$\n\\subset, \\Subset, \\sqsubset\n$\\subset, \\Subset, \\sqsubset$\n\\supset, \\Supset, \\sqsupset\n$\\supset, \\Supset, \\sqsupset$\n\\subseteq, \\nsubseteq, \\subsetneq, \\varsubsetneq, \\sqsubseteq\n$\\subseteq, \\nsubseteq, \\subsetneq, \\varsubsetneq, \\sqsubseteq$\n\\supseteq, \\nsupseteq, \\supsetneq, \\varsupsetneq, \\sqsupseteq\n$\\supseteq, \\nsupseteq, \\supsetneq, \\varsupsetneq, \\sqsupseteq$\n\\subseteqq, \\nsubseteqq, \\subsetneqq, \\varsubsetneqq\n$\\subseteqq, \\nsubseteqq, \\subsetneqq, \\varsubsetneqq$\n\\supseteqq, \\nsupseteqq, \\supsetneqq, \\varsupsetneqq\n$\\supseteqq, \\nsupseteqq, \\supsetneqq, \\varsupsetneqq$\n9. 几何符号           % comment $\\dots$ \\dots $\\KaTeX$ \\KaTeX   $\\%$ \\\\% $\\cdots$ \\cdots $\\LaTeX$ \\LaTeX   $\\#$ \\\\# $\\ddots$ \\ddots $\\TeX$ \\TeX   $\\\u0026amp;$ \\\\\u0026amp; $\\ldots$ \\ldots $\\nabla$ \\nabla   $\\_$ \\\\_ $\\vdots$ \\vdots $\\infty$ \\infty   $\\text{\\textunderscore}$ \\text{\\textunderscore} $\\dotsb$ \\dotsb $\\infin$ \\infin   $\\text{\u0026ndash;}$ \\text{--} $\\dotsc$ \\dotsc $\\checkmark$ \\checkmark   $\\text{\\textendash}$ \\text{\\textendash} $\\dotsi$ \\dotsi $\\dag$ \\dag   $\\text{\u0026mdash;}$ \\text{---} $\\dotsm$ \\dotsm $\\dagger$ \\dagger   $\\text{\\textemdash}$ \\text{\\textemdash} $\\dotso$ \\dotso $\\text{\\textdagger}$ \\text{\\textdagger}   $\\text{\\textasciitilde}$ \\text{\\textasciitilde} $\\sdot$ \\sdot $\\ddag$ \\ddag   $\\text{\\textasciicircum}$ \\text{\\textasciicircum} $\\mathellipsis$ \\mathellipsis $\\ddagger$ \\ddagger    $\\text{\\textellipsis}$ \\text{\\textellipsis} $\\text{\\textdaggerdbl}$ \\text{\\textdaggerdbl}   $\\text{\\textquoteleft}$ text{\\textquoteleft} $\\Box$ \\Box $\\Dagger$ \\Dagger   $\\lq$ \\lq $\\square$ \\square $\\angle$ \\angle   $\\text{\\textquoteright}$ \\text{\\textquoteright} $\\blacksquare$ \\blacksquare $\\measuredangle$ \\measuredangle   $\\rq$ \\rq $\\triangle$ \\triangle $\\sphericalangle$ \\sphericalangle   $\\text{\\textquotedblleft}$ \\text{\\textquotedblleft} $\\triangledown$ \\triangledown $\\top$ \\top   $\u0026quot;$ \u0026quot; $\\triangleleft$ \\triangleleft $\\bot$ \\bot   $\\text{\\textquotedblright}$ \\text{\\textquotedblright} $\\triangleright$ \\triangleright $$$ \\$   $\\colon$ \\colon $\\bigtriangledown$ \\bigtriangledown $\\text{\\textdollar}$ \\text{\\textdollar}   $\\backprime$ \\backprime $\\bigtriangleup$ \\bigtriangleup $\\pounds$ \\pounds   $\\prime$ \\prime $\\blacktriangle$ \\blacktriangle $\\mathsterling$ \\mathsterling   $\\text{\\textless}$ \\text{\\textless} $\\blacktriangledown$ \\blacktriangledown $\\text{\\textsterling}$ \\text{\\textsterling}   $\\text{\\textgreater}$ \\text{\\textgreater} $\\blacktriangleleft$ \\blacktriangleleft $\\yen$ \\yen   $\\text{\\textbar}$ \\text{\\textbar} $\\blacktriangleright$ \\blacktriangleright $\\surd$ \\surd   $\\text{\\textbardbl}$ \\text{\\textbardbl} $\\diamond$ \\diamond $\\degree$ \\degree   $\\text{\\textbraceleft}$ \\text{\\textbraceleft} $\\Diamond$ \\Diamond $\\text{\\textdegree}$ \\text{\\textdegree}   $\\text{\\textbraceright}$ \\text{\\textbraceright} $\\lozenge$ \\lozenge $\\mho$ \\mho   $\\text{\\textbackslash}$ \\text{\\textbackslash} $\\blacklozenge$ \\blacklozenge $\\diagdown$ \\diagdown   $\\text{\\P}$ \\text{\\P} or \\P $\\star$ \\star $\\diagup$ \\diagup   $\\text{\\S}$ \\text{\\S} or \\S $\\bigstar$ \\bigstar $\\flat$ \\flat   $\\text{\\sect}$ \\text{\\sect} $\\clubsuit$ \\clubsuit $\\natural$ \\natural   $\\copyright$ \\copyright $\\clubs$ \\clubs $\\sharp$ \\sharp   $\\circledR$ \\circledR $\\diamondsuit$ \\diamondsuit $\\heartsuit$ \\heartsuit   $\\text{\\textregistered}$ \\text{\\textregistered} $\\diamonds$ \\diamonds $\\hearts$ \\hearts   $\\circledS$ \\circledS $\\spadesuit$ \\spadesuit $\\spades$ \\spades   $\\text{\\textcircled a}$ \\text{\\textcircled a} $\\maltese$ \\maltese     Direct Input: § ¶ $ £ ¥ ∇ ∞ · ∠ ∡ ∢ ♠ ♡ ♢ ♣ ♭ ♮ ♯ ✓ … ⋮ ⋯ ⋱ !$ ‼ ⦵\n10. 逻辑符号 \\forall, \\exists, \\nexists\n$\\forall, \\exists, \\nexists$\n\\therefore, \\because, \\And\n$\\therefore, \\because, \\And$\n\\lor, \\vee, \\curlyvee, \\bigvee\n$\\lor, \\vee, \\curlyvee, \\bigvee$\n\\land, \\wedge, \\curlywedge, \\bigwedge\n$\\land, \\wedge, \\curlywedge, \\bigwedge$\n\\bar{q}, \\bar{abc}, \\overline{q}, \\overline{abc}, \\lnot \\neg, \\not\\operatorname{R}, \\bot, \\top\n$\\bar{q}, \\bar{abc}, \\overline{q}, \\overline{abc}, \\lnot \\neg, \\not\\operatorname{R}, \\bot, \\top$\n11. 箭头           $\\circlearrowleft$ \\circlearrowleft $\\leftharpoonup$ \\leftharpoonup $\\rArr$ \\rArr   $\\circlearrowright$ \\circlearrowright $\\leftleftarrows$ \\leftleftarrows $\\rarr$ \\rarr   $\\curvearrowleft$ \\curvearrowleft $\\leftrightarrow$ \\leftrightarrow $\\restriction$ \\restriction   $\\curvearrowright$ \\curvearrowright $\\Leftrightarrow$ \\Leftrightarrow $\\rightarrow$ \\rightarrow   $\\Darr$ \\Darr $\\leftrightarrows$ \\leftrightarrows $\\Rightarrow$ \\Rightarrow   $\\dArr$ \\dArr $\\leftrightharpoons$ \\leftrightharpoons $\\rightarrowtail$ \\rightarrowtail   $\\darr$ \\darr $\\leftrightsquigarrow$ \\leftrightsquigarrow $\\rightharpoondown$ \\rightharpoondown   $\\dashleftarrow$ \\dashleftarrow $\\Lleftarrow$ \\Lleftarrow $\\rightharpoonup$ \\rightharpoonup   $\\dashrightarrow$ \\dashrightarrow $\\longleftarrow$ \\longleftarrow $\\rightleftarrows$ \\rightleftarrows   $\\downarrow$ \\downarrow $\\Longleftarrow$ \\Longleftarrow $\\rightleftharpoons$ \\rightleftharpoons   $\\Downarrow$ \\Downarrow $\\longleftrightarrow$ \\longleftrightarrow $\\rightrightarrows$ \\rightrightarrows   $\\downdownarrows$ \\downdownarrows $\\Longleftrightarrow$ \\Longleftrightarrow $\\rightsquigarrow$ \\rightsquigarrow   $\\downharpoonleft$ \\downharpoonleft $\\longmapsto$ \\longmapsto $\\Rrightarrow$ \\Rrightarrow   $\\downharpoonright$ \\downharpoonright $\\longrightarrow$ \\longrightarrow $\\Rsh$ \\Rsh   $\\gets$ \\gets $\\Longrightarrow$ \\Longrightarrow $\\searrow$ \\searrow   $\\Harr$ \\Harr $\\looparrowleft$ \\looparrowleft $\\swarrow$ \\swarrow   $\\hArr$ \\hArr $\\looparrowright$ \\looparrowright $\\to$ \\to   $\\harr$ \\harr $\\Lrarr$ \\Lrarr $\\twoheadleftarrow$ \\twoheadleftarrow   $\\hookleftarrow$ \\hookleftarrow $\\lrArr$ \\lrArr $\\twoheadrightarrow$ \\twoheadrightarrow   $\\hookrightarrow$ \\hookrightarrow $\\lrarr$ \\lrarr $\\Uarr$ \\Uarr   $\\iff$ \\iff $\\Lsh$ \\Lsh $\\uArr$ \\uArr   $\\impliedby$ \\impliedby $\\mapsto$ \\mapsto $\\uarr$ \\uarr   $\\implies$ \\implies $\\nearrow$ \\nearrow $\\uparrow$ \\uparrow   $\\Larr$ \\Larr $\\nleftarrow$ \\nleftarrow $\\Uparrow$ \\Uparrow   $\\lArr$ \\lArr $\\nLeftarrow$ \\nLeftarrow $\\updownarrow$ \\updownarrow   $\\larr$ \\larr $\\nleftrightarrow$ \\nleftrightarrow $\\Updownarrow$ \\Updownarrow   $\\leadsto$ \\leadsto $\\nLeftrightarrow$ \\nLeftrightarrow $\\upharpoonleft$ \\upharpoonleft   $\\leftarrow$ \\leftarrow $\\nrightarrow$ \\nrightarrow $\\upharpoonright$ \\upharpoonright   $\\Leftarrow$ \\Leftarrow $\\nRightarrow$ \\nRightarrow $\\upuparrows$ \\upuparrows   $\\leftarrowtail$ \\leftarrowtail $\\nwarrow$ \\nwarrow    $\\leftharpoondown$ \\leftharpoondown $\\Rarr$ \\Rarr    $\\xleftarrow{abc}$ \\xleftarrow{abc} $\\xrightarrow[under]{over}$ \\xrightarrow[under]{over}    $\\xLeftarrow{abc}$ \\xLeftarrow{abc} $\\xRightarrow{abc}$ \\xRightarrow{abc}    $\\xleftrightarrow{abc}$ \\xleftrightarrow{abc} $\\xLeftrightarrow{abc}$ \\xLeftrightarrow{abc}    $\\xhookleftarrow{abc}$ \\xhookleftarrow{abc} $\\xhookrightarrow{abc}$ \\xhookrightarrow{abc}    $\\xtwoheadleftarrow{abc}$ \\xtwoheadleftarrow{abc} $\\xtwoheadrightarrow{abc}$ \\xtwoheadrightarrow{abc}    $\\xleftharpoonup{abc}$ \\xleftharpoonup{abc} $\\xrightharpoonup{abc}$ \\xrightharpoonup{abc}    $\\xleftharpoondown{abc}$ \\xleftharpoondown{abc} $\\xrightharpoondown{abc}$ \\xrightharpoondown{abc}    $\\xleftrightharpoons{abc}$ \\xleftrightharpoons{abc} $\\xrightleftharpoons{abc}$ \\xrightleftharpoons{abc}    $\\xtofrom{abc}$ \\xtofrom{abc} $\\xmapsto{abc}$ \\xmapsto{abc}    $\\xlongequal{abc}$ \\xlongequal{abc}      Direct Input: $← ↑ → ↓ ↔ ↕ ↖ ↗ ↘ ↙ ↚ ↛ ↞ ↠ ↢ ↣ ↦ ↩ ↪ ↫ ↬ ↭ ↮ ↰ ↱↶ ↷ ↺ ↻ ↼ ↽ ↾ ↾ ↿ ⇀ ⇁ ⇂ ⇃ ⇄ ⇆ ⇇ ⇈ ⇉ ⇊ ⇋ ⇌⇍ ⇎ ⇏ ⇐ ⇑ ⇒ ⇓ ⇔ ⇕ ⇚ ⇛ ⇝ ⇠ ⇢ ⟵ ⟶ ⟷ ⟸ ⟹ ⟺ ⟼$ ↽\n12. 上下标    功能 语法 效果     上标 a^2 $a^2$   下标 a_2 $a_2$   组合 a^{2+2} $a^{2+2}$   结合上下标 x_2^3 $x_2^3$   前置上下标 {}_1^2\\!X_3^4 ${}_1^2!X_3^4$   导数 x', \\dot{x}, \\ddot{x} $x', \\dot{x}, \\ddot{x}$   向量 \\vec{c}, \\overleftarrow{a b}, \\overrightarrow{c d}, \\overleftrightarrow{a b} $\\vec{c}, \\overleftarrow{a b}, \\overrightarrow{c d}, \\overleftrightarrow{a b}$   弧线 \\widehat{e f g}, \\overset{\\frown} {AB} $\\widehat{e f g}, \\overset{\\frown} {AB}$   上/下划线 \\overline{h i j}, \\underline{k l m} $\\overline{h i j}, \\underline{k l m}$   上括号 \\overbrace{1+2+\\cdots+100}\n\\begin{matrix} 5050 \\\\ \\overbrace{ 1+2+\\cdots+100 } \\end{matrix} $\\overbrace{1+2+\\cdots+100}$\n$\\begin{matrix} 5050 \\ \\overbrace{ 1+2+\\cdots+100 } \\end{matrix}$   下括号 \\underbrace{a+b+\\cdots+z}\n\\begin{matrix} \\underbrace{ a+b+\\cdots+z } \\\\ 26 \\end{matrix} $\\underbrace{a+b+\\cdots+z}$\n$\\begin{matrix} \\underbrace{ a+b+\\cdots+z } \\ 26 \\end{matrix}$   累加 \\sum_{k=1}^N k^2\n\\begin{matrix} \\sum_{k=1}^N k^2 \\end{matrix} $\\sum_{k=1}^N k^2$\n$\\begin{matrix} \\sum_{k=1}^N k^2 \\end{matrix}$   累加-格式 \\displaystyle\\sum_{\\substack{0\u0026lt;i\u0026lt;m\\\\\\\\0\u0026lt;j\u0026lt;n}}\\textstyle\\sum_{\\substack{0\u0026lt;i\u0026lt;m\\\\\\\\0\u0026lt;j\u0026lt;n}} $\\displaystyle\\sum_{\\substack{0\u0026lt;i\u0026lt;m\\\\0\u0026lt;j\u0026lt;n}}$$\\textstyle\\sum_{\\substack{0\u0026lt;i\u0026lt;m\\\\0\u0026lt;j\u0026lt;n}}$   累乘 \\prod_{i=1}^N x_i $\\prod_{i=1}^N x_i$   上积 \\coprod_{i=1}^N x_i $\\coprod_{i=1}^N x_i$   极限 \\lim_{n \\to \\infty}x_n $\\lim_{n \\to \\infty}x_n$   极限-格式 \\lim\\limits_{n \\to \\infty}x_n\\lim\\nolimits_{n \\to \\infty}x_n $\\lim\\limits_{n \\to \\infty}x_n$\n$\\lim\\nolimits_{n \\to \\infty}x_n$   积分 \\int_{-N}^{N} e^x\\, {\\rm d}x $\\int_{-N}^{N} e^x, {\\rm d}x$   双重积分 \\iint_{D}^{W} \\, \\mathrm{d}x\\,\\mathrm{d}y $\\iint_{D}^{W} , \\mathrm{d}x,\\mathrm{d}y$   三重积分 \\iiint_{E}^{V} \\, \\mathrm{d}x\\,\\mathrm{d}y\\,\\mathrm{d}z $\\iiint_{E}^{V} , \\mathrm{d}x,\\mathrm{d}y,\\mathrm{d}z$   闭合 \\oint_{C} x^3\\, \\mathrm{d}x + 4y^2\\, \\mathrm{d}y $\\oint_{C} x^3, \\mathrm{d}x + 4y^2, \\mathrm{d}y$   交集 \\bigcap_1^{n} p $\\bigcap_1^{n} p$   并集 \\bigcup_1^{k} p $\\bigcup_1^{k} p$    13. 分式 通常使用\\frac {分子} {分母} 来生成一个分数。如果分式比较复杂，也可以使用 分子 \\over 分母\n   功能 语法 效果     分数 \\frac{2}{4}=0.5 $\\frac{2}{4}=0.5$   小型分数 \\tfrac{2}{4} = 0.5 $\\tfrac{2}{4} = 0.5$   连分式 \\cfrac{2}{c + \\cfrac{2}{d + \\cfrac{2}{4}}} = a $\\cfrac{2}{c + \\cfrac{2}{d + \\cfrac{2}{4}}} = a$    \\binom{n}{k}, \\dbinom{n}{k}, \\tbinom{n}{k} $\\binom{n}{k}, \\dbinom{n}{k}, \\tbinom{n}{k}$    {n \\choose k}, {n\\brace k}, {n\\brack k} ${n \\choose k}, {n\\brace k}, {n\\brack k}$   二项式系数 \\dbinom{n}{r}=\\binom{n}{n-r}=\\mathrm{C}_n^r=\\mathrm{C}_n^{n-r} $\\dbinom{n}{r}=\\binom{n}{n-r}=\\mathrm{C}_n^r=\\mathrm{C}_n^{n-r}$   小型二项式系数 \\tbinom{n}{r}=\\tbinom{n}{n-r}=\\mathrm{C}_n^r=\\mathrm{C}_n^{n-r} $\\tbinom{n}{r}=\\tbinom{n}{n-r}=\\mathrm{C}_n^r=\\mathrm{C}_n^{n-r}$   大型二项式系数 \\binom{n}{r}=\\dbinom{n}{n-r}=\\mathrm{C}_n^r=\\mathrm{C}_n^{n-r} $\\binom{n}{r}=\\dbinom{n}{n-r}=\\mathrm{C}_n^r=\\mathrm{C}_n^{n-r}$    14. 矩阵    效果 语法 效果 语法     $\\begin{matrix} a \u0026amp; b \\\\ c \u0026amp; d \\end{matrix}$ \\begin{matrix}\na \u0026amp; b \\\\\\\nc \u0026amp; d\n\\end{matrix} $\\begin{array}{cc}a \u0026amp; b\\\\c \u0026amp; d\\end{array}$ \\begin{array}{cc}\na \u0026amp; b \\\\\\\nc \u0026amp; d\n\\end{array}   $\\begin{pmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\end{pmatrix}$ \\begin{pmatrix}\na \u0026amp; b \\\\\\\nc \u0026amp; d\n\\end{pmatrix} $\\begin{bmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\end{bmatrix}$ \\begin{bmatrix}\na \u0026amp; b \\\\\\\nc \u0026amp; d\n\\end{bmatrix}   $\\begin{vmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\end{vmatrix}$ \\begin{vmatrix}\na \u0026amp; b \\\\\\\nc \u0026amp; d\n\\end{vmatrix} $\\begin{Vmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\end{Vmatrix}$ \\begin{Vmatrix}\na \u0026amp; b \\\\\\\nc \u0026amp; d\n\\end{Vmatrix}   $\\begin{Bmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\end{Bmatrix}$ \\begin{Bmatrix}\na \u0026amp; b \\\\\\\nc \u0026amp; d\n\\end{Bmatrix} $\\def\\arraystretch{1.5}\\begin{array}{c:c:c} a \u0026amp; b \u0026amp; c \\\\ \\hline d \u0026amp; e \u0026amp; f \\\\ \\hdashline g \u0026amp; h \u0026amp; i \\end{array}$ \\def\\arraystretch{1.5}\n\\begin{array}{c:c:c}\na \u0026amp; b \u0026amp; c \\\\\\ \\hline\nd \u0026amp; e \u0026amp; f \\\\\\\n\\hdashline\ng \u0026amp; h \u0026amp; i\n\\end{array}   $x = \\begin{cases} a \u0026amp;\\text{if } b \\\\ c \u0026amp;\\text{if } d \\end{cases}$ x = \\begin{cases}\na \u0026amp;\\text{if } b \\\\\\\nc \u0026amp;\\text{if } d\n\\end{cases} 无效呢？ \\begin{rcases}\na \u0026amp;\\text{if } b \\\\\\\nc \u0026amp;\\text{if } d\n\\end{rcases}⇒…   $\\begin{smallmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\end{smallmatrix}$ \\begin{smallmatrix}\na \u0026amp; b \\\\\\\nc \u0026amp; d\n\\end{smallmatrix} $$\\sum_{\\begin{subarray}{l} i\\in\\Lambda\\\\ 0\u0026lt;j\u0026lt;n\\end{subarray}}$$ \\sum_{\n\\begin{subarray}{l}\ni\\in\\Lambda\\\\\\\n0\u0026lt;j\u0026lt;n\n\\end{subarray}}    15. 希腊字母 直接输入: $Α Β Γ Δ Ε Ζ Η Θ Ι \\allowbreak Κ Λ Μ Ν Ξ Ο Π Ρ Σ Τ Υ Φ Χ Ψ Ω$ $\\allowbreak α β γ δ ϵ ζ η θ ι κ λ μ ν ξ o π \\allowbreak ρ σ τ υ ϕ χ ψ ω ε ϑ ϖ ϱ ς φ ϝ$\n           $\\Alpha$ \\Alpha $\\Beta$ \\Beta $\\Gamma$ \\Gamma $\\Delta$ \\Delta   $\\Epsilon$ \\Epsilon $\\Zeta$ \\Zeta $\\Eta$ \\Eta $\\Theta$ \\Theta   $\\Iota$ \\Iota $\\Kappa$ \\Kappa $\\Lambda$ \\Lambda $\\Mu$ \\Mu   $\\Nu$ \\Nu $\\Xi$ \\Xi $\\Omicron$ \\Omicron $\\Pi$ \\Pi   $\\Rho$ \\Rho $\\Sigma$ \\Sigma $\\Tau$ \\Tau $\\Upsilon$ \\Upsilon   $\\Phi$ \\Phi $\\Chi$ \\Chi $\\Psi$ \\Psi $\\Omega$ \\Omega   $\\varGamma$ \\varGamma $\\varDelta$ \\varDelta $\\varTheta$ \\varTheta $\\varLambda$ \\varLambda   $\\varXi$ \\varXi $\\varPi$ \\varPi $\\varSigma$ \\varSigma $\\varUpsilon$ \\varUpsilon   $\\varPhi$ \\varPhi $\\varPsi$ \\varPsi $\\varOmega$ \\varOmega    $\\alpha$ \\alpha $\\beta$ \\beta $\\gamma$ \\gamma $\\delta$ \\delta   $\\epsilon$ \\epsilon $\\zeta$ \\zeta $\\eta$ \\eta $\\theta$ \\theta   $\\iota$ \\iota $\\kappa$ \\kappa $\\lambda$ \\lambda $\\mu$ \\mu   $\\nu$ \\nu $\\xi$ \\xi $\\omicron$ \\omicron $\\pi$ \\pi   $\\rho$ \\rho $\\sigma$ \\sigma $\\tau$ \\tau $\\upsilon$ \\upsilon   $\\phi$ \\phi $\\chi$ \\chi $\\psi$ \\psi $\\omega$ \\omega   $\\varepsilon$ \\varepsilon $\\varkappa$ \\varkappa $\\vartheta$ \\vartheta $\\thetasym$ \\thetasym   $\\varpi$ \\varpi $\\varrho$ \\varrho $\\varsigma$ \\varsigma $\\varphi$ \\varphi   $\\digamma $ \\digamma       Other Letters\n            $\\imath$ \\imath $\\nabla$ \\nabla $\\Im$ \\Im $\\Reals$ \\Reals $\\text{\\OE}$ \\text{\\OE}   $\\jmath$ \\jmath $\\partial$ \\partial $\\image$ \\image $\\wp$ \\wp $\\text{\\o}$ \\text{\\o}   $\\aleph$ \\aleph $\\Game$ \\Game $\\Bbbk$ \\Bbbk $\\weierp$ \\weierp $\\text{\\O}$ \\text{\\O}   $\\alef$ \\alef $\\Finv$ \\Finv $\\N$ \\N $\\Z$ \\Z $\\text{\\ss}$ \\text{\\ss}   $\\alefsym$ \\alefsym $\\cnums$ \\cnums $\\natnums$ \\natnums $\\text{\\aa}$ \\text{\\aa} $\\text{\\i}$ \\text{\\i}   $\\beth$ \\beth $\\Complex$ \\Complex $\\R$ \\R $\\text{\\AA}$ \\text{\\AA} $\\text{\\j}$ \\text{\\j}   $\\gimel$ \\gimel $\\ell$ \\ell $\\Re$ \\Re $\\text{\\ae}$ \\text{\\ae}    $\\daleth$ \\daleth $\\hbar$ \\hbar $\\real$ \\real $\\text{\\AE}$ \\text{\\AE}    $\\eth$ \\eth $\\hslash$ \\hslash $\\reals$ \\reals $\\text{\\oe}$ \\text{\\oe}     直接输入: $∂ ∇ ℑ Ⅎ ℵ ℶ ℷ ℸ ⅁ ℏ ð − ∗$ ÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖÙÚÛÜÝÞßàáâãäåçèéêëìíîïðñòóôöùúûüýþÿ\n16. 字体大小          $\\Huge AB$ \\Huge AB $\\normalsize AB$ \\normalsize AB   $\\huge AB$ \\huge AB $\\small AB$ \\small AB   $\\LARGE AB$ \\LARGE AB $\\footnotesize AB$ \\footnotesize AB   $\\Large AB$ \\Large AB $\\scriptsize AB$ \\scriptsize AB   $\\large AB$ \\large AB $\\tiny AB$ \\tiny AB    17. 颜色    语法 效果     \\color{blue} F=ma $\\color{blue} F=ma$   \\textcolor{blue}{F=ma} $\\textcolor{blue}{F=ma}$   \\textcolor{#228B22}{F=ma} $\\textcolor{#228B22}{F=ma}$   \\colorbox{aqua}{$F=ma$} $\\colorbox{aqua}{$F=ma$}$   \\fcolorbox{red}{aqua}{$F=ma$} $\\fcolorbox{red}{aqua}{$F=ma$}$    18. 字体           $\\mathrm{Ab0}$ \\mathrm{Ab0} $\\mathbf{Ab0}$ \\mathbf{Ab0} $\\mathit{Ab0}$ \\mathit{Ab0}   $\\mathnormal{Ab0}$ \\mathnormal{Ab0} $\\textbf{Ab0}$ \\textbf{Ab0} $\\textit{Ab0}$ \\textit{Ab0}   $\\textrm{Ab0}$ \\textrm{Ab0} $\\bf Ab0$ \\bf Ab0 $\\it Ab0$ \\it Ab0   $\\rm Ab0$ \\rm Ab0 $\\bold{Ab0}$ \\bold{Ab0} $\\textup{Ab0}$ \\textup{Ab0}   $\\textnormal{Ab0}$ \\textnormal{Ab0} $\\boldsymbol{Ab0}$ \\boldsymbol{Ab} $\\Bbb{AB}$ \\Bbb{AB}   $\\text{Ab0}$ \\text{Ab0} $\\bm{Ab0}$ \\bm{Ab0} $\\mathbb{AB}$ \\mathbb{AB}   $\\mathsf{Ab0}$ \\mathsf{Ab0} $\\textmd{Ab0}$ \\textmd{Ab0} $\\frak{Ab0}$ \\frak{Ab0}   $\\textsf{Ab0}$ \\textsf{Ab0} $\\mathtt{Ab0}$ \\mathtt{Ab0} $\\mathfrak{Ab0}$ \\mathfrak{Ab0}   $\\sf Ab0$ \\sf Ab0 $\\texttt{Ab0}$ \\texttt{Ab0} $\\mathcal{AB0}$ \\mathcal{AB0}    $\\tt Ab0$ \\tt Ab0 $\\cal AB0$ \\cal AB0     $\\mathscr{AB}$ \\mathscr{AB}    19. 括号             $(~)$ ( ) $\\lparen~\\rparen$ \\lparen\n$~~~~$\\rparen $⌈~⌉$ ⌈ ⌉ $\\lceil~\\rceil$ \\lceil\n$~~~~~$\\rceil $\\uparrow$ \\uparrow   $[~]$ [ ] $\\lbrack~\\rbrack$ \\lbrack\n$~~~~$\\rbrack $⌊~⌋$ ⌊ ⌋ $\\lfloor~\\rfloor$ \\lfloor\n$~~~~~$\\rfloor $\\downarrow$ \\downarrow   ${ }$ \\{ \\} $\\lbrace \\rbrace$ \\lbrace\n$~~~~$\\rbrace $⎰⎱$ ⎰⎱ $\\lmoustache \\rmoustache$ \\lmoustache\n$~~~~$\\rmoustache $\\updownarrow$ \\updownarrow   $⟨~⟩$ ⟨ ⟩ $\\langle~\\rangle$ \\langle\n$~~~~$\\rangle $⟮~⟯$ ⟮ ⟯ $\\lgroup~\\rgroup$ \\lgroup\n$~~~~~$\\rgroup $\\Uparrow$ \\Uparrow   $\\vert$ | $\\vert$ \\vert $┌ ┐$ ┌ ┐ $\\ulcorner \\urcorner$ \\ulcorner\n$~~~~$\\urcorner $\\Downarrow$ \\Downarrow   $\\Vert$ \\| $\\Vert$ \\Vert $└ ┘$ └ ┘ $\\llcorner \\lrcorner$ \\llcorner\n$~~~~$\\lrcorner $\\Updownarrow$ \\Updownarrow   $\\lvert~\\rvert$ \\lvert\n$~~~~$\\rvert $\\lVert~\\rVert$ \\lVert\n$~~~~~$\\rVert \\left. \\right. $\\backslash$ \\backslash   $\\lang~\\rang$ \\lang\n$~~~~$\\rang $\\lt~\\gt$ \\lt \\gt $⟦~⟧$ ⟦ ⟧ $\\llbracket~\\rrbracket$ \\llbracket\n$~~~~$\\rrbracket $\\lBrace~\\rBrace$ \\lBrace \\rBrace    调整尺寸\n$\\left(\\LARGE{AB}\\right)$ \\left(\\LARGE{AB}\\right)\n$( \\big( \\Big( \\bigg( \\Bigg($ ( \\big( \\Big( \\bigg( \\Bigg(\n            \\left \\big \\bigl \\bigm \\bigr   \\middle \\Big \\Bigl \\Bigm \\Bigr   \\right \\bigg \\biggl \\biggm \\biggr    \\Bigg \\Biggl \\Biggm \\Biggr    ","date":"June 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00020_toha-tutorial/0015_latax_formula/","summary":"官方文档\n线上工具\n一、基础篇 1. 输入公式   行内公式： 格式：$数学公式$ 例如：$x^2=1$ :  $x^2=1$\n  行间公式：\n$$\n数学公式\n$$\n例如: $$f(x)=\\int_{-\\infty}^\\infty\\widehat f\\xi\\ e^{2\\pi i\\xi x}\\ d\\xi$$ $$f(x)=\\int_{-\\infty}^\\infty\\widehat f\\xi\\ e^{2\\pi i\\xi x}\\ d\\xi$$\n  二、进阶篇 1. 声调/变音符号 \\dot{a}, \\ddot{a}, \\acute{a}, \\grave{a}\n$\\dot{a}, \\ddot{a}, \\acute{a}, \\grave{a}$\n\\check{a}, \\breve{a}, \\tilde{a}, \\bar{a}\n$\\check{a}, \\breve{a}, \\tilde{a}, \\bar{a}$\n\\hat{a}, \\widehat{a}, \\vec{a}, \\tilde{a}, \\widetilde{a}\n$\\hat{a}, \\widehat{a}, \\vec{a}, \\tilde{a}, \\widetilde{a}$\na', a'' \n$a', a''$\n2. 标准函数   指数/上下标","tags":["Latex","公式"],"title":"Katex公式"},{"categories":["Basic"],"contents":"🤑\nThis is a sample post intended to test the followings:\n Default hero image. Different shortcodes.  一、报警(Alert) The following alerts are available in this theme.\n这是 type=\u0026quot;success\u0026quot;的报警样例.\n格式:\n {{\u0026lt; alert type=\u0026ldquo;success\u0026rdquo; \u0026gt; }}\n内容\n {{\u0026lt; /alert \u0026gt; }}  这是 type=\u0026quot;danger\u0026quot;的报警样例.\n格式:\n {{\u0026lt; alert type=\u0026ldquo;danger\u0026rdquo; \u0026gt; }}\n内容\n {{\u0026lt; /alert \u0026gt; }}  这是 type=\u0026quot;warning\u0026quot;的报警样例.\n格式:\n {{\u0026lt; alert type=\u0026ldquo;warning\u0026rdquo; \u0026gt; }}\n内容\n {{\u0026lt; /alert \u0026gt; }}  这是 type=\u0026quot;info\u0026quot;的报警样例.\n格式:\n {{\u0026lt; alert type=\u0026ldquo;info\u0026rdquo; \u0026gt; }}\n内容\n {{\u0026lt; /alert \u0026gt; }}  这是 type=\u0026quot;dark\u0026quot;的报警样例.\n格式:\n {{\u0026lt; alert type=\u0026ldquo;dark\u0026rdquo; \u0026gt; }}\n内容\n {{\u0026lt; /alert \u0026gt; }}  这是 type=\u0026quot;primary\u0026quot;的报警样例.\n格式:\n {{\u0026lt; alert type=\u0026ldquo;primary\u0026rdquo; \u0026gt; }}\n内容\n {{\u0026lt; /alert \u0026gt; }}  这是 type=\u0026quot;secondary\u0026quot;的报警样例.\n格式:\n {{\u0026lt; alert type=\u0026ldquo;secondary\u0026rdquo; \u0026gt; }}\n内容\n {{\u0026lt; /alert \u0026gt; }}  二、插入图片(Image) 语法格式:\n {{\u0026lt; img src=\u0026quot;/datasets/moon.jpg\u0026quot; title=\u0026ldquo;鼠标停在图片上显示的\u0026rdquo; height=\u0026ldquo;尺寸\u0026rdquo; width=\u0026ldquo;尺寸\u0026rdquo; align=\u0026ldquo;center\u0026rdquo; float=\u0026ldquo;right\u0026rdquo;\u0026gt;}}\n 属性设置：\n float: 图片与文本内容之间的关系，值: right：表示图片在文本的右边(文本中插入图片，图片放在右边)\nalign: 图片排版方式，值：center：表示居中放置\n 例如：{{\u0026lt; img src=\u0026quot;/datasets/moon.jpg\u0026quot; height=\u0026ldquo;200\u0026rdquo; width=\u0026ldquo;300\u0026rdquo; float=\u0026ldquo;left\u0026rdquo; title=\u0026ldquo;A boat at the sea\u0026rdquo; \u0026gt;}}\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Cras egestas lectus sed leo ultricies ultricies. Praesent tellus risus, eleifend vel efficitur ac, venenatis sit amet sem. Ut ut egestas erat. Fusce ut leo turpis. Morbi consectetur sed lacus vitae vehicula. Cras gravida turpis id eleifend volutpat. Suspendisse nec ipsum eu erat finibus dictum. Morbi volutpat nulla purus, vel maximus ex molestie id. Nullam posuere est urna, at fringilla eros venenatis quis.\nFusce vulputate dolor augue, ut porta sapien fringilla nec. Vivamus commodo erat felis, a sodales lectus finibus nec. In a pulvinar orci. Maecenas suscipit eget lorem non pretium. Nulla aliquam a augue nec blandit. Curabitur ac urna iaculis, ornare ligula nec, placerat nulla. Maecenas aliquam nisi vitae tempus vulputate.\n三、页面分割(split) 这个主题支持将页面分割成你想要的任意多列。\n1. 分割成两列 语法格式：\n {{\u0026lt; split 6 6\u0026gt;}}\n---\n{{\u0026lt; /split \u0026gt;}}\n 例如：\n {{\u0026lt; split 6 6\u0026gt;}}\n这是左边列\n---\n这是右边列\n{{\u0026lt; /split \u0026gt;}}\n 结果样式：\n这是左边列  这是右边列   2. 分割3列 语法格式：\n {{\u0026lt; split 4 4 4 \u0026gt;}}\n---\n---\n{{\u0026lt; /split \u0026gt;}}\n 例如：\n {{\u0026lt; split 4 4 4 \u0026gt;}}\n这是左边列\n---\n这是中间列\n---\n这是右边列\n{{\u0026lt; /split \u0026gt;}}\n 结果样式：\n这是左边列  这是中间列  这是右边列   四、垂直方向-空行 在两行之间加入空行\n语法格式:\n {{\u0026lt; vs 4\u0026gt;}} ： 表示加入4个空行\n  五、hero 要显示自己的hero图：\n例如： 在一个块(CV)下创建子块(cv_sub)，路径如下：\n├── _index.en.md\n├── _index.zh-cn.md identifier: cv\n├── cv_sub\n│　├── _index.zh-cn.md identifier: cv_sub; parent: cv\n│　└── rich_content\n│　├── images hero图片位置\n│　│　├── forest.jpg\n│　│　└── hero.svg\n│　└── index.md 真正的博文内容，必须命名为index.md。identifier: rich-content; parent: cv_sub\n","date":"June 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00020_toha-tutorial/0020_shortcodes_samples/","summary":"🤑\nThis is a sample post intended to test the followings:\n Default hero image. Different shortcodes.  一、报警(Alert) The following alerts are available in this theme.\n这是 type=\u0026quot;success\u0026quot;的报警样例.\n格式:\n {{\u0026lt; alert type=\u0026ldquo;success\u0026rdquo; \u0026gt; }}\n内容\n {{\u0026lt; /alert \u0026gt; }}  这是 type=\u0026quot;danger\u0026quot;的报警样例.\n格式:\n {{\u0026lt; alert type=\u0026ldquo;danger\u0026rdquo; \u0026gt; }}\n内容\n {{\u0026lt; /alert \u0026gt; }}  这是 type=\u0026quot;warning\u0026quot;的报警样例.\n格式:\n {{\u0026lt; alert type=\u0026ldquo;warning\u0026rdquo; \u0026gt; }}\n内容\n {{\u0026lt; /alert \u0026gt; }}  这是 type=\u0026quot;info\u0026quot;的报警样例.","tags":["shortcodes"],"title":"区域块-实例"},{"categories":["Basic"],"contents":"一、小技巧 可以使用html的标签\nmarkdown中常用的html标签：    操作 标签     换行 测试\u0026lt;br\u0026gt;一下   标记 \u0026lt;mark\u0026gt;测试一下\u0026lt;/mark\u0026gt;   按钮 \u0026lt;kbd\u0026gt;测试一下\u0026lt;/kbd\u0026gt;   颜色 \u0026lt;font color=\u0026quot;#A020F0\u0026quot;\u0026gt;颜色\u0026lt;/font\u0026gt;   四号文字 \u0026lt;font size=\u0026quot;4\u0026quot;\u0026gt;四号文字\u0026lt;/font\u0026gt;   引用1 \u0026lt;cite\u0026gt;引用[^1]\u0026lt;/cite\u0026gt;   空格 \u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;测试一下   删除线 \u0026lt;s\u0026gt;测试一下\u0026lt;/s\u0026gt;   下划线 \u0026lt;u\u0026gt;测试一下\u0026lt;/u\u0026gt;   字体增大 \u0026lt;big\u0026gt;测试一下\u0026lt;/big\u0026gt;   字体减小 \u0026lt;small\u0026gt;测试一下\u0026lt;/small\u0026gt;   文字上标 测试\u0026lt;sup\u0026gt;一下\u0026lt;/sup\u0026gt;   文字下标 测试\u0026lt;sub\u0026gt;一下\u0026lt;/sub\u0026gt;   加n个空行 {{\u0026lt; vs n\u0026gt;}}   右对齐\n \u0026lt;p align=right\u0026gt;测试一下\u0026lt;/p\u0026gt;   文字居中 \u0026lt;center\u0026gt;测试一下\u0026lt;/center\u0026gt;   图片居中 \u0026lt;p align=\u0026quot;center\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;***.jpg\u0026quot; width=\u0026quot;60%\u0026quot;\u0026gt;\u0026lt;/p\u0026gt;   超链接 \u0026lt;a href=\u0026quot;center\u0026quot; target=\u0026quot;blank\u0026quot;\u0026gt;文本\u0026lt;/a\u0026gt; href指定跳转的目标路径；\ntarget属性指定超链接打开的位置，\n值blank: 表示在一个新的页面中打开；\n默认值self: 在当前页面中打开超链接   图片 \u0026lt;img src=\u0026quot;***.jpg\u0026quot; width=\u0026quot;60%\u0026quot; height=\u0026quot;图片高度(单位是像素级)\u0026quot; alt=\u0026quot;图片描述，当图片加载失败时显示\u0026quot;\u0026gt;   音频 \u0026lt;audio src=\u0026quot;音频的url\u0026quot; controls=\u0026quot;是否允许用户控制播放\u0026quot; autoplay=\u0026quot;音频文件是否自动播放\u0026quot; \nloop=\u0026quot;音频是否循环播放\u0026quot; preload=\u0026quot;音频在页面加载时进行加载-如果设置了autoplay则忽略该属性\u0026quot;\u0026gt;   视频 跟音频一样，只是多了width和height       操作 需求     Markdown只能识别一个空格(在半角输入状态下)。有两种方法插入更多空格\n方法一：手动输入空格(半个空格\u0026amp;nbsp;)(半角相当于1个空格\u0026amp;ensp;)(全角相当于2个空格\u0026amp;emsp;) 方法二：使用权角空格即：在全角状态下直接使用空格键就ok了 添加空格   如果行与行之间没有空行，则会被视为同一段落。\n方法一：段内换行，在上一行的结尾插入两个以上的空格然后回车; 或者直接用 \u0026lt;br\u0026gt; 方法二：新起一段，在上一行的结尾插入两个以上的空格然后回车+空行；或者直接用\u0026lt;/p\u0026gt; 换行    二、基本语法 教程\n1. 代码块 ​```语言名称```  并不一定真要放代码时才用这个标签，比如：\n 如果要重点突出某个字，可以用行内代码标签 如果不想让Markdown渲染某段文字，可以用代码块标签进行包裹   行内代码标签： `行内代码` 代码块：通过一对 ```包裹   2. 标题 # 一阶标题  ## 二阶标题  ### 三阶标题  #### 四阶标题  ##### 五阶标题 ###### 六阶标题 3. 字体  斜体： 格式：*文本*  示例： *斜体*： 斜体 加粗： 格式：**文本**  示例： **加粗**： 加粗 斜体+加粗： 格式：***文本***  示例：***斜体加粗***：斜体加粗 删除线： 格式：~~文本~~或者\u0026lt;s\u0026gt;文本\u0026lt;/s\u0026gt;  示例：\u0026lt;s\u0026gt;删除线\u0026lt;/s\u0026gt;： 删除线 背景高亮：格式：\u0026lt;mark\u0026gt;文本\u0026lt;/mark\u0026gt;  示例：\u0026lt;mark\u0026gt;高亮\u0026lt;/mark\u0026gt;：高亮 背景按钮形式：格式：\u0026lt;kbd\u0026gt;文本\u0026lt;/kbd\u0026gt;  示例：\u0026lt;kbd\u0026gt;按钮\u0026lt;/kbd\u0026gt;：按钮 上标：格式：\u0026lt;sup\u0026gt;文本\u0026lt;/sup\u0026gt;  示例：x\u0026lt;sup\u0026gt;20\u0026lt;/sup\u0026gt;y：x20y 下标：格式：\u0026lt;sub\u0026gt;文本\u0026lt;/sub\u0026gt;  示例：H\u0026lt;sub\u0026gt;2\u0026lt;/sub\u0026gt;O：H2O  4. 引用 a. 引用文本  语法：\u0026gt; 加一个空格。 多级引用是可以嵌套的。\n\u0026gt;  一级引用文本  \u0026gt;\u0026gt;  二级引用文本  \u0026gt;\u0026gt;\u0026gt;  三级引用文本  \u0026gt;\u0026gt;\u0026gt; *  三级引用，无序列表      b. 引用参考文献  语法：\n\u0026lt;cite\u0026gt;论文名[^1]\u0026lt;/cite\u0026gt; [^1]: 详细的内容   Don\u0026rsquo;t communicate by sharing memory, share memory by communicating. — Rob Pike2  引用第二篇论文 Matting3\n 5. 分割线  下面是分割线： ---\n   下面是分割线： ***\n  6. 插入图片  markdown语法：![图片名称](图片地址)  []: 里面的内容表示图片未加载时的提示文字  (): 表示图片地址 \n  html语法： 插入图片：\u0026lt;img src=\u0026quot;***.jpg\u0026quot; width=\u0026quot;60%\u0026quot; height=\u0026quot;图片高度(单位是像素级)\u0026quot; alt=\u0026quot;图片描述，当图片加载失败时显示\u0026quot;\u0026gt;  居中：\u0026lt;p align=\u0026quot;center\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;***.jpg\u0026quot; width=\u0026quot;60%\u0026quot;\u0026gt;\u0026lt;/p\u0026gt; \n  本项目中，图片统一放在根目录下的 /static/ 路径下：例如：图片路径: /static/datasets/moon.jpg \u0026lt;p align=\u0026quot;center\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;/datasets/moon.jpg\u0026quot; width=\u0026quot;30%\u0026quot; height=\u0026quot;30%\u0026quot; title=\u0026quot;moon\u0026quot; alt=\u0026quot;moon\u0026quot;\u0026gt;\u0026lt;/p\u0026gt;   如果图片与本文放在同一个路径下，例如：图片路径: /content/posts/***/moon.jpg  \u0026lt;p align=\u0026quot;center\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;/zh-cn/posts/***/moon.jpg\u0026quot; width=\u0026quot;30%\u0026quot; height=\u0026quot;30%\u0026quot; title=\u0026quot;moon\u0026quot; alt=\u0026quot;moon\u0026quot;\u0026gt;\u0026lt;/p\u0026gt;     图文混排\n 左图右文，例如： \u0026lt;p\u0026gt; \u0026lt;img src=\u0026quot;/datasets/moon.jpg\u0026quot; width=\u0026quot;30%\u0026quot; height=\u0026quot;30%\u0026quot; align=\u0026quot;left\u0026quot; /\u0026gt; 文字在右边，图片在左边。文字在右边，图片在左边。文字在右边，图片在左边。文字在右边，图片在左边。文字在右边，图片在左边。文字在右边，图片在左边。 \u0026lt;/p\u0026gt;  左文右图\n  文字在右边，图片在左边。文字在右边，图片在左边。文字在右边，图片在左边。文字在右边，图片在左边。文字在右边，图片在左边。文字在右边，图片在左边。文字在右边，图片在左边。文字在右边，图片在左边。文字在右边，图片在左边。文字在右边，图片在左边。文字在右边，图片在左边。文字在右边，图片在左边。 文字在右边，图片在左边。文字在右边，图片在左边。文字在右边，图片在左边。文字在右边，图片在左边。文字在右边，图片在左边。文字在右边，图片在左边。 文字在右边，图片在左边。文字在右边，图片在左边。文字在右边，图片在左边。  7. 多媒体  视频语法：{{\u0026lt; video src=\u0026quot;/videos/sample.mp4\u0026quot; \u0026gt;}}\n 这个没啥用： \u0026lt;video id=\u0026quot;video\u0026quot; controls=\u0026quot;\u0026quot; preload=\u0026quot;none\u0026quot; poster=\u0026quot;封面图链接\u0026quot;\u0026gt;\u0026lt;source id=\u0026quot;mp4\u0026quot; src=\u0026quot;视频地址\u0026quot; type=\u0026quot;video/mp4\u0026quot;\u0026gt;\u0026lt;/video\u0026gt; 这个有用： { {\u0026lt; video src=\u0026quot;/videos/sample.mp4\u0026quot; \u0026gt;} }   Video by Rahul Sharma from Pexels.\n 音频：\n \u0026lt;audio id=\u0026quot;audio\u0026quot; controls=\u0026quot;\u0026quot; preload=\u0026quot;none\u0026quot;\u0026gt;\u0026lt;source id=\u0026quot;mp3\u0026quot; src=\u0026quot;音频地址\u0026quot;\u0026gt;\u0026lt;/audio\u0026gt; 8. 超链接  markdown语法：[描述](https://xxxx.com)\n如果让项目默认：点击超链接，重新打开网页。\n可以在 themes/toha/layouts/_default/baseof.html 中的\u0026lt;head\u0026gt;中添加\u0026lt;base target=\u0026quot;_blank\u0026quot;\u0026gt;\n  html语法：\u0026lt;a href=\u0026quot;目标路径\u0026quot; target=\u0026quot;blank\u0026quot;\u0026gt;文本\u0026lt;/a\u0026gt;\n   本项目的地址，例如本地地址: /content/posts/***/latax_formula.zh-cn.md 例如：\u0026lt;a href=\u0026quot;/zh-cn/posts/***/latax_formula\u0026quot; target=\u0026quot;bland\u0026quot;\u0026gt;katex\u0026lt;/a\u0026gt;  本地路径：katex\n  外网地址，例如：\u0026lt;a href=\u0026quot;https://www.baidu.com/\u0026quot; target=\u0026quot;blank\u0026quot;\u0026gt;百度一下\u0026lt;/a\u0026gt; 百度一下\n  9. 表格  语法： |表头|表头|表头| |:--|:--:|--:| |内容|内容|内容| |内容|内容|内容|     表头 表头 表头     内容 内容 内容   内容 内容 内容    10. 列表 a. 无序列表  markdown语法：\n- 列表内容 + 列表内容 * 列表内容    效果一样\n  二级\n  三级\n 四级         html语法：太复杂 \u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;内容\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\n \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;书籍 \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;道德经\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; 效果：\n 书籍  道德经    b. 有序列表  markdown语法：数字加点，加空格\n例如：1. 有序列表内容   一级有序列表内容  二级有序列表  三级有序列表  四级有序列表       一级有序列表内容   html语法：太复杂 用 \u0026lt;ol\u0026gt;\u0026lt;/ol\u0026gt; 和 \u0026lt;li\u0026gt;\u0026lt;/li\u0026gt;  \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt;书籍 \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt;道德经\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; 效果：\n 书籍  道德经    11. 流程图 st=\u0026gt;start: 开始 跳转到：三、流程图\n12. 注释 被注释的文字不会显示出来。\n html注释：\u0026lt;!-- this is a comment --\u0026gt; 例如：\n\u0026lt;!--\n我是多行\n段落注释\n渲染时不会显示\n--\u0026gt;\n  html标签：style='display: none'\n  markdown注释：[](注释内容，渲染时不会显示)\n 13. 特殊字符    特殊字符 语法 字符     空格符 \u0026amp;nbsp;    小于号 \u0026amp;lt; \u0026lt;   大于号 \u0026amp;gt; \u0026gt;   和号 \u0026amp;amp; \u0026amp;   人民币 \u0026amp;yen; ¥   版权 \u0026amp;copy; ©   注册商标 \u0026amp;reg; ®   摄氏度 \u0026amp;deg; °   正负号 \u0026amp;plusmn; ±   乘号 \u0026amp;times; ×   除号 \u0026amp;divide; ÷   平方（上标²） \u0026amp;sup2; ²   立方（上标³） \u0026amp;sup3; ³    14. 公式 markdown的公式: 可以使用两个美元符 $$ 包裹 TeX 或 LaTeX 格式的数学公式来实现。提交后，问答和文章页会根据需要加载 Mathjax 对数学公式进行渲染，例如：\n公式katex文档\n   序号 大写 大写 小写 小写 英文 英语音标注音 汉语名称 常用指代意义     1 $$\\Alpha$$ \\Alpha $$\\alpha$$ \\alpha alpha /\u0026lsquo;ælfə/ 阿尔法 角度、系数、角加速度、第一个、电离度、转化率   2 $$\\Beta$$ \\Beta $$\\beta$$ \\beta beta /\u0026lsquo;beɪtə/ 贝塔 角度、系数、磁通系数   3 $$\\Gamma$$ \\Gamma $$\\gamma$$ \\gamma gamma /\u0026lsquo;gæmə/ 伽玛 电导系数、角度、比热容比   4 $$\\Delta$$ \\Delta $$\\delta$$ \\delta delta /\u0026lsquo;deltə/ 德尔塔 变化量、焓变、熵变、屈光度、一元二次方程中的判别式、化学位移   5 $$\\Epsilon$$ \\Epsilon $$\\epsilon, \\varepsilon$$ \\epsilon, \\varepsilon epsilon /\u0026lsquo;epsɪlɒn/ 艾普西隆 对数之基数、介电常数、电容率、应变   6 $$\\Zeta$$ \\Zeta $$\\zeta$$ \\zeta zeta /\u0026lsquo;zi:tə/ 泽塔 系数、方位角、阻抗、相对黏度   7 $$\\Eta$$ \\Eta $$\\eta$$ \\eta eta /\u0026lsquo;i:tə/ 伊塔 迟滞系数、机械效率   8 $$\\Theta$$ \\Theta $$\\theta, \\vartheta$$ \\theta, \\vartheta theta /\u0026lsquo;θi:tə/ 西塔 温度、角度   9 $$\\Iota$$ \\Iota $$\\iota$$ \\iota iota /aɪ\u0026rsquo;əʊtə/ 约(yāo)塔 微小、一点   10 $$\\Kappa$$ \\Kappa $$\\kappa, \\varkappa$$ \\kappa, \\varkappa kappa /\u0026lsquo;kæpə/ 卡帕 介质常数、绝热指数   11 $$\\Lambda$$ \\Lambda $$\\lambda$$ \\lambda lambda /\u0026lsquo;læmdə/ 拉姆达 波长、体积、导热系数   12 $$\\Mu$$ \\Mu $$\\mu$$ \\mu mu /mju:/ 谬 磁导率、微、动摩擦系（因）数、流体动力黏度、货币单位、莫比乌斯函数   13 $$\\Nu$$ \\Nu $$\\nu$$ \\nu nu /nju:/ 纽 磁阻系数、流体运动粘度、光波频率、化学计量数   14 $$\\Xi$$ \\Xi $$\\xi$$ \\xi xi /ksi/ 克西 随机变量、（小）区间内的一个未知特定值   15 $$\\Omicron$$ \\Omicron $$\\omicron$$ \\omicron omicron /əuˈmaikrən/ 奥米克戎 高阶无穷小函数   16 $$\\Pi$$ \\Pi $$\\pi, \\varpi$$ \\pi, \\varpi pi /paɪ/ 派 圆周率、π(n)表示不大于n的质数个数、连乘   17 $$\\Rho$$ \\Rho $$\\rho, \\varrho$$ \\rho, \\varrho rho /rəʊ/ 柔 电阻率、柱坐标和极坐标中的极径、密度、曲率半径   18 $$\\Sigma$$ \\Sigma $$\\sigma, \\varsigma$$ \\sigma, \\varsigma sigma /\u0026lsquo;sɪɡmə/ 西格马 总和、表面密度、跨导、应力、电导率   19 $$\\Tau$$ \\Tau $$\\tau$$ \\tau tau /taʊ/ 陶 时间常数、切应力、2π（两倍圆周率）   20 $$\\Upsilon$$ \\Upsilon $$\\upsilon$$ \\upsilon upsilon /ˈipsɪlon/ 宇普西隆 位移   21 $$\\Phi$$ \\Phi $$\\phi, \\varphi$$ \\phi, \\varphi phi /faɪ/ 斐 磁通量、电通量、角、透镜焦度、热流量、电势、直径、欧拉函数、相位、孔隙度   22 $$\\Chi$$ \\Chi $$\\chi$$ \\chi chi /kaɪ/ 希 /恺 统计学中有卡方(χ^2)分布   23 $$\\Psi$$ \\Psi $$\\psi$$ \\psi psi /psaɪ/ 普西 角速、介质电通量、ψ函数、磁链   24 $$\\Omega$$ \\Omega $$\\omega$$ \\omega omega /\u0026lsquo;əʊmɪɡə/ 欧米伽 欧姆、角速度、角频率、交流电的电角度、化学中的质量分数、有机物的不饱和度     我是一个公式 $$\\Gamma(n) = (n-1)!$$：$$\\Gamma(n) = (n-1)!$$\nBlock math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$ $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n$$\\alpha = \\frac a b$$: $$\\alpha = \\frac a b$$\n15. 切割成列 这个主题支持将页面分割成尽可能多的列。\n{\u0026lt; split 6 6\u0026gt;} 16. 任务清单  语法实例 - [ ] 未完成 - [x] 已完成   未完成 已完成  17. Markdown 变量  Markdown中文持变量定义和变量引用，且支持中文。一处定义，处处使用，而且方便，统一修改。\n语法：\n步骤1：定义链接：[百度]:https://www.baidu.com\n步骤2：引用链接：[自定义文本][百度]\n 自定义文本\n18. Markdown 锚点  场景：现在在写一篇博客，内容牵涉到以前的博文或者博文前面的章节。想设置一个超链接，跳转到前面博文的具体位置。\n步骤一： 在 需要跳至的位置 设置锚点(或者是前面的 标题)：\u0026lt;a id=\u0026quot;锚点1-id\u0026quot;\u0026gt;跳到此处\u0026lt;/a\u0026gt;\n步骤二： 从该位置调到 锚点位置：\u0026lt;a href=\u0026quot;#锚点1-id\u0026quot;\u0026gt;请看前博文\u0026lt;/a\u0026gt;\n 例如：Markdown语法，参考：基本语法\n三、流程图 1、设置 要是用流程图时，需要添加：mermaid: true\ntitle: \u0026#34;Mermaid Support\u0026#34; date: 2022-03-14T06:15:35+06:00 menu: sidebar: name: Mermaid identifier: writing-post-mermaid parent: writing-post weight: 60 mermaid: true 2、语法 {{\u0026lt; mermaid align=\u0026ldquo;left\u0026rdquo; \u0026gt;}}\n內容\n{{\u0026lt; /mermaid \u0026gt;}}\n参数：\n align：让您将图表对齐到左边、右边或中间(left, right, center)。默认对齐方式为居中。 background：让您更改图表的背景颜色。    3、实例 1）Graph []：表示矩形框 ()：表示圆角矩形框\n{}：表示菱形框\n`{`{\u0026lt; mermaid align=\u0026quot;left\u0026quot; \u0026gt;}} graph LR; A[Hard edge] --\u0026gt;|Link text| B(Round edge) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result one] C --\u0026gt;|Two| E[Result two] `{`{\u0026lt; /mermaid \u0026gt;}}  graph LR; A[Hard edge] --|Link text| B(Round edge) B -- C{Decision} C --|One| D[Result one] C --|Two| E[Result two]  b）序列图(Sequence Diagram) `{`{\u0026lt; mermaid \u0026gt;}} sequenceDiagram participant Alice participant Bob Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts \u0026lt;br/\u0026gt;prevail! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! `{`{\u0026lt; /mermaid \u0026gt;}}  sequenceDiagram participant Alice participant Bob Alice-John: Hello John, how are you? loop Healthcheck John-John: Fight against hypochondria end Note right of John: Rational thoughts prevail! John--Alice: Great! John-Bob: How about you? Bob--John: Jolly good!  c）甘特图 (Gantt diagram) `{`{\u0026lt; mermaid \u0026gt;}} gantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid excludes weekdays 2014-01-10 section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d `{`{\u0026lt; /mermaid \u0026gt;}}  gantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid excludes weekdays 2014-01-10 section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d  4）类图(class diagram) `{`{\u0026lt; mermaid \u0026gt;}} classDiagram Class01 \u0026lt;|-- AveryLongClass : Cool Class03 *-- Class04 Class05 o-- Class06 Class07 .. Class08 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla Class08 \u0026lt;--\u0026gt; C2: Cool label `{`{\u0026lt; /mermaid \u0026gt;}}  classDiagram Class01 C2 : Where am i? Class09 --* C3 Class09 --| Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla Class08  C2: Cool label  5）git图(git graph) `{`{\u0026lt; mermaid background=\u0026quot;black\u0026quot; align=\u0026quot;right\u0026quot; \u0026gt;}} gitGraph: options { \u0026quot;nodeSpacing\u0026quot;: 150, \u0026quot;nodeRadius\u0026quot;: 10 } end commit branch newbranch checkout newbranch commit commit checkout master commit commit merge newbranch `{`{\u0026lt; /mermaid \u0026gt;}}  gitGraph: options { \"nodeSpacing\": 150, \"nodeRadius\": 10 } end commit branch newbranch checkout newbranch commit commit checkout master commit commit merge newbranch  6）ER图(ER Diagram) `{`{\u0026lt; mermaid \u0026gt;}} erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses `{`{\u0026lt; /mermaid \u0026gt;}}  erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses    测试 \u0026#x21a9;\u0026#xfe0e;\n The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015. \u0026#x21a9;\u0026#xfe0e;\n 这是第二个引用的详细内容 \u0026#x21a9;\u0026#xfe0e;\n   ","date":"June 8, 2021","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00020_toha-tutorial/0013_markdown-tutorial/","summary":"一、小技巧 可以使用html的标签\nmarkdown中常用的html标签：    操作 标签     换行 测试\u0026lt;br\u0026gt;一下   标记 \u0026lt;mark\u0026gt;测试一下\u0026lt;/mark\u0026gt;   按钮 \u0026lt;kbd\u0026gt;测试一下\u0026lt;/kbd\u0026gt;   颜色 \u0026lt;font color=\u0026quot;#A020F0\u0026quot;\u0026gt;颜色\u0026lt;/font\u0026gt;   四号文字 \u0026lt;font size=\u0026quot;4\u0026quot;\u0026gt;四号文字\u0026lt;/font\u0026gt;   引用1 \u0026lt;cite\u0026gt;引用[^1]\u0026lt;/cite\u0026gt;   空格 \u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;测试一下   删除线 \u0026lt;s\u0026gt;测试一下\u0026lt;/s\u0026gt;   下划线 \u0026lt;u\u0026gt;测试一下\u0026lt;/u\u0026gt;   字体增大 \u0026lt;big\u0026gt;测试一下\u0026lt;/big\u0026gt;   字体减小 \u0026lt;small\u0026gt;测试一下\u0026lt;/small\u0026gt;   文字上标 测试\u0026lt;sup\u0026gt;一下\u0026lt;/sup\u0026gt;   文字下标 测试\u0026lt;sub\u0026gt;一下\u0026lt;/sub\u0026gt;   加n个空行 {{\u0026lt; vs n\u0026gt;}}   右对齐","tags":["MarkDown","教程"],"title":"MarkDown入门"},{"categories":["Basic"],"contents":"一、启动 模板项目: github\n# --force 即使本文件夹不为空，也会强制创建站点 hugo new site myblog -f=yaml --force # 初始化本地仓库，因为部署时要把该文件的内容push到远端仓库 git init # 添加toha主题 git submodule add https://github.com/hugo-toha/toha.git themes/toha # 在本地启动站点，浏览器中打开: http://localhost:1313 hugo server -t toha -w Demo样例\nHugo文档\nGithub项目\n二、配置 config.yaml: 配置样例\n这个主题的大部分内容是由data目录中的一些 YAML 文件驱动的。 在本节中，我们将添加一些示例数据。 由于我们正在构建一个多语言站点，因此我们会将每种语言的数据保存在各自的语言环境文件夹中。首先，在data目录中创建 en 文件夹(英语环境)/zh-cn(汉语环境)。 我们将在这里添加英语语境数据。\n1、主页配置 在目的环境文件夹中创建site.yaml\n英语环境：/data/en/site.yaml 汉语环境：/data/zh-cn/site.yaml\n# Copyright Notice copyright: © 2021 Copyright. # A disclaimer notice for the footer. Make sure you have set \u0026#34;params.footer.disclaimer.enable: true\u0026#34; in your `config.yaml` file. disclaimer: \u0026#34;这个主题是MIT许可的\u0026#34; # Meta description for your site. This will help the search engines to find your site. description: 机器学习、深度学习 探索者. # 指定要在顶部导航栏中显示的自定义菜单列表。它们将通过分隔线与主菜单分开。 customMenus: - name: 文档 url: https://toha-guides.netlify.app/posts/ # Specify OpenGraph Headers openGraph: title: biubiobiu\u0026#39;s Blog type: website description: biubiobiu的简历和私人博客. image: images/author/john.png url: https://***.github.io 2、作者信息配置 在目的语言环境路径中创建：author.yaml文件\n英语环境: /data/en/author.yaml\n汉语环境: /data/zh-cn/author.yaml\n3、区域块设置 在目的语言环境路径中创建：sections文件夹\n英语环境：data/en/sections/\n汉语环境：data/zh-cn/sections/\n三、部署 It\u0026rsquo;s coming soon \u0026hellip;\n","date":"June 8, 2020","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00020_toha-tutorial/0010_toha-config/","summary":"一、启动 模板项目: github\n# --force 即使本文件夹不为空，也会强制创建站点 hugo new site myblog -f=yaml --force # 初始化本地仓库，因为部署时要把该文件的内容push到远端仓库 git init # 添加toha主题 git submodule add https://github.com/hugo-toha/toha.git themes/toha # 在本地启动站点，浏览器中打开: http://localhost:1313 hugo server -t toha -w Demo样例\nHugo文档\nGithub项目\n二、配置 config.yaml: 配置样例\n这个主题的大部分内容是由data目录中的一些 YAML 文件驱动的。 在本节中，我们将添加一些示例数据。 由于我们正在构建一个多语言站点，因此我们会将每种语言的数据保存在各自的语言环境文件夹中。首先，在data目录中创建 en 文件夹(英语环境)/zh-cn(汉语环境)。 我们将在这里添加英语语境数据。\n1、主页配置 在目的环境文件夹中创建site.yaml\n英语环境：/data/en/site.yaml 汉语环境：/data/zh-cn/site.yaml\n# Copyright Notice copyright: © 2021 Copyright. # A disclaimer notice for the footer. Make sure you have set \u0026#34;params.footer.disclaimer.enable: true\u0026#34; in your `config.","tags":["Toha","配置"],"title":"Toha的配置"},{"categories":["Basic"],"contents":"一、创建类别 1、创建文章 在content文件夹中创建posts文件夹，在该文件夹中创建一个_index.zh-cn.md文件(中文环境)/_index.en.md(英文环境)。在里面添加如下内容：\n--- title: Posts --- 现在，假设你想写一篇文章。首先，创建一个文件，在末尾用markdown扩展名命名它。例如:我们创建了一个名为analytics-and-comments.en.md，并添加以下几行内容。如果在中文环境下创建，名字应该是analytics-and-comments.zh-cn.md:\n--- title: \u0026#34;Analytics and Comments\u0026#34; date: 2020-06-08T06:00:23+06:00 hero: /images/posts/writing-posts/analytics.svg description: Adding analytics and disquss comment in hugo theme Toha menu: sidebar: name: Analytics \u0026amp; Comments identifier: analytics-and-comments weight: 500 --- ### Complete Post Coming Soon... 在文件的头部以3个-开始和结束，称为前置内容。我们写得每一篇博客文章都需要有前置内容，在前置内容之后，可以开始写文章内容了，前置内容的参数有：\n   参数 解释     title 贴子的标题   date 显示博客发布时间，第一部分 year-month-date format   hero 文章封面图的位置路径。创建路径static/images/posts/writingposts/ 在其中放置图片文件   description 添加任意你喜欢的描述   menu 这个部分包含了另一个sidebar参数，该参数定义了侧边栏中文件结构的样子。该参数的子参数有：name,identifier,weight    name: 定义了侧边栏文件层次结构中，文档的名称    identifier: 标识符。有助于将文件与其他文件区分开来，有助于分类    weight: 权重值，对于多个文件，文档将基于该权重值以升序出现在文件层次结构中。    parent:        2、创建子类 刚刚我们创建了一个_index.zh-cn.md文件和一个博客文章的markdown文件，现在我们创建一个子类。创建一个文件夹 getting-started/_index.zh-cn.md，该文件中包含下面的前置内容:\n--- title: Deploy Site menu: sidebar: name: Deploy Site identifier: getting-started weight: 300 --- 上述代码块中各个参数的含义前面已经讨论过了。 只是，暂时请记住，我们将创建类别名称作为getting-started，这就是我们将其作为标识符包含在此 _index.md 中的原因。 接下来，我们将创建一个名为 github-pages.md 的 Markdown 文件。这将是我们此文件夹的博客文章文件。 github -pages.md 包括以下几行：\n--- title: \u0026#34;Deploy site in Github Pages\u0026#34; date: 2020-06-08T06:00:20+06:00 hero: /images/posts/writing-posts/git.svg menu: sidebar: name: Github Pages identifier: getting-started-github parent: getting-started weight: 10 --- 目录关系如下： getting-started |__ _index.md_ |__ github-pages.md 一个新参数：parent：该参数的值一定要与上一级的 标签(identifier)相匹配。\n3、作者信息 在默认情况下，文章的作者信息用的是 config.yaml文件中的相关信息。如果想修改作者信息，可以在前置内容中添加 author 块：\nauthor: name: Md.Habibur image: /images/authors/habib.jpg 二、创建子类 It\u0026rsquo;s coming soon \u0026hellip;\n","date":"June 8, 2020","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/posts/00020_toha-tutorial/0011_write-blogs/","summary":"一、创建类别 1、创建文章 在content文件夹中创建posts文件夹，在该文件夹中创建一个_index.zh-cn.md文件(中文环境)/_index.en.md(英文环境)。在里面添加如下内容：\n--- title: Posts --- 现在，假设你想写一篇文章。首先，创建一个文件，在末尾用markdown扩展名命名它。例如:我们创建了一个名为analytics-and-comments.en.md，并添加以下几行内容。如果在中文环境下创建，名字应该是analytics-and-comments.zh-cn.md:\n--- title: \u0026#34;Analytics and Comments\u0026#34; date: 2020-06-08T06:00:23+06:00 hero: /images/posts/writing-posts/analytics.svg description: Adding analytics and disquss comment in hugo theme Toha menu: sidebar: name: Analytics \u0026amp; Comments identifier: analytics-and-comments weight: 500 --- ### Complete Post Coming Soon... 在文件的头部以3个-开始和结束，称为前置内容。我们写得每一篇博客文章都需要有前置内容，在前置内容之后，可以开始写文章内容了，前置内容的参数有：\n   参数 解释     title 贴子的标题   date 显示博客发布时间，第一部分 year-month-date format   hero 文章封面图的位置路径。创建路径static/images/posts/writingposts/ 在其中放置图片文件   description 添加任意你喜欢的描述   menu 这个部分包含了另一个sidebar参数，该参数定义了侧边栏中文件结构的样子。该参数的子参数有：name,identifier,weight    name: 定义了侧边栏文件层次结构中，文档的名称    identifier: 标识符。有助于将文件与其他文件区分开来，有助于分类    weight: 权重值，对于多个文件，文档将基于该权重值以升序出现在文件层次结构中。    parent:        2、创建子类 刚刚我们创建了一个_index.","tags":["博文路径"],"title":"撰写文章"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://biubiobiu.github.io/zh-cn/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"}]