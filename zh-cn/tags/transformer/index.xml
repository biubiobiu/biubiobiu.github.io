<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Transformer on biubiobiu's Blog</title><link>https://biubiobiu.github.io/zh-cn/tags/transformer/</link><description>Recent content in Transformer on biubiobiu's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Everything is mine</copyright><lastBuildDate>Wed, 08 Sep 2021 12:30:40 +0800</lastBuildDate><atom:link href="https://biubiobiu.github.io/zh-cn/tags/transformer/index.xml" rel="self" type="application/rss+xml"/><item><title>位置编码</title><link>https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0030_transformer/0030_position/</link><pubDate>Wed, 08 Sep 2021 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0030_transformer/0030_position/</guid><description>位置编码 1、绝对位置编码 最早出现于Transformer文章中，目的是为了弥补模型中位置信息的缺失。
输入：$\bold{X} \in \R^{n \times d}$ 包含一个序列中n个词元的d维嵌入表示。
位置编码：$\bold{P} \in \R^{n \times d}$, 矩阵第i行 偶数列、奇数列：用不同的频率、偏移来记录位置信息。 $$p_{i,2j} = sin(\frac{i}{10000^{\frac{2j}{d}}})$$ $$p_{i,2j+1} = cos(\frac{i}{10000^{\frac{2j}{d}}})$$
在 $\bold{X} + \bold{P}$ 时，当$\bold{X}$的幅度值比$\bold{P}$小或者差不多时，可以增大$\bold{X}$的幅度值，以保证$\bold{X}$的主导性。 $$ \bold{X} \times M + \bold{P} $$
2、相对位置编码 Google于2018年提出的 《Self-Attention with Relative Position Representations》 。该方法出自Transformer的原班人马，通过在attention模块中加入可训练的参数，帮助模型来记住输入中的相对位置。
3、ALiBi ALiBi
4、旋转位置编码(RoPE) RoPE
个人理解：对embedding向量做一个角度旋转。由于d维的向量旋转太复杂，只对2维的向量做旋转。所以d维的向量，有d/2个小向量。 旋转的基本角度：
参考：苏剑林的blog def precompute_freqs_cis(dim: int, seq_len: int, theta: float = 10000.0): # 计算词向量元素两两分组之后，每组元素对应的旋转角度 freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].</description></item><item><title>code解析</title><link>https://biubiobiu.github.io/zh-cn/posts/00200_nlp/1000_code/bart_summary/</link><pubDate>Wed, 08 Sep 2021 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00200_nlp/1000_code/bart_summary/</guid><description>一、transformers Hugging Face公司发布的transformers包，能够超级方便的引入训练模型：BERT、GPT2、&amp;hellip; transformers英文文档 transformers中文文档
二、Tokenizer from transformers import BertTokenizerFast, BertTokenizer from transformers import GPT2TokenizerFast, GPT2LMHeadModel # 初始化tokenizer tokenizer = BertTokenizerFast(vocab_file=args.vocab_path, sep_token=&amp;#34;[SEP]&amp;#34;, pad_token=&amp;#34;[PAD]&amp;#34;, cls_token=&amp;#34;[CLS]&amp;#34;) # 对比 tokenizer.encode() 与 tokenizer.tokenize() sentence = &amp;#34;Hello, my son is cuting.&amp;#34; input_ids_1 = tokenizer.encode(sentence, add_special_tokens=False) # add_special_tokens=True 将句子转换成对应模型的输入形式，默认开启。就是首尾加上[cls]、[sep]。即：tensor([ 101, 7592, 1010, 2026, 2365, 2003, 3013, 2075, 1012, 102]) # add_special_tokens=False 首尾先不加[cls]、[sep] input_tokens = tokenizer.tokenize(sentence) # [&amp;#39;hello&amp;#39;, &amp;#39;,&amp;#39;, &amp;#39;my&amp;#39;, &amp;#39;son&amp;#39;, &amp;#39;is&amp;#39;, &amp;#39;cut&amp;#39;, &amp;#39;##ing&amp;#39;, &amp;#39;.&amp;#39;] input_ids_2 = tokenizer.</description></item><item><title>Transformer</title><link>https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0030_transformer/0010_transformer_summary/</link><pubDate>Wed, 08 Sep 2021 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0030_transformer/0010_transformer_summary/</guid><description>一、简介 谷歌大脑、谷歌研究院等团队于2017年联合发表文章《Attention Is All You Need》，提出了一种新的注意力 Seq2Deq 模型，以取代之前以RNN作为编/解码器实现的 Seq2Seq 模型。模型采用的也是编码器-解码器架构，但是在该模型中，编码器和解码器不再是 RNN结构，取而代之的是编码器栈（encoder stack）和解码器栈（decoder stack）（注：所谓的“栈”就是将同一结构重复多次，“stack”翻译为“堆叠”更为合适）。编码器栈和解码器栈中分别为连续N个具有相同结构的编码器和解码器。
编码器：由两部分组成（自注意力模块 + 前馈神经网络）
自注意力模块：具体来说是“Multi-Head Attention”，即“多头注意力”模块
全连接前馈网络 每个子网络都具有残差连接，其输出形式为 $LayerNorm(Sublayer(x)+x)$ ，其中 $Sublayer(x)$ 表示子网络对输入特征x进行的具体映射操作；$LayerNorm()$ 表示归一化操作。
解码器：由三部分组成（自注意力模块 + 编码-解码注意力模块 + 前馈神经网络）
解码器中多了一个编码-解码注意力模块，用来利用当前已有的输出，来匹配输入特征（即：attention操作），然后拿计算出的新特征来计算当前时间步的输出。解码器中的自注意力模块与编码器不同是：这里只能看到当前时间步之前的输入，而不是全部的输入，所以需要有mask的操作。
论文中图： 二、Transformer 输入：序列的embeding表示 + 位置编码
编码器：
多头注意力 + 残差连接(residual connection) &amp;ndash;&amp;gt; 层归一化(layer normalization) 基于位置的前馈网络(positionwise feed-forward network) + 残差连接(residual connection) &amp;ndash;&amp;gt; 层归一化(layer normalization) class PositionWiseFFN(nn.Module): &amp;#34;&amp;#34;&amp;#34;基于位置的前馈网络&amp;#34;&amp;#34;&amp;#34; def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs): super(PositionWiseFFN, self).</description></item></channel></rss>