<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>深度学习 on biubiobiu's Blog</title><link>https://biubiobiu.github.io/zh-cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link><description>Recent content in 深度学习 on biubiobiu's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Everything is mine</copyright><lastBuildDate>Sat, 05 Aug 2023 12:30:40 +0800</lastBuildDate><atom:link href="https://biubiobiu.github.io/zh-cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml"/><item><title>初始化</title><link>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0020_deeplearing_init/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0020_deeplearing_init/</guid><description>一、初始化 不合适的权重初始化会使得隐藏层数据的方差过大（例如，随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也增大），从而在经过sigmoid这种非线性层时离中心较远(导数接近0)，因此过早地出现梯度消失。所以，在深度学习中，神经网络的权重初始化方法（weight initialization）对模型的收敛速度和性能有着至关重要的影响。一个好的权重初始化虽然不能完全解决梯度消失或梯度爆炸的问题，但是对于处理这两个问题是有很大帮助的，并且十分有利于提升模型的收敛速度和性能表现。
过大/过小 问题：
如果权值的初始值过大，则loss function相对于权值参数的梯度值很大，每次利用梯度下降更新参数的时，参数更新的幅度也会很大，这就导致loss function的值在其最小值附近震荡。 而过小的初值值则相反，loss关于权值参数的梯度很小，每次更新参数时，更新的幅度也很小，着就会导致loss的收敛很缓慢，或者在收敛到最小值前在某个局部的极小值收敛了。 Glorot条件 ：优秀的初始化应该保证以下两个条件：
各个层的激活值h（输出值）的方差要保持一致， 各个层对状态z的梯度的方差要保持一致， 一个事实：方差与Layer的关系： 参考
各个层激活值h（输出值）的方差与网络的层数有关，激活值的方差逐层递减，就是越来越集中到一定范围，这个范围的概率会较大； 关于状态z的梯度的方差与网络的层数有关，状态的梯度在反向传播过程中越往下梯度越小（因为方差越来越小）。； 各个层权重参数W的梯度的方差与层数无关； 初始化的要求
参数不能全部初始化为0，也不能全部初始化同一个值； 最好保证参数初始化的均值为0，正负交错，正负参数大致上数量相等； 初始化参数不能太大或者是太小，参数太小会导致特征在每层间逐渐缩小而难以产生作用，参数太大会导致数据在逐层间传递时逐渐放大而导致梯度消失发散，不能训练； 1、Xavier初始化 Xavier初始化的基本思想：保持输入和输出的方差一致（服从相同的分布），这样就避免了所有输出值都趋向于0。它为了保证前向传播和反向传播时每一层的方差一致。
在全连接层的Xavier初始化：用 $N(0, 1/m)$ 的随机分布初始化。
2、MSRA Xavier初始化适合用tanh激活函数，对于Relu激活函数比使用。何凯明大神提出了 MSRA，可以适用于Relu激活函数。 主要想要解决的问题是由于经过relu后，方差会发生变化，因此我们初始化权值的方法也应该变化。只考虑输入个数时，MSRA初始化是一个均值为0，方差为2/n的高斯分布：$N(0, \sqrt{2/n})$ 。在ReLU网络中，假定每一层有一半的神经元被激活，另一半为0（x负半轴中是不激活的），所以要保持方差不变，只需要在Xavier的基础上再除以2
3、NTK参数化 除了直接用这种方式初始化外，还可以使用 参数化的方式：用 $N(0, 1)$ 的随机分布来初始化，但需要将输出结果除以 $\sqrt{m}$，即： $$ y_j = b_j + \frac{1}{\sqrt{m}} \sum_i{x_i w_{ij}} $$
这个高斯过程被称为 &amp;ldquo;NTK参数化&amp;rdquo;，可以参考 《Neural Tangent Kernel: Convergence and Generalization in Neural Networks》，《On the infinite width limit of neural networks with a standard parameterization》。利用NTK参数化后，所有参数都可以用方差为1的分布初始化，这意味着每个参数的尺度大致是一个级别，这样的话我们就可以设置较大的学习率，加快收敛。</description></item><item><title>综述</title><link>https://biubiobiu.github.io/zh-cn/posts/00033_reinforce/0001_reinforce_summary/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00033_reinforce/0001_reinforce_summary/</guid><description>一、简介 graph LR; A(动态规划) -- B(Monte Carlo) B -- C(TD) C -- D(Q学习) D -- E(SARSA) E -- F(DQN) F -- G(PPO) G -- H(AC/A2C/A3C) H -- I(DDPG) I -- J(SAC) 二、</description></item><item><title>归一化</title><link>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0015_deeplearning_norm/</link><pubDate>Thu, 05 Aug 2021 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0015_deeplearning_norm/</guid><description>一、Normlization介绍 一般而言，样本特征由于来源及度量单位不同，其尺度往往差异很大。如果尺度差异很大，神经网络就比较难训练。为了提高训练效率，对输入特征做归一化，把不同的尺度压缩到一定范围内，尺度统一后，大部分位置的梯度方向近似于最优解搜索方向。这样，在用梯度下降法进行求解时，每一步梯度的方向都基本上指向最小值，训练效率会大大提高。
归一化：泛指把数据特征转换为相同尺度的方法，比如：
把数据特征映射到 [0, 1] 或者 [-1, 1] 区间 映射为服从 N(0, 1) 的标准正态分布 1、逐层归一化 逐层归一化可以有效提高训练效率的原因：
更好的尺度不变形
深度神经网路中，一个神经层的输入是之前神经层的输出。给定一个神经层 $l$，它之前的神经层 $1, 2, &amp;hellip;, l-1$，的参数变化会导致其输入的分布发生很大的变化。当使用随机梯度下降法训练网络时，每次参数更新都会导致该神经层的输入分布发生变化，层数越高，其输入分布会改变得越明显。
为了缓解这个问题，可以对每个神经层的输入进行归一化，使其分布保持稳定。不管底层的参数如何变化，高层的输入相对稳定。另外，尺度不变性，可以使我们更加高效地进行参数初始化以及超参数选择。
更平滑的优化地形
逐层归一化，一方面可以是大部分神经层的输入处于不饱和区域，从而让梯度变大，避免梯度消失问题；另一方面可以使得神经网络的优化地形（Optimization Landscape）更加平滑，并使梯度变得更加稳定，从而允许使用更高的学习率，并加快收敛速度。
1. 批量归一化 批量归一化（Batch Normalization）对神经网络中的任意中间层进行归一化。
$$ a^{(l)} = f(z^{(l)}) = f(Wa^{(l-1)}+b) $$ $f(·)$ 是激活函数，$W$ 和 $b$ 是可学习的参数。
为了提高优化效率，就要使净输入 $z^l$ 的分布一致，比如：都归一化为标准正态分布。虽然归一化操作可以应用在输入 $a^{(l-1)}$ 上，但归一化 $z^l$ 更加有利于优化。因此，在实践中，归一化操作一般应用在仿射变换之后，激活函数之前。
2. 层归一化 层归一化（Layer Normalization）是和批量归一化非常类似的方法，与批量归一化不同的是，层归一化是对一个中间层的所有神经元进行归一化。
二、Norm的位置 在目前大模型中 Normalization 的位置：
pre Norm 的状态： $x_{t+1} = x_t + F_t(Norm(x_t))$</description></item><item><title>深度学习-结构</title><link>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0010_deeplearning_problem/</link><pubDate>Thu, 05 Aug 2021 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0010_deeplearning_problem/</guid><description>一、激活函数 为什么需要激活函数？
例如：两个感知机。 $h_1 = W_1 x + b_1, h_2 = W_2 h_1 + b_2$ 如果没有激活函数这个 非线性变换，由于感知机的计算时线性变换，可以转换为：$h_2 = W_2 W_1 x + W_2 b_1 + b_2$ 就是说：如果没有激活函数，模型就做不了太深。两层的权重完全可以用一层的权重来表示。
1、Sigmoid函数 logistic函数
$$ \varphi(v) = \frac{1}{1+e^{-av}} $$
tanh函数
$$ \varphi(v) = tanh(v) = \frac{1-e^{-v}}{1+e^{-v}} $$
分段线性函数
$ \varphi(v) = \begin{cases} 1 &amp;amp;\text{if } v \geqslant \theta \\ kv &amp;amp;\text{if } - \theta &amp;lt; v &amp;lt; \theta \\ 0 &amp;amp;\text{if } v \leqslant 0 \end{cases}$</description></item><item><title>深度学习开篇</title><link>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0005_deeplearning_summary/</link><pubDate>Thu, 05 Aug 2021 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0005_deeplearning_summary/</guid><description>论文入口
一、机器学习 目前，人工智能研究领域主要体现在一下几个方面：
智能感知：通过模拟人的感知能力（视觉、听觉、嗅觉）对外部信息进行感知和识别，并能够对信息进行加工和处理，从而做出反应。
智能学习：学习是人工智能的主要标志和获取知识的重要手段，研究机器通过模拟人的学习能力，如何从小样本、大数据中学习，主要有：
监督学习：（Supervised Learning）表示机器学习的数据是带有标记的，这些标记可以包括：数据类别、数据属性、特征点位置等。这些标记作为预期效果，不断修正机器的预测结果。常见的监督学习有分类、回归、结构化学习。 半监督学习：（Semi-Supervised Learning）利用少量标注数据和大量无标注数据进行学习的方式。常用的半监督学习算法有：自训练、协同训练 非监督学习：（Unsupervised Learning）表示机器学习的数据是没有标记的。常见的无监督学习有：聚类、降维 强化学习：（Reinforcement Learning）通过智能体和环境的交互，不断学习并调整策略的机器学习算法。这种算法带有一种激励机制，如果智能体根据环境做出一个正确的动作，则施予一定的“正激励”；如果是错误的动作，则给与一定的“负激励”。通过不断地累加激励，以获取激励最大化的回报。做火热的应用就是 AlphaGo Zero 认知推理：模拟人的认知能力，主要研究知识表示、推理、规划、决策等，主要有自然语言处理、脑科学。
二、表征学习 表征：为了提高机器学习系统的准确率，需要将输入信息转化为有效的特征，或者更一般性地称为 表征（Representation） 表征学习：如果有一种算法可以自动地学习有效的特征，并提高最终机器学习模型的性能，那么这种学习就可以叫做 表征学习。 表征学习的关键是解决语义鸿沟（Semantic Gap）问题。即：输入数据的底层特征与高层语义信息之间的不一致性和差异性。
机器学习中经常用两种方式表示特征：局部表示(Local Representation)、分布式表示(Distributed Representation)。
比如：颜色的表示。
局部表示：也称为离散表示或者符号表示，比如：one-hot向量的形式。假设所有颜色 构成一个词表 $\bold V$，此时，可以用一个 $|\bold V|$ 维的one-hot向量来表示一中颜色。但是，one-hot向量的维数很高，且不能扩展，如果有一种新的颜色，就需要增加一维来表示。不同颜色之间的相似度都为0，无法直到“红色”和“中国红”的相似度要高于“红色”和“黑色”的相似度。 分布式表示：另一种表示颜色的方法是用RGB值来表示颜色，不同颜色对应RGB三维空间中的一个点。分布式表示通常可以表示低维的稠密向量。 嵌入：神经网络将高维的局部表示空间 $\R^{|\bold V|}$，映射到一个非常低维的分布式表示空间 $\R^{D}$。在这个低维空间中，每个特征不再是坐标轴上的点，而是分散在整个低维空间中，在机器学习中，这个过程也成为嵌入（Embedding）。比如：自然语言中词的分布式表示也经常叫做词嵌入。
要学习到一种好的高层次语义表示（一般为分布式表示），通常只有从底层特征开始，经过多步非线性转换才能得到。深层结构的优点是可以提高特征的重用性，从而指数级增强表示能力。因此，表示学习的关键是构建具有一定深度的多层次特征表示。
三、深度学习 深度学习是机器学习的一个重要的、新的研究领域，源于对神经网络的进一步研究，通常采用包含多个隐藏层的神经网络结构，目的是建立、模拟人脑学习过程。
在描述深度学习之前，先回顾下机器学习和深度学习的关系。
机器学习：研究如何使用计算机系统利用经验改善性能。在机器学习的众多研究方向中，表征学习关注如何自动找出表示数据的合适方式，以便更好地将输入变换为正确的输出。
深度学习：是具有多级表示的表征学习方法。在每一级，深度学习通过简单的函数将该级的表示变换为更高级的表示。因此，深度学习模型也可以看作是由许多简单函数复合而成的函数。当这些复合函数足够多时，就可以表达非常复杂的变换。
作为表征学习的一种，深度学习将自动找出每一级表示数据的合适方式。逐级表示越来越抽象的概念或模式。高层特征是由底层特征通过推演归纳得到。
深度学习可通过学习一种深层非线性网络结构来表征输入数据，实现复杂函数逼近，具有很强的从少数样本集中学习数据集本质特征的能力。深度学习的主要思想：通过自学习的方法，学习到训练数据的结构，并在该结构上进行有监督训练微调。 以图像为例，它的输入是一堆原始像素值，模型中逐级表示为： graph LR; A(特定位置和角度的边缘) -- B(由边缘组合得出的花纹) B -- C(由多种花纹进一步汇合得到的特定部位) C -- D(由特定部位组合得到的整个目标) 1、神经元 神经元模型：</description></item></channel></rss>