<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>summary on biubiobiu's Blog</title><link>https://biubiobiu.github.io/zh-cn/tags/summary/</link><description>Recent content in summary on biubiobiu's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Everything is mine</copyright><lastBuildDate>Sat, 05 Aug 2023 12:30:40 +0800</lastBuildDate><atom:link href="https://biubiobiu.github.io/zh-cn/tags/summary/index.xml" rel="self" type="application/rss+xml"/><item><title>模型介绍</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0100_diffusion_model/0010_diffusion_summary_/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0100_diffusion_model/0010_diffusion_summary_/</guid><description>一、简介 It is coming soon.
hello</description></item><item><title>简介</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_aigc_summary/0010_aigc_summary/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_aigc_summary/0010_aigc_summary/</guid><description>一、LLM 1、LLaMa 使用RMSNorm（即Root Mean square Layer Normalization）对输入数据进行标准化，RMSNorm可以参考论文：Root mean square layer normalization。 使用激活函数SwiGLU， 该函数可以参考PALM论文：Glu variants improve transformer。 使用Rotary Embeddings进行位置编码，该编码可以参考论文 Roformer: Enhanced transformer with rotary position embedding。 使用了AdamW优化器，并使用cosine learning rate schedule， 使用因果多头注意的有效实现来减少内存使用和运行时间。该实现可在xformers
2、PaLM 采用SwiGLU激活函数：用于 MLP 中间激活，采用SwiGLU激活函数：用于 MLP 中间激活，因为与标准 ReLU、GELU 或 Swish 激活相比，《GLU Variants Improve Transformer》论文里提到：SwiGLU 已被证明可以显著提高模型效果 提出Parallel Layers：每个 Transformer 结构中的“并行”公式：与 GPT-J-6B 中一样，使用的是标准“序列化”公式。并行公式使大规模训练速度提高了大约 15%。消融实验显示在 8B 参数量下模型效果下降很小，但在 62B 参数量下没有模型效果下降的现象。 Multi-Query Attention：每个头共享键/值的映射，即“key”和“value”被投影到 [1, h]，但“query”仍被投影到形状 [k, h]，这种操作对模型质量和训练速度没有影响，但在自回归解码时间上有效节省了成本。 使用RoPE embeddings：使用的不是绝对或相对位置嵌入，而是RoPE，是因为 RoPE 嵌入在长文本上具有更好的性能 ， 采用Shared Input-Output Embeddings:输入和输出embedding矩阵是共享的，这个我理解类似于word2vec的输入W和输出W'：
3、GLM Layer Normalization的顺序和残差连接被重新排列， 用于输出标记预测的单个线性层； ReLU s替换为GELU s 二维位置编码</description></item><item><title>CAM</title><link>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0020_cam/</link><pubDate>Fri, 09 Sep 2022 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0020_cam/</guid><description>一、简介 二、模型 1、gradient-based 1. GAP 《Learning Deep Features for Discriminative Localizatiion》
# 代码非常简单， 提取到特征图和目标类别全连接的权重，直接加权求和，再经过relu操作去除负值，最后归一化获取CAM，具体如下: # 获取全连接层的权重 self._fc_weights = self.model._modules.get(fc_layer).weight.data # 获取目标类别的权重作为特征权重 weights=self._fc_weights[class_idx, :] # 这里self.hook_a为最后一层特征图的输出 batch_cams = (weights.unsqueeze(-1).unsqueeze(-1) * self.hook_a.squeeze(0)).sum(dim=0) # relu操作,去除负值 batch_cams = F.relu(batch_cams, inplace=True) # 归一化操作 batch_cams = self._normalize(batch_cams) 2. Grad-CAM 《Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization》
2、gradient-free</description></item><item><title>简介</title><link>https://biubiobiu.github.io/zh-cn/posts/00500_video/vidio_summary/</link><pubDate>Mon, 09 May 2022 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00500_video/vidio_summary/</guid><description>一、简介 It is coming soon.
二、网络 1、 2、 3、 4、</description></item><item><title>简介</title><link>https://biubiobiu.github.io/zh-cn/posts/00400_vlp/vlp_summary/</link><pubDate>Mon, 09 May 2022 06:00:20 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00400_vlp/vlp_summary/</guid><description>一、简介 多模态学习，英文全程MultiModal Machine Learning(MMML)，从1970年 起步，已经经历了多个发展阶段，在2010年后，全面进入深度学习的阶段。多模态机器学习，以机器学习实现处理和理解多源模态信息的能力。图像、视频、音频、语义之间的多模态学习比较热门。比如互联网大型视频平台，都会将多模态技术用于视频理解业务，可以加视频封面、视频抽帧、文本信息融合。当计算机能够看懂视频，就可以做很多事儿了，比如：视频分类、审核、推荐、搜索、特效。
多模态学习有5个研究方向：
多模态表示学习（Multimodal Representation） 模态转化（Translation） 对齐（Alignment） 多模态融合（Multimodal Fusion） 协同学习（Co-learning） 实际应用，比如：
视频网站上进度条，会显示那个时间段是高光时刻 自动驾驶领域，雷达、视觉与多传感器信息融合 视频的分类、审核、推荐、搜索、特效等等 1、VLP 微软发表的一篇文章《An Empirical Study of Training End-to-End Vision-and-Language Transformers》进行了大量的实验，对不同VLP模型、各个模块不同配置的效果。
VLP通常都会遵循同一个框架，包含5大模块：
Vision Encoder：主要有3中类型 使用object detection模型，比如：Faster R-CNN，识别图像中的目标区域，并生成每个目标区域的特征表示，输入到后续模型中 利用CNN模型提取grid feature作为图像输入 ViT采用的将图像分解成patch，每个patch生成embeding输入到模型。 随着Vision Transformer的发展，ViT的方式逐渐成为主流方式。 Text Encoder：包括BERT、RoBERTa、ELECTRA、ALBERT、DeBERTa等经典预训练语言模型结构。 Multimodel Fusion：主要指如何融合图像、文本，主要有2中： co-attention：图像、文本分别使用Transformer编码，在每个Transformer模块中加入图像、文本的cross attention merged attention model，图像、文本在开始就拼接在一起，输入到Transformer 模型结构：主要有2中： Encoder-only：这种比较常见 Encoder-Decoder 预训练任务：主要有3中： Masked Language Modeling（MLM）类似BERT，随机mask掉部分token，用剩余的预测出被mask掉的token Masked Image Modeling，对输入的部分图像patch进行mask，然后预测被mask的patchs Image-Text Matching（ITM），预测image和text的pair对是否匹配，对比学习的预训练方法可以属于这类。 二、网络 Open AI 在2021年1月份发布的DALL-E和CLIP，属于结合图像和文本的多模态模型，其中DALL-E是基于文本来生成模型的模型；CLIP是用文本作为监督信号来训练可迁移的视觉模型，这两个工作带动了一波新的研究高潮。</description></item></channel></rss>