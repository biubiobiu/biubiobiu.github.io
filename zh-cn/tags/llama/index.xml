<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>llama on biubiobiu's Blog</title><link>https://biubiobiu.github.io/zh-cn/tags/llama/</link><description>Recent content in llama on biubiobiu's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Everything is mine</copyright><lastBuildDate>Sat, 05 Aug 2023 12:30:40 +0800</lastBuildDate><atom:link href="https://biubiobiu.github.io/zh-cn/tags/llama/index.xml" rel="self" type="application/rss+xml"/><item><title>LLaMa</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0015_llama/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0015_llama/</guid><description>LLaMa2 翻译
一、简介 数据方面
LLaMa2训练了2000B的tokens，训练语料比LLaMa多了40% 2000B 个token的预训练集，提供了良好的性能和成本权衡；对最真实的来源进行上采样，以增加知识并抑制幻觉，保持真实 调查数据，以便用户更好地了解模型的潜在能力和局限性，保证安全。 上下文长度从2048提升到了4096 LLaMa2-chat 模型还接受了超过100w的人类标注的训练数据 开源数据选了 LLaMa2 使用监督微调 LLaMa2-chat 使用人类反馈强化学习(RLHF)进行迭代细化；包括拒绝采样、近端策略优化 网络方面
LLaMa2 vs LLaMa，主要改动体现在 GQA 和 FFN 上:
由MHA改成GQA：整体参数量会减少 FFN模块矩阵维度有扩充：增强泛化能力，整体参数量增加。 RMSNorm 归一化 FFN中用swiGLU激活函数替换原来的Relu 旋转位置编码 RoPE 增加上下文长度 分组查询注意力 GQA 原始的 多头注意力：MHA 具有单个KV投影的原始多查询格式：MQA 具有8个KV投影的分组查询注意力变体：GQA 训练方面 预训练细节：
用AdamW优化器进行训练，其中： $β_1 =0.9，β_2 = 0.95，eps = 10−5$。 使用余弦调整学习率，预热2000steps，$lr$ 衰减到峰值的10% 使用0.</description></item></channel></rss>