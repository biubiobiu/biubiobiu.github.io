<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>BART on biubiobiu's Blog</title><link>https://biubiobiu.github.io/zh-cn/tags/bart/</link><description>Recent content in BART on biubiobiu's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Everything is mine</copyright><lastBuildDate>Wed, 08 Sep 2021 06:00:20 +0600</lastBuildDate><atom:link href="https://biubiobiu.github.io/zh-cn/tags/bart/index.xml" rel="self" type="application/rss+xml"/><item><title>BART综述</title><link>https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0150_bart/bart_summary/</link><pubDate>Wed, 08 Sep 2021 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0150_bart/bart_summary/</guid><description>一、背景 二、BART BART 是一个去噪自动编码器，用于预训练seq-to-seq模型。Bart是标准的Transformer架构，Bart的预训练过程是：
用噪声函数破坏文本 通过学习，让模型重建原始文本。 1、模型架构 同GPT一样，把ReLU激活函数修改为GeLU 用 $N(0, 0.02)$ 初始化参数。 基础版：采用6层编码器；large版：采用12层编码器。与bert的差异 解码器的每层，对编码器的最终隐藏层，做交叉attention BERT在进行预测之前，会有一个前馈网络，而BART没有。总体而言，BART的参数比同等大小的BERT模型多了10% 2、总结 BART提出了各种各样的破坏方法，比如：
删掉某些单词(Delete)； 打乱输入多个句子的顺序(permutation)； (❌: 效果不好) 交换序列中单词的位置(rotation)； (❌: 效果不好) 随机插入MASK(比如：原来AB单词之间没有其他单词，故意插入一个MASK去误导模型)或一个MASK盖多个单词(误导模型这里只有一个单词)(Text Infilling)。 (✅: 效果最好)</description></item></channel></rss>