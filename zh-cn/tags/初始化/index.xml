<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>初始化 on biubiobiu's Blog</title><link>https://biubiobiu.github.io/zh-cn/tags/%E5%88%9D%E5%A7%8B%E5%8C%96/</link><description>Recent content in 初始化 on biubiobiu's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Everything is mine</copyright><lastBuildDate>Sat, 05 Aug 2023 12:30:40 +0800</lastBuildDate><atom:link href="https://biubiobiu.github.io/zh-cn/tags/%E5%88%9D%E5%A7%8B%E5%8C%96/index.xml" rel="self" type="application/rss+xml"/><item><title>初始化</title><link>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0020_deeplearing_init/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00030_deeplearning_summary/0020_deeplearing_init/</guid><description>一、初始化 1、Xavier初始化 在全连接层的Xavier初始化：用 $N(0, 1/m)$ 的随机分布初始化。
2、NTK参数化 除了直接用这种方式初始化外，还可以使用 参数化的方式：用 $N(0, 1)$ 的随机分布来初始化，但需要将输出结果除以 $\sqrt{m}$，即： $$ y_j = b_j + \frac{1}{\sqrt{m}} \sum_i{x_i w_{ij}} $$
这个高斯过程被称为 &amp;ldquo;NTK参数化&amp;rdquo;，可以参考 《Neural Tangent Kernel: Convergence and Generalization in Neural Networks》，《On the infinite width limit of neural networks with a standard parameterization》。利用NTK参数化后，所有参数都可以用方差为1的分布初始化，这意味着每个参数的尺度大致是一个级别，这样的话我们就可以设置较大的学习率，加快收敛。</description></item></channel></rss>