<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>GPT on biubiobiu's Blog</title><link>https://biubiobiu.github.io/zh-cn/tags/gpt/</link><description>Recent content in GPT on biubiobiu's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Everything is mine</copyright><lastBuildDate>Sat, 05 Aug 2023 12:30:40 +0800</lastBuildDate><atom:link href="https://biubiobiu.github.io/zh-cn/tags/gpt/index.xml" rel="self" type="application/rss+xml"/><item><title>GPT</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0011_gpt/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0011_gpt/</guid><description>一、简介 二、GPT-1 GPT(2018-06) 详细参考
其创造性的提出以Transformer的解码器来训练生成式模型，后面Bert的作者估计是看到了这篇论文，据说两个月时间就发表了以Transformer编码器训练的Bert模型。总结下GPT-1模型：
GPT-1 使用了一个仅有解码器的 Transformer 结构，每一个作为一个Layer，共有12层； 使用了一个 768 维的嵌入向量来表示输入序列中的每个词或标记，使用了 12 个并行的注意力头（attention heads）； 使用Adam优化器进行模型训练，在训练过程中，使用了学习率的 warmup 阶段和余弦退火调度机制，以平衡训练速度和模型性能； 模型权重被初始化为均值为 0、标准差为 0.02 的正态分布（N(0, 0.02)），使用字节对编码（Byte Pair Encoding，BPE）来对文本进行分词处理，分词后得到的词汇表大小为 40000； 激活函数是 GELU； 文本输入序列固定长度是512； 参数量 117M; 使用了学习得到的位置嵌入向量(position embedding)，而不是Attention is All You Need中使用的正弦位置嵌入向量； 三、GPT-2 GPT-2(2019-02)
GPT-2的改进:
GPT-2 是GPT语言模型开始变大的地方，这是 OpenAI 第一次训练超过 1B 个参数的模型。 通过提升模型的规模，来凸显GPT的优势。在 GPT-1 中，作者训练了单个模型，但在这里，作者训练了一系列模型。 与GPT-1相比，架构上有如下差异：
层归一化操作，有原来的post-norm换成了pre-norm，以加速训练和提高模型性能。此外，在最后一个自注意力块的输出上添加了额外的层归一化； 在权重初始化时，通过 $\frac{1}{\sqrt n}$ 进行缩放。这种缩放有助于减少梯度更新的方差，使训练过程更加稳定； 扩大了其词汇表的大小，词汇表大小约为 50,000（相比于约 40,000）； 增大文本输入序列长度 1024（相比于 512）这使得模型能够更好地理解和生成更长的文本； batch size大小为 512（相比于 64）较大的批次大小有助于提高训练效率和模型并行计算的能力。 最大的模型具有约 15 亿个参数。 数据集：GPT-2 构造了一个新数据集，WebText。全部来自于 Reddit 的外链，而且是那些获得至少三个赞的外链，经过清洗、去重后，得到8百万网页共计 40GB 文本数据。 WebText 数据集的特点在于全面而干净。 GPT-2的不同版本:</description></item><item><title>GPT综述</title><link>https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0080_gpt/0010_gpt_summary/</link><pubDate>Wed, 08 Sep 2021 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00200_nlp/0080_gpt/0010_gpt_summary/</guid><description>模型评估 评估指标：
困惑度：困惑度（perplexity）的基本思想是：给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好，公式如下 $PP(W)=P(w_1w_2&amp;hellip;w_N)^{\frac{-1}{N}}$ 。由公式可知，句子概率越大，语言模型越好，迷惑度越小。困惑度p可以理解为，如果每个时间步都根据语言模型计算的概率分布随机挑词，那么平均情况下，挑多少个词才能挑到正确的那个 Prompt ranking accuracy：这个指标的定义和评价方法，来自《Hierarchical Neural Story Generation》。主要是关注引导语和生成的故事之间的相关性。具体做法是：在测试集中选择一对（p，g），p表示引导语，g表示生成的故事，在随机选取其他的引导语p1-p9，然后计算p和g的likelihood。条件一：（p，g）的相似性比（p1，g）的相似性大。 那么就取10000个测试集中的（p，g），满足条件一的部分占比，就称为Prompt ranking accuracy。 句子嵌入的相似度：计算引导语和生成的故事的句子嵌入（用GloVe取每个词的平均嵌入值）的余弦相似度。 评价连贯性：连贯性的评价方法，来自《Modeling local coherence: An entity-based approach》，主要思想是，在测试数据集中，对于一个故事s0，选择前面15个句子，打乱顺序，生成14个乱序的故事s1-s14。然后用语言模型计算s0-s14的可能性。对于s1-s14，如果可能性大于s0，就称为反例。 错误率定义为反例的占比。 评价单词的重复性和rareness 一、简介 基于文本预训练的GPT-1，GPT-2，GPT-3三代模型都是采用的以Transformer为核心结构的模型，不同的是模型的层数和词向量长度等超参，它们具体的内容如下：
模型 发布时间 层数 head hidden 参数量 预训练数据量 GPT-1 2018年6月 12 12 768 1.17亿 5GB GPT-2 2019年2月 48 - 1600 15亿 40GB GPT-3 2020年5月 96 96 12888 175B 45TB 二、GPT GPT(2018-06) 其创造性的提出以Transformer的解码器来训练生成式模型，后面Bert的作者估计是看到了这篇论文，据说两个月时间就发表了以Transformer编码器训练的Bert模型。总结下GPT-1模型：</description></item></channel></rss>