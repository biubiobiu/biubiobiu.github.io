<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>训练 on biubiobiu's Blog</title><link>https://biubiobiu.github.io/zh-cn/tags/%E8%AE%AD%E7%BB%83/</link><description>Recent content in 训练 on biubiobiu's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Everything is mine</copyright><lastBuildDate>Fri, 08 Apr 2022 06:00:20 +0600</lastBuildDate><atom:link href="https://biubiobiu.github.io/zh-cn/tags/%E8%AE%AD%E7%BB%83/index.xml" rel="self" type="application/rss+xml"/><item><title>模型训练</title><link>https://biubiobiu.github.io/zh-cn/posts/w00035_programming_language/w0050_pytorch/w0050_train_model/</link><pubDate>Fri, 08 Apr 2022 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/w00035_programming_language/w0050_pytorch/w0050_train_model/</guid><description>一、</description></item><item><title>模型训练</title><link>https://biubiobiu.github.io/zh-cn/posts/w00035_programming_language/w0040_tf/w0010_compat/w0020_tf_compat_train/</link><pubDate>Wed, 08 Sep 2021 06:00:20 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/w00035_programming_language/w0040_tf/w0010_compat/w0020_tf_compat_train/</guid><description>一、tf.layers tf.layers模块在TensorFlow2.0中已经被完全移除了，用tf.keras.layers定义层是新的标准。
二、tf.losses tf.losses模块包含了经常使用的、能够实现独热编码的损失函数。
三、tf.train 1. Optimizer TensorFlow提供的优化器
优化器 功能 tf.train.Optimizer tf.train.GradientDescentOptimizer tf.train.AdadeltaOptimizer tf.train.AdagtadOptimizer tf.train.AdagradDAOptimizer tf.train.MomentumOptimizer tf.train.AdamOptimizer tf.train.FtrlOptimizer tf.train.ProximalGradientDescentOptimizer tf.train.ProximalAdagradOptimizer tf.train.RMSProOptimizer Optimizer类与其子类的继承关系：
def minimize(self, loss, # 损失值， tensor # 全局训练步数，随着模型迭代优化自增， variable global_step=None, # 待训练模型参数的列表， list var_list=None, # 计算梯度和更新参数模型时的并行化程度，可选值GATE_OP,GATE_NONE,GATE_GRAPH # GATE_NONE 无同步，最大化并行执行效率，将梯度计算和模型参数更新完全并行化。 # GATE_OP，操作级同步，对于每个操作，分别确保所有梯度在使用前都计算完成。 # GATE_GRAPH，图级同步，最小化并行执行效率，确保所有模型参数的梯度计算完成。 gate_gradients=GATE_OP, # 聚集梯度值的方法， Enum aggregation_methed=None, # 是否将梯度计算放置到对应操作所在同一个设备，默认否，Boolean colocate_gradients_with_ops=False, # 优化器在数据流图中的名称，string nmae=None, # 损失值的梯度 grad_loss=None) 属性 功能介绍 _name 表示优化器的名称 _use_locking 表示是否在并发更新模型参数时加锁 minimize 最小化损失函数，该方法会依次调用compute_gradients和apply_gradients compute_gradients 计算模型所有参数的梯度值,返回&amp;lt;梯度，响应参数&amp;gt;的键值对列表 apply_gradients 将梯度值更新到对应的模型参数，优化器的apply_gradients成员方法内部会调用tf.</description></item></channel></rss>