<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>torch on biubiobiu's Blog</title><link>https://biubiobiu.github.io/zh-cn/tags/torch/</link><description>Recent content in torch on biubiobiu's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Everything is mine</copyright><lastBuildDate>Fri, 08 Apr 2022 06:00:20 +0600</lastBuildDate><atom:link href="https://biubiobiu.github.io/zh-cn/tags/torch/index.xml" rel="self" type="application/rss+xml"/><item><title>Tensor和变量</title><link>https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0050_pytorch/0040_tensor/</link><pubDate>Fri, 08 Apr 2022 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0050_pytorch/0040_tensor/</guid><description>Tensor
每个张量Tensor都有一个相应的torch.Storage，用来保存数据。
torch.Storage: 是一个单一数据类型的连续一维数组。每个Tensor都有一个对应的相同数据类型的存储：class torch.FloatStorage 类tensor：提供了一个存储 多维的、横向视图，并定义了数值运算。 torch.Tensor.abs()：会在原地计算，并返回改变后的tensor
torch.Tensor.abd()：在一个新的tensor中计算结果 变量
Variable 在torch.autograd.Variable中，Variable的结构图：data：Variable的tensor数值 grad_fn：表示得到这个Variable的操作，
grad：表示Variable的反向传播梯度 示例1：x = Variable(torch.Tensor([1]), requires_grad=Ture) 其中：requires_grad=True ：这个参数表示是否对这个变量求梯度。
x.backward()：自动求导。自动求导不需要再去明确地写明那个函数对那个函数求导，直接通过这行代码就可以对所有的需要梯度的变量进行求导。
x.grad：存放的就是x的梯度值 示例2：y.backward(torch.FloatTensor([1,0.1,0.01]))，表示得到的梯度分别乘以1,0.1,0.01 Variable和Tensor本质上没有区别，不过Variable会被放入一个计算图中，然后进行前向传播、反向传播、自动求导。 tensor与Variable之间的转换： tensor —to—&amp;gt; Variable：b=Variable(a) 一、Tensor信息 torch.is_tensor(obj) 判断是否为tensor torch.is_storage(obj) 判断obj是一个pytorch storage对象 torch.set_default_tensor_type() torch.numel(Tensor) 返回张量中元素的个数 二、创建Tensor torch.Tensor([[1,2],[3,4]])。创建&amp;mdash;返回指定数值的张量 torch.randn(*sizes, out=None)。创建&amp;mdash;返回标准正态分布的随机数张量。标准正态分布，形状由sizes定义 torch.randperm(n, out=None)。创建&amp;mdash;返回0~n-1之间的随机整数1维张量。返回一个从0~n-1的随机整数排列 torch.rand(*sizes, out=None)。创建&amp;mdash;返回[0, 1)的均匀分布张量 torch.arange(start, end, step=1, out=None)。创建&amp;mdash;返回一个1维张量。[start, end) 以step为步长的一组序列值 torch.range(start, end, step=1, out=None)。创建&amp;mdash;返回一个1维张量。[start, end) 以step为步长的1维张量 torch.zeros(*sizes, out=None)。创建&amp;mdash;返回一个全为0的张量。生成一个tensor, 数值为0，形状由sizes定义 torch.</description></item><item><title>基础操作</title><link>https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0050_pytorch/0020_basic/</link><pubDate>Fri, 08 Apr 2022 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0050_pytorch/0020_basic/</guid><description>一、数据类型 1、torch的数据类型 torch.Tensor 是默认的torch.FloatTensor 的简称。
剥离出一个Tensor参与计算，不参与求导：Tensor后加 .detach()
各个数据类型之间的转换：
方法一：在Tensor后加，.long(), .int(), .float(), .double() 方法二：可以用 .to()函数 数据类型 CPU tensor GPU tensor 32-bit float torch.FloatTensor torch.cuda.FloatTensor 64-big float torch.DoubleTensor torch.cuda.DoubleTensor 16-bit float N/A torch.cuda.HalfTensor 8-bit integer(unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer(signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer(signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer(signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer(signed) torch.</description></item><item><title>数学计算</title><link>https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0050_pytorch/0030_mathematical/</link><pubDate>Fri, 08 Apr 2022 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0050_pytorch/0030_mathematical/</guid><description>一、数学计算 torch.abs(input)。 数学&amp;mdash;绝对值 torch.add(input, value)。数学&amp;mdash;对张量的每个元素加value值 torch.div(input, value)。数学&amp;mdash;逐元素除法，将input逐元素除以标量value torch.div(input, other)。数学&amp;mdash;逐元素除法。
两个张量input和other逐元素相除.这两个维度可以不同，但元素数量一定要一致。输出: 与input维度一致 torch.mul(input, value)。数学&amp;mdash;逐元素乘法 torch.mul(input, other)。数学&amp;mdash;逐元素乘法 torch.fmod(inpur, divisor, out)。数学&amp;mdash;取余 torch.remainder(input, divisor, out)。数学&amp;mdash;取余 相当于 %。
divisor: 标量或者张量 逐元素 torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None)。数学&amp;mdash; 像素点相除后相加。
out = tensor .+ value*(tensor1./tensor2) torch.addcmul(tensor, value=1, tensor1, tensor2, out=None)。数学&amp;mdash; 像素点相乘后相加。
out = tensor .+ value*(tensor1 .* tensor2) torch.neg(input)。数学&amp;mdash;取负。out = -1 * input。 torch.reciprocal(input)。数学&amp;mdash;倒数。out = 1.0 / input。 torch.sign(input)。数学&amp;mdash;取正负符号 torch.sin(Tensor)。数学&amp;mdash;正弦 torch.cos(Tensor)。数学&amp;mdash;余弦 torch.tan(Tensor)。数学&amp;mdash;正切 torch.sinh(Tensor)。数学&amp;mdash;双曲正弦 torch.cosh(Tensor)。数学&amp;mdash;双曲余弦 torch.tanh(Tensor)。数学&amp;mdash;双曲正切 torch.asin(Tensor)。数学&amp;mdash;反正弦 torch.acos(input)。数学&amp;mdash;反余弦 torch.</description></item><item><title>模型训练</title><link>https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0050_pytorch/0050_train_model/</link><pubDate>Fri, 08 Apr 2022 06:00:20 +0600</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00035_programming_language/0050_pytorch/0050_train_model/</guid><description>一、数据预处理 import torch from torch.utils.data import Dataset, DataLoader, TensorDataset from torch.autograd import Variable import numpy as np class MyDataset(Dataset): &amp;#34;&amp;#34;&amp;#34; 下载数据、初始化数据，都可以在这里完成 &amp;#34;&amp;#34;&amp;#34; def __init__(self): xy = np.loadtxt(&amp;#39;../dataSet/diabetes.csv.gz&amp;#39;, delimiter=&amp;#39;,&amp;#39;, dtype=np.float32) # 使用numpy读取数据 self.x_data = torch.from_numpy(xy[:, 0:-1]) self.y_data = torch.from_numpy(xy[:, [-1]]) self.len = xy.shape[0] def __getitem__(self, index): return self.x_data[index], self.y_data[index] def __len__(self): return self.len # 创建Dataset对象 dataset = MyDataset() # 创建DataLoadder对象 dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2) # 循环DataLoader对象 num_epoches = 100 for epoch in range(num_epoches) for img, label in dataloader: # 将数据从dataloader中读取出来，一次读取的样本数为32个 # class torch.</description></item></channel></rss>