<!doctype html><html><head><title>aigc</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0d88336a9cbd3e68c7714efae786b0a9_38044_42x0_resize_box_2.png><meta property="og:title" content="aigc"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://biubiobiu.github.io/zh-cn/tags/aigc/"><meta property="og:updated_time" content="2023-08-05T12:30:40+08:00"><link rel=stylesheet href=/css/layouts/list.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/zh-cn><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png alt=Logo>
biubiobiu's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu245540b1d52c0d501ae7bc0752a15caf_34633_42x0_resize_box_2.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu547c3206082786c1d32d36034e2a655a_40863_42x0_resize_box_2.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/zh-cn/search><input type=text name=keyword placeholder=搜索 data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/zh-cn/tags data-filter=all>标签</a></li><div class="subtree taxonomy-terms"><li><a class="taxonomy-term active" href=https://biubiobiu.github.io/zh-cn/tags/aigc/ data-taxonomy-term=aigc><span class=taxonomy-label>aigc</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/attention/ data-taxonomy-term=attention><span class=taxonomy-label>attention</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/backbone/ data-taxonomy-term=backbone><span class=taxonomy-label>backbone</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/bart/ data-taxonomy-term=bart><span class=taxonomy-label>BART</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/bert/ data-taxonomy-term=bert><span class=taxonomy-label>BERT</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/can/ data-taxonomy-term=can><span class=taxonomy-label>CAN</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/chatglm/ data-taxonomy-term=chatglm><span class=taxonomy-label>ChatGLM</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/claude/ data-taxonomy-term=claude><span class=taxonomy-label>Claude</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/clip/ data-taxonomy-term=clip><span class=taxonomy-label>CLIP</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/cnn/ data-taxonomy-term=cnn><span class=taxonomy-label>cnn</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/cohere/ data-taxonomy-term=cohere><span class=taxonomy-label>Cohere</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/contrastive-learning/ data-taxonomy-term=contrastive-learning><span class=taxonomy-label>contrastive learning</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/cuda/ data-taxonomy-term=cuda><span class=taxonomy-label>cuda</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/cv/ data-taxonomy-term=cv><span class=taxonomy-label>CV</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/dall-e/ data-taxonomy-term=dall-e><span class=taxonomy-label>DALL-E</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/diffusion/ data-taxonomy-term=diffusion><span class=taxonomy-label>Diffusion</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/electra/ data-taxonomy-term=electra><span class=taxonomy-label>ELECTRA</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/falcon/ data-taxonomy-term=falcon><span class=taxonomy-label>Falcon</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/family/ data-taxonomy-term=family><span class=taxonomy-label>Family</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/gan/ data-taxonomy-term=gan><span class=taxonomy-label>GAN</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/gpt/ data-taxonomy-term=gpt><span class=taxonomy-label>GPT</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/gpt-1/ data-taxonomy-term=gpt-1><span class=taxonomy-label>GPT-1</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/gru/ data-taxonomy-term=gru><span class=taxonomy-label>GRU</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/imagen/ data-taxonomy-term=imagen><span class=taxonomy-label>Imagen</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/importlib/ data-taxonomy-term=importlib><span class=taxonomy-label>importlib</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/ipdb/ data-taxonomy-term=ipdb><span class=taxonomy-label>ipdb</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/latex/ data-taxonomy-term=latex><span class=taxonomy-label>Latex</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/llama/ data-taxonomy-term=llama><span class=taxonomy-label>llama</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/logging/ data-taxonomy-term=logging><span class=taxonomy-label>logging</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/lstm/ data-taxonomy-term=lstm><span class=taxonomy-label>LSTM</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/markdown/ data-taxonomy-term=markdown><span class=taxonomy-label>MarkDown</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/matting/ data-taxonomy-term=matting><span class=taxonomy-label>matting</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/midjourney/ data-taxonomy-term=midjourney><span class=taxonomy-label>Midjourney</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/mllm/ data-taxonomy-term=mllm><span class=taxonomy-label>MLLM</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/mxnet/ data-taxonomy-term=mxnet><span class=taxonomy-label>mxnet</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/ndarray/ data-taxonomy-term=ndarray><span class=taxonomy-label>NdArray</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/nlp/ data-taxonomy-term=nlp><span class=taxonomy-label>NLP</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/opencv/ data-taxonomy-term=opencv><span class=taxonomy-label>OpenCV</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/optimizer/ data-taxonomy-term=optimizer><span class=taxonomy-label>optimizer</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/palm/ data-taxonomy-term=palm><span class=taxonomy-label>PaLM</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/pil/ data-taxonomy-term=pil><span class=taxonomy-label>PIL</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/pip/ data-taxonomy-term=pip><span class=taxonomy-label>pip</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/python/ data-taxonomy-term=python><span class=taxonomy-label>python</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/request/ data-taxonomy-term=request><span class=taxonomy-label>request</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/shortcodes/ data-taxonomy-term=shortcodes><span class=taxonomy-label>shortcodes</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/summary/ data-taxonomy-term=summary><span class=taxonomy-label>summary</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/t5/ data-taxonomy-term=t5><span class=taxonomy-label>T5</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/tensor/ data-taxonomy-term=tensor><span class=taxonomy-label>Tensor</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/tf/ data-taxonomy-term=tf><span class=taxonomy-label>TF</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/toha/ data-taxonomy-term=toha><span class=taxonomy-label>Toha</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/torch/ data-taxonomy-term=torch><span class=taxonomy-label>torch</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/transformer/ data-taxonomy-term=transformer><span class=taxonomy-label>Transformer</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/vicuna/ data-taxonomy-term=vicuna><span class=taxonomy-label>Vicuna</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/video/ data-taxonomy-term=video><span class=taxonomy-label>video</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/vision-transformer/ data-taxonomy-term=vision-transformer><span class=taxonomy-label>vision transformer</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/vlp/ data-taxonomy-term=vlp><span class=taxonomy-label>vlp</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/vqgan/ data-taxonomy-term=vqgan><span class=taxonomy-label>VQGAN</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/word-embedding/ data-taxonomy-term=word-embedding><span class=taxonomy-label>word embedding</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/ data-taxonomy-term=%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86><span class=taxonomy-label>中心极限定理</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/ data-taxonomy-term=%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81><span class=taxonomy-label>位置编码</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/ data-taxonomy-term=%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C><span class=taxonomy-label>假设检验</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%85%AC%E5%BC%8F/ data-taxonomy-term=%E5%85%AC%E5%BC%8F><span class=taxonomy-label>公式</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%86%85%E7%BD%AE%E6%A8%A1%E5%9D%97/ data-taxonomy-term=%E5%86%85%E7%BD%AE%E6%A8%A1%E5%9D%97><span class=taxonomy-label>内置模块</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%87%B8%E4%BC%98%E5%8C%96/ data-taxonomy-term=%E5%87%B8%E4%BC%98%E5%8C%96><span class=taxonomy-label>凸优化</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%88%86%E5%B8%83/ data-taxonomy-term=%E5%88%86%E5%B8%83><span class=taxonomy-label>分布</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%88%9D%E5%A7%8B%E5%8C%96/ data-taxonomy-term=%E5%88%9D%E5%A7%8B%E5%8C%96><span class=taxonomy-label>初始化</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%8D%9A%E6%96%87%E8%B7%AF%E5%BE%84/ data-taxonomy-term=%E5%8D%9A%E6%96%87%E8%B7%AF%E5%BE%84><span class=taxonomy-label>博文路径</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%8D%B7%E7%A7%AF/ data-taxonomy-term=%E5%8D%B7%E7%A7%AF><span class=taxonomy-label>卷积</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ data-taxonomy-term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C><span class=taxonomy-label>卷积神经网络</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/ data-taxonomy-term=%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90><span class=taxonomy-label>回归分析</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/ data-taxonomy-term=%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5><span class=taxonomy-label>基本概念</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/ data-taxonomy-term=%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C><span class=taxonomy-label>基础操作</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%A0%86/ data-taxonomy-term=%E5%A0%86><span class=taxonomy-label>堆</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/ data-taxonomy-term=%E5%A4%9A%E6%A8%A1%E6%80%81><span class=taxonomy-label>多模态</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B/ data-taxonomy-term=%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B><span class=taxonomy-label>大数定律</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/ data-taxonomy-term=%E5%A4%A7%E6%A8%A1%E5%9E%8B><span class=taxonomy-label>大模型</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/ data-taxonomy-term=%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81><span class=taxonomy-label>字符编码</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%B0%8F%E5%9E%8B%E5%8C%96/ data-taxonomy-term=%E5%B0%8F%E5%9E%8B%E5%8C%96><span class=taxonomy-label>小型化</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%B9%B3%E7%A8%B3%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B/ data-taxonomy-term=%E5%B9%B3%E7%A8%B3%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B><span class=taxonomy-label>平稳随机过程</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%B9%B6%E8%A1%8C/ data-taxonomy-term=%E5%B9%B6%E8%A1%8C><span class=taxonomy-label>并行</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%B9%BB%E8%A7%89/ data-taxonomy-term=%E5%B9%BB%E8%A7%89><span class=taxonomy-label>幻觉</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%BA%94%E7%94%A8%E7%AD%96%E7%95%A5/ data-taxonomy-term=%E5%BA%94%E7%94%A8%E7%AD%96%E7%95%A5><span class=taxonomy-label>应用策略</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%BC%82%E5%B8%B8/ data-taxonomy-term=%E5%BC%82%E5%B8%B8><span class=taxonomy-label>异常</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/ data-taxonomy-term=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0><span class=taxonomy-label>强化学习</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%BD%92%E4%B8%80%E5%8C%96/ data-taxonomy-term=%E5%BD%92%E4%B8%80%E5%8C%96><span class=taxonomy-label>归一化</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ data-taxonomy-term=%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C><span class=taxonomy-label>循环神经网络</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E6%95%99%E7%A8%8B/ data-taxonomy-term=%E6%95%99%E7%A8%8B><span class=taxonomy-label>教程</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E6%95%B0%E5%AD%A6%E8%AE%A1%E7%AE%97/ data-taxonomy-term=%E6%95%B0%E5%AD%A6%E8%AE%A1%E7%AE%97><span class=taxonomy-label>数学计算</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E6%96%87%E4%BB%B6/ data-taxonomy-term=%E6%96%87%E4%BB%B6><span class=taxonomy-label>文件</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90/ data-taxonomy-term=%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90><span class=taxonomy-label>方差分析</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ data-taxonomy-term=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0><span class=taxonomy-label>机器学习</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/ data-taxonomy-term=%E6%A6%82%E7%8E%87%E8%AE%BA><span class=taxonomy-label>概率论</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E6%A8%A1%E5%9E%8B/ data-taxonomy-term=%E6%A8%A1%E5%9E%8B><span class=taxonomy-label>模型</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E6%AD%A3%E5%88%99/ data-taxonomy-term=%E6%AD%A3%E5%88%99><span class=taxonomy-label>正则</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ data-taxonomy-term=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0><span class=taxonomy-label>深度学习</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83/ data-taxonomy-term=%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83><span class=taxonomy-label>混合精度训练</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E7%94%9F%E6%88%90%E5%BC%8F/ data-taxonomy-term=%E7%94%9F%E6%88%90%E5%BC%8F><span class=taxonomy-label>生成式</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ data-taxonomy-term=%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B><span class=taxonomy-label>目标检测</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E7%AE%80%E4%BB%8B/ data-taxonomy-term=%E7%AE%80%E4%BB%8B><span class=taxonomy-label>简介</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E7%BB%BC%E8%BF%B0/ data-taxonomy-term=%E7%BB%BC%E8%BF%B0><span class=taxonomy-label>综述</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8-%E6%9E%B6%E6%9E%84/ data-taxonomy-term=%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8-%E6%9E%B6%E6%9E%84><span class=taxonomy-label>编码器-解码器 架构</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/ data-taxonomy-term=%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84><span class=taxonomy-label>网络结构</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E8%AE%AD%E7%BB%83/ data-taxonomy-term=%E8%AE%AD%E7%BB%83><span class=taxonomy-label>训练</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6/ data-taxonomy-term=%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6><span class=taxonomy-label>训练框架</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/ data-taxonomy-term=%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2><span class=taxonomy-label>语义分割</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E8%BF%9B%E9%98%B6%E6%93%8D%E4%BD%9C/ data-taxonomy-term=%E8%BF%9B%E9%98%B6%E6%93%8D%E4%BD%9C><span class=taxonomy-label>进阶操作</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E9%85%8D%E7%BD%AE/ data-taxonomy-term=%E9%85%8D%E7%BD%AE><span class=taxonomy-label>配置</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E9%87%87%E6%A0%B7/ data-taxonomy-term=%E9%87%87%E6%A0%B7><span class=taxonomy-label>采样</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F/ data-taxonomy-term=%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F><span class=taxonomy-label>随机变量</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B/ data-taxonomy-term=%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B><span class=taxonomy-label>随机过程</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E9%9D%99%E6%80%81%E5%9B%BE/ data-taxonomy-term=%E9%9D%99%E6%80%81%E5%9B%BE><span class=taxonomy-label>静态图</span></a></li><li><a class=taxonomy-term href=https://biubiobiu.github.io/zh-cn/tags/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE/ data-taxonomy-term=%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE><span class=taxonomy-label>马尔科夫链</span></a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class="content container-fluid" id=content><div class="container-fluid post-card-holder" id=post-card-holder><div class=post-card><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0020_palm/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>PaLM</h5><p class="card-text post-summary">一、简介 1、PaLM 1 《PaLM: Scaling Language Modeling with Pathways》 这篇文章87页，并没有深度的讨论模型算法的结构，数据的清洗技巧，或者是训练的方式（估计感觉这块的创新性不是特别明显，也不是文章的主要目的）。 而是花了大量的篇幅去评估这个模型在multi-task的能力，比如翻译，代码修改，生成，问答等等。
其中模型版本于训练集大小：
Google PaLM 是一个 540B 参数密集型 Transformer 语言模型，在 780B 高质量、多样化文本的标记上进行训练。 它已经针对 3 种不同的尺寸进行了训练：8B、62B 和 540B，使用 6144 TPU v4 芯片使用 Pathways，这是一种新的 ML 系统，可跨多个 TPU（张量处理单元）Pod 进行高效训练。 当它被引入时，它在数百个 NLU 和 NLG 基准测试中产生了 SOTA 小样本学习结果。 这包括 Big-Bench 任务的性能大幅提升，以及多语言 NLG 和源代码生成功能的显着改进。 它还被证明可以使用思维链提示来解释笑话或逻辑推理，从而产生很好的解释。
PaLM超越了许多之前的SOTA。作者归功于
更好的数据的清理， 更多的数据， 模型规模的进一步提升。 模型算法的改进比较少，从Model Architecture那一章看出，其实模型结构的变化并不明显，在激活层，ShareEmbedding，PosEmbedding等模块做了一些结构优选。核心的TransformerBlock的变种选择也更多是为了优化模型的训练效率。谷歌作为搜索技术的天花板，数据清洗的积累，以及对于数据的理解肯定是OpenAI这些公司无法比拟的。个人感觉这块是个比较明显的优势。
与GPT-3相比的变化：
多查询注意力（Multi-query attention）：在每个注意力头中共享K/V（Key/Value）嵌入，但使用独立的Q（Query）嵌入。这样做可以在推理阶段显著提高模型的速度。 并行Transformer块：使用并行的Transformer块来提高训练时间，相较于传统的串行设计，可以减少约15%的训练时间。 SwiGLU激活函数：与GPT-3使用的GELU激活函数不同，这里采用了SwiGLU激活函数。 旋转位置编码RoPE嵌入：使用RoPE（Relative Positional Encodings）嵌入代替学习得到的嵌入方式，在长文本上具有更好的性能 。 输入-输出嵌入共享：输入和输出embedding矩阵是共享的。 无偏置向量：在mlp、normlayer等算法中，都不使用bias，对于大模型，可以提高训练稳定性。 SentencePiece与256k标记：使用SentencePiece进行分词处理，标记数量为256k。 2、PaLM 2 《PaLM 2 Technical Report》 这篇报告-总结：</p></div><div class=card-footer><span class=float-left>August 5, 2023</span>
<a href=/zh-cn/posts/00300_aigc/0010_generate_text/0020_palm/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div><div class=post-card><a href=/zh-cn/posts/00300_aigc/0010_generate_text/0045_vicuna/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>Vicuna</h5><p class="card-text post-summary">一、简介 二、网络结构</p></div><div class=card-footer><span class=float-left>August 5, 2023</span>
<a href=/zh-cn/posts/00300_aigc/0010_generate_text/0045_vicuna/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div><div class=post-card><a href=/zh-cn/posts/00300_aigc/0020_generate_image/1015_vqgan/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>VQGAN</h5><p class="card-text post-summary">一、简介 It is coming soon.</p></div><div class=card-footer><span class=float-left>August 5, 2023</span>
<a href=/zh-cn/posts/00300_aigc/0020_generate_image/1015_vqgan/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div><div class=post-card><a href=/zh-cn/posts/00300_aigc/0005_summary/0010_aigc_train_p/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>大模型训练框架</h5><p class="card-text post-summary">一、简介 模型并行：将模型参数分布到多个GPU上
张量并行：切分参数矩阵，每个GPU计算一部分。缺点是：需要额外通信，降低计算粒度 流水线并行：将网络分成多段并行。缺点是：引入流水线气泡 ZeRO-3：将参数分布到数据并行组中，计算之前先获取模型参数。缺点是：需要额外通信 为了能够提升训练的效率，目前都采用混合精度训练，然而混合精度训练，是非常不稳定的，很容易导致梯度爆炸。这个原因是：在做Forword或者Backword的时候，需要把FP32位，降低到FP16位。这个操作有可能会导致精度溢出，从而导致loss爆炸。
1、混合精度(AMP) 混合精度 (Automatically Mixed Precision, AMP)
为加速训练，模型的参数是以FP16半精度存储的； 然后，输入数据也是 FP16半精度，与模型参数 foreword计算，激活结果也是FP16半精度； 计算loss，然后backword。在backword之前，需要对loss进行缩放，让他变成Fp32位 二、Deepspeed 使用文档 DeepSpeed的核心就在于：GPU显存不够，CPU内存来凑。比方说，我们只有一张10GB的GPU，那么我们很可能需要借助80GB的CPU，才能够训练一个大模型。
具体点说，DeepSpeed将当前时刻，训练模型用不到的参数，缓存到CPU中，等到要用到了，再从CPU挪到GPU。这里的“参数”，不仅指的是模型参数，还指optimizer、梯度等。
越多的参数挪到CPU上，GPU的负担就越小；但随之的代价就是，更为频繁的CPU，GPU交互，极大增加了训练推理的时间开销。因此，DeepSpeed使用的一个核心要义是：时间开销和显存占用的权衡。
1、使用DeepSpeed deepspeed --master_port 29500 --num_gpus=2 run_s2s.py --deepspeed ds_config.json &ndash;master_port：端口号。最好显示指定，默认为29500，可能会被占用（i.e., 跑了多个DeepSpeed进程）。
&ndash;num_gpus: GPU数目，默认会使用当前所见的所有GPU。
&ndash;deepspeed: 提供的config文件，用来指定许多DeepSpeed的重要参数。
使用DeepSpeed的一个核心要点，就在于写一个config文件（可以是.json，也可以是类json格式的配置文件），在这个配置文件中，你可以指定你想要的参数，例如，权衡时间和显存。因此，上面几个参数里，最重要的便是&ndash;deepspeed，即你提供的config文件，即ZeRO。
2、ZeRO Zero Redundancy Optimizer (ZeRO)是DeepSpeed的workhorse. 用户可以提供不同的ZeRO config文件，来实现DeepSpeed的不同功能特性。
即，传统的深度学习，模型训练并行，是将模型参数复制多份到多张GPU上，只将数据拆分（如，torch的Dataparallel），这样就会有大量的显存冗余浪费。而ZeRO就是为了消除这种冗余，提高对memory的利用率。注意，这里的“memory”不仅指多张GPU memory，还包括CPU。
而ZeRO的实现方法，就是把参数占用，逻辑上分成三种类型。将这些类型的参数划分：
optimizer states：即优化器的参数状态。例如，Adam的动量参数。 gradients：梯度缓存，对应于optimizer。 parameters：模型参数。 对应的，DeepSpeed的ZeRO config文件就可以分为如下几类：
ZeRO Stage 1: 划分optimizer states。优化器参数被划分到多个memory上，每个momoey上的进程只负责更新它自己那部分参数。 ZeRO Stage 2: 划分gradient。每个memory，只保留它分配到的optimizer state所对应的梯度。这很合理，因为梯度和optimizer是紧密联系在一起的。只知道梯度，不知道optimizer state，是没有办法优化模型参数的。 ZeRO Stage 3: 划分模型参数，或者说，不同的layer.</p></div><div class=card-footer><span class=float-left>August 5, 2023</span>
<a href=/zh-cn/posts/00300_aigc/0005_summary/0010_aigc_train_p/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div><div class=post-card><a href=/zh-cn/posts/00300_aigc/0005_summary/0015_aigc_train_mini/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>模型小型化</h5><p class="card-text post-summary">一、简介 目前小型化的方案：
剪枝 Network Pruning 蒸馏 Knowledge Distillation 量化 Parameter Quantization Architecture Design Dynamic Computation 二、TensorRT</p></div><div class=card-footer><span class=float-left>August 5, 2023</span>
<a href=/zh-cn/posts/00300_aigc/0005_summary/0015_aigc_train_mini/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div><div class=post-card><a href=/zh-cn/posts/00300_aigc/0005_summary/0005_aigc_application/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>模型应用策略</h5><p class="card-text post-summary">一、简介 对于大语言模型应用的两种不同的使用方式：
“专才”：只精通指定任务。怎么让一个基础模型在指定任务上比较精通呢？有两种方式：
加外挂：比如：在bert后面添加几个fc层，完成指定任务 fine-tune： Adapter插件：固定原来模型，添加一个额外的模型插件。例如：Bitfit、AdapterBias、Houlsby、Prefix-tuning；ControlNet， LoRA，Text Inversion “全才”：模型有各种背景知识，用户可以通过使用prompt指令，来要求模型按照指令输出。
In-context Learning Instruction tuning Chain-of-Thought Prompting APE 1、Adapter插件 有人提出 Adaptor 的概念，在预训练的模型中加入一些叫Apt(Adaptor)的层，在微调的时候，只微调Apt层。这篇文章中，将Adapter插在Feed-forward层之后，在预训练的时候是没有Adapter的，只有在微调的时候才插进去。并且在微调的时候，只调整Adapter层的参数。
二、大模型-使用策略 1、In-Context Learning 1. 解释1 《Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?》 In-context learning是一种学习范式，它允许语言模型通过以演示形式组织的若干个示例或者指令来学习任务。In-context learning（ICL）的核心在于从任务相关的类比样本中学习，ICL要求若干示例以特定形式进行演示，然后将当前输入x跟上述示例通过prompt拼接到一起作为语言模型的输入。本质上，它利用训练有素的语言模型根据演示的示例来估计候选答案的可能性。简单理解，就是通过若干个完整的示例，让语言模型更好地理解当前的任务，从而做出更加准确的预测。
实验结论：
ICL 中Ground Truth信息无关紧要。
作者实验对比：没有示例、多个示例-且label是一一对应的、多个示例-且label是随机的。对比发现： 随机label 与 正确label 的效果相当，性能只下降了 $ 0 - 5\%$。 没有示例，效果下降较多。 2. ICL的性能收益主要来自 独立规范的输入空间和标签空间，以及正确一致的演示格式。</p></div><div class=card-footer><span class=float-left>August 5, 2023</span>
<a href=/zh-cn/posts/00300_aigc/0005_summary/0005_aigc_application/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div><div class=post-card><a href=/zh-cn/posts/00300_aigc/0005_summary/0012_aigc_amp/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>混合精度训练</h5><p class="card-text post-summary">一、简介 目前，混合精度 (Automatically Mixed Precision, AMP) 训练已经成为了炼丹师的标配工具，仅仅只需几行代码，就能让显存占用减半，训练速度加倍。 AMP 技术是由百度和 NIVDIA 团队在 2017 年提出的 (Mixed Precision Training)，该成果发表在 ICLR 上。PyTorch 1.6之前，大家都是用 NVIDIA 的 apex 库来实现 AMP 训练。1.6 版本之后，PyTorch 出厂自带 AMP。
# 原代码 output = net(input) loss = loss_fn(output, target) loss.backward() optimizer.step() optimizer.zero_grad() # 使用混合精度训练 with torch.cuda.amp.autocast(): output = net(input) loss = loss_fn(output, target) scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() optimizer.zero_grad() 半精度浮点数 (FP16)： 是一种计算机使用的二进制浮点数数据类型，使用 2 字节 (16 位) 存储。而 PyTorch 默认使用 单精度浮点数 (FP32) 来进行网络模型的计算和权重存储。FP32 在内存中用 4 字节 (32 位) 存储。</p></div><div class=card-footer><span class=float-left>August 5, 2023</span>
<a href=/zh-cn/posts/00300_aigc/0005_summary/0012_aigc_amp/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div><div class=post-card><a href=/zh-cn/posts/00300_aigc/0005_summary/0020_aigc_error/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>生成式-问题</h5><p class="card-text post-summary">一、简介 问题1：在文本生成是，模型会一本正经的胡说八道，这种现象叫做模型的幻觉。 问题2：训练一个大模型，需要多少数据量呢？ 问题3：数据预处理，怎么过滤、去重 问题4：模型大小 与 数据大小 的关系？ 二、模型问题 1、Calibration 问题1：在文本生成是，模型会一本正经的胡说八道，这种现象叫做模型的幻觉。
《Language Models (Mostly) Know What They Know》 这篇论文发现：模型够大后，说谎才会心虚。 对于大模型，模型输出是正确的概率 VS 模型的自信度，这两个是相关的。当模型比较自信时，输出的结果是正确的概率就比较大。 对于小模型，模型输出是正确的概率 VS 模型的自信度，这两个是不相关的 其中，横轴：模型输出时的自信程度；纵轴：模型输出是正确的概率。黄色表示最大模型，自身表示最小模型。 三、数据问题 问题2：训练一个大模型，需要多少数据量呢？
训练一个大模型，需要多少数据量呢？《When Do You Need Billions of Words of Pretraining Data?》 问题3：数据预处理，怎么过滤、去重?
数据预处理：《Scaling Language Models: Methods, Analysis & Insights from Training Gopher》 过滤有害的内容，通过Google的审核接口 去掉一些 HTML 前端的一些tag 规则过滤，去掉低质量的文本。 去重 剔除测试数据 问题4：模型大小 与 数据大小 的关系？ 《Training Compute-Optimal Large Language Models》 这篇文章发现：</p></div><div class=card-footer><span class=float-left>August 5, 2023</span>
<a href=/zh-cn/posts/00300_aigc/0005_summary/0020_aigc_error/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div><div class=post-card><a href=/zh-cn/posts/00300_aigc/0005_summary/0001_aigc_summary/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>综述</h5><p class="card-text post-summary">在大语言模型的训练中，如果增大数据量，相应的应该减少学习率，这个跟原来的经验相反。
模型大小与模型效果 《Emergent Abilities of Large Language Models》 这篇文章指出：随着模型大小的增大，模型效果先不会有明显提升；增加到一定程度，模型有个突然顿悟时刻。
为什么需要预训练 《Visualizing and Understanding the Effectiveness of BERT》 这篇文章指出:
首先，预训练能在下游任务中达到一个良好的初始点，与从头开始训练相比，预训练能带来更宽的最优点，更容易优化。尽管 BERT 对下游任务的参数设置过高，但微调程序对过拟合具有很强的鲁棒性。 其次，可视化结果表明，由于最佳值平坦且宽广，以及训练损失面和泛化误差面之间的一致性，微调 BERT 趋向于更好地泛化。 第三，在微调过程中，BERT 的低层更具不变性，这表明靠近输入的层学习到了更多可迁移的语言表征。 一、文本生成 1、GPT 参考
2、PaLM 《PaLM: Scaling Language Modeling with Pathways》 PaLM才是真正的“大”模型。它是迄今为止训练的最大的密集语言模型，参数为 540B，需要 6144 个 TPU 来训练（这是 3 个完整的 TPU pod，每个包含 2048 个 TPU）。这太贵了！可能只有谷歌拥有资源+基础设施来做到这一点。使用的Token高达7800亿。PaLM是使用Google新一代PathWay分布式训练框架训练出来。
与GPT-3相比的变化：
多查询注意力（Multi-query attention）：在每个注意力头中共享K/V（Key/Value）嵌入，但使用独立的Q（Query）嵌入。这样做可以在推理阶段显著提高模型的速度。 并行Transformer块：使用并行的Transformer块来提高训练时间，相较于传统的串行设计，可以减少约15%的训练时间。 SwiGLU激活函数：与GPT-3使用的GELU激活函数不同，这里采用了SwiGLU激活函数。 旋转位置编码RoPE嵌入：使用RoPE（Relative Positional Encodings）嵌入代替学习得到的嵌入方式，在长文本上具有更好的性能 。 输入-输出嵌入共享：输入和输出embedding矩阵是共享的。 无偏置向量：在mlp、normlayer等算法中，都不使用bias，对于大模型，可以提高训练稳定性。 SentencePiece与256k标记：使用SentencePiece进行分词处理，标记数量为256k。 所以，有很多变化！同样，其中很多都是常见的，例如使用 GPT-3 的学习嵌入向量已经非常过时了，现在几乎没有人这样做。</p></div><div class=card-footer><span class=float-left>August 5, 2023</span>
<a href=/zh-cn/posts/00300_aigc/0005_summary/0001_aigc_summary/ class="float-right btn btn-outline-info btn-sm">阅读</a></div></div></a></div></div><div class=paginator><ul class=pagination><li class=page-item><a href=/zh-cn/tags/aigc/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/zh-cn/tags/aigc/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/zh-cn/tags/aigc/>1</a></li><li class="page-item active"><a class=page-link href=/zh-cn/tags/aigc/page/2/>2</a></li><li class="page-item disabled"><a class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/zh-cn/tags/aigc/page/2/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>导航</h5><ul><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#about>关于</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#skills>技能</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#experiences>经验</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#projects>项目</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#education>教育</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#recent-posts>最新博客</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#accomplishments>造诣</a></li><li class=nav-item><a class=smooth-scroll href=https://biubiobiu.github.io/zh-cn/#achievements>业绩</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>联系方式:</h5><ul><li><span>电话:</span> <span>+0123456789</span></li><li><span>邮箱:</span> <span>bsgshyn8@163.com</span></li></ul></div></div></div><hr><div class=container><p id=disclaimer><strong>免责声明:</strong> 这个主题是MIT许可的。因此，您可以将其用于非商业、商业或者私人用途。您可以修改或分发主题，而无须经 作者任何许可。但是，主题作者不对主题的任何问题提供任何保证或承担任何责任。</p></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=/js/list.js></script></body></html>