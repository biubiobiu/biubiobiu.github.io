<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>PaLM on biubiobiu's Blog</title><link>https://biubiobiu.github.io/zh-cn/tags/palm/</link><description>Recent content in PaLM on biubiobiu's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Everything is mine</copyright><lastBuildDate>Sat, 05 Aug 2023 12:30:40 +0800</lastBuildDate><atom:link href="https://biubiobiu.github.io/zh-cn/tags/palm/index.xml" rel="self" type="application/rss+xml"/><item><title>PaLM</title><link>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0020_palm/</link><pubDate>Sat, 05 Aug 2023 12:30:40 +0800</pubDate><guid>https://biubiobiu.github.io/zh-cn/posts/00300_aigc/0010_generate_text/0020_palm/</guid><description>一、简介 1、PaLM 1 《PaLM: Scaling Language Modeling with Pathways》 这篇文章87页，并没有深度的讨论模型算法的结构，数据的清洗技巧，或者是训练的方式（估计感觉这块的创新性不是特别明显，也不是文章的主要目的）。 而是花了大量的篇幅去评估这个模型在multi-task的能力，比如翻译，代码修改，生成，问答等等。
其中模型版本于训练集大小：
Google PaLM 是一个 540B 参数密集型 Transformer 语言模型，在 780B 高质量、多样化文本的标记上进行训练。 它已经针对 3 种不同的尺寸进行了训练：8B、62B 和 540B，使用 6144 TPU v4 芯片使用 Pathways，这是一种新的 ML 系统，可跨多个 TPU（张量处理单元）Pod 进行高效训练。 当它被引入时，它在数百个 NLU 和 NLG 基准测试中产生了 SOTA 小样本学习结果。 这包括 Big-Bench 任务的性能大幅提升，以及多语言 NLG 和源代码生成功能的显着改进。 它还被证明可以使用思维链提示来解释笑话或逻辑推理，从而产生很好的解释。
PaLM超越了许多之前的SOTA。作者归功于
更好的数据的清理， 更多的数据， 模型规模的进一步提升。 模型算法的改进比较少，从Model Architecture那一章看出，其实模型结构的变化并不明显，在激活层，ShareEmbedding，PosEmbedding等模块做了一些结构优选。核心的TransformerBlock的变种选择也更多是为了优化模型的训练效率。谷歌作为搜索技术的天花板，数据清洗的积累，以及对于数据的理解肯定是OpenAI这些公司无法比拟的。个人感觉这块是个比较明显的优势。
与GPT-3相比的变化：
多查询注意力（Multi-query attention）：在每个注意力头中共享K/V（Key/Value）嵌入，但使用独立的Q（Query）嵌入。这样做可以在推理阶段显著提高模型的速度。 并行Transformer块：使用并行的Transformer块来提高训练时间，相较于传统的串行设计，可以减少约15%的训练时间。 SwiGLU激活函数：与GPT-3使用的GELU激活函数不同，这里采用了SwiGLU激活函数。 旋转位置编码RoPE嵌入：使用RoPE（Relative Positional Encodings）嵌入代替学习得到的嵌入方式，在长文本上具有更好的性能 。 输入-输出嵌入共享：输入和输出embedding矩阵是共享的。 无偏置向量：在mlp、normlayer等算法中，都不使用bias，对于大模型，可以提高训练稳定性。 SentencePiece与256k标记：使用SentencePiece进行分词处理，标记数量为256k。 2、PaLM 2 《PaLM 2 Technical Report》 这篇报告-总结：</description></item></channel></rss>